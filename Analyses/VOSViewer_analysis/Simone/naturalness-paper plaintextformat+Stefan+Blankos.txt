FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Miller, EJ
   Foo, YZ
   Mewton, P
   Dawel, A
AF Miller, Elizabeth J.
   Foo, Yong Zhi
   Mewton, Paige
   Dawel, Amy
TI How do people respond to computer-generated versus human faces? A
   systematic review and meta-analyses
SO COMPUTERS IN HUMAN BEHAVIOR REPORTS
LA English
DT Review
DE Virtual; Avatar; Trustworthiness; Generative adversarial network
ID UNCANNY VALLEY; FACIAL EXPRESSIONS; EXTRASTRIATE CORTEX; 1ST
   IMPRESSIONS; PERCEPTION; RACE; RECOGNITION; EMOTION; AVATAR; INFORMATION
AB Computer-generated (CG) beings are rapidly infiltrating the human social world. Yet evidence about how humans respond to CG faces is mixed. The present systematic review and meta-analyses aimed to synthesise empirical evidence from studies comparing people's responses to CG and human faces, across key face processing domains of interest to psychology, neuroscience, and computer science. We tested whether effects were moderated by the perceived realism of CG relative to human faces, and whether CG and human faces showed the same identity or not. We hypothesised that people would be able to tell CG and human faces apart, and that other types of responses would favour human over CG faces. While results supported our hypotheses across several domains (perceptions of human-likeness, face memory, first impressions, emotion labelling), some responses did not differ for CG and human faces (quality of interactions, emotion ratings, facial mimicry, looking behaviour). We also found a reduced inversion effect for CG relative to human faces, though only minimal data were available for hallmark face effects (ORE, N170 and FFA responses). Overall, findings highlight potential strengths and challenges of using CG faces across a range of applications, including e-health, social companionship, videogaming, and scientific work.
C1 [Miller, Elizabeth J.; Foo, Yong Zhi; Mewton, Paige; Dawel, Amy] Australian Natl Univ, Sch Med & Psychol, Canberra, ACT 2600, Australia.
   [Foo, Yong Zhi] Univ Western Australia, Sch Biol Sci, Crawley, WA 6009, Australia.
C3 Australian National University; University of Western Australia
RP Dawel, A (corresponding author), Australian Natl Univ, Sch Med & Psychol, Canberra, ACT 2600, Australia.
EM elizabeth.miller@anu.edu.au; fooyongzhi@gmail.com;
   paige.mewton@anu.edu.au; amy.dawel@anu.edu.au
OI Mewton, Paige/0000-0003-1744-1155; Miller,
   Elizabeth/0000-0003-2572-6134; Foo, Yong Zhi/0000-0001-7627-2991; Dawel,
   Amy/0000-0001-6668-3121
FU Australian Government through the Australian Research Council's
   Discovery Projects funding scheme [DP220101026]; TRANSFORM Career
   Development Fellowship from The Australian National University (ANU)
   College of Health and Medicine
FX This research is supported by the Australian Government through the
   Australian Research Council's Discovery Projects funding scheme (project
   DP220101026) and by a TRANSFORM Career Development Fellowship to AD from
   The Australian National University (ANU) College of Health and Medicine.
   The funders had no role in developing or conducting this research. We
   have no conflicts of interest to disclose.
CR Abbott Miriam Bowers, 2016, Online J Issues Nurs, V21, P7, DOI 10.3912/OJIN.Vol21No03PPT39,05
   Andrade AD, 2010, J PALLIAT MED, V13, P1415, DOI 10.1089/jpm.2010.0108
   [Anonymous], 2009, P 3 INT C AFF COMP I, DOI DOI 10.1109/ACII.2009.5349549
   Arsalidou M, 2011, BRAIN TOPOGR, V24, P149, DOI 10.1007/s10548-011-0171-4
   Aviezer H, 2008, PSYCHOL SCI, V19, P724, DOI 10.1111/j.1467-9280.2008.02148.x
   Bailenson J. N., 2003, J FORENSIC IDENTIFIC, V53, P722
   Bailenson JN, 2004, PRESENCE-VIRTUAL AUG, V13, P416, DOI 10.1162/1054746041944858
   Balas B, 2017, COMPUT HUM BEHAV, V77, P240, DOI 10.1016/j.chb.2017.08.045
   Balas B, 2015, COMPUT HUM BEHAV, V52, P331, DOI 10.1016/j.chb.2015.06.018
   Balas B, 2014, PERCEPTION, V43, P355, DOI 10.1068/p7696
   Balas B, 2012, PERCEPTION, V41, P361, DOI 10.1068/p7166
   Balas B, 2011, DEVELOPMENTAL SCI, V14, P892, DOI 10.1111/j.1467-7687.2011.01039.x
   Ballew CC, 2007, P NATL ACAD SCI USA, V104, P17948, DOI 10.1073/pnas.0705435104
   Balsters MJH, 2013, EVOL PSYCHOL-US, V11, P148, DOI 10.1177/147470491301100114
   Baltrusaitis T., 2010, P 3 INT WORKSHOP AFF, P27, DOI [10.1145/1877826.1877835, DOI 10.1145/1877826.1877835]
   Bartneck C, 2001, USER MODEL USER-ADAP, V11, P279, DOI 10.1023/A:1011811315582
   Bartneck C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P367
   Bentin S, 1996, J COGNITIVE NEUROSCI, V8, P551, DOI 10.1162/jocn.1996.8.6.551
   Biele C, 2006, EXP BRAIN RES, V171, P1, DOI 10.1007/s00221-005-0254-0
   Calder AJ, 2005, NAT REV NEUROSCI, V6, P641, DOI 10.1038/nrn1724
   Calvo MG, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-35259-w
   Calvo MG, 2016, EMOTION, V16, P1186, DOI 10.1037/emo0000192
   Carlson CA, 2012, APPL COGNITIVE PSYCH, V26, P525, DOI 10.1002/acp.2824
   Carter E. J., 2013, Proceedings of the ACM Symposium on Applied Perception, P35
   Chattopadhyay D, 2016, J VISION, V16, DOI 10.1167/16.11.7
   Cheetham M, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00981
   Cheetham M, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01219
   Cheetham Marcus, 2013, Front Psychol, V4, P108, DOI 10.3389/fpsyg.2013.00108
   Cheetham M, 2011, FRONT HUM NEUROSCI, V5, DOI 10.3389/fnhum.2011.00126
   Chen F, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0210858
   Christensen JL, 2013, J INT AIDS SOC, V16, DOI 10.7448/IAS.16.3.18716
   Cohen J, 1988, STAT POWER ANAL BEHA
   Cooper M, 2019, BRIT J GUID COUNS, V47, P446, DOI 10.1080/03069885.2018.1506567
   Costantini E, 2004, LECT NOTES COMPUT SC, V3068, P276
   Costantini E., 2005, P 10 INT C INTELLIGE, P20, DOI [10.1145/1040830.1040846, DOI 10.1145/1040830.1040846]
   Craft AJ, 2012, ARCH SEX BEHAV, V41, P939, DOI 10.1007/s10508-012-9933-7
   Craig BM, 2012, EMOTION, V12, P1303, DOI 10.1037/a0028622
   Crookes K, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141353
   Cruwys T, 2014, J AFFECT DISORDERS, V159, P139, DOI 10.1016/j.jad.2014.02.019
   Dai ZY, 2018, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.168
   Dawel A, 2022, BEHAV RES METHODS, V54, P1889, DOI 10.3758/s13428-021-01705-3
   de Ruiter A, 2021, Philosophy & Technology, DOI [10.1007/s13347-021-00459-2, DOI 10.1007/S13347-021-00459-2]
   Deffler SA, 2015, PSYCHON B REV, V22, P1041, DOI 10.3758/s13423-014-0769-0
   Dellazizzo L, 2018, FRONT PSYCHIATRY, V9, DOI 10.3389/fpsyt.2018.00131
   Demos KE, 2008, CEREB CORTEX, V18, P2729, DOI 10.1093/cercor/bhn034
   Diel A, 2022, ACM T HUM-ROBOT INTE, V11, DOI 10.1145/3470742
   DIMBERG U, 1982, PSYCHOPHYSIOLOGY, V19, P643, DOI 10.1111/j.1469-8986.1982.tb02516.x
   Dyck M, 2010, PSYCHIAT RES, V179, P247, DOI 10.1016/j.psychres.2009.11.004
   Dyck M, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003628
   Egger M, 1997, BMJ-BRIT MED J, V315, P629, DOI 10.1136/bmj.315.7109.629
   EKMAN P, 1992, PSYCHOL SCI, V3, P34, DOI 10.1111/j.1467-9280.1992.tb00253.x
   Ert E, 2016, TOURISM MANAGE, V55, P62, DOI 10.1016/j.tourman.2016.01.013
   Fabri M., 2004, Virtual Reality, V7, P66, DOI 10.1007/s10055-003-0116-7
   Fan S., 2012, SIGGRAPH ASIA 2012 T, V1, P3, DOI [10.1145/2407746.2407763, DOI 10.1145/2407746.2407763]
   Flückiger C, 2018, PSYCHOTHERAPY, V55, P316, DOI 10.1037/pst0000172
   Foo YZ, 2022, PERS SOC PSYCHOL B, V48, P1580, DOI 10.1177/01461672211048110
   Foo YZ, 2017, BIOL REV, V92, P551, DOI 10.1111/brv.12243
   Freeman JB, 2008, J EXP PSYCHOL GEN, V137, P673, DOI 10.1037/a0013875
   Freeman JB, 2014, J NEUROSCI, V34, P10573, DOI 10.1523/JNEUROSCI.5063-13.2014
   Freeman JB, 2010, PERS SOC PSYCHOL B, V36, P1318, DOI 10.1177/0146167210378755
   Freeman JB, 2010, J EXP SOC PSYCHOL, V46, P179, DOI 10.1016/j.jesp.2009.10.002
   Gaither SE, 2019, J SOC PSYCHOL, V159, P592, DOI 10.1080/00224545.2018.1538929
   Geiger AR, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-97527-6
   Gendron M., 2013, EMOTION PERCEPTION P, DOI DOI 10.1093/OXFORDHB/9780195376746.013.0034
   Gibert G, 2013, SPEECH COMMUN, V55, P135, DOI 10.1016/j.specom.2012.07.001
   Gómez-Leal R, 2021, PEERJ, V9, DOI 10.7717/peerj.11274
   Gong L, 2008, COMPUT HUM BEHAV, V24, P1494, DOI 10.1016/j.chb.2007.05.007
   Gonzalez-Franco M, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00392
   Gracanin A, 2021, J NONVERBAL BEHAV, V45, P83, DOI 10.1007/s10919-020-00347-x
   Green RD, 2008, COMPUT HUM BEHAV, V24, P2456, DOI 10.1016/j.chb.2008.02.019
   Guise V, 2012, NURS EDUC TODAY, V32, P683, DOI 10.1016/j.nedt.2011.09.004
   Gwinn OS, 2018, NEUROPSYCHOLOGIA, V119, P405, DOI 10.1016/j.neuropsychologia.2018.09.001
   Hernandez N, 2009, NEUROPSYCHOLOGIA, V47, P1004, DOI 10.1016/j.neuropsychologia.2008.10.023
   Hourihan KL, 2013, MEM COGNITION, V41, P1021, DOI 10.3758/s13421-013-0316-7
   Hyde J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1787, DOI 10.1145/2556288.2557280
   Itz ML, 2014, NEUROIMAGE, V102, P736, DOI 10.1016/j.neuroimage.2014.08.042
   Jackson PL, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00112
   Javor A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0165998
   Jianying Wang, 2019, Proceedings of the International Conferences. Interfaces and Human Computer Interaction 2019, Game and Entertainment Technologies 2019, Computer Graphics, Visualization, Computer Vision and Image Processing 2019, P115
   John LK, 2011, J CONSUM RES, V37, P858, DOI 10.1086/656423
   Joyal CC, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00787
   Kala S, 2021, FRONT PSYCHIATRY, V12, DOI 10.3389/fpsyt.2021.709382
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Katsyri J., 2003, P INT C AUD SPEECH P, P1
   Kätsyri J, 2020, NEUROIMAGE, V204, DOI 10.1016/j.neuroimage.2019.116216
   Kätsyri J, 2019, PERCEPTION, V48, P968, DOI 10.1177/0301006619869134
   Kätsyri J, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.01362
   Kegel L. C., 2020, SOCIAL COGNITIVE AFF
   Krumhuber EG, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137840
   Krumhuber EG, 2012, EMOTION, V12, P351, DOI 10.1037/a0026632
   Lewkowicz DJ, 2012, DEV PSYCHOBIOL, V54, P124, DOI 10.1002/dev.20583
   Lidestam B, 2001, SCAND AUDIOL, V30, P89, DOI 10.1080/010503901300112194
   Little AC, 2007, EVOL HUM BEHAV, V28, P18, DOI 10.1016/j.evolhumbehav.2006.09.002
   Lucas GM, 2017, FRONT ROBOT AI, V4, DOI 10.3389/frobt.2017.00051
   Lucas GM, 2014, COMPUT HUM BEHAV, V37, P94, DOI 10.1016/j.chb.2014.04.043
   MacDorman KF, 2019, COMPUT HUM BEHAV, V94, P140, DOI 10.1016/j.chb.2019.01.011
   MacDorman KF, 2017, COGNITION, V161, P132, DOI 10.1016/j.cognition.2017.01.009
   MacDorman KF, 2016, COGNITION, V146, P190, DOI 10.1016/j.cognition.2015.09.019
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   Malek N, 2019, EMOTION, V19, P234, DOI 10.1037/emo0000410
   Maras MH, 2019, INT J EVID PROOF, V23, P255, DOI 10.1177/1365712718807226
   Matheson HE, 2012, CAN J EXP PSYCHOL, V66, P51, DOI 10.1037/a0026062
   Matheson HE, 2011, BEHAV RES METHODS, V43, P224, DOI 10.3758/s13428-010-0029-9
   McKone E, 2007, TRENDS COGN SCI, V11, P8, DOI 10.1016/j.tics.2006.11.002
   McKone E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49202-0
   Meissner CA, 2001, PSYCHOL PUBLIC POL L, V7, P3, DOI 10.1037//1076-8971.7.1.3
   Milcent AS, 2019, PROCEEDINGS OF THE 19TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA' 19), P215, DOI 10.1145/3308532.3329446
   Miller EJ, 2022, EMOTION, V22, P907, DOI 10.1037/emo0000772
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Moser E, 2007, J NEUROSCI METH, V161, P126, DOI 10.1016/j.jneumeth.2006.10.016
   Mühlberger A, 2009, J NEURAL TRANSM, V116, P735, DOI 10.1007/s00702-008-0108-6
   Mundy ME, 2012, NEUROPSYCHOLOGIA, V50, P3053, DOI 10.1016/j.neuropsychologia.2012.07.006
   Mustafa M., 2016, P 13 EUR C VIS MED P, P1, DOI [10.1145/2998559.2998563, DOI 10.1145/2998559.2998563]
   Mustafa M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5098, DOI 10.1145/3025453.3026043
   Nakagawa S, 2007, BIOL REV, V82, P591, DOI 10.1111/j.1469-185X.2007.00027.x
   Naples A, 2015, BEHAV RES METHODS, V47, P562, DOI 10.3758/s13428-014-0491-x
   Ni H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON CYBORG AND BIONIC SYSTEMS (CBS), P298, DOI 10.1109/CBS.2018.8612271
   Nightingale SJ, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2120481119
   Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105
   Papesh MH, 2009, CAN J EXP PSYCHOL, V63, P253, DOI 10.1037/a0015802
   Parmar D, 2022, AUTON AGENT MULTI-AG, V36, DOI 10.1007/s10458-021-09539-1
   Patel H, 2015, PRESENCE-VIRTUAL AUG, V24, P1, DOI 10.1162/PRES_a_00212
   Pointet VCP, 2021, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.576852
   Peromaa T, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0215610
   Philip L, 2018, I-PERCEPTION, V9, DOI 10.1177/2041669518786527
   Pickard MD, 2016, COMPUT HUM BEHAV, V65, P23, DOI 10.1016/j.chb.2016.08.004
   Recio G, 2011, BRAIN RES, V1376, P66, DOI 10.1016/j.brainres.2010.12.041
   Riedl R, 2014, J MANAGE INFORM SYST, V30, P83, DOI 10.2753/MIS0742-1222300404
   Rohatgi A., 2021, WebPlotDigitizer
   Rossion B., 2012, The Oxford Handbook of Event-Related Potential Components, P1, DOI [DOI 10.1093/OXFORDHB/9780195374148.013.0064, 10.1093/oxfordhb/9780195374148.013.0064]
   Rossion B, 2008, ACTA PSYCHOL, V128, P274, DOI 10.1016/j.actpsy.2008.02.003
   Roth D., 2019, PERCEIVED AUTHENTICI, P21, DOI [10.1145/3340764.3340797, DOI 10.1145/3340764.3340797]
   Royer J, 2018, COGNITION, V181, P12, DOI 10.1016/j.cognition.2018.08.004
   Rubin A, 2022, J GEN INTERN MED, V37, P70, DOI 10.1007/s11606-021-06945-9
   Schindler S, 2017, SCI REP-UK, V7, DOI 10.1038/srep45003
   Schyns PG, 2003, NEUROREPORT, V14, P1665, DOI 10.1097/00001756-200309150-00002
   Seo Y, 2017, COMPUT HUM BEHAV, V69, P120, DOI 10.1016/j.chb.2016.12.020
   Shen BY, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9667066
   Shepherd RM, 2005, PERS INDIV DIFFER, V39, P949, DOI 10.1016/j.paid.2005.04.001
   Singh B, 2022, PERS SOC PSYCHOL B, V48, P865, DOI 10.1177/01461672211024463
   Sollfrank T, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651044
   Sutherland CAM, 2013, COGNITION, V127, P105, DOI 10.1016/j.cognition.2012.12.001
   Syrjämäki AH, 2020, COMPUT HUM BEHAV, V112, DOI 10.1016/j.chb.2020.106454
   TANAKA JW, 1993, Q J EXP PSYCHOL-A, V46, P225, DOI 10.1080/14640749308401045
   Thorstenson CA, 2019, EMOTION, V19, P799, DOI 10.1037/emo0000485
   Tinwell A., 2015, International Journal of Mechanisms and Robotic Systems, V2, P97, DOI [DOI 10.1504/IJMRS.2015, 10.1504/IJMRS.2015.068991, DOI 10.1504/IJMRS.2015.068991]
   Tinwell A, 2014, COMPUT HUM BEHAV, V36, P286, DOI 10.1016/j.chb.2014.03.073
   Tinwell A, 2013, COMPUT HUM BEHAV, V29, P1617, DOI 10.1016/j.chb.2013.01.008
   Tinwell A, 2011, COMPUT HUM BEHAV, V27, P741, DOI 10.1016/j.chb.2010.10.018
   Todorov A, 2005, SCIENCE, V308, P1623, DOI 10.1126/science.1110589
   Todorov A, 2008, TRENDS COGN SCI, V12, P455, DOI 10.1016/j.tics.2008.10.001
   Vaitonyte J, 2021, COMPUT HUM BEHAV REP, V3, DOI 10.1016/j.chbr.2021.100065
   VALENTINE T, 1991, Q J EXP PSYCHOL-A, V43, P161, DOI 10.1080/14640749108400966
   Valentine T, 2016, Q J EXP PSYCHOL, V69, P1996, DOI 10.1080/17470218.2014.990392
   Vernon RJW, 2014, P NATL ACAD SCI USA, V111, pE3353, DOI 10.1073/pnas.1409860111
   Viechtbauer W, 2010, J STAT SOFTW, V36, P1, DOI 10.18637/jss.v036.i03
   Wang Liz C., 2010, International Journal of Electronic Marketing and Retailing, V3, P341, DOI 10.1504/IJEMR.2010.036881
   Wang NB, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44058-w
   Wieser MJ, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00471
   Willis J, 2006, PSYCHOL SCI, V17, P592, DOI 10.1111/j.1467-9280.2006.01750.x
   Wilson JP, 2015, PSYCHOL SCI, V26, P1325, DOI 10.1177/0956797615590992
   YIN RK, 1969, J EXP PSYCHOL, V81, P141, DOI 10.1037/h0027474
   YOUNG AW, 1987, PERCEPTION, V16, P747, DOI 10.1068/p160747
   Zhao TH, 2020, NEUROREPORT, V31, P437, DOI 10.1097/WNR.0000000000001420
   Zhou Y., 2016, P 10 INT S COMM SYST, P1, DOI DOI 10.1109/CSNDSP.2016.7573913
NR 166
TC 3
Z9 3
U1 6
U2 10
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 2451-9588
J9 COMPUT HUM BEHAV REP
JI Comput. Hum. Behav. Rep.
PD MAY
PY 2023
VL 10
AR 100283
DI 10.1016/j.chbr.2023.100283
EA MAY 2023
PG 25
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Emerging Sources Citation Index (ESCI)
SC Psychology
GA J0WZ3
UT WOS:001006909700001
OA gold
DA 2024-01-09
ER

PT J
AU Nussbaum, C
   Pöhlmann, M
   Kreysa, H
   Schweinberger, SR
AF Nussbaum, Christine
   Poehlmann, Manuel
   Kreysa, Helene
   Schweinberger, Stefan R.
TI Perceived naturalness of emotional voice morphs
SO COGNITION & EMOTION
LA English
DT Article
DE Naturalness; parameter-specific voice morphing; vocal emotions; F0;
   Timbre
ID FUNDAMENTAL-FREQUENCY; AUDITORY ADAPTATION; SPEECH NATURALNESS;
   PERCEPTION; INTELLIGIBILITY; GENDER; COMMUNICATION; APERIODICITY;
   PSYTOOLKIT; EXPRESSION
AB Research into voice perception benefits from manipulation software to gain experimental control over acoustic expression of social signals such as vocal emotions. Today, parameter-specific voice morphing allows a precise control of the emotional quality expressed by single vocal parameters, such as fundamental frequency (F0) and timbre. However, potential side effects, in particular reduced naturalness, could limit ecological validity of speech stimuli. To address this for the domain of emotion perception, we collected ratings of perceived naturalness and emotionality on voice morphs expressing different emotions either through F0 or Timbre only. In two experiments, we compared two different morphing approaches, using either neutral voices or emotional averages as emotionally non-informative reference stimuli. As expected, parameter-specific voice morphing reduced perceived naturalness. However, perceived naturalness of F0 and Timbre morphs were comparable with averaged emotions as reference, potentially making this approach more suitable for future research. Crucially, there was no relationship between ratings of emotionality and naturalness, suggesting that the perception of emotion was not substantially affected by a reduction of voice naturalness. We hold that while these findings advocate parameter-specific voice morphing as a suitable tool for research on vocal emotion perception, great care should be taken in producing ecologically valid stimuli.
C1 [Nussbaum, Christine; Poehlmann, Manuel; Kreysa, Helene; Schweinberger, Stefan R.] Friedrich Schiller Univ Jena, Dept Gen Psychol & Cognit Neurosci, Jena, Germany.
   [Nussbaum, Christine; Kreysa, Helene; Schweinberger, Stefan R.] Friedrich Schiller Univ, Voice Res Unit, Jena, Germany.
   [Schweinberger, Stefan R.] Univ Geneva, Swiss Ctr Affect Sci, Geneva, Switzerland.
C3 Friedrich Schiller University of Jena; Friedrich Schiller University of
   Jena; University of Geneva
RP Nussbaum, C; Schweinberger, SR (corresponding author), Friedrich Schiller Univ Jena, Dept Gen Psychol & Cognit Neurosci, Steiger 3 Haus 1, D-07743 Jena, Germany.
EM christine.nussbaum@uni-jena.de; stefan.schweinberger@uni-jena.de
OI Nussbaum, Christine/0000-0003-2718-2898; Kreysa,
   Helene/0000-0001-7163-7023; Schweinberger, Stefan/0000-0001-5762-0188
CR Alku P, 1999, CLIN NEUROPHYSIOL, V110, P1329, DOI 10.1016/S1388-2457(99)00088-7
   Anand S, 2015, J SPEECH LANG HEAR R, V58, P1134, DOI 10.1044/2015_JSLHR-S-14-0243
   ANSI, 1973, PSYCH S3 20
   Arias P, 2021, EMOT REV, V13, P12, DOI 10.1177/1754073920934544
   Assmann P. F., 2006, INTERSPEECH S COND M
   Assmann PF, 2000, J ACOUST SOC AM, V108, P1856, DOI 10.1121/1.1289363
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Baird A, 2018, J AUDIO ENG SOC, V66, P277, DOI 10.17743/jaes.2018.0023
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Belin P, 2011, BRIT J PSYCHOL, V102, P711, DOI 10.1111/j.2044-8295.2011.02041.x
   Bestelmeyer PEG, 2010, COGNITION, V117, P217, DOI 10.1016/j.cognition.2010.08.008
   Boersma P., 2021, Glot International
   Bruckert L, 2010, CURR BIOL, V20, P116, DOI 10.1016/j.cub.2009.11.034
   BURTON MW, 1995, J EXP PSYCHOL HUMAN, V21, P1230, DOI 10.1037/0096-1523.21.5.1230
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Calder AJ, 2000, COGNITION, V76, P105, DOI 10.1016/S0010-0277(00)00074-3
   Coughlin-Woods S, 2005, PERCEPT MOTOR SKILL, V100, P295
   Crookes K, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141353
   Crumpton J, 2016, INT J SOC ROBOT, V8, P271, DOI 10.1007/s12369-015-0329-4
   Cumming G, 2014, PSYCHOL SCI, V25, P7, DOI 10.1177/0956797613504966
   Eadie TL, 2002, J SPEECH LANG HEAR R, V45, P1088, DOI 10.1044/1092-4388(2002/087)
   EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550
   Fritz CO, 2012, J EXP PSYCHOL GEN, V141, P30, DOI 10.1037/a0026092
   Frühholz S, 2015, CEREB CORTEX, V25, P2752, DOI 10.1093/cercor/bhu074
   Giordano BL, 2021, NAT HUM BEHAV, V5, P1203, DOI 10.1038/s41562-021-01073-0
   Gong L, 2008, COMPUT HUM BEHAV, V24, P1494, DOI 10.1016/j.chb.2007.05.007
   Grichkovtsova I, 2012, SPEECH COMMUN, V54, P414, DOI 10.1016/j.specom.2011.10.005
   Haubo Rune., 2015, Stand, V19, P2016
   Heider F, 1944, AM J PSYCHOL, V57, P243, DOI 10.2307/1416950
   Hortensius R, 2018, IEEE T COGN DEV SYST, V10, P852, DOI 10.1109/TCDS.2018.2826921
   Hubbard DJ, 2013, J ACOUST SOC AM, V133, P2367, DOI 10.1121/1.4792145
   Ilves M, 2013, BEHAV INFORM TECHNOL, V32, P117, DOI 10.1080/0144929X.2012.702285
   Ilves M, 2011, LECT NOTES COMPUT SC, V6974, P588, DOI 10.1007/978-3-642-24600-5_62
   Juslin PN, 2003, PSYCHOL BULL, V129, P770, DOI 10.1037/0033-2909.129.5.770
   Kätsyri J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00390
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Kawahara H., 2013, IEEE INT C AC SPEECH, P1, DOI [10.1109/APSIPA.2013.6694355, DOI 10.1109/APSIPA.2013.6694355]
   Kawahara H., 2019, The Oxford Handbook of Voice Perception, P685
   Klopfenstein M, 2020, CLIN LINGUIST PHONET, V34, P327, DOI 10.1080/02699206.2019.1652692
   Kloth N, 2017, NEUROIMAGE, V155, P1, DOI 10.1016/j.neuroimage.2017.04.049
   Lakens D., 2019, Simulation-based power-analysis for factorial ANOVA designs, DOI DOI 10.31234/OSF.IO/BAXSF
   Mackey LS, 1997, J SPEECH LANG HEAR R, V40, P349, DOI 10.1044/jslhr.4002.349
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   MATLAB, 2020, VERS 9 8 0 R2020A
   Mayo C, 2011, SPEECH COMMUN, V53, P311, DOI 10.1016/j.specom.2010.10.003
   McAleer P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0090779
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   Meltzner GS, 2005, J SPEECH LANG HEAR R, V48, P766, DOI 10.1044/1092-4388(2005/053)
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Nadler JT, 2015, J GEN PSYCHOL, V142, P71, DOI 10.1080/00221309.2014.994590
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nusbaum H. C., 1995, International Journal of Speech Technology, V1, P7, DOI 10.1007/BF02277176
   Nussbaum C, 2022, SOC COGN AFFECT NEUR, V17, P1145, DOI 10.1093/scan/nsac033
   Nussbaum C, 2022, COGNITION, V219, DOI 10.1016/j.cognition.2021.104967
   Paulmann S., 2018, The Oxford Handbook of Voice Perception, P458
   Pell MD, 2015, BIOL PSYCHOL, V111, P14, DOI 10.1016/j.biopsycho.2015.08.008
   Péron J, 2015, CORTEX, V63, P172, DOI 10.1016/j.cortex.2014.08.023
   SCHERER KR, 1986, PSYCHOL BULL, V99, P143, DOI 10.1037/0033-2909.99.2.143
   Schindler S, 2017, SCI REP-UK, V7, DOI 10.1038/srep45003
   Schirmer A, 2017, SOC COGN AFFECT NEUR, V12, P902, DOI 10.1093/scan/nsx020
   Schweinberger SR, 2008, CURR BIOL, V18, P684, DOI 10.1016/j.cub.2008.04.015
   Schweinberger SR, 2020, COMPUT HUM BEHAV, V106, DOI 10.1016/j.chb.2020.106256
   Skuk VG, 2020, J SPEECH LANG HEAR R, V63, P3155, DOI 10.1044/2020_JSLHR-20-00026
   Skuk VG, 2015, J ACOUST SOC AM, V138, P1180, DOI 10.1121/1.4927696
   Skuk VG, 2014, J SPEECH LANG HEAR R, V57, P285, DOI 10.1044/1092-4388(2013/12-0314)
   Spatola N, 2021, COMPUT HUM BEHAV, V124, DOI 10.1016/j.chb.2021.106934
   Stoet G, 2017, TEACH PSYCHOL, V44, P24, DOI 10.1177/0098628316677643
   Stoet G, 2010, BEHAV RES METHODS, V42, P1096, DOI 10.3758/BRM.42.4.1096
   Vojtech JM, 2019, AM J SPEECH-LANG PAT, V28, P875, DOI 10.1044/2019_AJSLP-MSC18-18-0052
   von Eiff CI, 2022, EAR HEARING, V43, P1178, DOI 10.1097/AUD.0000000000001181
   Webster MA, 1999, PSYCHON B REV, V6, P647, DOI 10.3758/BF03212974
   Whiting CM, 2020, COGNITION, V200, DOI 10.1016/j.cognition.2020.104249
   Yamagishi J, 2012, ACOUST SCI TECHNOL, V33, P1, DOI 10.1250/ast.33.1
   Yamasaki R, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.09.020
   Yorkston K.M., 1999, Management of motor speech disorders in children and adults
   YORKSTON KM, 1990, J SPEECH HEAR DISORD, V55, P550, DOI 10.1044/jshd.5503.550
   Young AW, 2011, BRIT J PSYCHOL, V102, P959, DOI 10.1111/j.2044-8295.2011.02045.x
NR 78
TC 2
Z9 2
U1 10
U2 13
PU ROUTLEDGE JOURNALS, TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND
SN 0269-9931
EI 1464-0600
J9 COGNITION EMOTION
JI Cogn. Emot.
PD MAY 19
PY 2023
VL 37
IS 4
BP 731
EP 747
DI 10.1080/02699931.2023.2200920
EA APR 2023
PG 17
WC Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA J5MB6
UT WOS:000975713700001
PM 37104118
DA 2024-01-09
ER

PT J
AU Im, H
   Sung, BLY
   Lee, GR
   Kok, KQX
AF Im, Hyunjoo
   Sung, Billy
   Lee, Garim
   Kok, Keegan Qi Xian
TI Let voice assistants sound like a machine: Voice and task type effects
   on perceived fluency, competence, and consumer attitude
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Voice assistant; Synthetic voice; Voice perception
ID WARMTH; PERCEPTION; COMPUTER; MODEL
AB Voice Assistants (VA) are increasingly penetrating consumers' daily lives. This study aimed to investigate the effects of synthetic vs. human voice on users' perception of the voice, social judgments of the VA, and attitudes towards VAs. Drawing from CASA(Computers-as-socialactors) framework and social perception literature, we developed a theoretical model that explains the psychological underlying mechanism of the voice effects. Through two online experiments, we rejected our initial hypotheses that human voice would increase users' perception and evaluation of the VAs. Instead, our findings support that the VAs were favored when they spoke with a synthetic voice only when the users engaged in functional tasks. There was no difference between the voices for social tasks. A further investigation revealed that participants perceived the synthetic voice to be more fluent when VA responds to functional tasks. This enhanced perception of fluency increased competence perception and attitudes. The findings imply that VAs should not be designed to closely resemble humans. Rather, consideration of usage contexts and consumer expectations should be prioritized in developing most likable VAs.
C1 [Im, Hyunjoo; Lee, Garim] Univ Minnesota Twin Cities, Retailing & Consumer Studies, 240 McNeal Hall, 1985 Buford Ave, St Paul, MN 55108 USA.
   [Sung, Billy; Kok, Keegan Qi Xian] Curtin Univ, Sch Management & Mkt, Consumer Res Lab, Bentley, WA 6102, Australia.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   Curtin University
RP Im, H (corresponding author), Univ Minnesota Twin Cities, Retailing & Consumer Studies, 240 McNeal Hall, 1985 Buford Ave, St Paul, MN 55108 USA.
EM hjim@umn.edu; billy.sung@curtin.edu.au; lee02169@umn.edu;
   keegan.kok@curtin.edu.au
OI Lee, Garim/0000-0002-7054-1967; Kok, Keegan/0000-0002-6623-8957; Sung,
   Billy/0000-0003-0028-6574; Im, Hyunjoo/0000-0002-7932-0087
CR Amick LJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01148
   [Anonymous], NUMB VOIC ASS US WOR
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Belanche D, 2021, PSYCHOL MARKET, V38, P2357, DOI 10.1002/mar.21532
   Cambre Julia, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359325
   Carolus Astrid, 2021, Frontiers in Computer Science, V3, P46, DOI DOI 10.3389/FCOMP.2021.682982
   Chattaraman V, 2019, COMPUT HUM BEHAV, V90, P315, DOI 10.1016/j.chb.2018.08.048
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Cohen J, 1988, STAT POWER ANAL BEHA
   Core dna, 2021, CHATBOTFAIL 4 CHATBO
   Craig Scotty D., 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P2272, DOI 10.1177/1071181319631517
   Demeure V, 2011, PRESENCE-VIRTUAL AUG, V20, P431, DOI 10.1162/PRES_a_00065
   Dickerson R, 2006, STUD HEALTH TECHNOL, V119, P114
   Dou X, 2021, INT J SOC ROBOT, V13, P615, DOI 10.1007/s12369-020-00654-9
   Fan A, 2016, J SERV MARK, V30, P713, DOI 10.1108/JSM-07-2015-0225
   Fernandes T, 2021, J BUS RES, V122, P180, DOI 10.1016/j.jbusres.2020.08.058
   Fiske ST, 2002, J PERS SOC PSYCHOL, V82, P878, DOI 10.1037//0022-3514.82.6.878
   Fiske ST, 2007, TRENDS COGN SCI, V11, P77, DOI 10.1016/j.tics.2006.11.005
   Fraj S., 2009, 2009 INTERSPEECH, P2907, DOI [10.21437/Interspeech.2009-736, DOI 10.21437/INTERSPEECH.2009-736]
   Gambino A., 2020, HumanMachine Communication, V1, P71, DOI [DOI 10.30658/HMC.1.5, 10.30658/hmc.1.5]
   Go E, 2019, COMPUT HUM BEHAV, V97, P304, DOI 10.1016/j.chb.2019.01.020
   Guha A, 2023, J ACAD MARKET SCI, V51, P843, DOI 10.1007/s11747-022-00874-7
   Hayes A. F., 2013, Introduction to Mediation, Moderation, and Conditional Process Analysis: Methodology in the Social Sciences, DOI DOI 10.1111/JEDM.12050
   Higgins D, 2022, COMPUT GRAPH-UK, V104, P116, DOI 10.1016/j.cag.2022.03.009
   Imhof M., 2010, INT J LIST, V24, P19, DOI [10.1080/10904010903466295, DOI 10.1080/10904010903466295]
   Kim SY, 2019, MARKET LETT, V30, P1, DOI 10.1007/s11002-019-09485-9
   Krenn B, 2017, AI SOC, V32, P65, DOI 10.1007/s00146-014-0569-0
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   Latinus M, 2011, CURR BIOL, V21, pR143, DOI 10.1016/j.cub.2010.12.033
   Lee JER, 2010, TRUST TECHNOLOGY UBI, P1, DOI DOI 10.4018/978-1-61520-901-9.CH001
   Leigh T. W., 2002, J PERS SELL SALES M, V22, P41, DOI [10.1080/08853134.2002.10754292, DOI 10.1080/08853134.2002.10754292]
   Li M., 2021, Proceedings of the 54th Hawaii International Conference on System Sciences, P4053, DOI [https://doi.org/10.24251/HICSS.2021.493, DOI 10.24251/HICSS.2021.493]
   Longoni C, 2022, J MARKETING, V86, P91, DOI 10.1177/0022242920957347
   McDonough M., 2020, CURR CONTENTS
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   Meppelink CS, 2019, J HEALTH COMMUN, V24, P129, DOI 10.1080/10810730.2019.1583701
   Mourey JA, 2017, J CONSUM RES, V44, P414, DOI 10.1093/jcr/ucx038
   Nass C, 2000, J SOC ISSUES, V56, P81, DOI 10.1111/0022-4537.00153
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Nijholt A, 2003, 2003 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P128, DOI 10.1109/CYBER.2003.1253445
   Nusbaum H. C., 1995, International Journal of Speech Technology, V1, P7, DOI 10.1007/BF02277176
   PETTY RE, 1983, J CONSUM RES, V10, P135, DOI 10.1086/208954
   Pitardi V, 2021, PSYCHOL MARKET, V38, P626, DOI 10.1002/mar.21457
   PricewaterhouseCoopers, 2018, CONS INT SER PREP VO
   Reeves B., 1996, The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Pla, DOI DOI 10.1007/S42452-020-2192-7
   Rella S., 2021, SPEECH TECHNOLO 1026
   Salem M, 2013, LECT NOTES ARTIF INT, V8239, P531, DOI 10.1007/978-3-319-02675-6_53
   Schreibelmayr S, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.787499
   Seymour William, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3479515
   Sherman JW, 2009, J PERS SOC PSYCHOL, V96, P305, DOI 10.1037/a0013778
   Sohn H, 2019, NEURON, V103, P934, DOI 10.1016/j.neuron.2019.06.012
   Stern SE, 2006, INT J HUM-COMPUT ST, V64, P43, DOI 10.1016/j.ijhcs.2005.07.002
   Stroessner SJ, 2019, INT J SOC ROBOT, V11, P305, DOI 10.1007/s12369-018-0502-7
   Torre I., 2021, Voice Attractiveness, P299, DOI [10.1007/978-981-15-6627-1_16, DOI 10.1007/978-981-15-6627-1_16]
   Torre I, 2020, IEEE ROMAN, P215, DOI 10.1109/RO-MAN47096.2020.9223449
   Urakami Jacqueline, 2020, Human-Computer Interaction. Multimodal and Natural Interaction. Thematic Area, HCI 2020 Held as Part of the 22nd International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12182), P244, DOI 10.1007/978-3-030-49062-1_17
   Vernuccio M, 2023, J CONSUM BEHAV, V22, P1074, DOI 10.1002/cb.1984
   Voorveld HAM, 2020, CYBERPSYCH BEH SOC N, V23, P689, DOI 10.1089/cyber.2019.0205
   Wang WH, 2017, COMPUT HUM BEHAV, V68, P334, DOI 10.1016/j.chb.2016.11.022
   WASON PC, 1966, BRIT J PSYCHOL, V57, P413, DOI 10.1111/j.2044-8295.1966.tb01044.x
   Whang C, 2021, PSYCHOL MARKET, V38, P581, DOI 10.1002/mar.21437
   Williams R., 2019, MARKETING DIVE 0919
   Xie ZH, 2022, PSYCHOL MARKET, V39, P1902, DOI 10.1002/mar.21706
NR 64
TC 4
Z9 4
U1 30
U2 39
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD AUG
PY 2023
VL 145
AR 107791
DI 10.1016/j.chb.2023.107791
EA APR 2023
PG 11
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA F8LK0
UT WOS:000984806300001
DA 2024-01-09
ER

PT J
AU Ko, S
   Barnes, J
   Dong, J
   Park, CH
   Howard, A
   Jeon, M
AF Ko, Sangjin
   Barnes, Jaclyn
   Dong, Jiayuan
   Park, Chung Hyuk
   Howard, Ayanna
   Jeon, Myounghoon
TI The Effects of Robot Voices and Appearances on Users' Emotion
   Recognition and Subjective Perception
SO INTERNATIONAL JOURNAL OF HUMANOID ROBOTICS
LA English
DT Article
DE Social robots; conversational agent; emotive voices; user perception;
   user preference
ID TRUST; FACE
AB As the influence of social robots in people's daily lives grows, research on understanding people's perception of robots including sociability, trust, acceptance, and preference becomes more pervasive. Research has considered visual, vocal, or tactile cues to express robots' emotions, whereas little research has provided a holistic view in examining the interactions among different factors influencing emotion perception. We investigated multiple facets of user perception on robots during a conversational task by varying the robots' voice types, appearances, and emotions. In our experiment, 20 participants interacted with two robots having four different voice types. While participants were reading fairy tales to the robot, the robot gave vocal feedback with seven emotions and the participants evaluated the robot's profiles through post surveys. The results indicate that (1) the accuracy of emotion perception differed depending on presented emotions, (2) a regular human voice showed higher user preferences and naturalness, (3) but a characterized voice was more appropriate for expressing emotions with significantly higher accuracy in emotion perception, and (4) participants showed significantly higher emotion recognition accuracy with the animal robot than the humanoid robot. A follow-up study (N=10) with voice-only conditions confirmed that the importance of embodiment. The results from this study could provide the guidelines needed to design social robots that consider emotional aspects in conversations between robots and users.
C1 [Ko, Sangjin; Dong, Jiayuan; Jeon, Myounghoon] Virginia Polytech Inst & State Univ, Grad Dept Ind & Syst Engn, Blacksburg, VA 24061 USA.
   [Barnes, Jaclyn; Jeon, Myounghoon] Michigan Technol Univ, Dept Comp Sci, Houghton, MI 49931 USA.
   [Park, Chung Hyuk] George Washington Univ, Dept Biomed Engn, Dept Comp Sci, Washington, DC USA.
   [Howard, Ayanna] Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH USA.
C3 Virginia Polytechnic Institute & State University; Michigan
   Technological University; George Washington University; University
   System of Ohio; Ohio State University
RP Jeon, M (corresponding author), Virginia Polytech Inst & State Univ, Grad Dept Ind & Syst Engn, Blacksburg, VA 24061 USA.; Jeon, M (corresponding author), Michigan Technol Univ, Dept Comp Sci, Houghton, MI 49931 USA.
EM myounghoonjeon@vt.edu
OI Jeon, Myounghoon/0000-0003-2908-671X
FU National Institutes of Health (US) [1 R01 HD082914-01]
FX This work was partly supported by National Institutes of Health (US)
   (No.1 R01 HD082914-01).
CR Bachorowski JA, 2003, ANN NY ACAD SCI, V1000, P244, DOI 10.1196/annals.1280.012
   Bänziger T, 2009, EMOTION, V9, P691, DOI 10.1037/a0017088
   Barnes J., 2018, P 24 INT C AUD DISPL, P271
   Barnes J, 2017, INT CONF UBIQ ROBOT, P51
   Barnes JA, 2021, INT J HUM-COMPUT INT, V37, P249, DOI 10.1080/10447318.2020.1819667
   Beer JS, 2006, BRAIN RES, V1079, P98, DOI 10.1016/j.brainres.2006.01.002
   Bevill R, 2016, ACMIEEE INT CONF HUM, P421, DOI 10.1109/HRI.2016.7451786
   Birkholz P, 2015, J ACOUST SOC AM, V137, P1503, DOI 10.1121/1.4906836
   Bryant D, 2020, ACMIEEE INT CONF HUM, P13, DOI 10.1145/3319502.3374778
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Coeckelbergh M, 2010, ETHICS INF TECHNOL, V12, P235, DOI 10.1007/s10676-010-9221-y
   Conti D, 2020, INTERACT STUD, V21, P220, DOI 10.1075/is.18024.con
   Cordaro DT, 2016, EMOTION, V16, P117, DOI 10.1037/emo0000100
   Darwin C., 1872, P374
   Decety J, 2010, DEV NEUROSCI-BASEL, V32, P257, DOI 10.1159/000317771
   Diaz M., 2011, FACE GESTURE, V27, P927, DOI [10.1109/FG.2011.5771375, DOI 10.1109/FG.2011.5771375]
   DiSalvo C.F., 2002, P DIS02 DES INT SYST, P321
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Ekman P, 2011, EMOT REV, V3, P364, DOI 10.1177/1754073911410740
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   Eyssel F., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P851, DOI 10.1109/ROMAN.2012.6343858
   Eyssel F, 2012, ACMIEEE INT CONF HUM, P125
   FakhrHosseini, 2017, 2017 26 IEEE INT S R
   Feldmaier, 2017, PROC COMPANION 2017
   Fischer K, 2019, ACMIEEE INT CONF HUM, P29, DOI 10.1109/HRI.2019.8673078
   Fong T, 2003, ROBOT AUTON SYST, V42, P143, DOI 10.1016/S0921-8890(02)00372-X
   Fraune MR, 2015, ACMIEEE INT CONF HUM, P109, DOI [10.1145/2696454.2696483, 10.1145/2696454.2696472]
   Frith CD, 2006, NEURON, V50, P531, DOI 10.1016/j.neuron.2006.05.001
   Ham J, 2015, INT J SOC ROBOT, V7, P479, DOI 10.1007/s12369-015-0280-4
   Haring KS, 2013, ACMIEEE INT CONF HUM, P131, DOI 10.1109/HRI.2013.6483536
   Jeon M, 2011, LECT NOTES COMPUT SC, V6762, P523
   Jung MF, 2017, ACMIEEE INT CONF HUM, P263, DOI 10.1145/2909824.3020224
   Kishi T, 2013, IEEE INT CONF ROBOT, P1663, DOI 10.1109/ICRA.2013.6630793
   Kraus M, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P112
   Kwak S. S., 2013, 2013 IEEE RO MAN, P180, DOI DOI 10.1109/ROMAN.2013.6628441
   LEWIS JD, 1985, SOC FORCES, V63, P967, DOI 10.2307/2578601
   Li J, 2015, INT J HUM-COMPUT ST, V77, P23, DOI 10.1016/j.ijhcs.2015.01.001
   Liu ZT, 2016, CHIN CONTR CONF, P6363, DOI 10.1109/ChiCC.2016.7554357
   Lowe, 2016, GROUNDING EMOTIONS R
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   Mitchell RLC, 2015, NEUROPSYCHOLOGIA, V70, P1, DOI 10.1016/j.neuropsychologia.2015.02.018
   Nabe S., 2006, ROB HUM INT COMM 200, P384, DOI DOI 10.1109/ROMAN.2006.314464
   Nass Clifford, 2001, P AAAI S EM INT 2 TA
   Nonaka S, 2004, IEEE INT CONF ROBOT, P2770, DOI 10.1109/ROBOT.2004.1307480
   Ortony A, 2022, PERSPECT PSYCHOL SCI, V17, P41, DOI 10.1177/1745691620985415
   Pereira A, 2008, P 7 INT JOINT C AUT, V3, P1253
   Phillips ML, 2003, BRIT J PSYCHIAT, V182, P190, DOI 10.1192/bjp.182.3.190
   Plutchik Robert, 1980, Theories of Emotion, P3, DOI DOI 10.1016/B978-0-12-558701-3.50007-7
   Reisenzein R, 2013, IEEE T AFFECT COMPUT, V4, P246, DOI 10.1109/T-AFFC.2013.14
   Rodriguez-Lizundia E, 2015, INT J HUM-COMPUT ST, V82, P83, DOI 10.1016/j.ijhcs.2015.06.001
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Russell JA, 2017, EMOTIONS AND AFFECT IN HUMAN FACTORS AND HUMAN-COMPUTER INTERACTION, P123, DOI 10.1016/B978-0-12-801851-4.00004-5
   Saint-Aime Sebastien, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P919
   Sakai K, 2022, IEEE ROBOT AUTOM LET, V7, P366, DOI 10.1109/LRA.2021.3128233
   Sangjin Ko, 2020, HCI International 2020 - Late Breaking Papers. Multimodality and Intelligence. 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12424), P174, DOI 10.1007/978-3-030-60117-1_13
   Schilhab, 2002, ANIM BEHAV
   Schirmer A, 2017, TRENDS COGN SCI, V21, P216, DOI 10.1016/j.tics.2017.01.001
   Seo SH, 2015, ACMIEEE INT CONF HUM, P125, DOI 10.1145/2696454.2696471
   Sharma, 2013, PROC 8 ACMIEEE INT C
   Sims, 2009, PROC HUMAN FACTORS E
   Song SC, 2017, ACMIEEE INT CONF HUM, P2, DOI 10.1145/2909824.3020239
   Susskind JM, 2008, NAT NEUROSCI, V11, P843, DOI 10.1038/nn.2138
   Thomaz A., 2016, Foundations and Trends in Robotics, V4, P105, DOI 10.1561/2300000049
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Waytz A, 2014, J EXP SOC PSYCHOL, V52, P113, DOI 10.1016/j.jesp.2014.01.005
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   ZIGMOND AS, 1983, ACTA PSYCHIAT SCAND, V67, P361, DOI 10.1111/j.1600-0447.1983.tb09716.x
NR 67
TC 1
Z9 2
U1 20
U2 24
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0219-8436
EI 1793-6942
J9 INT J HUM ROBOT
JI Int. J. Humanoid Robot.
PD FEB
PY 2023
VL 20
IS 01
DI 10.1142/S0219843623500019
EA FEB 2023
PG 33
WC Robotics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Robotics
GA C2FG7
UT WOS:000937023400001
OA Green Published, hybrid
DA 2024-01-09
ER

PT J
AU van Prooije, T
   Knuijt, S
   Oostveen, J
   Kapteijns, K
   Vogel, AP
   van de Warrenburg, B
AF van Prooije, Teije
   Knuijt, Simone
   Oostveen, Judith
   Kapteijns, Kirsten
   Vogel, Adam P. P.
   van de Warrenburg, Bart
TI Perceptual and Acoustic Analysis of Speech in Spinocerebellar ataxia
   Type 1
SO CEREBELLUM
LA English
DT Article; Early Access
DE Spinocerebellar ataxia type 1; Speech; Dysarthria
ID FRIEDREICH ATAXIA; DYSARTHRIA; OUTCOMES; MARKERS; DISEASE
AB This study characterizes the speech phenotype of spinocerebellar ataxia type 1 (SCA1) using both perceptual and objective acoustic analysis of speech in a cohort of SCA1 patients. Twenty-seven symptomatic SCA1 patients in various disease stages (SARA score range: 3-32 points) and 18 sex and age matched healthy controls underwent a clinical assessment addressing ataxia severity, non-ataxia signs, cognitive functioning, and speech. Speech samples were perceptually rated by trained speech therapists, and acoustic metrics representing speech timing, vocal control, and voice quality were extracted. Perceptual analysis revealed reduced intelligibility and naturalness in speech samples of SCA1 patients. Acoustically, SCA1 patients presented with slower speech rate and diadochokinetic rate as well as longer syllable duration compared to healthy controls. No distinct abnormalities in voice quality in the acoustic analysis were detected at group level. Both the affected perceptual and acoustic variables correlated with ataxia severity. Longitudinal assessment of speech is needed to place changes in speech in the context of disease progression and potential response to treatment.
C1 [van Prooije, Teije; Kapteijns, Kirsten; van de Warrenburg, Bart] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Dept Neurol, Med Ctr, Nijmegen, Netherlands.
   [Knuijt, Simone; Oostveen, Judith] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Dept Rehabil, Med Ctr, Nijmegen, Netherlands.
   [Vogel, Adam P. P.] Univ Melbourne, Ctr Neurosci Speech, Melbourne, Australia.
   [Vogel, Adam P. P.] Univ Tubingen, Hertie Inst Clin Brain Res, Translat Genom Neurodegenerat Dis, Tubingen, Germany.
   [Vogel, Adam P. P.] Univ Tubingen, Ctr Neurol, Tubingen, Germany.
   [Vogel, Adam P. P.] Redenlab Inc, Melbourne, Australia.
C3 Radboud University Nijmegen; Radboud University Nijmegen; University of
   Melbourne; Eberhard Karls University of Tubingen; Eberhard Karls
   University Hospital; Eberhard Karls University of Tubingen
RP van de Warrenburg, B (corresponding author), Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Dept Neurol, Med Ctr, Nijmegen, Netherlands.
EM Bart.vandeWarrenburg@radboudumc.nl
RI Vogel, Adam/ABU-8530-2022
OI Kapteijns, Kirsten/0000-0001-5735-1934; van Prooije,
   Teije/0000-0002-6372-0801; Vogel, Adam/0000-0002-3505-2631
CR Boersma P., 2021, Glot International
   Brendel B, 2013, CEREBELLUM, V12, P475, DOI 10.1007/s12311-012-0440-0
   Brooker SM, 2021, ANN CLIN TRANSL NEUR, V8, P1543, DOI 10.1002/acn3.51370
   Bürk K, 2003, J NEUROL, V250, P207, DOI 10.1007/s00415-003-0976-5
   Chan JCS, 2022, J HUNTINGTONS DIS, V11, P71, DOI 10.3233/JHD-210501
   Chan JCS, 2019, NEUROSCI BIOBEHAV R, V107, P450, DOI 10.1016/j.neubiorev.2019.08.009
   Hoche F, 2018, BRAIN, V141, P248, DOI 10.1093/brain/awx317
   Jacobi H, 2013, CEREBELLUM, V12, P418, DOI 10.1007/s12311-012-0421-3
   Jacobi H, 2015, LANCET NEUROL, V14, P1101, DOI 10.1016/S1474-4422(15)00202-1
   Klockgether T, 2019, NAT REV DIS PRIMERS, V5, DOI 10.1038/s41572-019-0074-3
   Knuijt S, 2017, FOLIA PHONIATR LOGO, V69, P143, DOI 10.1159/000484556
   Luo L, 2017, CEREBELLUM, V16, P615, DOI 10.1007/s12311-016-0836-3
   Nasreddine ZS, 2005, J AM GERIATR SOC, V53, P695, DOI 10.1111/j.1532-5415.2005.53221.x
   Nigri A, 2022, CEREBELLUM, V21, P133, DOI 10.1007/s12311-021-01285-0
   ORR HT, 1993, NAT GENET, V4, P221, DOI 10.1038/ng0793-221
   Rosen KM, 2012, J NEUROL, V259, P2471, DOI 10.1007/s00415-012-6547-x
   Schalling E, 2013, BRAIN LANG, V127, P317, DOI 10.1016/j.bandl.2013.10.002
   Schmahmann JD, 1998, BRAIN, V121, P561, DOI 10.1093/brain/121.4.561
   Schmitz-Hübsch T, 2006, NEUROLOGY, V66, P1717, DOI 10.1212/01.wnl.0000219042.60538.92
   Sidtis JJ, 2011, J COMMUN DISORD, V44, P478, DOI 10.1016/j.jcomdis.2011.03.002
   Vogel AP, 2020, NEUROLOGY, V95, pE194, DOI 10.1212/WNL.0000000000009776
   Vogel AP, 2018, J NEUROL, V265, P2060, DOI 10.1007/s00415-018-8950-4
   Vogel AP, 2017, MITOCHONDRION, V37, P1, DOI 10.1016/j.mito.2017.06.002
   Vogel AP, 2017, NEUROLOGY, V89, P837, DOI 10.1212/WNL.0000000000004248
   Vogel AP, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.04.015
   Vogel Adam P., 2015, Frontiers in Bioengineering and Biotechnology, V3, P98, DOI 10.3389/fbioe.2015.00098
   Vogel AP, 2014, SPEECH COMMUN, V65, P1, DOI 10.1016/j.specom.2014.05.002
   Vogel AP, 2012, NEUROPSYCHOLOGIA, V50, P3273, DOI 10.1016/j.neuropsychologia.2012.09.011
   Vogel AP, 2011, J VOICE, V25, P137, DOI 10.1016/j.jvoice.2009.09.003
   Walton MK, 2015, VALUE HEALTH, V18, P741, DOI 10.1016/j.jval.2015.08.006
   Weismer G, 2002, J SPEECH LANG HEAR R, V45, P421, DOI 10.1044/1092-4388(2002/033)
NR 31
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1473-4222
EI 1473-4230
J9 CEREBELLUM
JI Cerebellum
PD 2023 JAN 12
PY 2023
DI 10.1007/s12311-023-01513-9
EA JAN 2023
PG 9
WC Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Neurosciences & Neurology
GA 8C9KD
UT WOS:000917918400001
PM 36633828
OA hybrid
DA 2024-01-09
ER

PT J
AU Li, MM
   Guo, F
   Wang, XS
   Chen, JH
   Ham, J
AF Li, Mingming
   Guo, Fu
   Wang, Xueshuang
   Chen, Jiahao
   Ham, Jaap
TI Effects of robot gaze and voice human-likeness on users' subjective
   perception, visual attention, and cerebral activity in voice
   conversations
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Robot gaze; Voice human-likeness; Subjective perception; Eye-tracking;
   fNIRS
ID PREFRONTAL CORTEX; PERCEIVED DIFFICULTY; OLDER-PEOPLE; EYE CONTACT;
   EMOTION; FNIRS; STEREOTYPES; INTEGRATION; ATTITUDES; COGNITION
AB Robot gaze and voice are essential anthropomorphic features to promote users' engagement in voice conver-sations. Earlier research chiefly examined how robot gaze and voice human-likeness separately influenced users' subjective perception. When implementing gaze on robots with different human-like voices, there has little evidence of their possible interaction effects, particularly on users' visual attention and cerebral activity, which could help to understand the perceptual and cognitive processing of anthropomorphic features. Therefore, a within-subject experiment of voice conversations with diverse robot gaze (gaze versus no gaze) and human-like voices (high human-like versus low human-like) using subjective reporting, eye-tracker, and fNIRS was con-ducted. The results showed that the robot with gaze or a high human-like voice evoked more pleasure, higher arousal, more perceived likability, and less negative attitudes. Robot gaze significantly increased users' average fixation durations and total fixation time, while voice human-likeness prolonged first fixation durations. Moreover, the robot with a high human-like voice (or gaze) induced increased activity in the left DLPFC and decreased activity in the right Broca's area than that had no gaze (or a low human-like voice). The results suggest that robot gaze might chiefly capture users' sustained attention, voice human-likeness might attract users' initial attention, and they might jointly influence users' perceptual processing of prosodic features and emotional processing.
C1 [Li, Mingming; Guo, Fu; Chen, Jiahao] Northeastern Univ, Sch Business Adm, Dept Ind Engn, Shenyang, Peoples R China.
   [Wang, Xueshuang] Shenyang Univ Technol, Sch Mech Engn, Shenyang, Peoples R China.
   [Ham, Jaap] Eindhoven Univ Technol, Dept Ind Engn & Innovat Sci, Res Grp Human Technol Interact, Eindhoven, Netherlands.
   [Guo, Fu] 195 Chuangxin Rd, Shenyang 110167, Peoples R China.
C3 Northeastern University - China; Shenyang University of Technology;
   Eindhoven University of Technology
RP Guo, F (corresponding author), 195 Chuangxin Rd, Shenyang 110167, Peoples R China.
EM fguo@mail.neu.edu.cn
RI Ham, Jaap/H-4754-2011
OI Chen, Jiahao/0000-0002-6110-7851; Ham, Jaap/0000-0003-1703-5165; Li,
   Mingming/0000-0002-4702-6350
FU National Natural Science Foundation of China [72071035, 72171042]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 72071035 and Grant No. 72171042) . No conflict of
   interest exists in submitting this paper, and all authors approve it for
   publication. We are grateful to all the experimental participants for
   this study. Furthermore, we are genuinely pleased to extend our
   gratitude to editors and reviewers for their valuable work.
CR Admoni H, 2017, J HUM-ROBOT INTERACT, V6, P25, DOI 10.5898/JHRI.6.1.Admoni
   Amso D, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0085701
   Andrist S, 2014, ACMIEEE INT CONF HUM, P25, DOI 10.1145/2559636.2559666
   Argyle M., 1972, Non-verbal communication, V2
   Babel F, 2021, INT J SOC ROBOT, V13, P1485, DOI 10.1007/s12369-020-00730-0
   Baillon A, 2013, EVOL HUM BEHAV, V34, P146, DOI 10.1016/j.evolhumbehav.2012.12.001
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Balconi M, 2021, J MOTOR BEHAV, V53, P296, DOI 10.1080/00222895.2020.1774490
   Balconi M, 2015, BRAIN COGNITION, V95, P67, DOI 10.1016/j.bandc.2015.02.001
   Bartneck C., 2010, Paladyn, Journal of Behavioral Robotics, V1, P109, DOI DOI 10.2478/S13230-010-0011-3
   Behe BK, 2015, J RETAIL CONSUM SERV, V24, P10, DOI 10.1016/j.jretconser.2015.01.002
   Belin P, 2006, PHILOS T R SOC B, V361, P2091, DOI 10.1098/rstb.2006.1933
   Belkaid M, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abc5044
   Bourguet Marie-Luce, 2020, HAI '20: Proceedings of the 8th International Conference on Human-Agent Interaction, P60, DOI 10.1145/3406499.3415073
   Burleigh TJ, 2013, COMPUT HUM BEHAV, V29, P759, DOI 10.1016/j.chb.2012.11.021
   Burton MW, 2000, J COGNITIVE NEUROSCI, V12, P679, DOI 10.1162/089892900562309
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Callaway C, 2006, COMPUT LINGUIST, V32, P451, DOI 10.1162/coli.2006.32.3.451
   Chang RCS, 2018, COMPUT HUM BEHAV, V84, P194, DOI 10.1016/j.chb.2018.02.025
   Charest I, 2009, BMC NEUROSCI, V10, DOI 10.1186/1471-2202-10-127
   COPE M, 1988, MED BIOL ENG COMPUT, V26, P289, DOI 10.1007/BF02447083
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Degutyte Z, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.616471
   Dou X, 2022, INT J SOC ROBOT, V14, P229, DOI 10.1007/s12369-021-00782-w
   Dou X, 2021, INT J SOC ROBOT, V13, P615, DOI 10.1007/s12369-020-00654-9
   Duffy BR, 2003, ROBOT AUTON SYST, V42, P177, DOI 10.1016/S0921-8890(02)00374-3
   Emery NJ, 2000, NEUROSCI BIOBEHAV R, V24, P581, DOI 10.1016/S0149-7634(00)00025-7
   Eyssel, 2012, P 7 ANN ACMIEEE INT, DOI DOI 10.1145/2157689.2157717
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Ferrari M, 2012, NEUROIMAGE, V63, P921, DOI 10.1016/j.neuroimage.2012.03.049
   Forbes CE, 2010, ANNU REV NEUROSCI, V33, P299, DOI 10.1146/annurev-neuro-060909-153230
   Friederici AD, 2004, BRAIN LANG, V89, P267, DOI 10.1016/S0093-934X(03)00351-1
   Gameiro RR, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-02526-1
   Garza R, 2016, EVOL PSYCHOL-US, V14, DOI 10.1177/1474704916631614
   Ghiglino Davide, 2020, Paladyn, Journal of Behavioral Robotics, V11, P31, DOI 10.1515/pjbr-2020-0004
   Ghiglino D, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.642796
   Glotzbach Evelyn, 2011, Open Neuroimag J, V5, P33, DOI 10.2174/1874440001105010033
   Gray JR, 2002, P NATL ACAD SCI USA, V99, P4115, DOI 10.1073/pnas.062381899
   Guo F, 2019, INT J HUM-COMPUT INT, V35, P1947, DOI 10.1080/10447318.2019.1587938
   Ham J, 2015, INT J SOC ROBOT, V7, P479, DOI 10.1007/s12369-015-0280-4
   Heim S, 2003, COGNITIVE BRAIN RES, V16, P285, DOI 10.1016/S0926-6410(02)00284-7
   Herrington JD, 2005, EMOTION, V5, P200, DOI 10.1037/1528-3542.5.2.200
   Hietanen JK, 2008, NEUROPSYCHOLOGIA, V46, P2423, DOI 10.1016/j.neuropsychologia.2008.02.029
   Hoshi Y, 2003, PSYCHOPHYSIOLOGY, V40, P511, DOI 10.1111/1469-8986.00053
   Hou X, 2021, NEUROPHOTONICS, V8, DOI 10.1117/1.NPh.8.1.010802
   Huang JL, 2022, HUM FACTORS, V64, P1051, DOI 10.1177/0018720820987443
   Husic-Mehmedovic M, 2017, J BUS RES, V80, P145, DOI 10.1016/j.jbusres.2017.04.019
   Jiang J, 2017, SOC COGN AFFECT NEUR, V12, P319, DOI 10.1093/scan/nsw127
   Jiang J, 2012, J NEUROSCI, V32, P16064, DOI 10.1523/JNEUROSCI.2926-12.2012
   JUST MA, 1976, COGNITIVE PSYCHOL, V8, P441, DOI 10.1016/0010-0285(76)90015-3
   Kawasaki M, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00318
   Kelley MS, 2021, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.599581
   Keshmiri S, 2019, IEEE ROBOT AUTOM LET, V4, P4108, DOI 10.1109/LRA.2019.2930495
   Keshmiri S, 2019, IEEE ROBOT AUTOM LET, V4, P3263, DOI 10.1109/LRA.2019.2925732
   KLEINKE CL, 1986, PSYCHOL BULL, V100, P78, DOI 10.1037/0033-2909.100.1.78
   Klüber K, 2022, COMPUT HUM BEHAV, V128, DOI 10.1016/j.chb.2021.107128
   Kompatsiari K, 2019, IEEE INT C INT ROBOT, P6979, DOI [10.1109/IROS40897.2019.8967747, 10.1109/iros40897.2019.8967747]
   Kompatsiari K, 2021, SOC COGN AFFECT NEUR, V16, P383, DOI 10.1093/scan/nsab001
   Kompatsiari K, 2021, INT J SOC ROBOT, V13, P525, DOI 10.1007/s12369-019-00565-4
   Kompatsiari K, 2017, LECT NOTES ARTIF INT, V10652, P443, DOI 10.1007/978-3-319-70022-9_44
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   Kuo JY, 2021, APPL ERGON, V94, DOI 10.1016/j.apergo.2021.103393
   Kwak SS, 2017, INT J SOC ROBOT, V9, P359, DOI 10.1007/s12369-016-0388-1
   Leong V, 2017, P NATL ACAD SCI USA, V114, P13290, DOI 10.1073/pnas.1702493114
   Levy DA, 2003, PSYCHOPHYSIOLOGY, V40, P291, DOI 10.1111/1469-8986.00031
   Li MM, 2022, INT J IND ERGONOM, V88, DOI 10.1016/j.ergon.2021.103159
   Liew TW, 2021, TELEMAT INFORM, V65, DOI 10.1016/j.tele.2021.101721
   Loh HW, 2022, COMPUT METH PROG BIO, V226, DOI 10.1016/j.cmpb.2022.107161
   Lu XP, 2019, HUM BRAIN MAPP, V40, P1942, DOI 10.1002/hbm.24503
   Manzi F, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-69140-6
   Mavridis N, 2015, ROBOT AUTON SYST, V63, P22, DOI 10.1016/j.robot.2014.09.031
   Mehrabian A., 1974, APPROACH ENV PSYCHOL
   Morillo-Mendez L, 2021, LECT NOTES ARTIF INT, V13086, P350, DOI 10.1007/978-3-030-90525-5_30
   Murray LJ, 2007, J NEUROSCI, V27, P5515, DOI 10.1523/JNEUROSCI.0406-07.2007
   Mutlu Bilge, 2012, ACM Trans Interact Intell Syst (TiiS), V2, P1, DOI DOI 10.1145/2070719.2070725
   NASS C, 1995, INT J HUM-COMPUT ST, V43, P223, DOI 10.1006/ijhc.1995.1042
   Nomura T, 2006, INTERACT STUD, V7, P437, DOI 10.1075/is.7.3.14nom
   Okafuji Y, 2020, ADV ROBOTICS, V34, P931, DOI 10.1080/01691864.2020.1769724
   Perugia G, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.645956
   Piva M, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00571
   Quaresima V, 2019, ORGAN RES METHODS, V22, P46, DOI 10.1177/1094428116658959
   Rapp A, 2021, INT J HUM-COMPUT ST, V151, DOI 10.1016/j.ijhcs.2021.102630
   Roesler E, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abj5425
   Sarigul B., 2020, COMPANION 2020 ACMIE
   Schreibelmayr S, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.787499
   Seaborn K, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3386867
   Singh AK, 2005, NEUROIMAGE, V27, P842, DOI 10.1016/j.neuroimage.2005.05.019
   Tatarian K, 2022, INT J SOC ROBOT, V14, P893, DOI 10.1007/s12369-021-00839-w
   Tay B, 2014, COMPUT HUM BEHAV, V38, P75, DOI 10.1016/j.chb.2014.05.014
   Thepsoonthorn C, 2021, INT J SOC ROBOT, V13, P1443, DOI 10.1007/s12369-020-00726-w
   Tiberio L, 2013, ROBOTICS, V2, P92, DOI 10.3390/robotics2020092
   Tsiourti C, 2019, INT J SOC ROBOT, V11, P555, DOI 10.1007/s12369-019-00524-z
   Unema PJA, 2005, VIS COGN, V12, P473, DOI 10.1080/13506280444000409
   Vollmer AL, 2018, SCI ROBOT, V3, DOI 10.1126/scirobotics.aat7111
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Wang JH, 2020, COMPUT EDUC, V146, DOI 10.1016/j.compedu.2019.103779
   WASSERTHEIL S, 1970, BIOMETRICS, V26, P588, DOI 10.2307/2529115
   Wei Y, 2016, IND ROBOT, V43, P380, DOI 10.1108/IR-08-2015-0164
   WHEELER RE, 1993, PSYCHOPHYSIOLOGY, V30, P82, DOI 10.1111/j.1469-8986.1993.tb03207.x
   Wiese E, 2019, PHILOS T R SOC B, V374, DOI 10.1098/rstb.2018.0430
   Winkle K, 2020, INT J SOC ROBOT, V12, P847, DOI [10.1007/s12369-019-00536-9, 10.1080/10494820.2019.1696844]
   Wood LJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059448
   Xu K, 2019, NEW MEDIA SOC, V21, P2522, DOI 10.1177/1461444819851479
   Yang GZ, 2018, SCI ROBOT, V3, DOI 10.1126/scirobotics.aar7650
   Zhang DD, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-18683-2
   Zhang H, 2020, BIOL PSYCHOL, V150, DOI 10.1016/j.biopsycho.2019.107827
   Zhang RH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4951, DOI 10.24963/ijcai.2020/689
   Zhang YX, 2017, LECT NOTES ARTIF INT, V10652, P556, DOI 10.1007/978-3-319-70022-9_55
NR 108
TC 2
Z9 2
U1 39
U2 85
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD APR
PY 2023
VL 141
AR 107645
DI 10.1016/j.chb.2022.107645
EA JAN 2023
PG 15
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA 8C0RC
UT WOS:000917324100001
DA 2024-01-09
ER

PT J
AU Shiramizu, VKM
   Lee, AJ
   Altenburg, D
   Feinberg, DR
   Jones, BC
AF Shiramizu, Victor Kenji M.
   Lee, Anthony J.
   Altenburg, Daria
   Feinberg, David R.
   Jones, Benedict C.
TI The role of valence, dominance, and pitch in perceptions of artificial
   intelligence (AI) conversational agents' voices
SO SCIENTIFIC REPORTS
LA English
DT Article
ID BIAS
AB There is growing concern that artificial intelligence conversational agents (e.g., Siri, Alexa) reinforce voice-based social stereotypes. Because little is known about social perceptions of conversational agents' voices, we investigated (1) the dimensions that underpin perceptions of these synthetic voices and (2) the role that acoustic parameters play in these perceptions. Study 1 (N = 504) found that perceptions of synthetic voices are underpinned by Valence and Dominance components similar to those previously reported for natural human stimuli and that the Dominance component was strongly and negatively related to voice pitch. Study 2 (N = 160) found that experimentally manipulating pitch in synthetic voices directly influenced dominance-related, but not valence-related, perceptions. Collectively, these results suggest that greater consideration of the role that voice pitch plays in dominance-related perceptions when designing conversational agents may be an effective method for controlling stereotypic perceptions of their voices and the downstream consequences of those perceptions.
C1 [Shiramizu, Victor Kenji M.; Jones, Benedict C.] Univ Strathclyde, Sch Psychol Sci & Hlth, Glasgow, Lanark, Scotland.
   [Lee, Anthony J.] Univ Stirling, Div Psychol, Stirling, Scotland.
   [Altenburg, Daria] Univ Ghent, Dept Mkt Innovat & Org, Ghent, Belgium.
   [Feinberg, David R.] McMaster Univ, Dept Psychol Neurosci & Behav, Hamilton, ON, Canada.
C3 University of Strathclyde; University of Stirling; Ghent University;
   McMaster University
RP Jones, BC (corresponding author), Univ Strathclyde, Sch Psychol Sci & Hlth, Glasgow, Lanark, Scotland.
EM benedict.jones@strath.ac.uk
RI Shiramizu, Victor/GPP-0919-2022; Altenburg, Daria/ISU-0474-2023;
   Feinberg, David R/C-1249-2009; Lee, Anthony J./I-8220-2012; Jones,
   Benedict C/A-7850-2008
OI Feinberg, David R/0000-0003-4179-1446; Lee, Anthony
   J./0000-0001-8288-3393; Jones, Benedict C/0000-0001-7777-0220
FU EPSRC grant 'Designing Conversational Assistants to Reduce Gender Bias'
   [EP/T023783/1]; Ghent University [BOF.24Y.2019.0006.01]
FX This research was supported by the EPSRC grant 'Designing Conversational
   Assistants to Reduce Gender Bias' (EP/T023783/1), awarded to Benedict
   Jones. Daria Altenburg was supported by Grant BOF.24Y.2019.0006.01 of
   Ghent University, awarded to Adriaan Spruyt. For the purpose of Open
   Access, the authors have applied a Creative Commons Attribution (CC BY)
   to any Author Accepted Manuscript (AAM) version arising from this
   submission.
CR Apicella CL, 2009, P ROY SOC B-BIOL SCI, V276, P1077, DOI 10.1098/rspb.2008.1542
   Armstrong MM, 2019, ANIM BEHAV, V147, P43, DOI 10.1016/j.anbehav.2018.11.005
   Aung T, 2020, CURR OPIN PSYCHOL, V33, P154, DOI 10.1016/j.copsyc.2019.07.028
   Balas B, 2018, COMPUT HUM BEHAV, V88, P236, DOI 10.1016/j.chb.2018.07.013
   Balas B, 2017, COMPUT HUM BEHAV, V77, P240, DOI 10.1016/j.chb.2017.08.045
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Baus C, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36518-6
   Boersma P., 2018, PRAAT DOING PHONETIC
   Bolker B., 2022, R package version 0.2.9.4
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Dinno A., 2018, paran: Horn's Test of Principal Components/Factors
   Feinberg DR, 2005, ANIM BEHAV, V69, P561, DOI 10.1016/j.anbehav.2004.06.012
   Hester N, 2021, J EXP PSYCHOL GEN, V150, P1147, DOI 10.1037/xge0000989
   Hodges-Simeon CR, 2010, HUM NATURE-INT BIOS, V21, P406, DOI 10.1007/s12110-010-9101-5
   Jones BC, 2021, NAT HUM BEHAV, V5, P159, DOI 10.1038/s41562-020-01007-2
   Jones BC, 2010, ANIM BEHAV, V79, P57, DOI 10.1016/j.anbehav.2009.10.003
   Kuznetsova A, 2017, J STAT SOFTW, V82, P1, DOI 10.18637/jss.v082.i13
   Long J., 2020, JTOOLS ANAL PRESENTA
   McAleer P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0090779
   Oh D, 2019, PSYCHOL SCI, V30, P65, DOI 10.1177/0956797618813092
   Olivola CY, 2014, TRENDS COGN SCI, V18, P566, DOI 10.1016/j.tics.2014.09.007
   Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105
   Puts DA, 2006, EVOL HUM BEHAV, V27, P283, DOI 10.1016/j.evolhumbehav.2005.11.003
   R Core Team, 2018, R: A Language and Environment for Statistical Computing
   Revelle W, 2016, Psych: procedures for personality and psychological research, DOI DOI 10.1109/TEM.2010.2048913
   Rhodes G, 2006, ANNU REV PSYCHOL, V57, P199, DOI 10.1146/annurev.psych.57.102904.190208
   Schild C, 2022, ADAPT HUM BEHAV PHYS, V8, P538, DOI 10.1007/s40750-022-00194-8
   Schild C, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-77940-z
   Sutherland CAM, 2013, COGNITION, V127, P105, DOI 10.1016/j.cognition.2012.12.001
   West Mark, 2019, I'd blush if I could: closing gender divides in digital skills through education
   Wester Mirjam, 2017, P 19 ACM INT C MULT, P506
   Wickham H., 2017, R PACKAGE VERSION, V1, P1, DOI DOI 10.1590/S0100-204X2017000300003
   Wickham H., 2019, readxl: read Excel files
   Wilson JP, 2015, PSYCHOL SCI, V26, P1325, DOI 10.1177/0956797615590992
   Xie Y., 2014, Implementing Reproducible Computational Research, DOI DOI 10.1201/9781315373461-1
   Zhu H., 2019, kableExtra: Construct Complex Table with 'kable' and Pipe Syntax
NR 36
TC 0
Z9 0
U1 1
U2 6
PU NATURE PORTFOLIO
PI BERLIN
PA HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD DEC 28
PY 2022
VL 12
IS 1
AR 22479
DI 10.1038/s41598-022-27124-8
PG 9
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA 7L1VX
UT WOS:000905762900011
PM 36577918
OA Green Accepted, gold, Green Published
DA 2024-01-09
ER

PT J
AU Duville, MM
   Alonso-Valerdi, LM
   Ibarra-Zarate, DI
AF Duville, Mathilde Marie
   Alonso-Valerdi, Luz Maria
   Ibarra-Zarate, David I. I.
TI Neuronal and behavioral affective perceptions of human and
   naturalness-reduced emotional prosodies
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
LA English
DT Article
DE electroencephalography (EEG); single-trial event-related potential
   (ERP); affective prosody; emotions; naturalness; valence; arousal;
   synthesized speech
ID DISCRIMINATION; SEMANTICS; DYNAMICS; STRESS
AB Artificial voices are nowadays embedded into our daily lives with latest neural voices approaching human voice consistency (naturalness). Nevertheless, behavioral, and neuronal correlates of the perception of less naturalistic emotional prosodies are still misunderstood. In this study, we explored the acoustic tendencies that define naturalness from human to synthesized voices. Then, we created naturalness-reduced emotional utterances by acoustic editions of human voices. Finally, we used Event-Related Potentials (ERP) to assess the time dynamics of emotional integration when listening to both human and synthesized voices in a healthy adult sample. Additionally, listeners rated their perceptions for valence, arousal, discrete emotions, naturalness, and intelligibility. Synthesized voices were characterized by less lexical stress (i.e., reduced difference between stressed and unstressed syllables within words) as regards duration and median pitch modulations. Besides, spectral content was attenuated toward lower F2 and F3 frequencies and lower intensities for harmonics 1 and 4. Both psychometric and neuronal correlates were sensitive to naturalness reduction. (1) Naturalness and intelligibility ratings dropped with emotional utterances synthetization, (2) Discrete emotion recognition was impaired as naturalness declined, consistent with P200 and Late Positive Potentials (LPP) being less sensitive to emotional differentiation at lower naturalness, and (3) Relative P200 and LPP amplitudes between prosodies were modulated by synthetization. Nevertheless, (4) Valence and arousal perceptions were preserved at lower naturalness, (5) Valence (arousal) ratings correlated negatively (positively) with Higuchi's fractal dimension extracted on neuronal data under all naturalness perturbations, (6) Inter-Trial Phase Coherence (ITPC) and standard deviation measurements revealed high inter-individual heterogeneity for emotion perception that is still preserved as naturalness reduces. Notably, partial between-participant synchrony (low ITPC), along with high amplitude dispersion on ERPs at both early and late stages emphasized miscellaneous emotional responses among subjects. In this study, we highlighted for the first time both behavioral and neuronal basis of emotional perception under acoustic naturalness alterations. Partial dependencies between ecological relevance and emotion understanding outlined the modulation but not the annihilation of emotional integration by synthetization.
C1 [Duville, Mathilde Marie; Alonso-Valerdi, Luz Maria; Ibarra-Zarate, David I. I.] Tecnol Monterrey, Escuela Ingn & Ciencias, Monterrey, NL, Mexico.
C3 Tecnologico de Monterrey
RP Duville, MM (corresponding author), Tecnol Monterrey, Escuela Ingn & Ciencias, Monterrey, NL, Mexico.
EM A00829725@tec.mx
OI Ibarra Zarate, David Isaac/0000-0002-9870-9645
CR Akçay MB, 2020, SPEECH COMMUN, V116, P56, DOI 10.1016/j.specom.2019.12.001
   Aldeneh Z, 2023, IEEE T AFFECT COMPUT, V14, P1351, DOI 10.1109/TAFFC.2021.3086050
   Amin N, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0061417
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Boersma P., 2020, Praat: Doing phonetics by computer: Version 6.1.28
   Boersma P, 1993, P I PHONETIC SCI, V17, P97, DOI DOI 10.1371/JOURNAL.PONE.0069107
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Brück C, 2011, NEUROIMAGE, V58, P259, DOI 10.1016/j.neuroimage.2011.06.005
   Chang CY, 2020, IEEE T BIO-MED ENG, V67, P1114, DOI 10.1109/TBME.2019.2930186
   Chou LC, 2020, COGN AFFECT BEHAV NE, V20, P1294, DOI 10.3758/s13415-020-00835-z
   Delorme A, 2007, NEUROIMAGE, V34, P1443, DOI 10.1016/j.neuroimage.2006.11.004
   DiIeva A, 2016, SPR SER COMPUT NEURO, P1, DOI 10.1007/978-1-4939-3995-4
   Dong L, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00601
   Duville MM, 2021, IEEE ENG MED BIO, P1644, DOI 10.1109/EMBC46164.2021.9629934
   Duville MM, 2021, DATA, V6, DOI 10.3390/data6120130
   Elmer S, 2021, NEUROIMAGE, V235, DOI 10.1016/j.neuroimage.2021.118051
   Gao XQ, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00026
   Gatti E, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.120
   Gervain J, 2019, TRENDS NEUROSCI, V42, P56, DOI 10.1016/j.tins.2018.09.004
   Gervain J, 2016, NEUROIMAGE, V133, P144, DOI 10.1016/j.neuroimage.2016.03.001
   Goldman JP, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P3240
   Gutiérrez-Palma N, 2016, LEARN INDIVID DIFFER, V45, P144, DOI 10.1016/j.lindif.2015.11.026
   Hardy TLD, 2020, J VOICE, V34, DOI 10.1016/j.jvoice.2018.10.002
   Herbert C, 2011, SOC NEUROSCI-UK, V6, P277, DOI 10.1080/17470919.2010.523543
   Huang KL, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.664925
   Iseli M, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P669
   James J, 2018, IEEE ROMAN, P632, DOI 10.1109/ROMAN.2018.8525652
   Kappenman E.S., 2011, The Oxford Handbook of Event-Related Potential Components, DOI DOI 10.1093/OXFORDHB/9780195374148.001.0001
   Kotz SA, 2007, BRAIN RES, V1151, P107, DOI 10.1016/j.brainres.2007.03.015
   Kranzbuhler AM, 2020, J ACAD MARKET SCI, V48, P478, DOI 10.1007/s11747-019-00707-0
   Ku LC, 2020, COGN AFFECT BEHAV NE, V20, P371, DOI 10.3758/s13415-020-00774-9
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   Liu R, 2021, INTERSPEECH, P4648, DOI 10.21437/Interspeech.2021-1236
   Liu ZT, 2018, NEUROCOMPUTING, V273, P271, DOI 10.1016/j.neucom.2017.07.050
   Mariooryad S, 2014, SPEECH COMMUN, V57, P1, DOI 10.1016/j.specom.2013.07.011
   Mauchand M, 2021, COGN AFFECT BEHAV NE, V21, P74, DOI 10.3758/s13415-020-00849-7
   McDonald J. H., 2014, Handbook of biological statistics, V3rd
   Moore B., 2007, SPRINGER HDB ACOUSTI, DOI 10.1007/978-0-387-30425-0_13
   MOULINES E, 1990, SPEECH COMMUN, V9, P453, DOI 10.1016/0167-6393(90)90021-Z
   Nash-Kille A, 2014, CLIN NEUROPHYSIOL, V125, P1459, DOI 10.1016/j.clinph.2013.11.017
   Ning YS, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9194050
   Oostenveld R, 2011, COMPUT INTEL NEUROSC, V2011, DOI 10.1155/2011/156869
   Paulmann S, 2008, BRAIN LANG, V105, P59, DOI 10.1016/j.bandl.2007.11.005
   Paulmann S, 2017, PSYCHOPHYSIOLOGY, V54, P555, DOI 10.1111/psyp.12812
   Paulmann S, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00345
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   Pell MD, 2015, BIOL PSYCHOL, V111, P14, DOI 10.1016/j.biopsycho.2015.08.008
   Pell MD, 2021, EMOT REV, V13, P51, DOI 10.1177/1754073920954288
   Pereira DR, 2021, COGN AFFECT BEHAV NE, V21, P172, DOI 10.3758/s13415-020-00858-6
   PERRIN F, 1989, ELECTROEN CLIN NEURO, V72, P184, DOI 10.1016/0013-4694(89)90180-6
   Pinheiro AP, 2015, BRAIN LANG, V140, P24, DOI 10.1016/j.bandl.2014.10.009
   Reddy VR, 2016, NEUROCOMPUTING, V171, P1323, DOI 10.1016/j.neucom.2015.07.053
   Renard Y, 2010, PRESENCE-VIRTUAL AUG, V19, P35, DOI 10.1162/pres.19.1.35
   Rodero E, 2021, NEW MEDIA SOC, DOI 10.1177/14614448211024142
   Ruiz-Padial E, 2018, BIOL PSYCHOL, V137, P42, DOI 10.1016/j.biopsycho.2018.06.008
   Schirmer A, 2006, TRENDS COGN SCI, V10, P24, DOI 10.1016/j.tics.2005.11.009
   Schirmer A, 2013, COGN AFFECT BEHAV NE, V13, P80, DOI 10.3758/s13415-012-0132-8
   Schuller DM, 2021, EMOT REV, V13, P44, DOI 10.1177/1754073919898526
   Schwa S, 2017, J ACOUST SOC AM, V142, P2419, DOI 10.1121/1.5008849
   Selvam V.S., 2022, COMPLETE HIGUCHI FRA
   Singh P, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107316
   Sorati M, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02562
   Steber S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-62761-x
   Striepe H, 2021, INT J SOC ROBOT, V13, P441, DOI 10.1007/s12369-019-00570-7
   Tamura Y, 2015, SCI REP-UK, V5, DOI 10.1038/srep08799
   Treder MS, 2016, NEUROIMAGE, V129, P279, DOI 10.1016/j.neuroimage.2016.01.019
   Viswanathan M, 2005, COMPUT SPEECH LANG, V19, P55, DOI 10.1016/j.csl.2003.12.001
   Vos RR, 2018, J VOICE, V32, pE126, DOI 10.1016/j.jvoice.2017.03.017
   Wang C, 2021, PSYCHOPHYSIOLOGY, V58, DOI 10.1111/psyp.13775
   Xue YW, 2018, SPEECH COMMUN, V102, P54, DOI 10.1016/j.specom.2018.06.006
   Yasoda K, 2020, SOFT COMPUT, V24, P16011, DOI 10.1007/s00500-020-04920-w
   Zhao GZ, 2018, FRONT BEHAV NEUROSCI, V12, DOI 10.3389/fnbeh.2018.00225
   Zhao TC, 2019, BRAIN LANG, V194, P77, DOI 10.1016/j.bandl.2019.05.002
   Zheng XW, 2021, INT J INTELL SYST, V36, P152, DOI 10.1002/int.22295
   Zhou SL, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00046
NR 75
TC 1
Z9 1
U1 3
U2 6
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
EI 1662-5188
J9 FRONT COMPUT NEUROSC
JI Front. Comput. Neurosci.
PD NOV 18
PY 2022
VL 16
AR 1022787
DI 10.3389/fncom.2022.1022787
PG 22
WC Mathematical & Computational Biology; Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Mathematical & Computational Biology; Neurosciences & Neurology
GA 6S6IX
UT WOS:000893089900001
PM 36465969
OA gold, Green Published
DA 2024-01-09
ER

PT J
AU Abdulrahman, A
   Richards, D
AF Abdulrahman, Amal
   Richards, Deborah
TI Is Natural Necessary? Human Voice versus Synthetic Voice for Intelligent
   Virtual Agents
SO MULTIMODAL TECHNOLOGIES AND INTERACTION
LA English
DT Article
DE embodied conversational agent; stress management; voice; text-to-speech;
   co-presence; trust; working alliance
ID SOCIAL PRESENCE; ANTHROPOMORPHISM; USERS; CONSISTENCY; VALIDATION;
   MODEL; TRUST; SENSE; FACE
AB The use of intelligent virtual agents (IVA) to support humans in social contexts will depend on their social acceptability. Acceptance will be related to the human's perception of the IVAs as well as the IVAs' ability to respond and adapt their conversation appropriately to the human. Adaptation implies computer-generated speech (synthetic speech), such as text-to-speech (TTS). In this paper, we present the results of a study to investigate the effect of voice type (human voice vs. synthetic voice) on two aspects: (1) the IVA's likeability and voice impression in the light of co-presence, and (2) the interaction outcome, including human-agent trust and behavior change intention. The experiment included 118 participants who interacted with either the virtual advisor with TTS or the virtual advisor with human voice to gain tips for reducing their study stress. Participants in this study found the voice of the virtual advisor with TTS to be more eerie, but they rated both agents, with recorded voice and with TTS, similarly in terms of likeability. They further showed a similar attitude towards both agents in terms of co-presence and building trust. These results challenge previous studies that favor human voice over TTS, and suggest that even if human voice is preferred, TTS can deliver equivalent benefits.
C1 [Abdulrahman, Amal; Richards, Deborah] Macquarie Univ, Sch Comp, 4 Res Pk Dr, Macquarie Pk, NSW 2113, Australia.
C3 Macquarie University
RP Richards, D (corresponding author), Macquarie Univ, Sch Comp, 4 Res Pk Dr, Macquarie Pk, NSW 2113, Australia.
EM amal.abdulrahman@students.mq.edu.au; deborah.richards@mq.edu.au
OI Abdulrahman, Amal/0000-0001-5360-0833; Richards,
   Deborah/0000-0002-7363-1511
FU International Macquarie University Research Training Program (iMQRTP)
   [2015113]
FX This research was funded by an International Macquarie University
   Research Training Program (iMQRTP) scholarship-No. 2015113.
CR Abdulrahman A., 2021, P 20 INT C AUTONOMOU, P10
   Abdulrahman A., 2019, P 40 INT C INF SYST, P1
   Abdulrahman A, 2022, AUTON AGENT MULTI-AG, V36, DOI 10.1007/s10458-022-09553-x
   Abdulrahman A, 2021, J MULTIMODAL USER IN, V15, P189, DOI 10.1007/s12193-020-00359-3
   Aljameel SS, 2017, IEEE INT CONF COMP, P24, DOI 10.1109/CIVEMSA.2017.7995296
   Barcelos RH, 2018, J INTERACT MARK, V41, P60, DOI 10.1016/j.intmar.2017.10.001
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   Black A.W., 2000, LTD DOMAIN SYNTHESIS
   Blascovich J, 2002, PSYCHOL INQ, V13, P103, DOI 10.1207/S15327965PLI1302_01
   Brenton H., 2005, P C HUM COMP INT WOR
   Cambre J, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376789
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Ciechanowski L, 2019, FUTURE GENER COMP SY, V92, P539, DOI 10.1016/j.future.2018.01.055
   Clore GL, 2013, EMOT REV, V5, P335, DOI 10.1177/1754073913489751
   Cowan BR, 2015, INT J HUM-COMPUT ST, V83, P27, DOI 10.1016/j.ijhcs.2015.05.008
   de Visser EJ, 2016, J EXP PSYCHOL-APPL, V22, P331, DOI 10.1037/xap0000092
   Dickerson R, 2006, STUD HEALTH TECHNOL, V119, P114
   Diederich S, 2019, Online
   Georgila K, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3519
   Goffman E, 1978, PRESENTATION SELF EV
   Gong L, 2003, P HUMAN COMPUTER INT, P160
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   GREEN EJ, 1981, PERCEPT PSYCHOPHYS, V30, P459, DOI 10.3758/BF03204842
   Hatcher RL, 2006, PSYCHOTHER RES, V16, P12, DOI 10.1080/10503300500352500
   Higgins D, 2022, COMPUT GRAPH-UK, V104, P116, DOI 10.1016/j.cag.2022.03.009
   Ho CC, 2010, COMPUT HUM BEHAV, V26, P1508, DOI 10.1016/j.chb.2010.05.015
   HORVATH AO, 1989, J COUNS PSYCHOL, V36, P223, DOI 10.1037/0022-0167.36.2.223
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   Jia H., 2013, P CHI 13 HUM FACT CO, DOI DOI 10.1145/2468356.2468649
   Kang H, 2020, INT J HUM-COMPUT ST, V133, P45, DOI 10.1016/j.ijhcs.2019.09.002
   Kim S, 2019, IEEE WCNC, P1, DOI [DOI 10.1080/09593985.2019.1566940, DOI 10.1109/wcnc.2019.8885834]
   Lee EJ, 2010, COMPUT HUM BEHAV, V26, P665, DOI 10.1016/j.chb.2010.01.003
   Li M., 2021, Proceedings of the 54th Hawaii International Conference on System Sciences, P4053, DOI [https://doi.org/10.24251/HICSS.2021.493, DOI 10.24251/HICSS.2021.493]
   MacDorman KF, 2006, INTERACT STUD, V7, P297, DOI 10.1075/is.7.3.03mac
   Mascarenhas S, 2022, ACM T INTERACT INTEL, V12, DOI 10.1145/3510822
   Mayer RC, 1999, J APPL PSYCHOL, V84, P123, DOI 10.1037/0021-9010.84.1.123
   McNaughton H, 2021, CLIN REHABIL, V35, P1021, DOI 10.1177/0269215521993648
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Moore RK, 2012, SCI REP-UK, V2, DOI 10.1038/srep00864
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   NASS C, 1993, HUM COMMUN RES, V19, P504, DOI 10.1111/j.1468-2958.1993.tb00311.x
   Nelekar S, 2022, BRIT J EDUC TECHNOL, V53, P491, DOI 10.1111/bjet.13174
   Ning YS, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9194050
   Noah Ben, 2021, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, P1448, DOI 10.1177/1071181321651128
   Nowak K, 2001, PRES 2001 C PHIL PA, DOI 10.1.1.19.5482
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Oh CS, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00114
   Picard Rosalind W, 2000, Affective Computing
   Pitardi V, 2021, PSYCHOL MARKET, V38, P626, DOI 10.1002/mar.21457
   Provoost S, 2017, J MED INTERNET RES, V19, DOI 10.2196/jmir.6553
   Ranjbartabar H, 2020, MULTIMODAL TECHNOLOG, V4, DOI 10.3390/mti4030055
   Reeves B., 1998, MEDIA EQUATION PEOPL
   Richards D, 2018, IEEE J BIOMED HEALTH, V22, P1699, DOI 10.1109/JBHI.2017.2782210
   Rothstein N., 2020, P AHFE 2020 VIRTUAL, V28, P190, DOI [10.1007/978-3-030-51041-1_26, DOI 10.1007/978-3-030-51041-1_26]
   Schmitt A., 2021, ECIS 2021 RES PAPERS
   Schultze U, 2019, INFORM SYST J, V29, P707, DOI 10.1111/isj.12230
   Seaborn K, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3386867
   Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779, DOI 10.1109/ICASSP.2018.8461368
   Sisman B, 2018, IEEE W SP LANG TECH, P282, DOI 10.1109/SLT.2018.8639507
   Smith Barry, 1988, FDN GESTALT THEORY
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   ter Stal S, 2020, JMIR HUM FACTORS, V7, DOI 10.2196/19987
   Torre I, 2020, IEEE ROMAN, P215, DOI 10.1109/RO-MAN47096.2020.9223449
   Vaidyam AN, 2019, CAN J PSYCHIAT, V64, P456, DOI 10.1177/0706743719828977
   Van Pinxteren MME, 2020, J SERV MANAGE, V31, P203, DOI 10.1108/JOSM-06-2019-0175
   Wagner P., 2019, P 10 SPEECH SYNTHESI
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Xie Y, 2020, J RETAIL CONSUM SERV, V55, DOI 10.1016/j.jretconser.2020.102119
   Yuan X, 2005, COMPUT ANIMAT VIRT W, V16, P109, DOI 10.1002/cav.65
   Zanbaka C., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1153
NR 71
TC 4
Z9 4
U1 1
U2 15
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2414-4088
J9 MULTIMODAL TECHNOLOG
JI Multimodal Technol. Interaction
PD JUL
PY 2022
VL 6
IS 7
AR 51
DI 10.3390/mti6070051
PG 17
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Information Systems
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA 3L0EF
UT WOS:000834443500001
OA gold
DA 2024-01-09
ER

PT J
AU Schreibelmayr, S
   Mara, M
AF Schreibelmayr, Simon
   Mara, Martina
TI Robot Voices in Daily Life: Vocal Human-Likeness and Application Context
   as Determinants of User Acceptance
SO FRONTIERS IN PSYCHOLOGY
LA English
DT Article
DE speech interface; voice assistant; human-robot interaction; synthetic
   voice; anthropomorphism; uncanny valley; application context; user
   acceptance
ID UNCANNY VALLEY; INDIVIDUAL-DIFFERENCES; SYNTHESIZED SPEECH; SOCIAL
   ROBOTS; PERSONALITY; ANTHROPOMORPHISM; RESPONSES; EERINESS;
   METAANALYSIS; HABITUATION
AB The growing popularity of speech interfaces goes hand in hand with the creation of synthetic voices that sound ever more human. Previous research has been inconclusive about whether anthropomorphic design features of machines are more likely to be associated with positive user responses or, conversely, with uncanny experiences. To avoid detrimental effects of synthetic voice design, it is therefore crucial to explore what level of human realism human interactors prefer and whether their evaluations may vary across different domains of application. In a randomized laboratory experiment, 165 participants listened to one of five female-sounding robot voices, each with a different degree of human realism. We assessed how much participants anthropomorphized the voice (by subjective human-likeness ratings, a name-giving task and an imagination task), how pleasant and how eerie they found it, and to what extent they would accept its use in various domains. Additionally, participants completed Big Five personality measures and a tolerance of ambiguity scale. Our results indicate a positive relationship between human-likeness and user acceptance, with the most realistic sounding voice scoring highest in pleasantness and lowest in eeriness. Participants were also more likely to assign real human names to the voice (e.g., "Julia" instead of "T380") if it sounded more realistic. In terms of application context, participants overall indicated lower acceptance of the use of speech interfaces in social domains (care, companionship) than in others (e.g., information & navigation), though the most human-like voice was rated significantly more acceptable in social applications than the remaining four. While most personality factors did not prove influential, openness to experience was found to moderate the relationship between voice type and user acceptance such that individuals with higher openness scores rated the most human-like voice even more positively. Study results are discussed in the light of the presented theory and in relation to open research questions in the field of synthetic voice design.
C1 [Schreibelmayr, Simon; Mara, Martina] Johannes Kepler Univ Linz, LIT Robopsychol Lab, Linz, Austria.
C3 Johannes Kepler University Linz
RP Schreibelmayr, S (corresponding author), Johannes Kepler Univ Linz, LIT Robopsychol Lab, Linz, Austria.
EM simon.schreibelmayr@jku.at
CR Aaltonen I, 2017, ACMIEEE INT CONF HUM, P53, DOI 10.1145/3029798.3038362
   Amazon, 2017, AM POLL INTR NEW GER
   [Anonymous], 2012, EUR SPEC
   [Anonymous], 2019, ONL RES PLATF
   [Anonymous], 2019, ADOBE AUDITION
   Anthony LM, 2000, COMPUT HUM BEHAV, V16, P31, DOI 10.1016/S0747-5632(99)00050-3
   Appel M, 2016, ACMIEEE INT CONF HUM, P411, DOI 10.1109/HRI.2016.7451781
   Atkinson RK, 2005, CONTEMP EDUC PSYCHOL, V30, P117, DOI 10.1016/j.cedpsych.2004.07.001
   Audacity, 2019, FREE OP SOURC CROSS
   Audiveris, 2019, TOOL AUD LOUDN
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Baltes-Gotz B., 2017, MEDIATOR MODERATORAN
   Bartneck C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P367
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   Bendel O., 2021, SOZIALE ROBOTER, DOI [10.1007/978-3-658-31114-8, DOI 10.1007/978-3-658-31114-8_1]
   Blut M, 2021, J ACAD MARKET SCI, V49, P632, DOI 10.1007/s11747-020-00762-y
   BOCHNER S, 1965, PSYCHOL REC, V15, P393, DOI 10.1007/BF03393605
   Brédart S, 2021, ADV COGN PSYCHOL, V17, P33, DOI 10.5709/acp-0314-1
   Broadbent E., 2011, 25 C ARTIFICIAL INTE
   Burleigh TJ, 2013, COMPUT HUM BEHAV, V29, P759, DOI 10.1016/j.chb.2012.11.021
   Carpenter J., 2019, Interactions, V26, P56
   Carpinella CM, 2017, ACMIEEE INT CONF HUM, P254, DOI 10.1145/2909824.3020208
   Chang M, 2020, IEEE T NEUR SYS REH, V28, P2805, DOI 10.1109/TNSRE.2020.3038175
   Charness N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02589
   COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155
   Cohn M, 2020, INTERSPEECH, P1733, DOI 10.21437/Interspeech.2020-1336
   Costa P. T., 1985, The NEO Personality Inventory manual
   Couper M. P., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P412, DOI 10.1145/365024.365306
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Davison A.C., 1997, BOOTSTRAP METHODS TH, DOI [DOI 10.1017/CBO9780511802843, 10.1017/CBO9780511802843]
   de Graaf MMA, 2015, LECT NOTES ARTIF INT, V9388, P184, DOI 10.1007/978-3-319-25554-5_19
   Devaraj S, 2008, INFORM SYST RES, V19, P93, DOI 10.1287/isre.1070.0153
   Diel A, 2021, J VISION, V21, DOI 10.1167/jov.21.4.1
   DIGMAN JM, 1990, ANNU REV PSYCHOL, V41, P417, DOI 10.1146/annurev.ps.41.020190.002221
   Douven I, 2018, PSYCHON B REV, V25, P1203, DOI 10.3758/s13423-017-1344-2
   Duffy BR, 2003, ROBOT AUTON SYST, V42, P177, DOI 10.1016/S0921-8890(02)00374-3
   EAGLY AH, 1982, J PERS SOC PSYCHOL, V43, P915, DOI 10.1037/0022-3514.43.5.915
   Elkins AC, 2013, GROUP DECIS NEGOT, V22, P897, DOI 10.1007/s10726-012-9339-x
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   European Commission, 2019, Ethics Guidelines for Trustworthy AL
   Eyssel F, 2012, ACMIEEE INT CONF HUM, P125
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   FESTINGER L, 1962, SCI AM, V207, P93, DOI 10.1038/scientificamerican1062-93
   Fink J, 2012, 2012 IEEE WORKSHOP ON ADVANCED ROBOTICS AND ITS SOCIAL IMPACTS (ARSO), P54, DOI 10.1109/ARSO.2012.6213399
   FREESTON MH, 1994, PERS INDIV DIFFER, V17, P791, DOI 10.1016/0191-8869(94)90048-5
   Furnham A, 1995, CURR PSYCHOL, V14, P179, DOI 10.1007/BF02686907
   Gambino A, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312870
   Gaudiello I, 2016, COMPUT HUM BEHAV, V61, P633, DOI 10.1016/j.chb.2016.03.057
   Giles H., 1979, Social Markers in Speech. Ed. by, P343
   Goetz J, 2003, RO-MAN 2003: 12TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P55
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   Google Duplex, 2018, AI ASS CALLS LOC BUS
   Hayes AF, 2007, BEHAV RES METHODS, V39, P709, DOI 10.3758/BF03192961
   Hedda, 2019, MICR SPEECH PLATF
   Hentschel T, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00011
   Ho CC, 2017, INT J SOC ROBOT, V9, P129, DOI 10.1007/s12369-016-0380-9
   Ho CC, 2010, COMPUT HUM BEHAV, V26, P1508, DOI 10.1016/j.chb.2010.05.015
   HOPE ACA, 1968, J ROY STAT SOC B, V30, P582
   Ilves M, 2013, BEHAV INFORM TECHNOL, V32, P117, DOI 10.1080/0144929X.2012.702285
   Imhof M., 2010, BERUFLICHE BILDUNG F, P15
   Jia JW, 2021, IND MANAGE DATA SYST, V121, P1457, DOI 10.1108/IMDS-11-2020-0664
   John OP., 1991, BIG 5 INVENTORY VERS
   Jung Y., 2018, 22 BIENN C INT TEL S
   KAPLAN PS, 1995, DEV PSYCHOBIOL, V28, P45, DOI 10.1002/dev.420280105
   Kaur Ravneet, 2020, Smart Systems and IoT: Innovations in Computing. Proceeding of SSIC 2019. Smart Innovation, Systems and Technologies (SIST 141), P401, DOI 10.1007/978-981-13-8406-6_38
   Kiesler S., 2002, CHI 02 EXTENDED ABST, P576, DOI DOI 10.1145/506443.506491
   Kohlberg L., 1987, Child psychology and childhood education: A cognitive-developmental view
   Krauss RM, 2002, J EXP SOC PSYCHOL, V38, P618, DOI 10.1016/S0022-1031(02)00510-3
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lischetzke T, 2017, J RES PERS, V68, P96, DOI 10.1016/j.jrp.2017.02.001
   Lopatovska I, 2019, J LIBR INF SCI, V51, P984, DOI 10.1177/0961000618759414
   MacDorman KF, 2015, INTERACT STUD, V16, P141, DOI 10.1075/is.16.2.01mac
   Mara M, 2022, Z PSYCHOL, V230, P33, DOI 10.1027/2151-2604/a000486
   Mara M, 2020, ACMIEEE INT CONF HUM, P355, DOI 10.1145/3371382.3378285
   Mara M, 2015, COMPUT HUM BEHAV, V48, P156, DOI 10.1016/j.chb.2015.01.007
   Mara M, 2015, COMPUT HUM BEHAV, V44, P326, DOI 10.1016/j.chb.2014.09.025
   Maricutoiu LP, 2014, PROCD SOC BEHV, V127, P311, DOI 10.1016/j.sbspro.2014.03.262
   Marikyan D, 2023, INFORM SYST FRONT, V25, P1101, DOI 10.1007/s10796-020-10042-3
   Mathur MB, 2016, COGNITION, V146, P22, DOI 10.1016/j.cognition.2015.09.008
   Mayer RE, 2003, J EDUC PSYCHOL, V95, P419, DOI 10.1037/0022-0663.95.2.419
   McGee TJ, 2001, PSYCHOPHYSIOLOGY, V38, P653, DOI 10.1017/S0048577201990973
   Meah LFS, 2014, LECT NOTES ARTIF INT, V8755, P256, DOI 10.1007/978-3-319-11973-1_26
   Meinecke C., 2019, DELOITTE READER 2022
   Mejia C, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7121316
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Morsunbul U., 2019, J. Hum. Sci, V16, P499
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   NORTON RW, 1975, J PERS ASSESS, V39, P607, DOI 10.1207/s15327752jpa3906_11
   Nov O., 2008, P 41 ANN HAWAII INT, P448, DOI DOI 10.1109/HICSS.2008.348
   Oshio A, 2009, SOC BEHAV PERSONAL, V37, P729, DOI 10.2224/sbp.2009.37.6.729
   Oyedele A, 2007, CYBERPSYCHOL BEHAV, V10, P624, DOI 10.1089/cpb.2007.9977
   Pérula-Martínez R, 2017, ACMIEEE INT CONF HUM, P259, DOI 10.1145/3029798.3038434
   Pinker S, 2003, LANGUAGE INSTINCT MI
   Polly, 2019, PHYLOGENETICS MATH V
   Process, 2019, SPSS MACR
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Questback, 2018, FEEDB PLATTF UNT FUN
   Radant M., 2003, 7 DPPD TAG DTSCH GES
   Reeves B., 1998, MEDIA EQUATION PEOPL
   Robinson MD, 2003, PSYCHOLOGY OF EVALUATION, P275
   Robinson MD, 2004, CURR DIR PSYCHOL SCI, V13, P127, DOI 10.1111/j.0963-7214.2004.00290.x
   Roesler E, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abj5425
   Roesler E, 2022, INT J SOC ROBOT, V14, P1155, DOI 10.1007/s12369-021-00860-z
   Romportl Jan, 2014, Text, Speech and Dialogue. 17th International Conference, TSD 2014. Proceedings: LNCS 8655, P595, DOI 10.1007/978-3-319-10816-2_72
   Schlink S, 2007, Z SOZIALPSYCHOL, V38, P153, DOI 10.1024/0044-3514.38.3.153
   Schupp J., 2014, Zusammenstellung sozialwissenschaftlicher Items und Skalen, DOI [10.6102/zis54, DOI 10.6102/ZIS54]
   Seaborn K., 2021, 2021 CHI C HUMAN FAC, P1, DOI DOI 10.1145/3411763.3451712
   Shao J., 1995, The Jackknife and Bootstrap. Springer Series in Statistics, P23, DOI DOI 10.1007/978-1-4612-0795-5
   Smith HMJ, 2016, EVOL PSYCHOL-US, V14, DOI 10.1177/1474704916630317
   Sporer SL, 2006, APPL COGNITIVE PSYCH, V20, P421, DOI 10.1002/acp.1190
   Sprent P, 2007, J ROY STAT SOC A STA, V170, P1178, DOI 10.1111/j.1467-985X.2007.00506_2.x
   Statista, 2021, ONL RES PLATF
   Statistical CJ., 1992, Curr. Dir. Psychol. Sci, V1, P98, DOI [10.1111/1467-8721.ep10768783, DOI 10.1111/1467-8721.EP10768783]
   Sutton SJ, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300833
   Tiwari Manjul, 2012, J Nat Sci Biol Med, V3, P3, DOI 10.4103/0976-9668.95933
   Torre I, 2018, PROCEEDINGS OF THE TECHNOLOGY, MIND, AND SOCIETY CONFERENCE (TECHMINDSOCIETY'18), DOI 10.1145/3183654.3183691
   Torre Ilaria, 2015, ICPHS
   Tourangeau R, 2003, COMPUT HUM BEHAV, V19, P1, DOI 10.1016/S0747-5632(02)00032-8
   Ullman D, 2021, 2021 16TH ACM/IEEE INTERNATIONAL CONFERENCE ON HUMAN-ROBOT INTERACTION, HRI, P110, DOI 10.1145/3434073.3444652
   van den Oord A., 2016, WAVENET GEN MODEL RA, P125
   Vlachos E, 2016, INTERACT STUD, V17, P390, DOI 10.1075/is.17.3.04vla
   Voxal, 2019, VOIC CHANG
   Wada K, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2847
   Wada K, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P4940, DOI 10.1109/IROS.2006.282455
   Waytz A, 2010, PERSPECT PSYCHOL SCI, V5, P219, DOI 10.1177/1745691610369336
   West Mark, 2019, I'd blush if I could: closing gender divides in digital skills through education
   Whang C, 2021, PSYCHOL MARKET, V38, P581, DOI 10.1002/mar.21437
   Yang H., 2021, Proceedings of the 2021 CHI conference on human factors in computing systems, P1
   Zhang TR, 2020, TRANSPORT RES C-EMER, V112, P220, DOI 10.1016/j.trc.2020.01.027
NR 136
TC 8
Z9 8
U1 24
U2 70
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 1664-1078
J9 FRONT PSYCHOL
JI Front. Psychol.
PD MAY 13
PY 2022
VL 13
AR 787499
DI 10.3389/fpsyg.2022.787499
PG 17
WC Psychology, Multidisciplinary
WE Social Science Citation Index (SSCI)
SC Psychology
GA 1N4BQ
UT WOS:000800602900001
PM 35645911
OA gold, Green Published
DA 2024-01-09
ER

PT J
AU Higgins, D
   Zibrek, K
   Cabral, J
   Egan, D
   McDonnell, R
AF Higgins, Darragh
   Zibrek, Katja
   Cabral, Joao
   Egan, Donal
   McDonnell, Rachel
TI Sympathy for the digital: Influence of synthetic voice on affinity,
   social presence and empathy for photorealistic virtual humans
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual humans; Perception; Synthetic voice; Virtual reality
ID UNCANNY VALLEY; HUMAN REALISM; FACE; CONSISTENCY; APPEARANCE; AGENT
AB In this paper, we investigate the effect of a realism mismatch in the voice and appearance of a photorealistic virtual character in both immersive and screen-mediated virtual contexts. While many studies have investigated voice attributes for robots, not much is known about the effect voice naturalness has on the perception of realistic virtual characters. We conducted the first experiment in Virtual Reality (VR) with over two hundred participants investigating the mismatch between realistic appearance and unrealistic voice on the feeling of presence, and the emotional response of the user to the character expressing a strong negative emotion. We predicted that the mismatched voice would lower social presence and cause users to have a negative emotional reaction and feelings of discomfort towards the character. We found that the concern for the virtual character was indeed altered by the unnatural voice, though interestingly it did not affect social presence. The second experiment was conducted with a view towards heightening the appearance realism of the same character for the same scenarios, with an additional lower level of voice realism employed to strengthen the mismatch of perceptual cues. While voice type did not appear to impact reports of empathic responses towards the character, there was an observed effect of voice realism on reported social presence, which was not detected in the first study. There were also significant results on affinity and voice trait measurements that provide evidence in support of perceptual mismatch theories of the Uncanny Valley. (c) 2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
C1 [Higgins, Darragh; Cabral, Joao; Egan, Donal; McDonnell, Rachel] Trinity Coll Dublin, Dublin, Ireland.
   [Zibrek, Katja] INRIA Rennes, Rennes, France.
C3 Trinity College Dublin; Universite de Rennes
RP McDonnell, R (corresponding author), Trinity Coll Dublin, Dublin, Ireland.
EM HIGGIND3@tcd.ie; katja.zibrek@inria.fr; CABRALJ@tcd.ie; doegan@tcd.ie;
   ramcdonn@tcd.ie
RI McDonnell, Rachel/HGC-4337-2022; Zibrek, Katja/JQW-2981-2023
OI McDonnell, Rachel/0000-0002-1957-2506; Zibrek,
   Katja/0000-0002-0204-3472; Cabral, Joao Paulo/0000-0002-9298-1787; Egan,
   Donal/0000-0001-6491-9575
FU Sci-ence Foundation Ireland Centre for Research Training in
   Digitally-Enhanced Reality (d-real) [18/CRT/6224]; ADAPT Centre for
   Digital Content Technology [13/RC/2106_P2, 19/FFP/6409]
FX Acknowledgment This work was conducted with the financial support of the
   Sci-ence Foundation Ireland Centre for Research Training in
   Digitally-Enhanced Reality (d-real) under Grant No. 18/CRT/6224 and
   un-der the ADAPT Centre for Digital Content Technology (Grant No.
   13/RC/2106_P2) and RADICal (Grant No. 19/FFP/6409) .
CR [Anonymous], 2001, CRITERIA SCOPE CONDI
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   Baxter M., 2021, INVESTIGATING IMPACT, DOI [10.1145/3469595.3469609, DOI 10.1145/3469595.3469609]
   Biocca F., 2001, 4 ANN INT WORKSH PRE, P1
   Bouchard S, 2013, CYBERPSYCH BEH SOC N, V16, P61, DOI 10.1089/cyber.2012.1571
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Ciechanowski L, 2019, FUTURE GENER COMP SY, V92, P539, DOI 10.1016/j.future.2018.01.055
   DAVIS MH, 1983, J PERS SOC PSYCHOL, V44, P113, DOI 10.1037/0022-3514.44.1.113
   Devesse A, 2018, INT J AUDIOL, V57, P908, DOI 10.1080/14992027.2018.1511922
   Ferstl Y, 2021, PROCEEDINGS OF THE 21ST ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA), P76, DOI 10.1145/3472306.3478338
   Go E, 2019, COMPUT HUM BEHAV, V97, P304, DOI 10.1016/j.chb.2019.01.020
   Golan O, 2006, J AUTISM DEV DISORD, V36, P169, DOI 10.1007/s10803-005-0057-y
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   Hall E. T., 1966, HIDDEN DIMENSION, V609
   Hartmann T, 2010, MEDIA PSYCHOL, V13, P339, DOI 10.1080/15213269.2010.524912
   Higgins D, REMOTELY PERCEIVED I, DOI [10.3389/frvir.2021.668499, DOI 10.3389/FRVIR.2021.668499]
   Ho JCF, 2022, BEHAV INFORM TECHNOL, V41, P1185, DOI 10.1080/0144929X.2020.1864018
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Kawahara H, 2013, ASIAPAC SIGN INFO PR
   Kawahara H, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P38
   Lee KM., 2003, DESIGNING SOCIAL PRE
   MacDorman KF, 2016, COGNITION, V146, P190, DOI 10.1016/j.cognition.2015.09.019
   Macdorman KF, 2008, TOO REAL COMFORT UNC
   Maloney D., 2020, PROC ACM HUM COMPUT, V4, P175, DOI DOI 10.1145/3415246
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Meah LFS, 2014, LECT NOTES ARTIF INT, V8755, P256, DOI 10.1007/978-3-319-11973-1_26
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Moore RK., 2016, IS SPOKEN LANGUAGE A
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Oh CS, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00114
   Parmar D, 2020, NAVIGATING COMBINATO
   Potard B, 2016, LECT NOTES ARTIF INT, V10011, P190, DOI 10.1007/978-3-319-47665-0_17
   Sallnäs EL, 2010, LECT NOTES COMPUT SC, V6192, P178, DOI 10.1007/978-3-642-14075-4_26
   Sarigul B, 2020, ACMIEEE INT CONF HUM, P430, DOI 10.1145/3371382.3378302
   Saygin AP, 2012, SOC COGN AFFECT NEUR, V7, P413, DOI 10.1093/scan/nsr025
   Seyama J, 2007, PRESENCE-TELEOP VIRT, V16, P337, DOI 10.1162/pres.16.4.337
   Shen LJ, 2010, HUM COMMUN RES, V36, DOI 10.1111/j.1468-2958.2010.01381.x
   Skalski P, 2010, PSYCHNOLOGY J, V8, P67
   Skarbez R, 2017, IEEE T VIS COMPUT GR, V23, P1322, DOI 10.1109/TVCG.2017.2657158
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Thézé R, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-72375-y
   Torre I, HRI 2021 ROBO IDENTI
   Urgen BA, 2018, NEUROPSYCHOLOGIA, V114, P181, DOI 10.1016/j.neuropsychologia.2018.04.027
   van Loon A, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202442
   Volonte M, 2016, IEEE T VIS COMPUT GR, V22, P1326, DOI 10.1109/TVCG.2016.2518158
   Wang SS, 2015, REV GEN PSYCHOL, V19, P393, DOI 10.1037/gpr0000056
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zibrek K., 2019, Motion, interaction and games, P1, DOI DOI 10.1145/3359566.3360064
   Zibrek K, P MOTION INTERACTION
   Zibrek K, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3349609
   Zibrek K, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119887
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 54
TC 12
Z9 12
U1 18
U2 60
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2022
VL 104
BP 116
EP 128
DI 10.1016/j.cag.2022.03.009
EA APR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 1J6KP
UT WOS:000798026700004
OA Green Published, hybrid
DA 2024-01-09
ER

PT J
AU Lavado-Nalvaiz, N
   Lucia-Palacios, L
   Pérez-López, R
AF Lavado-Nalvaiz, Natalia
   Lucia-Palacios, Laura
   Perez-Lopez, Raul
TI The role of the humanisation of smart home speakers in the
   personalisation-privacy paradox
SO ELECTRONIC COMMERCE RESEARCH AND APPLICATIONS
LA English
DT Article
DE Smart home speakers; Privacy paradox; Personalisation; Humanisation;
   Uncanny valley
ID UNCANNY VALLEY; INFORMATION DISCLOSURE; CONSUMER ADOPTION; INTERNET
   USERS; E-COMMERCE; CALCULUS; SERVICE; WILLINGNESS; ACCEPTANCE; INTENTION
AB This article examines the personalisation-privacy paradox through the privacy calculus lens in the context of smart home speakers. It also considers the direct and moderating role of humanisation in the personalisation-privacy paradox. This characteristic refers to how human the device is perceived to be, given its voice's tone and pacing, original responses, sense of humour, and recommendations. The model was tested on a sample of 360 users of different brands of smart home speakers. These users were heterogeneous in terms of age, gender, income, and frequency of use of the device. The results confirm the personalisation-privacy paradox and verify uncanny valley theory, finding the U-shaped effect that humanisation has on risks of information disclosure. They also show that humanisation increases benefits, which supports the realism maximisation theory. Specifically, they reveal that users will perceive the messages received as more useful and credible if the devices seem human. However, the human-likeness of these devices should not exceed certain levels as it increases perceived risk. These results should be used to highlight the importance of the human-like communication of smart home speakers.
C1 [Lavado-Nalvaiz, Natalia; Lucia-Palacios, Laura; Perez-Lopez, Raul] Univ Zaragoza, Dept Mkt, Fac Econ & Business Adm, Gran Via 2, Zaragoza 50005, Spain.
C3 University of Zaragoza
RP Lavado-Nalvaiz, N (corresponding author), Univ Zaragoza, Dept Mkt, Fac Econ & Business Adm, Gran Via 2, Zaragoza 50005, Spain.
EM 651789@unizar.es; llucia@unizar.es; raperez@unizar.es
RI Palacios, Laura Lucia/K-8921-2017
OI Palacios, Laura Lucia/0000-0002-8798-3294; Perez Lopez,
   Raul/0000-0001-6441-2504
FU MCIN/AEI [PID2020-114874GB-I00]; Government of Aragon; European Social
   Fund (GENERES Group) [S-54_20R]
FX The authors wish to express their gratitude for financial support
   received from the Grant PID2020-114874GB-I00 funded by
   MCIN/AEI/10.13039/501100011033; the funding received from the Government
   of Aragon and the European Social Fund (GENERES Group S-54_20R) .
CR Acquisti A., 2004, P 5 ACM C EL COMM, P21, DOI DOI 10.1145/988772.988777
   Adapa S, 2020, J RETAIL CONSUM SERV, V52, DOI 10.1016/j.jretconser.2019.101901
   Aguirre E, 2016, J CONSUM MARK, V33, P98, DOI 10.1108/JCM-06-2015-1458
   Anic ID, 2019, ELECTRON COMMER R A, V36, DOI 10.1016/j.elerap.2019.100868
   [Anonymous], 1979, RELIABILITY VALIDITY, DOI DOI 10.4135/9781412985642
   Bagozzi R.P., 1988, Principles of Marketing Research, V16, P74, DOI [DOI 10.1007/BF02723327, 10.1007/bf02723327]
   Bandara R, 2020, J RETAIL CONSUM SERV, V52, DOI 10.1016/j.jretconser.2019.101947
   Bavaresco R, 2020, COMPUT SCI REV, V36, DOI 10.1016/j.cosrev.2020.100239
   Benlian A, 2020, INFORM SYST J, V30, P1010, DOI 10.1111/isj.12243
   Bhatia J, 2018, ACM T COMPUT-HUM INT, V25, DOI 10.1145/3267808
   Bhattacherjee A, 2001, DECIS SUPPORT SYST, V32, P201, DOI 10.1016/S0167-9236(01)00111-7
   Burleigh TJ, 2013, COMPUT HUM BEHAV, V29, P759, DOI 10.1016/j.chb.2012.11.021
   Cazier J.A., 2009, TECHNIQUES APPL ADV
   Cazier JA, 2008, COMMUN ASSOC INF SYS, V23, P235
   Cheetham M, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01738
   Chen SC, 2015, TECHNOL FORECAST SOC, V96, P40, DOI 10.1016/j.techfore.2014.11.011
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Culnan MJ, 1999, ORGAN SCI, V10, P104, DOI 10.1287/orsc.10.1.104
   Davenport T, 2020, J ACAD MARKET SCI, V48, P24, DOI 10.1007/s11747-019-00696-0
   Diederich S., 2020, P 28 EUR C INF SYST, P1
   Dinev T, 2006, INFORM SYST RES, V17, P61, DOI 10.1287/isre.1060.0080
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   Feldman R., 2008, P SERV SYST SERV MAN, P1
   Foehr J, 2020, J ASSOC CONSUM RES, V5, P181, DOI 10.1086/707731
   Forde M, 2018, Relig Cult Afr Afr D, P1, DOI 10.1007/s00779-018-1174-x
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Frick NRJ, 2021, ELECTRON COMMER R A, V47, DOI 10.1016/j.elerap.2021.101046
   Gartner, 2018, WHATS AHEAD SMART SP
   GEISSER S, 1974, BIOMETRIKA, V61, P101, DOI 10.1093/biomet/61.1.101
   Gironda JT, 2018, ELECTRON COMMER R A, V29, P64, DOI 10.1016/j.elerap.2018.03.007
   Go E, 2019, COMPUT HUM BEHAV, V97, P304, DOI 10.1016/j.chb.2019.01.020
   Groom V, 2009, INT J HUM-COMPUT ST, V67, P842, DOI 10.1016/j.ijhcs.2009.07.001
   Guzman A. L., 2018, NETWORKED SELF HUMAN, P83
   Hagen P, 1999, Smart Personalization-The Forrester Report
   Han S, 2018, IND MANAGE DATA SYST, V118, P618, DOI 10.1108/IMDS-05-2017-0214
   Hayes JL, 2021, J INTERACT MARK, V55, P16, DOI 10.1016/j.intmar.2021.01.001
   Henseler J, 2015, J ACAD MARKET SCI, V43, P115, DOI 10.1007/s11747-014-0403-8
   Holtrop N, 2017, INT J RES MARK, V34, P154, DOI 10.1016/j.ijresmar.2016.06.001
   Hong JC, 2017, COMPUT HUM BEHAV, V67, P264, DOI 10.1016/j.chb.2016.11.001
   Hsu CL, 2016, COMPUT HUM BEHAV, V62, P516, DOI 10.1016/j.chb.2016.04.023
   Hsu MH, 2014, INTERNET RES, V24, P332, DOI 10.1108/IntR-01-2013-0007
   Jai TM, 2016, J RETAIL CONSUM SERV, V28, P296, DOI 10.1016/j.jretconser.2015.01.005
   Kaplan A, 2020, BUS HORIZONS, V63, P37, DOI 10.1016/j.bushor.2019.09.003
   Keh HT, 2010, J MARKETING, V74, P55, DOI 10.1509/jmkg.74.2.55
   Kim D, 2019, COMPUT HUM BEHAV, V92, P273, DOI 10.1016/j.chb.2018.11.022
   Kim MS, 2018, COMPUT HUM BEHAV, V88, P143, DOI 10.1016/j.chb.2018.06.031
   Kim SY, 2017, SUSTAINABILITY-BASEL, V9, DOI 10.3390/su9122262
   Kim YJ, 2014, COMPUT HUM BEHAV, V33, P256, DOI 10.1016/j.chb.2014.01.015
   Klaus P, 2022, J RETAIL CONSUM SERV, V65, DOI 10.1016/j.jretconser.2021.102490
   Kowalczuk P, 2018, J RES INTERACT MARK, V12, P418, DOI 10.1108/JRIM-01-2018-0022
   Krafft M, 2017, J INTERACT MARK, V39, P39, DOI 10.1016/j.intmar.2017.03.001
   Lee AR, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su131910679
   Lee CH, 2011, TOURISM MANAGE, V32, P987, DOI 10.1016/j.tourman.2010.08.011
   Lee JM, 2016, COMPUT HUM BEHAV, V63, P453, DOI 10.1016/j.chb.2016.05.056
   Lee S, 2021, J BUS RES, V129, P455, DOI 10.1016/j.jbusres.2019.09.053
   Liyanaarachchi G, 2021, J RETAIL CONSUM SERV, V60, DOI 10.1016/j.jretconser.2021.102500
   López G, 2018, ADV INTELL SYST, V592, P241, DOI 10.1007/978-3-319-60366-7_23
   Lu L, 2019, INT J HOSP MANAG, V80, P36, DOI 10.1016/j.ijhm.2019.01.005
   Luo XM, 2019, MARKET SCI, V38, P937, DOI 10.1287/mksc.2019.1192
   MacDorman KF, 2019, COMPUT HUM BEHAV, V94, P140, DOI 10.1016/j.chb.2019.01.011
   Malhotra NK, 2004, INFORM SYST RES, V15, P336, DOI 10.1287/isre.1040.0032
   Manheim K., 2019, YALE J LAW TECHNOLOG, V21, P106
   Mani Z, 2019, J MARKET MANAG-UK, V35, P1460, DOI 10.1080/0267257X.2019.1667856
   Martin BAS, 2020, J HOSP TOUR MANAG, V44, P108, DOI 10.1016/j.jhtm.2020.06.004
   Maya BM, 2020, COMPUT HUM BEHAV, V103, P21, DOI 10.1016/j.chb.2019.08.029
   McLean G, 2019, COMPUT HUM BEHAV, V99, P28, DOI 10.1016/j.chb.2019.05.009
   Min S, 2019, J TRAVEL TOUR MARK, V36, P770, DOI 10.1080/10548408.2018.1507866
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Morosan C, 2015, INT J HOSP MANAG, V47, P120, DOI 10.1016/j.ijhm.2015.03.008
   Norberg PA, 2007, J CONSUM AFF, V41, P100, DOI 10.1111/j.1745-6606.2006.00070.x
   Nunnally J. C., 1978, Psychometric Methods, V2nd
   Pantano E, 2020, J RETAIL CONSUM SERV, V55, DOI 10.1016/j.jretconser.2020.102096
   Poushneh A, 2021, J RETAIL CONSUM SERV, V58, DOI 10.1016/j.jretconser.2020.102283
   Puzakova M, 2013, J MARKETING, V77, P81, DOI 10.1509/jm.11.0510
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Rosenthal-von der Pütten AM, 2014, COMPUT HUM BEHAV, V36, P422, DOI 10.1016/j.chb.2014.03.066
   Sharma S, 2014, ELECTRON COMMER R A, V13, P305, DOI 10.1016/j.elerap.2014.06.007
   Sheehan B, 2020, J BUS RES, V115, P14, DOI 10.1016/j.jbusres.2020.04.030
   Sheng H, 2008, J ASSOC INF SYST, V9, P344, DOI 10.17705/1jais.00161
   Stein JP, 2017, COGNITION, V160, P43, DOI 10.1016/j.cognition.2016.12.010
   STONE M, 1974, J R STAT SOC B, V36, P111, DOI 10.1111/j.2517-6161.1974.tb00994.x
   Strait MK, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01366
   Sun YQ, 2015, COMPUT HUM BEHAV, V52, P278, DOI 10.1016/j.chb.2015.06.006
   Sweeney JC, 1999, J RETAILING, V75, P77, DOI 10.1016/S0022-4359(99)80005-0
   Teng WC, 2010, INT J MOB COMMUN, V8, P1, DOI 10.1504/IJMC.2010.030517
   Turel O, 2010, INFORM MANAGE-AMSTER, V47, P53, DOI 10.1016/j.im.2009.10.002
   Wang T, 2016, INT J INFORM MANAGE, V36, P531, DOI 10.1016/j.ijinfomgt.2016.03.003
   Waytz A, 2010, J PERS SOC PSYCHOL, V99, P410, DOI 10.1037/a0020240
   Xie Y, 2020, J RETAIL CONSUM SERV, V55, DOI 10.1016/j.jretconser.2020.102119
   Xu H, 2011, J ASSOC INF SYST, V12, P798
   Yang H, 2016, TELEMAT INFORM, V33, P256, DOI 10.1016/j.tele.2015.08.007
   Yang SQ, 2012, COMPUT HUM BEHAV, V28, P129, DOI 10.1016/j.chb.2011.08.019
   Yee N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1
   Yu J, 2017, TELEMAT INFORM, V34, P206, DOI 10.1016/j.tele.2015.11.004
   Zeng FE, 2021, J BUS RES, V124, P667, DOI 10.1016/j.jbusres.2020.02.006
   Zhu H, 2017, INFORM MANAGE-AMSTER, V54, P427, DOI 10.1016/j.im.2016.10.001
   Zlotowski JA, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00883
NR 97
TC 8
Z9 8
U1 30
U2 75
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1567-4223
EI 1873-7846
J9 ELECTRON COMMER R A
JI Electron. Commer. Res. Appl.
PD MAY-JUN
PY 2022
VL 53
AR 101146
DI 10.1016/j.elerap.2022.101146
EA APR 2022
PG 11
WC Business; Computer Science, Information Systems; Computer Science,
   Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Business & Economics; Computer Science
GA 2R9TQ
UT WOS:000821446800007
OA Green Published, hybrid
DA 2024-01-09
ER

PT J
AU Parmar, D
   Olafsson, S
   Utami, D
   Murali, P
   Bickmore, T
AF Parmar, Dhaval
   Olafsson, Stefan
   Utami, Dina
   Murali, Prasanth
   Bickmore, Timothy
TI Designing empathic virtual agents: manipulating animation, voice,
   rendering, and empathy to create persuasive agents
SO AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS
LA English
DT Article
DE Virtual agents; Animation fidelity; Voice quality; Rendering style;
   Simulated empathy; Agent perception
ID HUMAN REALISM; FEEDBACK; PERSONALITY; CONSISTENCY; CHARACTERS
AB Designers of virtual agents have a combinatorically large space of choices for the look and behavior of their characters. We conducted two between-subjects studies to explore the systematic manipulation of animation quality, speech quality, rendering style, and simulated empathy, and its impact on perceptions of virtual agents in terms of naturalness, engagement, trust, credibility, and persuasion within a health counseling domain. In the first study, animation was varied between manually created, procedural, or no animations; voice quality was varied between recorded audio and synthetic speech; and rendering style was varied between realistic and toon-shaded. In the second study, simulated empathy of the agent was varied between no empathy, verbal-only empathic responses, and full empathy involving verbal, facial, and immediacy feedback. Results show that natural animations and recorded voice are more appropriate for the agent's general acceptance, trust, credibility, and appropriateness for the task. However, for a brief health counseling task, animation might actually be distracting from the persuasive message, with the highest levels of persuasion found when the amount of agent animation is minimized. Further, consistent and high levels of empathy improve agent perception but may interfere with forming a trusting bond with the agent.
C1 [Parmar, Dhaval; Olafsson, Stefan; Utami, Dina; Murali, Prasanth; Bickmore, Timothy] Northeastern Univ, Boston, MA 02115 USA.
   [Olafsson, Stefan] Reykjavik Univ, Reykjavik, Iceland.
C3 Northeastern University; Reykjavik University
RP Parmar, D (corresponding author), Northeastern Univ, Boston, MA 02115 USA.
EM d.parmar@northeastern.edu; olafsson.s@northeastern.edu;
   utami.d@northeastern.edu; murali.pr@northeastern.edu;
   t.bickmore@northeastern.edu
OI Olafsson, Stefan/0000-0002-3549-8705
FU US National Institutes of Health [R01NR016131]
FX This work was supported by the US National Institutes of Health Grant
   R01NR016131.
CR Adobe, 2020, AD CREAT MARK DOC MA
   Amazon, 2020, AM MECH TURK
   Bickmore T, 2007, P 2007 HUM FACT COMP, P2291, DOI [10.1145/1240866.1240996, DOI 10.1145/1240866.1240996]
   Bickmore T. W., 2005, ACM Transactions on Computer-Human Interaction, V12, P293, DOI 10.1145/1067860.1067867
   Bickmore TW, 2013, J AM GERIATR SOC, V61, P1676, DOI 10.1111/jgs.12449
   Bickmore TW, 2010, IEEE T AFFECT COMPUT, V1, P60, DOI 10.1109/T-AFFC.2010.4
   Bigi B, 2012, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON SPEECH PROSODY, VOLS I AND II, P19
   Cassell J, 2004, COG TECH, P163
   Cassell J, 1999, APPL ARTIF INTELL, V13, P519, DOI 10.1080/088395199117360
   Cassell Justine, 2000, Embodied conversational agents
   Cereproc, 2020, CER TEXT TO SPEECH
   Dai ZY, 2018, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.168
   Fogg B.J., 2001, CHI 20001 EXTENDED A, P295, DOI 10.1145/634067.634242
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   Kätsyri J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00390
   Kim J, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3383075
   Kimani E, 2016, LECT NOTES ARTIF INT, V10011, P120, DOI 10.1007/978-3-319-47665-0_11
   Klein J, 2002, INTERACT COMPUT, V14, P119, DOI 10.1016/S0953-5438(01)00053-4
   Lane HC, 2013, J EDUC PSYCHOL, V105, P1026, DOI 10.1037/a0031506
   Lee A., 2009, Proceedings of the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), P131
   Li Gong, 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P158, DOI 10.1145/365024.365090
   MacDorman KF, 2016, COGNITION, V146, P190, DOI 10.1016/j.cognition.2015.09.019
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Nass C., 1999, AUDITORY VISUAL SPEE, P1
   Nguyen H., 2009, P INT C ULTR TEL WOR, P7
   Parmar D., 2020, P 19 INT C AUT AG MU, P1010, DOI [10.5555/3398761.3398879, DOI 10.5555/3398761.3398879]
   Petty R.E., 2010, ADV SOCIAL PSYCHOL S, P217, DOI DOI 10.1016/B978-0-12-375000-6.00040-9
   Richmond VP., 1995, IMMEDIACY
   Ring L, 2014, LECT NOTES ARTIF INT, V8637, P374, DOI 10.1007/978-3-319-09767-1_49
   Shams L, 2010, PHYS LIFE REV, V7, P269, DOI 10.1016/j.plrev.2010.04.006
   Slote M., 2003, TIME ETHICS ESSAYS I, P179, DOI [10.1007/978-94-017-3530-8_12, DOI 10.1007/978-94-017-3530-8_12]
   Stern SE, 1999, HUM FACTORS, V41, P588, DOI 10.1518/001872099779656680
   Taipale J, 2015, CONT PHILOS REV, V48, P161, DOI 10.1007/s11007-015-9327-3
   Tinwell A, 2010, J GAMING VIRTUAL WOR, V2, P3, DOI 10.1386/jgvw.2.1.3_1
   Vinayagamoorthy Vinoba, 2005, P SOC MECH ANDR SCI, P119
   Volonte M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P25, DOI 10.1109/VR.2018.8446364
   Volonte M, 2016, IEEE T VIS COMPUT GR, V22, P1326, DOI 10.1109/TVCG.2016.2518158
   Welch RB, 1996, PRESENCE-TELEOP VIRT, V5, P263, DOI 10.1162/pres.1996.5.3.263
   Wheeless Lawrence R, 1977, Human Communication Research, V3, P250, DOI [DOI 10.1111/J.1468-2958.1977.TB00523.X, 10.1111/j.1468-2958.1977.tb00523.x]
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Wu YX, 2014, IEEE T VIS COMPUT GR, V20, P626, DOI 10.1109/TVCG.2014.19
   Yee N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1
   Zanbaka C., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1153
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zibrek K., 2014, S APPL PERC 2014, P111, DOI DOI 10.1145/2628257.2628270
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 48
TC 3
Z9 4
U1 7
U2 30
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1387-2532
EI 1573-7454
J9 AUTON AGENT MULTI-AG
JI Auton. Agents Multi-Agent Syst.
PD APR
PY 2022
VL 36
IS 1
AR 17
DI 10.1007/s10458-021-09539-1
PG 24
WC Automation & Control Systems; Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Automation & Control Systems; Computer Science
GA ZF4OX
UT WOS:000759550300001
PM 35387204
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Mawalim, CO
   Galajit, K
   Karnjana, J
   Kidani, S
   Unoki, M
AF Mawalim, Candy Olivia
   Galajit, Kasorn
   Karnjana, Jessada
   Kidani, Shunsuke
   Unoki, Masashi
TI Speaker anonymization by modifying fundamental frequency and x-vector
   singular value
SO COMPUTER SPEECH AND LANGUAGE
LA English
DT Article
DE Speaker anonymization; X-vector singular value; Fundamental frequency;
   Clustering; Subjective evaluation
ID SPEECH; TRANSFORMATION; VERIFICATION
AB Speaker anonymization is a method of protecting voice privacy by concealing individual speaker characteristics while preserving linguistic information. The VoicePrivacy Challenge 2020 was initiated to generalize the task of speaker anonymization. In the challenge, two frameworks for speaker anonymization were introduced; in this study, we propose a method of improving the primary framework by modifying the state-of-the-art speaker individuality feature (namely, x-vector) in a neural waveform speech synthesis model. Our proposed method is constructed based on x-vector singular value modification with a clustering model. We also propose a technique of modifying the fundamental frequency and speech duration to enhance the anonymization performance. To evaluate our method, we carried out objective and subjective tests. The overall objective test results show that our proposed method improves the anonymization performance in terms of the speaker verifiability, whereas the subjective evaluation results show improvement in terms of the speaker dissimilarity. The intelligibility and naturalness of the anonymized speech with speech prosody modification were slightly reduced (less than 5% of word error rate) compared to the results obtained by the baseline system.
C1 [Mawalim, Candy Olivia; Galajit, Kasorn; Kidani, Shunsuke; Unoki, Masashi] Japan Adv Inst Sci & Technol, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.
   [Galajit, Kasorn; Karnjana, Jessada] Natl Sci & Technol Dev Agcy, NECTEC, Pathum Thani, Thailand.
C3 Japan Advanced Institute of Science & Technology (JAIST); National
   Science & Technology Development Agency - Thailand; National Electronics
   & Computer Technology Center (NECTEC)
RP Mawalim, CO (corresponding author), Japan Adv Inst Sci & Technol, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.
EM candyolivia@jaist.ac.jp; kasorn@jaist.ac.jp;
   jessada.karnjana@nectec.or.th; kidani@jaist.ac.jp; unoki@jaist.ac.jp
RI Kidani, Shunsuke/AAS-2593-2021
OI Kidani, Shunsuke/0000-0001-6491-9540; GALAJIT,
   Kasorn/0000-0003-3723-5658; Mawalim, Candy Olivia/0000-0001-9853-8893
FU JSPS, Japan KAKENHI Grant [20J20580]; Fund for the Promotion of Joint
   International Research (Fostering Joint International Research (B)),
   Japan [20KK0233]; JSPS-NSFC Bilateral Joint Research Projects/Seminars,
   Japan [JSJSBP120197416]; KDDI foundation, Japan;  [17H01761];
   Grants-in-Aid for Scientific Research [20KK0233] Funding Source: KAKEN
FX This work was supported by a Grant-in-Aid for Scientific Research (B),
   Japan (No. 17H01761) and JSPS, Japan KAKENHI Grant (No. 20J20580). This
   work was also supported by Fund for the Promotion of Joint International
   Research (Fostering Joint International Research (B)), Japan (20KK0233),
   JSPS-NSFC Bilateral Joint Research Projects/Seminars, Japan
   (JSJSBP120197416), and KDDI foundation, Japan (Research Grant Program).
CR Abou-Zleikha M, 2015, EUR SIGNAL PR CONF, P2102, DOI 10.1109/EUSIPCO.2015.7362755
   Akagi H., 1997, Journal of the Acoustical Society of Japan (E), V18, P73, DOI 10.1250/ast.18.73
   Bottou L., 1994, ADV NEURAL INFORM PR, P585
   Brümmer N, 2006, COMPUT SPEECH LANG, V20, P230, DOI 10.1016/j.csl.2005.08.001
   Buttle F., 1996, European Journal of Marketing, V30, DOI DOI 10.1108/03090569610105762
   Champion P., PPAI 2021 2 AAAI WOR
   Chung JS, 2018, INTERSPEECH, P1086
   COX EP, 1980, J MARKETING RES, V17, P407, DOI 10.1177/002224378001700401
   Das RK, 2020, INTERSPEECH, P4213, DOI 10.21437/Interspeech.2020-1052
   Das RK, 2018, IETE TECH REV, V35, P599, DOI 10.1080/02564602.2017.1357507
   Das RK, 2018, INT J SPEECH TECHNOL, V21, P401, DOI 10.1007/s10772-017-9474-5
   Dellwo Volker, 2007, Speaker Classification I. Fundamentals, Features, and Methods. (Lecture Notes in Artificial Intelligence vol. 4343), P1, DOI 10.1007/978-3-540-74200-5_1
   Dubagunta S.P., 2020, ADJUSTABLE DETERMINI
   Eriksson A., 1995, The frequency range of the voice fundamental in the speech of male and female adults
   Espinoza-Cuadros F.M., 2020, ARXIV201104696 CORR
   Fabian Pedregosa, 2011, Journal of Machine Learning Research, V12, P2825
   Fang F., 2019, P 10 ISCA WORKSHOP S, P155, DOI 10.21437/SSW.2019-28
   Fang FM, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5279, DOI 10.1109/ICASSP.2018.8462342
   GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gussenhoven C, 1997, J ACOUST SOC AM, V102, P3009, DOI 10.1121/1.420355
   Han YW, 2020, CCS '20: PROCEEDINGS OF THE 2020 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P2125, DOI 10.1145/3372297.3420025
   Hautamäki RG, 2020, INTERSPEECH, P4313, DOI 10.21437/Interspeech.2020-2715
   Hirose K, 2002, SPEECH COMMUN, V36, P97, DOI 10.1016/S0167-6393(01)00028-0
   Irum Amna, 2019, International Journal of Machine Learning and Computing, V9, P20, DOI 10.18178/ijmlc.2019.9.1.760
   Jin Q, 2008, INT CONF ACOUST SPEE, P4845
   Jin Q, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P529, DOI 10.1109/ASRU.2009.5373356
   Magariños C, 2017, COMPUT SPEECH LANG, V46, P36, DOI 10.1016/j.csl.2017.05.001
   Mawalim CO, 2020, INTERSPEECH, P1703, DOI 10.21437/Interspeech.2020-1887
   McAdams S., 1984, THESIS STANFORD
   Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Patino J., 2020, ARXIV PREPRINT ARXIV
   Peddinti V, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3214
   Pobar M, 2014, 2014 37TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P1264, DOI 10.1109/MIPRO.2014.6859761
   Povey Daniel, 2011, IEEE WORKSH AUT SPEE
   Sahidullah M, 2016, DIGIT SIGNAL PROCESS, V50, P1, DOI 10.1016/j.dsp.2015.10.011
   Sathiyamurthi P, 2017, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-017-0118-0
   Snyder D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5329
   Tomashenko N, 2020, INTERSPEECH, P1693, DOI 10.21437/Interspeech.2020-1333
   Tomashenko N., 2021, VOICEPRIVACY 2020 CH
   Turner H., 2020, ARXIV PREPRINT ARXIV
   Veaux C., 2017, University of Edinburgh. The Centre for Speech Technology Research (CSTR)
   Vestman V, 2020, COMPUT SPEECH LANG, V59, P36, DOI 10.1016/j.csl.2019.05.005
   Wang X, 2019, INT CONF ACOUST SPEE, P5916, DOI 10.1109/ICASSP.2019.8682298
   Zen HG, 2019, INTERSPEECH, P1526, DOI 10.21437/Interspeech.2019-2441
NR 47
TC 4
Z9 4
U1 0
U2 0
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0885-2308
EI 1095-8363
J9 COMPUT SPEECH LANG
JI Comput. Speech Lang.
PD MAY
PY 2022
VL 73
AR 101326
DI 10.1016/j.csl.2021.101326
EA DEC 2021
PG 17
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YI8MP
UT WOS:000744097300003
DA 2024-01-09
ER

PT J
AU Abur, D
   Subaciute, A
   Daliri, A
   Lester-Smith, RA
   Lupiani, AA
   Cilento, D
   Enos, NM
   Weerathunge, HR
   Tardif, MC
   Stepp, CE
AF Abur, Defne
   Subaciute, Austeja
   Daliri, Ayoub
   Lester-Smith, Rosemary A.
   Lupiani, Ashling A.
   Cilento, Dante
   Enos, Nicole M.
   Weerathunge, Hasini R.
   Tardif, Monique C.
   Stepp, Cara E.
TI Feedback and Feedforward Auditory-Motor Processes for Voice and
   Articulation in Parkinson's Disease
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
ID VOCAL PITCH; SENSORIMOTOR ADAPTATION; INTELLIGIBILITY SCORES;
   FUNDAMENTAL-FREQUENCY; SENSORY MEMORY; SPEECH RATE; RESPONSES; LEVODOPA;
   SPEAKERS; PERTURBATIONS
AB Purpose: Unexpected and sustained manipulations of auditory feedback during speech production result in "reflexive" and "adaptive" responses, which can shed light on feedback and feedforward auditory-motor control processes, respectively. Persons with Parkinson's disease (PwPD) have shown aberrant reflexive and adaptive responses, but responses appear to differ for control of vocal and articulatory features. However, these responses have not been examined for both voice and articulation in the same speakers and with respect to auditory acuity and functional speech outcomes (speech intelligibility and naturalness).
   Method: Here, 28 PwPD on their typical dopaminergic medication schedule and 28 age-, sex-, and hearing-matched controls completed tasks yielding reflexive and adaptive responses as well as auditory acuity for both vocal and articulatory features.
   Results: No group differences were found for any measures of auditory-motor control, conflicting with prior findings in PwPD while off medication. Auditory-motor measures were also compared with listener ratings of speech function: first formant frequency acuity was related to speech intelligibility, whereas adaptive responses to vocal fundamental frequency manipulations were related to speech naturalness.
   Conclusions: These results support that auditory-motor processes for both voice and articulatory features are intact for PwPD receiving medication. This work is also the first to suggest associations between measures of auditory-motor control and speech intelligibility and naturalness.
C1 [Abur, Defne; Daliri, Ayoub; Lester-Smith, Rosemary A.; Lupiani, Ashling A.; Cilento, Dante; Tardif, Monique C.; Stepp, Cara E.] Boston Univ, Dept Speech Language & Hearing Sci, Boston, MA 02215 USA.
   [Subaciute, Austeja; Enos, Nicole M.; Weerathunge, Hasini R.; Stepp, Cara E.] Boston Univ, Dept Biomed Engn, Boston, MA 02215 USA.
   [Daliri, Ayoub] Arizona State Univ, Coll Hlth Solut, Tempe, AZ USA.
   [Lester-Smith, Rosemary A.] Univ Texas Austin, Moody Coll Commun, Dept Speech Language & Hearing Sci, Austin, TX USA.
   [Lupiani, Ashling A.] Univ N Carolina, Joint Dept Biomed Engn, Chapel Hill, NC 27515 USA.
   [Lupiani, Ashling A.] North Carolina State Univ, Raleigh, NC USA.
   [Enos, Nicole M.] Boston Univ, Dept Elect & Comp Engn, Boston, MA USA.
   [Tardif, Monique C.] Univ Pittsburgh, Dept Commun Sci & Disorders, Pittsburgh, PA USA.
   [Stepp, Cara E.] Boston Univ, Dept Otolaryngol Head & Neck Surg, Sch Med, Boston, MA USA.
C3 Boston University; Boston University; Arizona State University; Arizona
   State University-Tempe; University of Texas System; University of Texas
   Austin; University of North Carolina; University of North Carolina
   Chapel Hill; North Carolina State University; Boston University;
   Pennsylvania Commonwealth System of Higher Education (PCSHE); University
   of Pittsburgh; Boston University
RP Abur, D (corresponding author), Boston Univ, Dept Speech Language & Hearing Sci, Boston, MA 02215 USA.
EM dabur@bu.edu
RI Weerathunge, Hasini/GQZ-2356-2022
OI Weerathunge, Hasini/0000-0002-0240-9104; Lester-Smith,
   Rosemary/0000-0002-9111-7399; Tardif, Monique/0000-0002-6029-6507;
   Stepp, Cara/0000-0002-8045-252X
FU National Institute of Deafness and Other Communication Disorders [R01
   DC016270, T32 DC013017, F31 DC019032]; Rafik B. Hariri Institute for
   Computing and Computational Science and Engineering; ASH Foundation New
   Century Doctoral Scholarship; Dudley Allen Sargent Research Fund Award
FX The authors would like to thank Talia Mittelman, Paige Clabby, Katherine
   Brown, and Halle Duggan for help with participant recruitment and data
   collection. This work was supported by grants R01 DC016270 (Cara E.
   Stepp and Frank H. Guenther), T32 DC013017 (Cara E. Stepp and
   Christopher A. Moore), and F31 DC019032 (Defne Abur) from the National
   Institute of Deafness and Other Communication Disorders. It was also
   supported by a Graduate Fellow Award from the Rafik B. Hariri Institute
   for Computing and Computational Science and Engineering (Defne Abur), an
   ASH Foundation New Century Doctoral Scholarship (Defne Abur), and a
   Dudley Allen Sargent Research Fund Award (Defne Abur).
CR Abur D, 2020, J SPEECH LANG HEAR R, V63, P3208, DOI 10.1044/2020_JSLHR-20-00003
   Abur D, 2019, AM J SPEECH-LANG PAT, V28, P1222, DOI 10.1044/2019_AJSLP-18-0275
   Abur D, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191839
   American Speech-Language-Hearing Association, 2005, Guidelines for Manual Pure-Tone Threshold Audiometry
   Anand S, 2015, J SPEECH LANG HEAR R, V58, P1134, DOI 10.1044/2015_JSLHR-S-14-0243
   [Anonymous], 2007, SPEECH INTELLIGIBILI
   Behroozmand R, 2009, CLIN NEUROPHYSIOL, V120, P1303, DOI 10.1016/j.clinph.2009.04.022
   Beverly D, 2010, J MED SPEECH-LANG PA, V18, P9
   Broadfoot C K, 2019, Perspect ASHA Spec Interest Groups, V4, P825, DOI 10.1044/2019_pers-sig3-2019-0001
   Bunton K, 2006, FOLIA PHONIATR LOGO, V58, P323, DOI 10.1159/000094567
   Burnett TA, 1997, J VOICE, V11, P202, DOI 10.1016/S0892-1997(97)80079-3
   Burnett TA, 1998, J ACOUST SOC AM, V103, P3153, DOI 10.1121/1.423073
   Cai S., 2008, 8 INT SEM SPEECH PRO
   Cai SQ, 2011, J NEUROSCI, V31, P16483, DOI 10.1523/JNEUROSCI.3653-11.2011
   Cannito MP, 2012, J VOICE, V26, P214, DOI 10.1016/j.jvoice.2011.08.014
   Chang EF, 2013, P NATL ACAD SCI USA, V110, P2653, DOI 10.1073/pnas.1216827110
   Chen X, 2013, BRAIN RES, V1527, P99, DOI 10.1016/j.brainres.2013.06.030
   De Letter M, 2006, ACTA NEUROL BELG, V106, P19
   Eadie TL, 2016, HEAD NECK-J SCI SPEC, V38, pE1955, DOI 10.1002/hed.24353
   García-Pérez MA, 1998, VISION RES, V38, P1861, DOI 10.1016/S0042-6989(97)00340-4
   Goberman A, 2002, J COMMUN DISORD, V35, P217, DOI 10.1016/S0021-9924(01)00072-7
   Goberman AM, 2003, J FLUENCY DISORD, V28, P55, DOI 10.1016/S0094-730X(03)00005-6
   Hain TC, 2000, EXP BRAIN RES, V130, P133, DOI 10.1007/s002219900237
   Ho AK, 2008, MOVEMENT DISORD, V23, P574, DOI 10.1002/mds.21899
   Hustad KC, 2007, FOLIA PHONIATR LOGO, V59, P306, DOI 10.1159/000108337
   Jones JA, 2008, EXP BRAIN RES, V190, P279, DOI 10.1007/s00221-008-1473-y
   Karlsen KH, 2000, J NEUROL NEUROSUR PS, V69, P584, DOI 10.1136/jnnp.69.5.584
   Kearney E, 2020, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02995
   Kempster GB, 2009, AM J SPEECH-LANG PAT, V18, P124, DOI 10.1044/1058-0360(2008/08-0017)
   KENT RD, 1989, J SPEECH HEAR DISORD, V54, P482, DOI 10.1044/jshd.5404.482
   Kiran S, 2001, J SPEECH LANG HEAR R, V44, P975, DOI 10.1044/1092-4388(2001/076)
   Lametti DR, 2012, J NEUROSCI, V32, P9351, DOI 10.1523/JNEUROSCI.0404-12.2012
   Lester-Smith RA, 2020, J SPEECH LANG HEAR R, V63, P3628, DOI 10.1044/2020_JSLHR-20-00192
   Liu HJ, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0033629
   Liu HJ, 2010, J ACOUST SOC AM, V127, P1042, DOI 10.1121/1.3273880
   Micheyl C, 2006, HEARING RES, V219, P36, DOI 10.1016/j.heares.2006.05.004
   Mollaei F, 2019, J SPEECH LANG HEAR R, V62, P4256, DOI 10.1044/2019_JSLHR-S-18-0425
   Mollaei F, 2016, BRAIN RES, V1646, P269, DOI 10.1016/j.brainres.2016.06.013
   Mollaei F, 2013, MOVEMENT DISORD, V28, P1668, DOI 10.1002/mds.25588
   Moore BCJ, 2006, HEARING RES, V222, P16, DOI 10.1016/j.heares.2006.08.007
   Murray ESH, 2019, J SPEECH LANG HEAR R, V62, P2270, DOI 10.1044/2019_JSLHR-S-18-0408
   Nasreddine ZS, 2005, J AM GERIATR SOC, V53, P695, DOI 10.1111/j.1532-5415.2005.53221.x
   Olsen Wayne O, 1998, Am J Audiol, V7, P21, DOI 10.1044/1059-0889(1998/012)
   PEKKONEN E, 1994, NEUROREPORT, V5, P2537, DOI 10.1097/00001756-199412000-00033
   Pekkonen E, 2001, NEUROIMAGE, V14, P376, DOI 10.1006/nimg.2001.0805
   Pichora-Fuller M. K., 2003, INT J AUDIOL, V42, p2S11
   Plowman-Prine EK, 2009, NEUROREHABILITATION, V24, P131, DOI 10.3233/NRE-2009-0462
   SCHOW RL, 1991, EAR HEARING, V12, P337, DOI 10.1097/00003446-199110000-00006
   SHEARD C, 1991, J SPEECH HEAR RES, V34, P285, DOI 10.1044/jshr.3402.285
   Skodda S, 2013, PARKINSONS DIS-US, V2013, DOI 10.1155/2013/389195
   Skodda S, 2011, J VOICE, V25, pE199, DOI 10.1016/j.jvoice.2010.04.007
   Skodda S, 2010, J NEURAL TRANSM, V117, P197, DOI 10.1007/s00702-009-0351-5
   Spencer KA, 2009, J MED SPEECH-LANG PA, V17, P125
   Stebbins GT, 2013, MOVEMENT DISORD, V28, P668, DOI 10.1002/mds.25383
   Stepp CE, 2017, J SPEECH LANG HEAR R, V60, P1545, DOI 10.1044/2017_JSLHR-S-16-0282
   Stipancic KL, 2018, J SPEECH LANG HEAR R, V61, P2757, DOI 10.1044/2018_JSLHR-S-17-0366
   Tjaden K, 2014, J SPEECH LANG HEAR R, V57, P779, DOI 10.1044/2014_JSLHR-S-12-0372
   Tourville JA, 2008, NEUROIMAGE, V39, P1429, DOI 10.1016/j.neuroimage.2007.09.054
   Tourville JA, 2011, LANG COGNITIVE PROC, V26, P952, DOI 10.1080/01690960903498424
   Villacorta VM, 2007, J ACOUST SOC AM, V122, P2306, DOI 10.1121/1.2773966
   Wang WD, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00815
   Weerathunge HR, 2020, J SPEECH LANG HEAR R, V63, P2846, DOI 10.1044/2020_JSLHR-19-00407
   Yorkston K., 1996, Sentence Intelligibility Test for Windows
   YORKSTON KM, 1990, J SPEECH HEAR DISORD, V55, P550, DOI 10.1044/jshd.5503.550
   YORKSTON KM, 2010, MANAGEMENT MOTOR SPE
   Zarate JM, 2005, ANN NY ACAD SCI, V1060, P404, DOI 10.1196/annals.1360.058
NR 66
TC 9
Z9 10
U1 1
U2 2
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 2200 RESEARCH BLVD, #271, ROCKVILLE, MD 20850-3289 USA
SN 1092-4388
EI 1558-9102
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD DEC
PY 2021
VL 64
IS 12
BP 4682
EP 4694
DI 10.1044/2021_JSLHR-21-00153
PG 13
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA ZA4NO
UT WOS:000756142500009
PM 34731577
OA Green Published
DA 2024-01-09
ER

PT J
AU Cistola, G
   Peiro-Lilja, A
   Cambara, G
   van der Meulen, I
   Farrus, M
AF Cistola, Giorgia
   Peiro-Lilja, Alex
   Cambara, Guillermo
   van der Meulen, Ineke
   Farrus, Mireia
TI Influence of TTS Systems Performance on Reaction Times in People with
   Aphasia
SO APPLIED SCIENCES-BASEL
LA English
DT Article
DE aphasia; intelligibility; jitter; naturalness; reading impairments;
   shimmer; text-to-speech systems
ID SPEECH; COMPREHENSION
AB Text-to-speech (TTS) systems provide fundamental reading support for people with aphasia and reading difficulties. However, artificial voices are more difficult to process than natural voices. The current study is an extended analysis of the results of a clinical experiment investigating which, among three artificial voices and a digitised human voice, is more suitable for people with aphasia and reading impairments. Such results show that the voice synthesised with Ogmios TTS, a concatenative speech synthesis system, caused significantly slower reaction times than the other three voices used in the experiment. The present study explores whether and what voice quality metrics are linked to delayed reaction times. For this purpose, the voices were analysed using an automatic assessment of intelligibility, naturalness, and jitter and shimmer voice quality parameters. This analysis revealed that Ogmios TTS, in general, performed worse than the other voices in all parameters. These observations could explain the significantly delayed reaction times in people with aphasia and reading impairments when listening to Ogmios TTS and could open up consideration about which TTS to choose for compensative devices for these patients based on the voice analysis of these parameters.
C1 [Cistola, Giorgia; Peiro-Lilja, Alex; Cambara, Guillermo] Univ Pompeu Fabra, TALN Res Grp, Barcelona 08017, Spain.
   [van der Meulen, Ineke] Erasmus MC, Univ Med Ctr Rotterdam, Dept Rehabil Med, NL-3015 GD Rotterdam, Netherlands.
   [van der Meulen, Ineke] Rijndam Rehabil, NL-3015 LJ Rotterdam, Netherlands.
   [Farrus, Mireia] Univ Barcelona, Language & Computat Ctr, Barcelona 08007, Spain.
C3 Pompeu Fabra University; Erasmus University Rotterdam; Erasmus MC;
   University of Barcelona
RP Cistola, G (corresponding author), Univ Pompeu Fabra, TALN Res Grp, Barcelona 08017, Spain.
EM giorgia.cistola@upf.edu; alex.peiro@upf.edu; guillermo.cambara@upf.edu;
   ivdmeulen@rijndam.nl; mfarrus@ub.edu
RI Farrús, Mireia/O-1402-2019; Cistola, Giorgia/GQQ-1362-2022
OI Farrús, Mireia/0000-0002-7160-9513; Cistola, Giorgia/0000-0002-6990-5786
FU Agencia Estatal de Investigacion (AEI); Ministerio de Ciencia,
   Innovacion y Universidades; Fondo Social Europeo (FSE) [RYC-2015-17239]
FX The last author has been funded by the Agencia Estatal de Investigacion
   (AEI), Ministerio de Ciencia, Innovacion y Universidades and the Fondo
   Social Europeo (FSE) under grant RYC-2015-17239 (AEI/FSE, UE).
CR Baby A., 2020, ARXIV200601463
   Boersma P., 2021, Glot International
   Bonafonte A, P TC STAR WORKSH SPE, P199
   Brown JA, 2019, AM J SPEECH-LANG PAT, V28, P278, DOI 10.1044/2018_AJSLP-17-0132
   Carlsen K., 1994, J MED SPEECH-LANG PA, V2, P105
   Cistola G, 2021, INT J LANG COMM DIS, V56, P161, DOI 10.1111/1460-6984.12569
   Cohn M, 2020, INTERSPEECH, P1733, DOI 10.21437/Interspeech.2020-1336
   Farrus M, P INTERSPEECH 2007 8, P778
   Forster KI, 2003, BEHAV RES METH INS C, V35, P116, DOI 10.3758/BF03195503
   Hux K, 2021, J COMMUN DISORD, V91, DOI 10.1016/j.jcomdis.2021.106098
   Hux K, 2017, J COMMUN DISORD, V69, P15, DOI 10.1016/j.jcomdis.2017.06.006
   King S, 2011, SADHANA-ACAD P ENG S, V36, P837, DOI 10.1007/s12046-011-0048-y
   Knollman-Porter K, 2019, AM J SPEECH-LANG PAT, V28, P1206, DOI 10.1044/2019_AJSLP-19-0013
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   McNeil M. R., 1991, CLIN APHASIOL, V20, P21
   Mittag G, 2020, INTERSPEECH, P1748, DOI 10.21437/Interspeech.2020-2382
   Murray LL, 1999, APHASIOLOGY, V13, P91, DOI 10.1080/026870399402226
   Pratap V, 2020, INTERSPEECH, P2757, DOI 10.21437/Interspeech.2020-2826
   Pratap V, 2019, INT CONF ACOUST SPEE, P6460, DOI 10.1109/ICASSP.2019.8683535
   Sanders W.R, 1968, U LEVEL COMPUTER ASS
   Slyh R.E, P 1999 IEEE INT C AC, VVolume 4, P2091, DOI [10.1109/ICASSP.1999.758345, DOI 10.1109/ICASSP.1999.758345]
   Soni M.H, P 9 ISCA SPEECH SYNT
   Taylor P, P 3 ESCA WORKSH SPEE, P147
   Teixeira JP, 2014, PROC TECH, V16, P1190, DOI 10.1016/j.protcy.2014.10.134
   Teixeira JP., 2013, Proc. Technol, V9, P1112, DOI DOI 10.1016/J.PROTCY.2013.12.124
   Wallace SE, 2019, APHASIOLOGY, V33, P731, DOI 10.1080/02687038.2018.1506088
   Ziegler A., 2011, GEN ESTIMATING EQUAT, DOI DOI 10.1007/978-1-4614-0499-6
   Zwetsch IC, 2006, SCI MED, V16, P109
NR 28
TC 1
Z9 1
U1 0
U2 1
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2076-3417
J9 APPL SCI-BASEL
JI Appl. Sci.-Basel
PD DEC
PY 2021
VL 11
IS 23
AR 11320
DI 10.3390/app112311320
PG 11
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Chemistry; Engineering; Materials Science; Physics
GA XW5KH
UT WOS:000735657500001
OA Green Published, gold
DA 2024-01-09
ER

PT J
AU Kapolowicz, MR
   Guest, DR
   Montazeri, V
   Baese-Berk, MM
   Assmann, PF
AF Kapolowicz, Michelle R.
   Guest, Daniel R.
   Montazeri, Vahid
   Baese-Berk, Melissa M.
   Assmann, Peter F.
TI Effects of Spectral Envelope and Fundamental Frequency Shifts on the
   Perception of Foreign-Accented Speech
SO LANGUAGE AND SPEECH
LA English
DT Article
DE Foreign-accented speech; fundamental frequency; spectral envelope;
   talker discrimination
ID ACOUSTIC DIFFERENCES; VOWEL NORMALIZATION; TALKER VARIABILITY; RAPID
   ADAPTATION; WORD RECOGNITION; SPEAKING RATE; VOICE; INDIVIDUALITY;
   PARAMETERS; RESOLUTION
AB To investigate the role of spectral pattern information in the perception of foreign-accented speech, we measured the effects of spectral shifts on judgments of talker discrimination, perceived naturalness, and intelligibility when listening to Mandarin-accented English and native-accented English sentences. In separate conditions, the spectral envelope and fundamental frequency (F0) contours were shifted up or down in three steps using coordinated scale factors (multiples of 8% and 30%, respectively). Experiment 1 showed that listeners perceive spectrally shifted sentences as coming from a different talker for both native-accented and foreign-accented speech. Experiment 2 demonstrated that downward shifts applied to male talkers and the largest upward shifts applied to all talkers reduced the perceived naturalness, regardless of accent. Overall, listeners rated foreign-accented speech as sounding less natural even for unshifted speech. In Experiment 3, introducing spectral shifts further lowered the intelligibility of foreign-accented speech. When speech from the same foreign-accented talker was shifted to simulate five different talkers, increased exposure failed to produce an improvement in intelligibility scores, similar to the pattern observed when listeners actually heard five foreign-accented talkers. Intelligibility of spectrally shifted native-accented speech was near ceiling performance initially, and no further improvement or decrement was observed. These experiments suggest a mechanism that utilizes spectral envelope and F0 cues in a talker-dependent manner to support the perception of foreign-accented speech.
C1 [Kapolowicz, Michelle R.] Univ Calif Irvine, 114 Med Sci E, Irvine, CA 92697 USA.
   [Guest, Daniel R.] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Montazeri, Vahid; Assmann, Peter F.] Univ Texas Dallas, Richardson, TX 75083 USA.
   [Baese-Berk, Melissa M.] Univ Oregon, Eugene, OR 97403 USA.
C3 University of California System; University of California Irvine;
   University of Minnesota System; University of Minnesota Twin Cities;
   University of Texas System; University of Texas Dallas; University of
   Oregon
RP Kapolowicz, MR (corresponding author), Univ Calif Irvine, 114 Med Sci E, Irvine, CA 92697 USA.
EM m.kapolowicz@uci.edu
RI Kapolowicz, Michelle/IUP-4526-2023
OI Kapolowicz, Michelle/0000-0003-4404-5270; Baese-Berk,
   Melissa/0000-0002-4855-9319
CR Abdi H, 2009, EXPT DESIGN ANAL PSY
   ANDERSONHSIEH J, 1988, LANG LEARN, V38, P561, DOI 10.1111/j.1467-1770.1988.tb00167.x
   [Anonymous], 1969, IEEE T ACOUST SPEECH, VAU17, P225
   Assmann Peter F., 2008, Canadian Acoustics, V36, P148
   Assmann P.F., 2007, Journal of the Acoustical Society of America, V122, P3064
   Assmann PF, 2008, J ACOUST SOC AM, V124, P3203, DOI 10.1121/1.2980456
   Assmann PF, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P889
   Bachorowski JA, 1999, J ACOUST SOC AM, V106, P1054, DOI 10.1121/1.427115
   Baese-Berk MM, 2021, JASA EXPRESS LETT, V1, DOI 10.1121/10.0003326
   Baese-Berk MM, 2015, J ACOUST SOC AM, V138, pEL223, DOI 10.1121/1.4929622
   Baese-Berk MM, 2013, J ACOUST SOC AM, V133, pEL174, DOI 10.1121/1.4789864
   Barreda S, 2020, LANGUAGE, V96, P224, DOI 10.1353/lan.2020.0018
   Barreda S, 2018, J ACOUST SOC AM, V143, pEL361, DOI 10.1121/1.5037614
   Barreda S, 2012, J ACOUST SOC AM, V132, P3453, DOI 10.1121/1.4747011
   Baskent D., 2016, SCI FDN AUDIOLOGY PE, P285
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Baumann O, 2010, PSYCHOL RES-PSYCH FO, V74, P110, DOI 10.1007/s00426-008-0185-z
   Bent T, 2013, J ACOUST SOC AM, V133, P1677, DOI 10.1121/1.4776212
   Best CT., 1995, SPEECH PERCEPTION LI, P171
   Bradlow AR, 2008, COGNITION, V106, P707, DOI 10.1016/j.cognition.2007.04.005
   Chang YP, 2006, J SPEECH LANG HEAR R, V49, P1331, DOI 10.1044/1092-4388(2006/095)
   Clarke CM, 2004, J ACOUST SOC AM, V116, P3647, DOI 10.1121/1.1815131
   Cooper A, 2016, J ACOUST SOC AM, V140, pEL378, DOI 10.1121/1.4966585
   Flege JE., 1995, SPEECH PERCEPTION LI, P233, DOI DOI 10.1111/J.1600-0404.1995.TB01710.X
   Friendly Michael, 2023, CRAN
   Fu QJ, 1999, EAR HEARING, V20, P332, DOI 10.1097/00003446-199908000-00006
   Fuller CD, 2014, JARO-J ASSOC RES OTO, V15, P1037, DOI 10.1007/s10162-014-0483-7
   Gaudrain E, 2018, EAR HEARING, V39, P226, DOI 10.1097/AUD.0000000000000480
   Gaudrain E, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P152
   Graddol David., 2006, English next
   Hillenbrand JM, 2009, ATTEN PERCEPT PSYCHO, V71, P1150, DOI 10.3758/APP.71.5.1150
   Holmes E, 2018, PSYCHOL SCI, V29, P1575, DOI 10.1177/0956797618779083
   Hothorn T, 2008, BIOMETRICAL J, V50, P346, DOI 10.1002/bimj.200810425
   Jenkins J., 2000, The phonology of English as an international language: New models, new norms, new goals
   Ji CL, 2014, J SPEECH LANG HEAR R, V57, P532, DOI 10.1044/2014_JSLHR-H-12-0404
   Johnsrude IS, 2013, PSYCHOL SCI, V24, P1995, DOI 10.1177/0956797613482467
   Joos M.A., 1948, Language, V24, P1, DOI [10.2307/522229, DOI 10.2307/522229]
   Kaiser AR, 2003, J SPEECH LANG HEAR R, V46, P390, DOI 10.1044/1092-4388(2003/032)
   Kapolowicz MR, 2020, J ACOUST SOC AM, V148, pEL267, DOI 10.1121/10.0001941
   Kapolowicz MR, 2018, J ACOUST SOC AM, V143, pEL99, DOI 10.1121/1.5023594
   Kapolowicz MR, 2016, INTERSPEECH, P3289, DOI 10.21437/Interspeech.2016-1585
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   Kawahara H., 2005, Proceedings from Interspeech 2005: The 9th Biennial Conference of the International Speech Communication Association ISCA, P537
   Kreiman J., 2011, FDN VOICE STUDIES IN, DOI DOI 10.1002/9781444395068
   KUWABARA H, 1991, SPEECH COMMUN, V10, P491, DOI 10.1016/0167-6393(91)90052-U
   KUWABARA H, 1995, SPEECH COMMUN, V16, P165, DOI 10.1016/0167-6393(94)00053-D
   LANE H, 1963, J ACOUST SOC AM, V35, P451, DOI 10.1121/1.1918501
   Lawrence Michael A, 2016, CRAN
   Lenth R., 2020, emmeans: Estimated marginal means, aka least-squares means
   Lenth Russell V, 2023, CRAN
   Mackey LS, 1997, J SPEECH LANG HEAR R, V40, P349, DOI 10.1044/jslhr.4002.349
   Magnuson JS, 2007, J EXP PSYCHOL HUMAN, V33, P391, DOI 10.1037/0096-1523.33.2.391
   Nearey T.M., 2007, Experimental Approaches to Phonology, P246
   NEAREY TM, 1989, J ACOUST SOC AM, V85, P2088, DOI 10.1121/1.397861
   Nygaard LC, 1998, PERCEPT PSYCHOPHYS, V60, P355, DOI 10.3758/BF03206860
   Sidaras SK, 2009, J ACOUST SOC AM, V125, P3306, DOI 10.1121/1.3101452
   Sjerps MJ, 2011, NEUROPSYCHOLOGIA, V49, P3831, DOI 10.1016/j.neuropsychologia.2011.09.044
   Smith DRR, 2005, J ACOUST SOC AM, V117, P305, DOI 10.1121/1.1828637
   STUDEBAKER GA, 1985, J SPEECH HEAR RES, V28, P455, DOI 10.1044/jshr.2803.455
   Tamati TN, 2021, J SPEECH LANG HEAR R, V64, P683, DOI 10.1044/2020_JSLHR-20-00496
   Tamati TN, 2020, J ACOUST SOC AM, V147, pEL370, DOI 10.1121/10.0001097
   Thomson RI, 2009, J ACOUST SOC AM, V126, P1447, DOI 10.1121/1.3177260
   TITZE IR, 1989, J ACOUST SOC AM, V85, P1699, DOI 10.1121/1.397959
   Trude AM, 2013, J MEM LANG, V69, P349, DOI 10.1016/j.jml.2013.05.002
   VANDOMMELEN WA, 1990, LANG SPEECH, V33, P259, DOI 10.1177/002383099003300302
   Wong PCM, 2003, J SPEECH LANG HEAR R, V46, P413, DOI 10.1044/1092-4388(2003/034)
   Xie X, 2018, LANG COGN NEUROSCI, V33, P196, DOI 10.1080/23273798.2017.1369551
NR 67
TC 1
Z9 1
U1 0
U2 5
PU SAGE PUBLICATIONS LTD
PI LONDON
PA 1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND
SN 0023-8309
EI 1756-6053
J9 LANG SPEECH
JI Lang. Speech
PD JUN
PY 2022
VL 65
IS 2
BP 418
EP 443
AR 00238309211029679
DI 10.1177/00238309211029679
EA JUL 2021
PG 26
WC Audiology & Speech-Language Pathology; Linguistics; Psychology,
   Experimental
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Psychology
GA 0N4KD
UT WOS:000672553400001
PM 34240630
DA 2024-01-09
ER

PT J
AU Seaborn, K
   Miyake, NP
   Pennefather, P
   Otake-Matsuura, M
AF Seaborn, Katie
   Miyake, Norihisa P.
   Pennefather, Peter
   Otake-Matsuura, Mihoko
TI Voice in Human-Agent Interaction: A Survey
SO ACM COMPUTING SURVEYS
LA English
DT Article
DE Computer agent; computer voice; synthetic speech; voice perception;
   vocalics; conversational agents; voice assistants; robots; embodied AI;
   embodied agents; voice-user interface (VUI); human-agent interaction
   (HAI); human-computer interaction (HCI); human-robot interaction (HRI);
   human-machine communication (HMC)
ID SOCIAL ROBOTS; COMPUTER; SPEECH; GENDER; PERCEPTION; FRAMEWORK; USERS;
   CUES; STEREOTYPES; RECOGNITION
AB Social robots, conversational agents, voice assistants, and other embodied AI are increasingly a feature of everyday life. What connects these various types of intelligent agents is their ability to interact with people through voice. Voice is becoming an essential modality of embodiment, communication, and interaction between computer-based agents and end-users. This survey presents a meta-synthesis on agent voice in the design and experience of agents from a human-centered perspective: voice-based human-agent interaction (vHAI). Findings emphasize the social role of voice in HAI as well as circumscribe a relationship between agent voice and body, corresponding to human models of social psychology and cognition. Additionally, changes in perceptions of and reactions to agent voice over time reveals a generational shift coinciding with the commercial proliferation of mobile voice assistants. The main contributions of this work are a vHAI classification framework for voice across various agent forms, contexts, and user groups, a critical analysis grounded in key theories, and an identification of future directions for the oncoming wave of vocal machines.
C1 [Seaborn, Katie] Tokyo Inst Technol, Meguru Ku, W9-51,2-12-1 Ookayama, Tokyo 1528552, Japan.
   [Seaborn, Katie] RIKEN, Ctr Adv Intelligence Project AIP, Meguru Ku, W9-51,2-12-1 Ookayama, Tokyo 1528552, Japan.
   [Miyake, Norihisa P.; Otake-Matsuura, Mihoko] RIKEN, Ctr Adv Intelligence Project AIP, Chuo Ku, Nihonbashi 1 Chome Mitsui Bldg,15th Floor, Tokyo 1030027, Japan.
   [Pennefather, Peter] gDial Inc, 87 Earl Grey Rd, Toronto, ON M4J 3L6, Canada.
C3 Tokyo Institute of Technology; RIKEN; RIKEN
RP Seaborn, K (corresponding author), Tokyo Inst Technol, Meguru Ku, W9-51,2-12-1 Ookayama, Tokyo 1528552, Japan.; Seaborn, K (corresponding author), RIKEN, Ctr Adv Intelligence Project AIP, Meguru Ku, W9-51,2-12-1 Ookayama, Tokyo 1528552, Japan.
EM seaborn.k.aa@m.titech.ac.jp; norihisa.miyake@riken.jp;
   p.pennefather@gmail.com; mihoko.otake@riken.jp
RI Otake-Matsuura, Mihoko/N-4686-2017; Seaborn, Katie/JFK-2295-2023
OI Otake-Matsuura, Mihoko/0000-0003-3644-276X; Seaborn,
   Katie/0000-0002-7812-9096
FU Japanese Society for the Promotion of Science (JSPS)
FX We thank the editors and reviewers. We also thank the Japanese Society
   for the Promotion of Science (JSPS) for supporting this work.
CR Abdul-Kader SA, 2015, INT J ADV COMPUT SC, V6, P72
   Abdulrahman A, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364754
   ACM Council, ACM ETH
   AI for the Social Good, 2019, DAGSTH DECL
   AJZEN I, 1991, ORGAN BEHAV HUM DEC, V50, P179, DOI 10.1016/0749-5978(91)90020-T
   Amershi S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300233
   Andrist S, 2015, ACMIEEE INT CONF HUM, P157, DOI 10.1145/2696454.2696464
   [Anonymous], 2019, PARTNERSHIP, DOI Buffalo Bayou
   [Anonymous], 1993, PARALANGUAGE LINGUIS
   [Anonymous], 2003, CHI Letters, DOI [10.1145/642611.642662, DOI 10.1145/642611.642662]
   [Anonymous], 2010, SOCIAL IDENTITY INTE
   [Anonymous], 1980, The Phonetic Description of Voice Quality
   Arik SO, 2017, PR MACH LEARN RES, V70
   Atkinson RK, 2005, CONTEMP EDUC PSYCHOL, V30, P117, DOI 10.1016/j.cedpsych.2004.07.001
   Baird A, 2018, J AUDIO ENG SOC, V66, P277, DOI 10.17743/jaes.2018.0023
   Baird Alice, 2017, P 12 INT AUDIO MOSTL, DOI [10.1145/3123514.3123528, DOI 10.1145/3123514.3123528]
   Barsalou LW, 2003, PSYCHOL LEARN MOTIV, V43, P43, DOI 10.1016/S0079-7421(03)01011-9
   Beauvoir S, 2011, The Second Sex
   Behrens SI, 2018, ACMIEEE INT CONF HUM, P63, DOI 10.1145/3173386.3177009
   Bhagdikar S, 2019, INT CONF SIM SEMI PR, P13, DOI 10.1109/sispad.2019.8870524
   Bracken CC, 2004, CYBERPSYCHOL BEHAV, V7, P349, DOI 10.1089/1094931041291358
   Braun M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300270
   Breazeal C, 2016, SPRINGER HANDBOOK OF ROBOTICS, P1935
   BRENNAN SE, 1995, KNOWL-BASED SYST, V8, P143, DOI 10.1016/0950-7051(95)98376-H
   Burgin Mark, 2009, SYSTEMATIC APPROACH
   Cambre Julia, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359325
   Cambre J, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376789
   Cangelosi Angelo, 2016, HUMANOID ROBOTICS RE, P1
   Cao HL, 2019, INTEL SERV ROBOT, V12, P45, DOI 10.1007/s11370-018-0266-9
   Chang RCS, 2018, COMPUT HUM BEHAV, V84, P194, DOI 10.1016/j.chb.2018.02.025
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Chidambaram V, 2012, ACMIEEE INT CONF HUM, P293
   Chiou EK, 2020, COMPUT EDUC, V146, DOI 10.1016/j.compedu.2019.103756
   Chita-Tegmark M, 2019, ACMIEEE INT CONF HUM, P230, DOI [10.1109/HRI.2019.8673222, 10.1109/hri.2019.8673222]
   Coeckelbergh M, 2011, INT J SOC ROBOT, V3, P197, DOI 10.1007/s12369-010-0075-6
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Cosentino Sarah, 2016, IEEE Rev Biomed Eng, V9, P148, DOI 10.1109/RBME.2016.2527638
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   Creed C, 2008, INTERACT COMPUT, V20, P225, DOI 10.1016/j.intcom.2007.11.004
   Creswell J. W., 2016, QUAL INQ
   Crowell CR, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3735, DOI 10.1109/IROS.2009.5354204
   Crumpton J, 2016, INT J SOC ROBOT, V8, P271, DOI 10.1007/s12369-015-0329-4
   Dahlbäck N, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P294
   DAHLBACK N, 1993, KNOWL-BASED SYST, V6, P258, DOI 10.1016/0950-7051(93)90017-N
   Dahlbäck N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1553
   Data for Democracy, 2019, GLOB DAT ETH PLEDG G
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Davis RO, 2019, COMPUT EDUC, V140, DOI 10.1016/j.compedu.2019.103605
   Donahue TJ, 2015, INT CONF COGN INFO, P397, DOI 10.1109/CogInfoCom.2015.7390626
   Dou X, 2021, INT J SOC ROBOT, V13, P615, DOI 10.1007/s12369-020-00654-9
   Drager KDR, 2010, AM J SPEECH-LANG PAT, V19, P259, DOI 10.1044/1058-0360(2010/09-0024)
   Elkins AC, 2013, GROUP DECIS NEGOT, V22, P897, DOI 10.1007/s10726-012-9339-x
   Elmlinger SL, 2019, J CHILD LANG, V46, P998, DOI 10.1017/S0305000919000291
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   Evans RE, 2010, INTERACT COMPUT, V22, P606, DOI 10.1016/j.intcom.2010.07.001
   Eyssel F., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P851, DOI 10.1109/ROMAN.2012.6343858
   Eyssel F, 2012, ACMIEEE INT CONF HUM, P125
   Feine J, 2019, INT J HUM-COMPUT ST, V132, P138, DOI 10.1016/j.ijhcs.2019.07.009
   Fessler Leah, 2017, Quartz
   Fields C, 2018, COGN SYST RES, V47, P186, DOI 10.1016/j.cogsys.2017.10.003
   Firestone W.A., 1993, Educational Researcher, V22, P16, DOI DOI 10.3102/0013189X022004016
   Fitch WT, 2017, PSYCHON B REV, V24, P3, DOI 10.3758/s13423-017-1236-5
   Fox Sarah, 2016, P 2016 CHI C HUM FAC, P3293, DOI DOI 10.1145/2851581
   Gangamohan P, 2016, INTEL SYST REF LIBR, V105, P205, DOI 10.1007/978-3-319-31056-5_11
   Ghazali AS, 2019, ACMIEEE INT CONF HUM, P586, DOI [10.1109/hri.2019.8673266, 10.1109/HRI.2019.8673266]
   Goble H, 2018, COMMUN RES REP, V35, P256, DOI 10.1080/08824096.2018.1447454
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   Govind D, 2013, INT J SPEECH TECHNOL, V16, P237, DOI 10.1007/s10772-012-9180-2
   Gravano A, 2011, COMPUT SPEECH LANG, V25, P601, DOI 10.1016/j.csl.2010.10.003
   Grifoni P., 2009, MULTIMODAL HUMAN COM
   Grudin J, 2009, AI MAG, V30, P48, DOI 10.1609/aimag.v30i4.2271
   Gunkel D.J., 2012, Communication +1, V1, P1, DOI [10.7275/R5QJ7F7R, DOI 10.7275/R5QJ7F7R]
   Guzman AL, 2020, NEW MEDIA SOC, V22, P70, DOI 10.1177/1461444819858691
   Guzman AL, 2019, COMPUT HUM BEHAV, V90, P343, DOI 10.1016/j.chb.2018.08.009
   Guzman AL, 2017, SOCIALBOTS AND THEIR FRIENDS: DIGITAL MEDIA AND THE AUTOMATION OF SOCIALITY, P69
   Hall Edward Twitchell, 1974, Handbook for proxemic research
   Hare B, 2017, ANNU REV PSYCHOL, V68, P155, DOI 10.1146/annurev-psych-010416-044201
   Hennig S., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P589, DOI 10.1109/ROMAN.2012.6343815
   Heyvaert M, 2013, QUAL QUANT, V47, P659, DOI 10.1007/s11135-011-9538-6
   HOCKETT CF, 1960, SCI AM, V203, P88, DOI 10.1038/scientificamerican0960-88
   Hoegen R, 2019, PROCEEDINGS OF THE 19TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA' 19), P111, DOI 10.1145/3308532.3329473
   Howard A, 2018, SCI ENG ETHICS, V24, P1521, DOI 10.1007/s11948-017-9975-2
   Hyde JS, 2019, AM PSYCHOL, V74, P171, DOI 10.1037/amp0000307
   IEEE Robotics and Automation Society, 2019, ROBOT ETHICS
   JACOBY S, 1995, RES LANG SOC INTERAC, V28, P171, DOI 10.1207/s15327973rlsi2803_1
   James J, 2018, IEEE ROMAN, P632, DOI 10.1109/ROMAN.2018.8525652
   Jensen LA, 1996, QUAL HEALTH RES, V6, P553, DOI 10.1177/104973239600600407
   Jeong Y, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312913
   Johar S, 2015, EMOTION AFFECT PERSO
   Kim S, 2018, ACMIEEE INT CONF HUM, P151, DOI 10.1145/3173386.3176970
   Kitchenham B., 2004, Keele University, P1, DOI DOI 10.5144/0256-4947.2017.79
   Knote R, 2019, PROCEEDINGS OF THE 52ND ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, P2024
   Komatsu T, 2011, INT J HUM-COMPUT INT, V27, P260, DOI 10.1080/10447318.2011.537209
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   Kreiman Jody, 2003, VOQUAL'03 Geneva, August 27-29, 2003, P115
   Krenn B, 2017, AI SOC, V32, P65, DOI 10.1007/s00146-014-0569-0
   Krosnick JA, 2010, HANDBOOK OF SURVEY RESEARCH, 2ND EDITION, P263
   Kumar Neha, 2019, Interactions, V26, P50, DOI DOI 10.1145/3305360
   Latinus M, 2011, CURR BIOL, V21, pR143, DOI 10.1016/j.cub.2010.12.033
   Lazzeri N, 2018, INT J ADV ROBOT SYST, V15, DOI 10.1177/1729881418783158
   Lee Eun Ju, 2000, P CHI 00 HUM FACT CO, P289, DOI [10.1145/633292.633461, DOI 10.1145/633292.633461]
   Lee KM, 2004, HUM COMMUN RES, V30, P182, DOI 10.1093/hcr/30.2.182
   Lee S. C., 2019, P 11 INT C AUT US IN, P209
   Lee S, 2019, MULTIMODAL TECHNOLOG, V3, DOI 10.3390/mti3010020
   Lewis M, 1998, AI MAG, V19, P67
   Li Gong, 2003, International Journal of Speech Technology, V6, P123, DOI 10.1023/A:1022382413579
   Lorber Judith, 1994, Paradoxes of gender, P32
   Lubold N, 2016, ACMIEEE INT CONF HUM, P255, DOI 10.1109/HRI.2016.7451760
   Mara M, 2020, ACMIEEE INT CONF HUM, P355, DOI 10.1145/3371382.3378285
   Marakas GM, 2000, INT J HUM-COMPUT ST, V52, P719, DOI 10.1006/ijhc.1999.0348
   MAYER RC, 1995, ACAD MANAGE REV, V20, P709, DOI 10.2307/258792
   Mayer RE, 2003, J EDUC PSYCHOL, V95, P419, DOI 10.1037/0022-0663.95.2.419
   McColl D, 2016, J INTELL ROBOT SYST, V82, P101, DOI 10.1007/s10846-015-0259-2
   McCrae R. R., 2008, Handbook of Personality: Theory and Research, V3rd, P159, DOI DOI 10.3905/JPE.2000.319978
   McDonald JD., 2008, Enquire, V1, P75
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   McTear Michael, 2016, The Conversational Interface: Talking to Smart Devices, DOI 10.1007/978-3-319-32967-3
   Mendelson J, 2017, INTERSPEECH, P249, DOI 10.21437/Interspeech.2017-1438
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Miller Blanca, 2019, Humanoid Robotics: A Reference, P2313, DOI 10.1007/978-94-007-7194-9_130-1
   Mitchell T, 2018, COMMUN ACM, V61, P103, DOI 10.1145/3191513
   Moher D, 2009, ANN INTERN MED, V151, P264, DOI [10.1016/j.ijsu.2010.02.007, 10.1136/bmj.i4086, 10.1016/j.ijsu.2010.07.299, 10.1186/2046-4053-4-1, 10.1371/journal.pmed.1000097, 10.1136/bmj.b2535, 10.1136/bmj.b2700]
   Molnár C, 2008, ANIM COGN, V11, P389, DOI 10.1007/s10071-007-0129-9
   Moreno R, 2001, COGNITION INSTRUCT, V19, P177, DOI 10.1207/S1532690XCI1902_02
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Moses DA, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-10994-4
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Murad C, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376522
   Murad Christine, 2020, P 2 C CONVERSATIONAL, P1, DOI [10.1145/3405755.3406137, DOI 10.1145/3405755.3406137]
   Nadal Kevin L., 2017, NEUROSEXISM, P1243
   Nass C, 1997, J APPL SOC PSYCHOL, V27, P864, DOI 10.1111/j.1559-1816.1997.tb00275.x
   Nass C., 2003, International Journal of Speech Technology, V6, P113, DOI 10.1023/A:1022378312670
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass Clifford, 2001, P AAAI S EM INT
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Nass Clifford, 2005, P HUM FACT COMP SYST, P1973, DOI [DOI 10.1145/1056808.1057070, 10.1145/1056808.1057070]
   Niculescu A., 2011, Proceedings of the 2011 International Conference on User Science and Engineering (i-USEr 2011), P18, DOI 10.1109/iUSEr.2011.6150529
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Nilsson N.J., 2009, The Quest for Artificial Intelligence
   Ohshima N, 2015, 2015 24TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P325, DOI 10.1109/ROMAN.2015.7333677
   Ohta Kengo, 2014, INT J COMPUTERS, V8, P136
   Parviainen E, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376187
   Pfeifer R., 2001, Understanding Intelligence.
   Pieraccini R, 2012, VOICE IN THE MACHINE: BUILDING COMPUTERS THAT UNDERSTAND SPEECH, P1
   Pittam Jeff, 1994, Voice in social interaction
   Polit DF, 2010, INT J NURS STUD, V47, P1451, DOI 10.1016/j.ijnurstu.2010.06.004
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Read R, 2013, ACMIEEE INT CONF HUM, P209, DOI 10.1109/HRI.2013.6483575
   Read R, 2012, ACMIEEE INT CONF HUM, P219
   Rode JA, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P123
   Rogers Yvonne, 2017, Research in the wild, DOI 10.1007/978-3-031-02220-3
   Rosenberg-Kima RB, 2007, LECT NOTES COMPUT SC, V4744, P214
   Rosenthal-von der Pütten AM, 2016, LECT NOTES ARTIF INT, V10011, P256, DOI 10.1007/978-3-319-47665-0_23
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Ryoko S., 2012, Proceedings of the 2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel & Distributed Computing (SNPD 2012), P19, DOI 10.1109/SNPD.2012.72
   Sandygulova A, 2018, INT J SOC ROBOT, V10, P687, DOI 10.1007/s12369-018-0472-9
   Sandygulova A, 2015, LECT NOTES ARTIF INT, V9388, P594, DOI 10.1007/978-3-319-25554-5_59
   Sarigul B, 2020, ACMIEEE INT CONF HUM, P430, DOI 10.1145/3371382.3378302
   Schlesinger A, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5412, DOI 10.1145/3025453.3025766
   Sezgin Emre, 2019, SSRN J, V10, P3, DOI [10.2139/ssrn.3381183, DOI 10.2139/SSRN.3381183]
   Shamekhi A, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173965
   Shi Y, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188560
   Shiwa T., 2008, 2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI 2008), P153
   Siegel M, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2563, DOI 10.1109/IROS.2009.5354116
   Sims Valerie K., 2009, P HUMAN FACTORS ERGO, V53, P1418, DOI 10.1177/154193120905301853
   Soraa RA, 2017, GEND TECHNOL DEV, V21, P99, DOI 10.1080/09718524.2017.1385320
   STEELE CM, 1995, J PERS SOC PSYCHOL, V69, P797, DOI 10.1037/0022-3514.69.5.797
   Stern SE, 2006, INT J HUM-COMPUT ST, V64, P43, DOI 10.1016/j.ijhcs.2005.07.002
   Stern SE, 1999, HUM FACTORS, V41, P588, DOI 10.1518/001872099779656680
   Sutton SJ, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300833
   SWELLER J, 1988, COGNITIVE SCI, V12, P257, DOI 10.1207/s15516709cog1202_4
   Tamagawa R, 2011, INT J SOC ROBOT, V3, P253, DOI 10.1007/s12369-011-0100-4
   Tannen Deborah, 2005, Conversational style: Analyzing talk among friends
   Tay B, 2014, COMPUT HUM BEHAV, V38, P75, DOI 10.1016/j.chb.2014.05.014
   Tomasev N, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-15871-z
   Torre I, 2018, PROCEEDINGS OF THE TECHNOLOGY, MIND, AND SOCIETY CONFERENCE (TECHMINDSOCIETY'18), DOI 10.1145/3183654.3183691
   Torrey C, 2013, ACMIEEE INT CONF HUM, P275, DOI 10.1109/HRI.2013.6483599
   Trivers R., 2002, Natural selection and social theory
   Trovato Gabriele, 2017, Paladyn, Journal of Behavioral Robotics, V8, P1, DOI 10.1515/pjbr-2017-0001
   Tsiourti C, 2019, INT J SOC ROBOT, V11, P555, DOI 10.1007/s12369-019-00524-z
   Vannucci F., 2018, P IEEE RAS 18 INT C, P1, DOI DOI 10.1109/HUMANOIDS.2018.8625004
   Wada K, 2004, P IEEE, V92, P1780, DOI 10.1109/JPROC.2004.835378
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Whittlestone J., 2019, Ethical and Societal Implications of Algorithms, Data, and Artificial Intelligence: A Roadmap for Research
   Wieringa S, 2018, J EVAL CLIN PRACT, V24, P930, DOI 10.1111/jep.13010
   Wigdor N, 2016, IEEE ROMAN, P219, DOI 10.1109/ROMAN.2016.7745134
   Wintre MG, 2001, CAN PSYCHOL, V42, P216
   Xu K, 2019, NEW MEDIA SOC, V21, P2522, DOI 10.1177/1461444819851479
   Yarosh S, 2018, PROCEEDINGS OF THE 2018 ACM CONFERENCE ON INTERACTION DESIGN AND CHILDREN (IDC 2018), P300, DOI 10.1145/3202185.3202207
   Yilmazyildiz S, 2016, INT J HUM-COMPUT INT, V32, P63, DOI 10.1080/10447318.2015.1093856
   Yilmazyildiz S, 2015, MULTIMED TOOLS APPL, V74, P9959, DOI 10.1007/s11042-014-2165-1
   Yohanan Steve, 2005, Proceedings of the 7th international conference on Multimodal interfaces, P222, DOI DOI 10.1145/1088463.1088502
   Yu Q, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312918
   Zaga C, 2016, ACMIEEE INT CONF HUM, P541, DOI 10.1109/HRI.2016.7451846
   Zanbaka C., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1153
   Zimman L, 2018, LANG LINGUIST COMPAS, V12, DOI 10.1111/lnc3.12284
NR 198
TC 40
Z9 43
U1 46
U2 149
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 0360-0300
EI 1557-7341
J9 ACM COMPUT SURV
JI ACM Comput. Surv.
PD JUL
PY 2021
VL 54
IS 4
AR 81
DI 10.1145/3386867
PG 43
WC Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA TF3DS
UT WOS:000670593100001
OA Bronze
DA 2024-01-09
ER

PT J
AU Rodero, E
   Lucas, I
AF Rodero, Emma
   Lucas, Ignacio
TI Synthetic versus human voices in audiobooks: The human emotional
   intimacy effect
SO NEW MEDIA & SOCIETY
LA English
DT Article; Early Access
DE Audiobooks; cognitive processing; human voice; perception; physiological
   response; synthetic voice; talking books
ID EVOKED IMAGERY; RADIO; TRANSPORTATION; ATTENTION; SPEECH;
   PERSUASIVENESS; RECALL; MEMORY; CUES; AGE
AB Human voices narrate most audiobooks, but the fast development of speech synthesis technology has enabled the possibility of using artificial voices. This raises the question of whether the listeners' cognitive processing is the same when listening to a synthetic or a human voice telling a story. This research aims to compare the listeners' perception, creation of mental images, narrative engagement, physiological response, and recognition of information when listening to stories conveyed by human and synthetic voices. The results showed that listeners enjoyed stories narrated by a human voice more than a synthetic one. Also, they created more mental images, were more engaged, paid more attention, had a more positive emotional response, and remembered more information. Speech synthesis has experienced considerable progress. However, there are still significant differences versus human voices, so that using them to narrate long stories, such as audiobooks do, is difficult.
C1 [Rodero, Emma] Pompeu Fabra Univ UPF, Dept Commun, Media Psychol Lab, Barcelona, Spain.
   [Lucas, Ignacio] Inst Invest Biomed Bellvitge IDIBELL, Neurosci Program, Psychiat & Mental Hlth Grp, Barcelona, Spain.
C3 Pompeu Fabra University; Institut d'Investigacio Biomedica de Bellvitge
   (IDIBELL)
RP Rodero, E (corresponding author), Pompeu Fabra Univ UPF, Dept Commun, Roc Boronat 138, Barcelona 08108, Spain.
EM emma.rodero@upf.edu
RI Lucas Adell, Ignacio/P-9102-2016
OI Lucas Adell, Ignacio/0000-0001-9426-5082
FU Leonardo Grants for Researchers and Cultural Creators, BBVA, Spain
FX The author(s) disclosed receipt of the following financial support for
   the research, authorship, and/or publication of this article: This
   research was supported by Leonardo Grants for Researchers and Cultural
   Creators, BBVA, Spain.
CR [Anonymous], 2012, PSYCHOPHYSIOLOGICAL, DOI DOI 10.4324/9780203181027
   [Anonymous], 2005, MORE ONE VOICE PHILO
   Assmann P., P 9 INT C SPOK LANG
   Audio Publishers Association (APA), 2021, VOIC IND
   Babin LA, 1998, PSYCHOL MARKET, V15, P261, DOI 10.1002/(SICI)1520-6793(199805)15:3<261::AID-MAR4>3.3.CO;2-X
   Barker P., 2015, VOICE STUDIES CRITIC, P16
   Barthes Roland, 1985, Responsibility of forms: Critical essays on music, art, and representation
   Bilandzic H, 2008, J COMMUN, V58, P508, DOI 10.1111/j.1460-2466.2008.00397.x
   Bolls PD, 2007, J ADVERTISING, V36, P35, DOI 10.2753/JOA0091-3367360403
   Bolls PD, 2003, MEDIA PSYCHOL, V5, P33, DOI 10.1207/S1532785XMEP0501_2
   BONE PF, 1992, J CONSUM RES, V19, P93, DOI 10.1086/209289
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Chen F., 2006, Designing human interface in speech technology
   Chion M., 2019, Audio-Vision: Sound on Screen
   Connor Stephen, 2000, Dumbstruck. A Cultural History of Ventriloquism
   Craig SD, 2019, J EDUC COMPUT RES, V57, P1534, DOI 10.1177/0735633118802877
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   Delogu C, 1998, SPEECH COMMUN, V24, P153, DOI 10.1016/S0167-6393(98)00009-0
   Di Matteo P., 2015, VOICE STUDIES CRITIC, P104
   Edison Research, 2020, SMART AUD REP 2020
   Edison Research, 2019, INF DIAL 2019
   ELLEN PS, 1991, ADV CONSUM RES, V18, P806
   Goossens C., 1994, Journal of Mental Imagery, V18, P119
   Green MC, 2000, J PERS SOC PSYCHOL, V79, P701, DOI 10.1037/0022-3514.79.5.701
   Green MC, 2004, COMMUN THEOR, V14, P311, DOI 10.1111/j.1468-2885.2004.tb00317.x
   Have I, 2020, NEW MEDIA SOC, V22, P409, DOI 10.1177/1461444819863407
   Humphry J, 2021, NEW MEDIA SOC, V23, P1971, DOI 10.1177/1461444820923679
   JENKINS JJ, 1982, B PSYCHONOMIC SOC, V20, P203
   Kosslyn S. M., 1994, IMAGE BRAIN RESOLUTI
   Kosslyn SM, 2001, NAT REV NEUROSCI, V2, P635, DOI 10.1038/35090055
   Lai J., P SIGCHI C HUM FACT, P321
   Lang A, 2015, COMMUN RES, V42, P759, DOI 10.1177/0093650213490722
   LUCE PA, 1983, HUM FACTORS, V25, P17, DOI 10.1177/001872088302500102
   Luce PA., 1981, 7 IND U SPEECH RES L, P229
   Mar RA, 2008, PERSPECT PSYCHOL SCI, V3, P173, DOI 10.1111/j.1745-6924.2008.00073.x
   Mayer R. E., 2014, The Cambridge handbook of multimedia learning, V2, P345
   Miller DW, 1997, PSYCHOL MARKET, V14, P337, DOI 10.1002/(SICI)1520-6793(199707)14:4<337::AID-MAR3>3.0.CO;2-A
   Mulac A, 1996, HEALTH COMMUN, V8, P199, DOI 10.1207/s15327027hc0803_2
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   PAIVIO A, 1991, CAN J PSYCHOL, V45, P255, DOI 10.1037/h0084295
   Paris CR, 2000, HUM FACTORS, V42, P421, DOI 10.1518/001872000779698132
   Parker B., 2013, SHOULD YOU HIRE COMP
   Pisoni DB., IEEE INT C AC SPEECH, P572
   Rodero E, 2020, UNESCO COURIER, V1, P18
   Rodero E, 2017, COMPUT HUM BEHAV, V77, P336, DOI 10.1016/j.chb.2017.08.044
   Rodero E, 2017, HUM COMMUN RES, V43, P397, DOI 10.1111/hcre.12109
   Rodero E, 2016, MEDIA PSYCHOL, V19, P224, DOI 10.1080/15213269.2014.1002942
   Rodero E, 2012, COMMUN RES, V39, P458, DOI 10.1177/0093650210386947
   Roring RW, 2007, HUM FACTORS, V49, P25, DOI 10.1518/001872007779598055
   Sanderman AA, 1997, LANG SPEECH, V40, P391, DOI 10.1177/002383099704000405
   Stern SE, 1999, HUM FACTORS, V41, P588, DOI 10.1518/001872099779656680
   Syrdal A. K., 1994, APPL SPEECH TECHNOLO
   Taake KP., 2009, THESIS WASHINGTON U
   Thoet A., 2017, SHORT HIST AUDIOBOOK
   vanDommelen WA, 1995, LANG SPEECH, V38, P267, DOI 10.1177/002383099503800304
   Wallin ET, 2020, NEW MEDIA SOC, V22, P470, DOI 10.1177/1461444819864691
   Winters SJ., 2004, RES SPOKEN LANGUAGE
   Wolters MK, 2015, J AM MED INFORM ASSN, V22, P35, DOI 10.1136/amiajnl-2014-002820
   Xu K, 2019, NEW MEDIA SOC, V21, P2522, DOI 10.1177/1461444819851479
NR 59
TC 4
Z9 4
U1 20
U2 47
PU SAGE PUBLICATIONS LTD
PI LONDON
PA 1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND
SN 1461-4448
EI 1461-7315
J9 NEW MEDIA SOC
JI New Media Soc.
PD 2021 JUN 28
PY 2021
AR 14614448211024142
DI 10.1177/14614448211024142
EA JUN 2021
PG 19
WC Communication
WE Social Science Citation Index (SSCI)
SC Communication
GA TD4VP
UT WOS:000669326300001
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Birkholz, P
   Drechsel, S
AF Birkholz, Peter
   Drechsel, Susanne
TI Effects of the piriform fossae, transvelar acoustic coupling, and
   laryngeal wall vibration on the naturalness of articulatory speech
   synthesis
SO SPEECH COMMUNICATION
LA English
DT Article
DE Transvelar coupling; Piriform fossae; Vocal tract model
ID VOCAL-TRACT; HYPOPHARYNGEAL CAVITIES; MODEL; FREQUENCY; SIMULATION;
   SEQUENCES; PHONOLOGY; SINUSES; TONGUE; LENGTH
AB Acoustic models of the vocal tract for articulatory speech synthesis often neglect a range of acoustic effects that are known to exist in the human vocal tract. Here we extended a basic acoustic vocal tract model by three features: the piriform fossae, transvelar acoustic coupling of the oral and nasal cavities, and sound radiation from the skin of the neck. The main goal was to find out how these features affect the naturalness of the synthesized speech. To this end, ten German words were synthesized with different combinations of the additional features, and listeners compared the naturalness of these stimuli. Surprisingly, all three features reduced the perceived naturalness, although they should make the synthesis more realistic. A closer analysis revealed that all new features emphasized the low frequencies compared to the high frequencies of the synthetic speech, leading to slightly more muffled speech with the used glottal excitation. An additional perception experiment with synthetic stimuli with a slightly more tense voice revealed no perceptual preference for the synthesis with or without the piriform fossae. These results indicate that the examined features play a minor role for the naturalness of articulatory synthesis compared to the voice source characteristics.
C1 [Birkholz, Peter; Drechsel, Susanne] Tech Univ Dresden, Inst Acoust & Speech Commun, Dresden, Germany.
C3 Technische Universitat Dresden
RP Birkholz, P (corresponding author), Tech Univ Dresden, Inst Acoust & Speech Commun, Dresden, Germany.
EM peter.birkholz@tu-dresden.de
CR Alexander R, 2019, J ACOUST SOC AM, V146, P4458, DOI 10.1121/1.5139413
   [Anonymous], 2005, 3D ARTIKULATORISCHE
   [Anonymous], 2011, P INTERSPEECH
   Badin P, 2002, J PHONETICS, V30, P533, DOI 10.1006/jpho.2002.0166
   Beautemps D, 2001, J ACOUST SOC AM, V109, P2165, DOI 10.1121/1.1361090
   Birkholz P., 2014, P 10 INT SEM SPEECH, P37
   Birkholz P., 2004, P INTERSPEECH 2004, P1125, DOI [10.21437/Interspeech.2004-409, DOI 10.21437/INTERSPEECH.2004-409]
   Birkholz P., 2011, STUDIENTEXTE SPRACHK, P47
   Birkholz P., 2007, 8 ANN C INT SPEECH C, P2865
   Birkholz P, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00597-w
   Birkholz P, 2019, INTERSPEECH, P3765, DOI 10.21437/Interspeech.2019-2410
   Birkholz P, 2019, SPEECH COMMUN, V110, P108, DOI 10.1016/j.specom.2019.04.009
   Birkholz P, 2017, COMPUT SPEECH LANG, V41, P116, DOI 10.1016/j.csl.2016.06.004
   Birkholz P, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060603
   Birkholz P, 2011, IEEE T AUDIO SPEECH, V19, P1422, DOI 10.1109/TASL.2010.2091632
   Birkhoz P, 2007, IEEE T AUDIO SPEECH, V15, P1218, DOI 10.1109/TASL.2006.889731
   Blandin R, 2015, J ACOUST SOC AM, V137, P832, DOI 10.1121/1.4906166
   Bouabana S, 1998, SPEECH COMMUN, V24, P227, DOI 10.1016/S0167-6393(98)00012-0
   BROWMAN CP, 1992, PHONETICA, V49, P155, DOI 10.1159/000261913
   Cranen B, 1996, SPEECH COMMUN, V19, P1, DOI 10.1016/0167-6393(96)00016-7
   Dang J, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P965, DOI 10.1109/ICSLP.1996.607763
   Dang JW, 2016, J ACOUST SOC AM, V139, P441, DOI 10.1121/1.4939964
   Dang JW, 1997, J ACOUST SOC AM, V101, P456, DOI 10.1121/1.417990
   Dang JW, 2004, J ACOUST SOC AM, V115, P853, DOI 10.1121/1.1639325
   Dang JW, 1996, J ACOUST SOC AM, V100, P3374, DOI 10.1121/1.416978
   Delvaux B, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102680
   Deng L, 1998, SPEECH COMMUN, V24, P299, DOI 10.1016/S0167-6393(98)00023-5
   Elie B, 2016, SPEECH COMMUN, V82, P85, DOI 10.1016/j.specom.2016.06.002
   Erath BD, 2013, SPEECH COMMUN, V55, P667, DOI 10.1016/j.specom.2013.02.002
   Fant G., 1976, STL QPSR, V4, P13
   FLANAGAN JL, 1975, AT&T TECH J, V54, P485, DOI 10.1002/j.1538-7305.1975.tb02852.x
   Fleischer M, 2015, BIOMECH MODEL MECHAN, V14, P719, DOI 10.1007/s10237-014-0632-2
   Freixes M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9214535
   Fujita S, 2005, ACOUST SCI TECHNOL, V26, P353, DOI 10.1250/ast.26.353
   Godoy E, 2016, INTERSPEECH, P948, DOI 10.21437/Interspeech.2016-1362
   ISHIZAKA K, 1972, AT&T TECH J, V51, P1233, DOI 10.1002/j.1538-7305.1972.tb02651.x
   Iskarous K, 2003, INT C PHON SCI BARC, P185
   Kitamura T, 2005, ACOUST SCI TECHNOL, V26, P16, DOI 10.1250/ast.26.16
   Kröger BJ, 2004, HNO, V52, P837, DOI 10.1007/s00106-004-1097-x
   KROGER BJ, 1993, PHONETICA, V50, P213
   KROGER BJ, 1998, PHONETISCHES MODELL
   Liu Li-Juan, 2017, BLIZZ CHALL WORKSH
   MAEDA S, 1990, NATO ADV SCI I D-BEH, V55, P131
   Maeda S., 1982, Speech Communication, V1, P199, DOI 10.1016/0167-6393(82)90017-6
   Meltzner GS, 2003, J ACOUST SOC AM, V114, P1035, DOI 10.1121/1.1582440
   MERMELSTEIN P, 1973, J ACOUST SOC AM, V53, P1070, DOI 10.1121/1.1913427
   Monson BB, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00587
   Murphy D.T., 2015, 18 INT C DIG AUD EFF, P1
   Okadome T, 2001, J ACOUST SOC AM, V110, P453, DOI 10.1121/1.1377633
   Payan Y, 1997, SPEECH COMMUN, V22, P185, DOI 10.1016/S0167-6393(97)00019-8
   Piepho HP, 2018, AGRON J, V110, P431, DOI 10.2134/agronj2017.10.0580
   Pont A, 2020, INT J NUMER METH BIO, V36, DOI 10.1002/cnm.3302
   Saltzman EL., 1989, Ecological Psychology, V1, P333, DOI DOI 10.1207/S15326969EC00104_
   Shadle C. H., 2001, 4 ISCA TUTORIAL RES, P121
   Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779, DOI 10.1109/ICASSP.2018.8461368
   SONDHI MM, 1987, IEEE T ACOUST SPEECH, V35, P955, DOI 10.1109/TASSP.1987.1165240
   Stavness I, 2011, INT J NUMER METH BIO, V27, P367, DOI 10.1002/cnm.1423
   Stevens K., 1998, Acoustic phonetics
   Stone S, 2018, IEEE-ACM T AUDIO SPE, V26, P1381, DOI 10.1109/TASLP.2018.2825601
   Story BH, 2019, J ACOUST SOC AM, V146, P2522, DOI 10.1121/1.5127756
   Story BH, 2018, J ACOUST SOC AM, V143, P3079, DOI 10.1121/1.5038264
   Suzuki H., 1990, 1 INT C SPOK LANG PR, P437
   Svec JG, 2005, J ACOUST SOC AM, V117, P1386, DOI 10.1121/1.1850074
   Takemoto H, 2013, J ACOUST SOC AM, V134, P2955, DOI 10.1121/1.4818744
   Takemoto H, 2010, J ACOUST SOC AM, V128, P3724, DOI 10.1121/1.3502470
   Teixeira AJS, 2005, EURASIP J APPL SIG P, V2005, P1435, DOI 10.1155/ASP.2005.1435
   TITZE IR, 1989, SPEECH COMMUN, V8, P191, DOI 10.1016/0167-6393(89)90001-0
   Toutios A, 2011, J ACOUST SOC AM, V129, P3245, DOI 10.1121/1.3569714
   Vampola T, 2020, J ACOUST SOC AM, V148, P3218, DOI 10.1121/10.0002487
   Vampola T, 2015, ACTA ACUST UNITED AC, V101, P594, DOI 10.3813/AAA.918855
   van den Doel K, 2008, IEEE T AUDIO SPEECH, V16, P1163, DOI 10.1109/TASL.2008.2001107
   Wu L, 2014, J ACOUST SOC AM, V136, P350, DOI 10.1121/1.4883355
   Xu Y, 2006, ITAL J LINGUIST, V18, P125
   Zhang C., 2016, P 15 INT C COMPUTER, P1, DOI [10.1109/MESA.2016.7587178, DOI 10.1109/ICIS.2016.7550911]
   Zhang J, 2019, J ACOUST SOC AM, V145, P734, DOI 10.1121/1.5089220
NR 75
TC 5
Z9 6
U1 0
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-6393
EI 1872-7182
J9 SPEECH COMMUN
JI Speech Commun.
PD SEP
PY 2021
VL 132
BP 96
EP 105
DI 10.1016/j.specom.2021.06.002
EA JUN 2021
PG 10
WC Acoustics; Computer Science, Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Acoustics; Computer Science
GA TT5VK
UT WOS:000680415400009
DA 2024-01-09
ER

PT J
AU Lu, L
   Zhang, P
   Zhang, TT
AF Lu, Lu
   Zhang, Pei
   Zhang, Tingting (Christina)
TI Leveraging "<i>human</i>-<i>likeness</i>" of robotic service at
   restaurants
SO INTERNATIONAL JOURNAL OF HOSPITALITY MANAGEMENT
LA English
DT Article
DE Service robot; Human-likeness; Physical appearance; Voice; Language;
   Restaurant
ID UNCANNY VALLEY; BEHAVIORAL INTENTIONS; SOCIAL ROBOTS; ANTHROPOMORPHISM;
   ACCEPTANCE; EXPERIENCES; APPEARANCE; QUALITY; STYLES
AB Despite the rise of human-robot interaction research, the mixed findings of human-likeness in consumer evaluation exist. Focusing on the restaurant sector, this research investigates how service robots' varying levels of human-likeness of attributes (i.e., visual, vocal and verbal) influence consumption outcomes (e.g., service encounter evaluation, revisit intentions and positive word of mouth intentions) and the underlying mechanisms through cognition (i.e., perceived credibility) and positive emotion per Appraisal Theory. Drawing on a consumer experiment involving a total of 587 participants, results suggest that humanlike voice emerges as a dominant attribute affecting all three consumption outcomes. Humanlike language style positively affects service encounter evaluation but barely affects the other two outcomes. The significant effect of humanlike voice on three consumption outcomes is only explained by positive emotion whereas the effect of humanlike language style on service encounter evaluation is explained by both cognition (i.e., perceived credibility) and emotion.
C1 [Lu, Lu] Temple Univ, Sch Sport Tourism & Hospitality Management, 1810 N 13th St,Speakman Hall 329, Philadelphia, PA 19122 USA.
   [Zhang, Pei] Univ Kentucky, Dept Retailing & Tourism Management, Coll Agr Food & Environm, Erikson Hall, Lexington, KY 40506 USA.
   [Zhang, Tingting (Christina)] Univ Cent Florida, Rosen Coll Hospitality Management, 9907 Universal Blvd, Orlando, FL 32821 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple
   University; University of Kentucky; State University System of Florida;
   University of Central Florida
RP Lu, L (corresponding author), Temple Univ, Sch Sport Tourism & Hospitality Management, 1810 N 13th St,Speakman Hall 329, Philadelphia, PA 19122 USA.
EM lu.lu0001@temple.edu; pei.zhang@uky.edu; tingting.zhang@ucf.edu
RI Lu, Lu/AAR-8010-2021
OI Lu, Lu/0000-0001-9852-3560
CR [Anonymous], 1995, J SERV MARK
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   Bartsch S., 2008, CURRENT RES QUESTION, P45
   Belk R, 2016, RECH APPL MARKET-ENG, V31, P83, DOI 10.1177/2051570716658467
   Berezina K, 2019, ROBOTS, ARTIFICIAL INTELLIGENCE, AND SERVICE AUTOMATION IN TRAVEL, TOURISM AND HOSPITALITY, P185, DOI 10.1108/978-1-78756-687-320191010
   Bischoff R., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P999, DOI 10.1109/ICSMC.1999.825399
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Burgers A, 2000, INT J SERV IND MANAG, V11, P142, DOI 10.1108/09564230010323642
   Choi S, 2019, INT J HOSP MANAG, V82, P32, DOI 10.1016/j.ijhm.2019.03.026
   Choi Y, 2021, CURR ISSUES TOUR, V24, P717, DOI 10.1080/13683500.2020.1735318
   Conti D, 2017, INT J SOC ROBOT, V9, P51, DOI 10.1007/s12369-016-0359-6
   Dautenhahn K., 2014, The Encyclopedia of Human -Computer Interaction, V2
   Duffy BR, 2003, ROBOT AUTON SYST, V42, P177, DOI 10.1016/S0921-8890(02)00374-3
   Edwards C, 2019, COMPUT HUM BEHAV, V90, P357, DOI 10.1016/j.chb.2018.08.027
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   Eyssel F., 2011, 2011 RO-MAN: The 20th IEEE International Symposium on Robot and Human Interactive Communication, P467, DOI 10.1109/ROMAN.2011.6005233
   Eyssel F, 2012, ACMIEEE INT CONF HUM, P125
   Ferrari F, 2016, INT J SOC ROBOT, V8, P287, DOI 10.1007/s12369-016-0338-y
   Fink Julia, 2012, Social Robotics. 4th International Conference (ICSR 2012). Proceedings, P199, DOI 10.1007/978-3-642-34103-8_20
   Fong T, 2003, ROBOT AUTON SYST, V42, P143, DOI 10.1016/S0921-8890(02)00372-X
   Gilbody-Dickerson C., 2019, RESTAURANT WAITERS H
   Gockley R., 2006, 1st Annual Conference on Human-Robot Interaction, P186
   Goetz J, 2003, RO-MAN 2003: 12TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P55
   Goudey A, 2016, RECH APPL MARKET-ENG, V31, P2, DOI 10.1177/2051570716643961
   Graefe A., 2016, Guide to Automated Journalism, DOI DOI 10.7916/D80G3XDJ
   Hebesberger D, 2017, INT J SOC ROBOT, V9, P417, DOI 10.1007/s12369-016-0391-6
   Ho TH, 2020, INT J HOSP MANAG, V87, DOI 10.1016/j.ijhm.2020.102501
   Hosany S, 2012, J TRAVEL RES, V51, P303, DOI 10.1177/0047287511410320
   Hutchinson J, 2009, TOURISM MANAGE, V30, P298, DOI 10.1016/j.tourman.2008.07.010
   Ivanov S., 2017, Revista Turismo & Desenvolvimento, P1501
   Jensen ML, 2013, J MANAGE INFORM SYST, V30, P293, DOI 10.2753/MIS0742-1222300109
   Kim SY, 2019, MARKET LETT, V30, P1, DOI 10.1007/s11002-019-09485-9
   Larivière B, 2017, J BUS RES, V79, P238, DOI 10.1016/j.jbusres.2017.03.008
   LAZARUS RS, 1991, AM PSYCHOL, V46, P352, DOI 10.1037/0003-066X.46.4.352
   Lee KM, 2005, HUM COMMUN RES, V31, P538, DOI 10.1111/j.1468-2958.2005.tb00882.x
   Lee M, 2019, INT J CONTEMP HOSP M, V31, P4313, DOI 10.1108/IJCHM-03-2018-0263
   Lin HX, 2020, J HOSP MARKET MANAG, V29, P530, DOI 10.1080/19368623.2020.1685053
   Lu L, 2019, INT J HOSP MANAG, V80, P36, DOI 10.1016/j.ijhm.2019.01.005
   Luna N., 2020, IS PENNY ROBOT FOOD
   Luo XM, 2019, MARKET SCI, V38, P937, DOI 10.1287/mksc.2019.1192
   Mathur M. B., 2009, 2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI), P313
   Matthews K, 2020, PANDEMIC PROVES UTIL
   Mende M, 2019, J MARKETING RES, V56, P535, DOI 10.1177/0022243718822827
   Montoya RM, 2017, PSYCHOL BULL, V143, P459, DOI 10.1037/bul0000085
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Murphy J, 2019, J TRAVEL TOUR MARK, V36, P784, DOI 10.1080/10548408.2019.1571983
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Nickson Dennis, 2005, Managing Service Quality, V15, P195
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Perfect TJ, 2002, APPL COGNITIVE PSYCH, V16, P973, DOI 10.1002/acp.920
   Phillips E, 2018, ACMIEEE INT CONF HUM, P105, DOI 10.1145/3171221.3171268
   Poliakoff E, 2013, PERCEPTION, V42, P998, DOI 10.1068/p7569
   Pouliot L., 2016, ROBOTS RESTAURANTS F
   Prakash A, 2015, INT J SOC ROBOT, V7, P309, DOI 10.1007/s12369-014-0269-4
   Prentice C, 2020, J HOSP MARKET MANAG, V29, P739, DOI 10.1080/19368623.2020.1722304
   Qiu HL, 2020, J HOSP MARKET MANAG, V29, P247, DOI 10.1080/19368623.2019.1645073
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Roseman Ira J., 2001, Appraisal Processes in Emotion: Theory, Methods, Research, P68
   Ryu K. S., 2007, Journal of Hospitality & Tourism Research, V31, P56, DOI 10.1177/1096348006295506
   Seiter JS, 2020, INT J HOSP MANAG, V84, DOI 10.1016/j.ijhm.2019.102320
   Shnabel N, 2013, PERS SOC PSYCHOL B, V39, P663, DOI 10.1177/0146167213480816
   Singer, 2016, ARTIFICIAL INTELLIGE
   SMITH CA, 1993, COGNITION EMOTION, V7, P233, DOI 10.1080/02699939308409189
   Sugiura K, 2015, ADV ROBOTICS, V29, P449, DOI 10.1080/01691864.2015.1009164
   SURPRENANT CF, 1987, J MARKETING, V51, P86, DOI 10.2307/1251131
   Tamagawa R, 2011, INT J SOC ROBOT, V3, P253, DOI 10.1007/s12369-011-0100-4
   Trovato G, 2015, 2015 24TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P235, DOI 10.1109/ROMAN.2015.7333573
   Tsaur SH, 2015, INT J HOSP MANAG, V51, P115, DOI 10.1016/j.ijhm.2015.08.015
   Tung VWS, 2017, INT J CONTEMP HOSP M, V29, P2498, DOI 10.1108/IJCHM-09-2016-0520
   Tussyadiah I, 2020, ANN TOURISM RES, V81, DOI 10.1016/j.annals.2020.102883
   Tussyadiah SP, 2018, TOURISM MANAGE, V67, P261, DOI 10.1016/j.tourman.2018.02.002
   van Pinxteren MME, 2019, J SERV MARK, V33, P507, DOI 10.1108/JSM-01-2018-0045
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Walters ML, 2008, AUTON ROBOT, V24, P159, DOI 10.1007/s10514-007-9058-3
   Wan J., 2015, STRONG BRANDS STRONG, P119, DOI DOI 10.4324/9781315767079
   Webster C., 2017, ADOPTION ROBOTS ARTI
   Wirtz J, 2018, J SERV MANAGE, V29, P907, DOI 10.1108/JOSM-04-2018-0119
   Yamada Y, 2013, JPN PSYCHOL RES, V55, P20, DOI 10.1111/j.1468-5884.2012.00538.x
   Youngtaek Kim, 2011, 2011 International Symposium on Low Power Electronics and Design (ISLPED 2011), P253, DOI 10.1109/ISLPED.2011.5993645
   Yu CE, 2020, J HOSP MARKET MANAG, V29, P22, DOI 10.1080/19368623.2019.1592733
   Zeng ZJ, 2020, TOURISM GEOGR, V22, P724, DOI 10.1080/14616688.2020.1762118
   Zhang T, 2008, IEEE INT CON AUTO SC, P674, DOI 10.1109/COASE.2008.4626532
   Zhu DH, 2020, INT J CONTEMP HOSP M, V32, P1367, DOI 10.1108/IJCHM-10-2019-0904
   Zlotowski J, 2015, INT J SOC ROBOT, V7, P347, DOI 10.1007/s12369-014-0267-6
NR 84
TC 72
Z9 74
U1 53
U2 224
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0278-4319
EI 1873-4693
J9 INT J HOSP MANAG
JI Int. J. Hosp. Manag.
PD APR
PY 2021
VL 94
AR 102823
DI 10.1016/j.ijhm.2020.102823
PG 9
WC Hospitality, Leisure, Sport & Tourism
WE Social Science Citation Index (SSCI)
SC Social Sciences - Other Topics
GA RA2OQ
UT WOS:000631257900026
DA 2024-01-09
ER

PT J
AU Hu, P
   Lu, YB
   Gong, YM
AF Hu, Peng
   Lu, Yaobin
   Gong, Yeming (Yale)
TI Dual humanness and trust in conversational AI: A person-centered
   approach
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Artificial intelligence; Humanness perception; Trust; Person-centered
   approach; Finite mixture modeling
ID EXPLORATORY FACTOR-ANALYSIS; LATENT PROFILE ANALYSIS; SOCIAL RESPONSES;
   ANTHROPOMORPHISM; MODELS; ROBOTS; INFORMATION; VARIABLES; IDENTITY;
   ALEXA
AB Conversational Artificial Intelligence (AI) is digital agents that interact with users by natural language. To advance the understanding of trust in conversational AI, this study focused on two humanness factors manifested by conversational AI: speaking and listening. First, we explored users? heterogeneous perception patterns based on the two humanness factors. Next, we examined how this heterogeneity relates to trust in conversational AI. A two-stage survey was conducted to collect data. Latent profile analysis revealed three distinct patterns: parahuman perception, para-machine perception, and asymmetric perception. Finite mixture modeling demonstrated that the benefit of humanizing AI?s voice for competence-related trust can evaporate once AI?s language understanding is perceived as poor. Interestingly, the asymmetry between humanness perceptions in speaking and listening can impede morality-related trust. By adopting a person-centered approach to address the relationship between dual humanness and user trust, this study contributes to the literature on trust in conversational AI and the practice of trust-inducing AI design.
C1 [Hu, Peng; Lu, Yaobin] Huazhong Univ Sci & Technol, Sch Management, Luoyu Rd 1037, Wuhan, Peoples R China.
   [Gong, Yeming (Yale)] EMLYON Business Sch, Business Intelligence Ctr, Yon, France.
C3 Huazhong University of Science & Technology; emlyon business school
RP Lu, YB (corresponding author), Huazhong Univ Sci & Technol, Sch Management, Luoyu Rd 1037, Wuhan, Peoples R China.
EM hpeng@hust.edu.cn; luyb@hust.edu.cn; gong@em-lyon.com
RI GONG, Yeming/I-7148-2012; Hu, Peng/HQZ-8723-2023
OI GONG, Yeming/0000-0001-9270-5507; Hu, Peng/0000-0003-1624-466X
FU National Natural Science Foundation of China [71810107003]; National
   Social Science Fund of China [18ZDA109]; Business Intelligence Center of
   EMLYON; Modern Information Management Research Center at HUST
FX This work was supported by the grants from National Natural Science
   Foundation of China (Project No. 71810107003) and National Social
   Science Fund of China (Project No. 18ZDA109) . This work was also
   supported by Business Intelligence Center of EMLYON and Modern
   Information Management Research Center at HUST.
CR Alashoor T. M., 2017, P 38 INT C INF SYST
   [Anonymous], COMMUNICATION Q
   Asparouhov T, 2014, STRUCT EQU MODELING, V21, P329, DOI 10.1080/10705511.2014.915181
   Bakk Z, 2016, STRUCT EQU MODELING, V23, P20, DOI 10.1080/10705511.2014.955104
   Braun M, 2019, J MULTIMODAL USER IN, V13, P71, DOI 10.1007/s12193-019-00301-2
   Califf CB, 2020, TECHNOL FORECAST SOC, V154, DOI 10.1016/j.techfore.2020.119968
   Chang RCS, 2018, COMPUT HUM BEHAV, V84, P194, DOI 10.1016/j.chb.2018.02.025
   Cheng HF, 2014, ELECTRON COMMER R A, V13, P1, DOI 10.1016/j.elerap.2013.07.002
   Cho E, 2019, CYBERPSYCH BEH SOC N, V22, P515, DOI 10.1089/cyber.2018.0571
   Culley KE, 2013, COMPUT HUM BEHAV, V29, P577, DOI 10.1016/j.chb.2012.11.023
   de Kleijn R, 2019, KNOWL-BASED SYST, V163, P794, DOI 10.1016/j.knosys.2018.10.006
   de Visser EJ, 2016, J EXP PSYCHOL-APPL, V22, P331, DOI 10.1037/xap0000092
   Demetis DS, 2018, J ASSOC INF SYST, V19, P929, DOI 10.17705/1jais.00514
   Dietvorst BJ, 2018, MANAGE SCI, V64, P1155, DOI 10.1287/mnsc.2016.2643
   Edwards C, 2019, COMPUT HUM BEHAV, V90, P357, DOI 10.1016/j.chb.2018.08.027
   Evermann J, 2011, J ASSOC INF SYST, V12, P632
   Fabrigar LR, 1999, PSYCHOL METHODS, V4, P272, DOI 10.1037/1082-989X.4.3.272
   Faraon M., 2019, P 2019 2 ART INT CLO
   Foehr J, 2020, J ASSOC CONSUM RES, V5, P181, DOI 10.1086/707731
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Gabriel AS, 2015, J APPL PSYCHOL, V100, P863, DOI 10.1037/a0037408
   Gefen D, 2003, MIS QUART, V27, P51, DOI 10.2307/30036519
   Gillath O, 2021, COMPUT HUM BEHAV, V115, DOI 10.1016/j.chb.2020.106607
   Go E, 2019, COMPUT HUM BEHAV, V97, P304, DOI 10.1016/j.chb.2019.01.020
   Gursoy D, 2019, INT J INFORM MANAGE, V49, P157, DOI 10.1016/j.ijinfomgt.2019.03.008
   Haas J.W., 1995, J BUS COMMUN, V32, P123, DOI DOI 10.1177/002194369503200202
   Henson RK, 2006, EDUC PSYCHOL MEAS, V66, P393, DOI 10.1177/0013164405282485
   HINKIN TR, 1995, J MANAGE, V21, P967, DOI 10.1177/014920639502100509
   Howard MC, 2018, ORGAN RES METHODS, V21, P846, DOI 10.1177/1094428117744021
   Khatri C, 2018, AI MAG, V39, P40, DOI 10.1609/aimag.v39i3.2810
   Klaus P, 2020, J SERV MARK, V34, P389, DOI 10.1108/JSM-01-2019-0043
   Lankton NK, 2015, J ASSOC INF SYST, V16, P880, DOI 10.17705/1jais.00411
   Lanza ST, 2013, STRUCT EQU MODELING, V20, P1, DOI 10.1080/10705511.2013.742377
   Lee KM, 2006, J COMMUN, V56, P754, DOI 10.1111/j.1460-2466.2006.00318.x
   Lee O. K. D, 2017, P 23 AM C INF SYST
   Lortie CL, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025085
   MacKenzie SB, 2011, MIS QUART, V35, P293
   Maedche A, 2018, P EUR C INF SYST ECI
   Marsh HW, 2009, STRUCT EQU MODELING, V16, P191, DOI 10.1080/10705510902751010
   Martin T, 2019, 73 DONT TRUST CONVER
   MAYER RC, 1995, ACAD MANAGE REV, V20, P709, DOI 10.2307/258792
   McKnight DH, 2002, INFORM SYST RES, V13, P334, DOI 10.1287/isre.13.3.334.81
   McKone E, 2001, J EXP PSYCHOL HUMAN, V27, P573, DOI 10.1037/0096-1523.27.3.573
   Meade AW, 2012, PSYCHOL METHODS, V17, P437, DOI 10.1037/a0028085
   Meyer JP, 2013, HUM RESOUR MANAGE R, V23, P190, DOI 10.1016/j.hrmr.2012.07.007
   Morin AJS, 2011, ORGAN RES METHODS, V14, P58, DOI 10.1177/1094428109356476
   Nass C, 2000, J SOC ISSUES, V56, P81, DOI 10.1111/0022-4537.00153
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Nylund KL, 2007, STRUCT EQU MODELING, V14, P535, DOI 10.1080/10705510701575396
   Nylund-Gibson K, 2014, STRUCT EQU MODELING, V21, P439, DOI 10.1080/10705511.2014.915375
   Olson P., 2019, NEW REPORT TACKLES T
   Peugh J, 2013, STRUCT EQU MODELING, V20, P616, DOI 10.1080/10705511.2013.824780
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Rhee CE, 2020, COMPUT HUM BEHAV, V109, DOI [10.1016/j.chb.2020.102659, 10.1016/j.chb.2020.106359]
   ROTTER JB, 1971, AM PSYCHOL, V26, P443, DOI 10.1037/h0031464
   Santos R, 2020, MULTIMED TOOLS APPL, V79, P35689, DOI 10.1007/s11042-020-08710-2
   Schuetz S, 2020, J ASSOC INF SYST, V21, P460, DOI 10.17705/1jais.00608
   Schuetzler RM, 2020, J MANAGE INFORM SYST, V37, P875, DOI 10.1080/07421222.2020.1790204
   Schwartz E. H., 2020, KIDS DONT TRUST VOIC
   Sheehan B, 2020, J BUS RES, V115, P14, DOI 10.1016/j.jbusres.2020.04.030
   Shin M, 2019, COMPUT HUM BEHAV, V94, P100, DOI 10.1016/j.chb.2019.01.016
   Silva AD, 2020, EXPERT SYST APPL, V147, DOI 10.1016/j.eswa.2020.113193
   Sonpar K, 2009, J BUS ETHICS, V90, P345, DOI 10.1007/s10551-009-0045-9
   Specht J, 2014, J PERS SOC PSYCHOL, V107, P540, DOI 10.1037/a0036863
   Srivastava SC, 2018, MIS QUART, V42, P779, DOI 10.25300/MISQ/2018/11914
   Strohmann T., 2019, AIS Transactions on Human-Computer Interaction, V11, P54, DOI 10.17705/1thci.00113
   Tamagawa R, 2011, INT J SOC ROBOT, V3, P253, DOI 10.1007/s12369-011-0100-4
   Torre I, 2020, COMPUT HUM BEHAV, V105, DOI 10.1016/j.chb.2019.106215
   Wang WQ, 2016, J MANAGE INFORM SYST, V33, P744, DOI 10.1080/07421222.2016.1243949
   Waytz A, 2014, J EXP SOC PSYCHOL, V52, P113, DOI 10.1016/j.jesp.2014.01.005
   Westerman D, 2020, COMMUN STUD, V71, P393, DOI 10.1080/10510974.2020.1749683
   Westerman D, 2019, COMMUN STUD, V70, P295, DOI 10.1080/10510974.2018.1557233
   Wiese E, 2020, INT J HUM-COMPUT ST, V133, P1, DOI 10.1016/j.ijhcs.2019.08.002
   Wise AF, 2019, COMPUT HUM BEHAV, V96, P273, DOI 10.1016/j.chb.2018.01.034
   Woo SE, 2018, ORGAN RES METHODS, V21, P814, DOI 10.1177/1094428117752467
   Xie H, 2020, INT J HUM-COMPUT INT, V36, P1095, DOI 10.1080/10447318.2020.1712061
   Xu K, 2019, NEW MEDIA SOC, V21, P2522, DOI 10.1177/1461444819851479
   Yang SQ, 2018, COMPUT HUM BEHAV, V89, P16, DOI 10.1016/j.chb.2018.07.032
   Zyphur MJ, 2009, ACAD MANAGE REV, V34, P677, DOI 10.5465/AMR.2009.44885862
NR 79
TC 25
Z9 25
U1 34
U2 187
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD JUN
PY 2021
VL 119
AR 106727
DI 10.1016/j.chb.2021.106727
EA FEB 2021
PG 18
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA RC2XB
UT WOS:000632667000002
OA Green Published
DA 2024-01-09
ER

PT J
AU Euler, HA
   Merkel, A
   Hente, K
   Neef, N
   von Gudenberg, AW
   Neumann, K
AF Euler, Harald A.
   Merkel, Anna
   Hente, Katja
   Neef, Nicole
   von Gudenberg, Alexander Wolff
   Neumann, Katrin
TI Speech restructuring group treatment for 6-to-9-year-old children who
   stutter: A therapeutic trial
SO JOURNAL OF COMMUNICATION DISORDERS
LA English
DT Article
DE Stuttering; Treatment; Effectiveness; Speech restructuring; Fluency
   shaping; Children
ID SCHOOL-AGE-CHILDREN; PHASE-II TRIAL; LIDCOMBE-PROGRAM; LONG-TERM;
   GRAMMATICAL COMPLEXITY; ADOLESCENTS; LENGTH; OUTCOMES; QUALITY; IMPACT
AB For children who stutter (CWS), there is good evidence of the benefits of treatment for pre-school age, but an evidence gap for elementary school age. Here we report on the effectiveness of a fluency shaping treatment for 6to 9-year-old children. The main treatment component is the reinforcement of soft voice onsets. An intensive in-patient group treatment phase lasts 6 days, followed by a 6-month maintenance phase with 3 in-patient weekend group refresher courses. Child and a parent participate together in various treatment activities. In this controlled intervention study (waitlist control, intention-to-treat design) assessments were performed before treatment (T1), 4 weeks after the intensive phase (T2), at the end of the maintenance phase (T3), and 1 year later (T4). Participants were 119 children (108 boys, 11 girls, age 5.5-10.4 years). Control conditions included a subgroup with delayed treatment (N=25) as well as the assessment of complexity of utterances, inter-rater reliability, and speech naturalness. From before treatment to 1-year follow-up, percent stuttered syllables and OASES-S (Overall Assessment of the Speaker's Experience with Stuttering School-age) scores decreased with large effect size. Speech naturalness improved during this period but did not reach the level of non-stuttering children. Complexity of utterances increased during the intensive phase, but only temporarily. Twenty children (16.8 %, including dropouts) showed no demonstrable treatment benefit. Fluency shaping treatment can be effectively applied to young school children. It is assumed that parental support, group therapy, intensive treatment, and regular exercises at home are essential.
C1 [Euler, Harald A.; Neumann, Katrin] Westphalian Wilhelms Univ Munster, Univ Hosp Munster, Dept Phoniatr & Pediat Audiol, Kardinal von Galen Ring 10, D-48149 Munster, Germany.
   [Merkel, Anna; Hente, Katja; von Gudenberg, Alexander Wolff] Inst Kassel Stuttering Therapy, Feriendorfstr 1, D-34208 Bad Emstal, Germany.
   [Neef, Nicole] Georg August Univ, Dept Diagnost & Intervent Neuroradiol, Robert Koch Str 40, D-37075 Gottingen, Germany.
C3 University of Munster; University of Gottingen
RP Euler, HA (corresponding author), Westphalian Wilhelms Univ Munster, Univ Hosp Munster, Dept Phoniatr & Pediat Audiol, Kardinal von Galen Ring 10, D-48149 Munster, Germany.
EM euler@uni-kassel.de; anna.merkel@kasseler-stottertherapie.de;
   katja.hente@kasseler-stottertherapie.de; nneef@gwdg.de;
   awvgudenberg@kasseler-stottertherapie.de; Katrin.Neumann@uni-muenster.de
RI Neef, Nicole E./F-4103-2018
OI Neef, Nicole E./0000-0003-2414-7595
CR Andrews C, 2016, J FLUENCY DISORD, V48, P44, DOI 10.1016/j.jfludis.2016.06.001
   Andrews C, 2012, LANG SPEECH HEAR SER, V43, P359, DOI 10.1044/0161-1461(2012/11-0038)
   [Anonymous], Public Health, DOI DOI 10.1016/S0033-3506(02)00027-6
   [Anonymous], 2011, OXFORD 2011 LEVELS E
   Bakker K., 2009, COMPUTERIZED SCORING
   Block S, 2005, INT J LANG COMM DIS, V40, P455, DOI 10.1080/03093640500088161
   Bloodstein O., 2008, A Handbook on Stuttering, V6th
   BUDD KS, 1986, BEHAV THER, V17, P538, DOI 10.1016/S0005-7894(86)80093-4
   Carey B, 2010, INT J LANG COMM DIS, V45, P108, DOI 10.3109/13682820902763944
   Cook S., 2013, LOGOS, V21, P97
   Craig A, 1996, J SPEECH HEAR RES, V39, P808, DOI 10.1044/jshr.3904.808
   de Sonneville-Koedoot C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133758
   Downs SH, 1998, J EPIDEMIOL COMMUN H, V52, P377, DOI 10.1136/jech.52.6.377
   Dunlap WP, 1996, PSYCHOL METHODS, V1, P170, DOI 10.1037/1082-989X.1.2.170
   Euler HA, 2009, SPRACHE-STIMME-GEHOR, V33, P193, DOI 10.1055/s-0029-1242747
   Euler H. A., 2016, LOGOS, V24, P84, DOI 10.7345/prolog-1602084.
   Euler HA, 2014, J FLUENCY DISORD, V39, P1, DOI 10.1016/j.jfludis.2014.01.002
   FESTINGER L, 1961, AM PSYCHOL, V16, P1, DOI 10.1037/h0045112
   Franken MCJ, 2005, J FLUENCY DISORD, V30, P189, DOI 10.1016/j.jfludis.2005.05.002
   Harris V, 2002, J FLUENCY DISORD, V27, P203, DOI 10.1016/S0094-730X(02)00127-4
   Harrison E, 2004, INT J LANG COMM DIS, V39, P257, DOI 10.1080/13682820310001644551
   Haynes B, 1999, BRIT MED J, V319, P652, DOI 10.1136/bmj.319.7211.652
   HUNT KW, 1970, MONOGR SOC RES CHILD, V35, P1
   Jones M, 2008, INT J LANG COMM DIS, V43, P649, DOI 10.1080/13682820801895599
   Kalinowski J, 2005, INT J LANG COMM DIS, V40, P349, DOI 10.1080/13693780400027779
   Karimi H., 2013, J SPEECH LANG HEAR R, V56
   Keilmann A, 2018, LOGOP PHONIATR VOCO, V43, P155, DOI 10.1080/14015439.2018.1498917
   Koushik S, 2009, J FLUENCY DISORD, V34, P279, DOI 10.1016/j.jfludis.2009.11.001
   Laiho A, 2007, INT J LANG COMM DIS, V42, P367, DOI 10.1080/13682820600939028
   Langevin M, 2006, J FLUENCY DISORD, V31, P229, DOI 10.1016/j.jfludis.2006.06.001
   Langevin M, 2010, J FLUENCY DISORD, V35, P123, DOI 10.1016/j.jfludis.2010.04.002
   Lattermann C, 2008, J FLUENCY DISORD, V33, P52, DOI 10.1016/j.jfludis.2007.12.002
   Lewis C, 2008, AM J SPEECH-LANG PAT, V17, P139, DOI 10.1044/1058-0360(2008/014)
   LOGAN KJ, 1995, J FLUENCY DISORD, V20, P35, DOI 10.1016/0094-730X(94)00008-H
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   Melnick KS, 2000, J FLUENCY DISORD, V25, P21, DOI 10.1016/S0094-730X(99)00028-5
   Metten C, 2007, SPRACHE-STIMME-GEHOR, V31, P72, DOI 10.1055/s-2007-958631
   Millard SK, 2018, AM J SPEECH-LANG PAT, V27, P1211, DOI 10.1044/2018_AJSLP-ODC11-17-0199
   Neumann K., 2016, Pathogenesis, diagnostic and treatment of speech fluency disorders. Evidence- and consensus-based S3-guidelines
   Neumann K, 2017, DTSCH ARZTEBL INT, V114, P383, DOI 10.3238/arztebl.2017.0383
   Nippold MA, 2008, AM J SPEECH-LANG PAT, V17, P356, DOI 10.1044/1058-0360(2008/07-0049)
   Nippold MA, 2012, LANG SPEECH HEAR SER, V43, P549, DOI 10.1044/0161-1461(2012/12-0054)
   Nippold MA, 2012, LANG SPEECH HEAR SER, V43, P338, DOI 10.1044/0161-1461(2012/12-0035)
   Nippold MA, 2011, LANG SPEECH HEAR SER, V42, P99, DOI 10.1044/0161-1461(2011/ed-02)
   Nye C, 2013, J SPEECH LANG HEAR R, V56, P921, DOI 10.1044/1092-4388(2012/12-0036)
   Onslow M., 2003, The Lidcombe Program of Early Stuttering Intervention: A Clinician's Guide
   Riley G. D., 2009, SSI-4 stuttering severity instrument fourth edition, V4th
   Riley GD, 2000, J SPEECH LANG HEAR R, V43, P965, DOI 10.1044/jslhr.4304.965
   RYAN BP, 1995, J SPEECH HEAR RES, V38, P61, DOI 10.1044/jshr.3801.61
   Sawilowsky SS, 2009, J MOD APPL STAT METH, V8, P597, DOI 10.22237/jmasm/1257035100
   Senkal OA, 2018, HEARING BALANC COMMU, V16, P134, DOI 10.1080/21695717.2018.1484627
   Sprangers MAG, 1999, SOC SCI MED, V48, P1507, DOI 10.1016/S0277-9536(99)00045-3
   Webster R. L, 1980, PRECISION FLUENCY PR
   Wolff von, 2006, FORUM LOGOPADIE, V20, P24
   Wolff von Gudenberg A, 2000, SPRACHE STIMME GEHOR, V24, P71
   World Health Organization, 2018, INT CLASSIFICATION F
   Yaruss J. S, 2010, Overall Assessment of the Speaker's Experience of Stuttering: Manual
   Yaruss JS, 2006, J FLUENCY DISORD, V31, P90, DOI 10.1016/j.jfludis.2006.02.002
   Yaruss JS, 2012, LANG SPEECH HEAR SER, V43, P536, DOI 10.1044/0161-1461(2012/11-0044)
   Yaruss J. Scott, 2007, Seminars in Speech and Language, V28, P312, DOI 10.1055/s-2007-986528
   Yaruss JS, 1999, J SPEECH LANG HEAR R, V42, P329, DOI 10.1044/jslhr.4202.329
   Yaruss JS., 2014, OASES OVERALL ASSESS
NR 62
TC 3
Z9 4
U1 2
U2 13
PU ELSEVIER SCIENCE INC
PI NEW YORK
PA STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN 0021-9924
EI 1873-7994
J9 J COMMUN DISORD
JI J. Commun. Disord.
PD JAN-FEB
PY 2021
VL 89
AR 106073
DI 10.1016/j.jcomdis.2020.106073
EA JAN 2021
PG 12
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA QP4XM
UT WOS:000623840000001
PM 33444874
OA hybrid
DA 2024-01-09
ER

PT J
AU Kühne, K
   Fischer, MH
   Zhou, YF
AF Kuehne, Katharina
   Fischer, Martin H.
   Zhou, Yuefang
TI The Human Takes It All: Humanlike Synthesized Voices Are Perceived as
   Less Eerie and More Likable. Evidence From a Subjective Ratings Study
SO FRONTIERS IN NEUROROBOTICS
LA English
DT Article
DE human-robot interaction; paralinguistic features; synthesized voice;
   text-to-speech; uncanny valley
ID SOCIAL ROBOTS; INDIVIDUAL-DIFFERENCES; SPEECH SYNTHESIS; PERSONALITY;
   PERCEPTION; RECOGNITION; ATTRACTION; LIKABILITY; JUDGMENTS; ATTITUDES
AB Background: The increasing involvement of social robots in human lives raises the question as to how humans perceive social robots. Little is known about human perception of synthesized voices.
   Aim: To investigate which synthesized voice parameters predict the speaker's eeriness and voice likability; to determine if individual listener characteristics (e.g., personality, attitude toward robots, age) influence synthesized voice evaluations; and to explore which paralinguistic features subjectively distinguish humans from robots/artificial agents.
   Methods: 95 adults (62 females) listened to randomly presented audio-clips of three categories: synthesized (Watson, IBM), humanoid (robot Sophia, Hanson Robotics), and human voices (five clips/category). Voices were rated on intelligibility, prosody, trustworthiness, confidence, enthusiasm, pleasantness, human-likeness, likability, and naturalness. Speakers were rated on appeal, credibility, human-likeness, and eeriness. Participants' personality traits, attitudes to robots, and demographics were obtained.
   Results: The human voice and human speaker characteristics received reliably higher scores on all dimensions except for eeriness. Synthesized voice ratings were positively related to participants' agreeableness and neuroticism. Females rated synthesized voices more positively on most dimensions. Surprisingly, interest in social robots and attitudes toward robots played almost no role in voice evaluation. Contrary to the expectations of an uncanny valley, when the ratings of human-likeness for both the voice and the speaker characteristics were higher, they seemed less eerie to the participants. Moreover, when the speaker's voice was more humanlike, it was more liked by the participants. This latter point was only applicable to one of the synthesized voices. Finally, pleasantness and trustworthiness of the synthesized voice predicted the likability of the speaker's voice. Qualitative content analysis identified intonation, sound, emotion, and imageability/embodiment as diagnostic features.
   Discussion: Humans clearly prefer human voices, but manipulating diagnostic speech features might increase acceptance of synthesized voices and thereby support human-robot interaction. There is limited evidence that human-likeness of a voice is negatively linked to the perceived eeriness of the speaker.
C1 [Kuehne, Katharina; Fischer, Martin H.; Zhou, Yuefang] Univ Potsdam, Div Cognit Sci, Potsdam, Germany.
C3 University of Potsdam
RP Kühne, K (corresponding author), Univ Potsdam, Div Cognit Sci, Potsdam, Germany.
EM kkuehne@uni-potsdam.de
RI Fischer, Martin H./D-9965-2017
OI Kuehne, Katharina/0000-0002-1334-1563
CR [Anonymous], 1998, Handbook of Social Psychology
   Antona M, 2019, 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2019), P416, DOI 10.1145/3316782.3322777
   Aylett MP, 2020, ACMIEEE INT CONF HUM, P110, DOI 10.1145/3371382.3378330
   Aylett MP, 2020, IEEE T AFFECT COMPUT, V11, P361, DOI 10.1109/TAFFC.2017.2763134
   Aylett MP, 2019, PROCEEDINGS OF THE 1ST INTERNATIONAL CONFERENCE ON CONVERSATIONAL USER INTERFACES (CUI 2019), DOI 10.1145/3342775.3342806
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Bartneck C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P367
   Belpaeme T, 2018, INT J SOC ROBOT, V10, P325, DOI 10.1007/s12369-018-0467-6
   Bendel O., 2020, MENSCH ROBOTER KOLLA, P1, DOI [10.1007/978-3-658-28307-0_1, DOI 10.1007/978-3-658-28307-0_1]
   Bernier Emily P., 2010, 2010 IEEE 9th International Conference on Development and Learning (ICDL 2010), P286, DOI 10.1109/DEVLRN.2010.5578828
   Beskow J., 2019, P 10 SPEECH SYNTH WO, P105, DOI DOI 10.21437/SSW.2019-19
   Birkholz P, 2017, COMPUT SPEECH LANG, V41, P116, DOI 10.1016/j.csl.2016.06.004
   Bombelli Griselda, 2013, DELTA, V29, P267
   Braun M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300270
   Breazeal C. L., 2002, DESIGNING SOCIABLE R, DOI [10.1007/0-306-47373-9_18, DOI 10.1007/0-306-47373-9_18]
   Breazeal C, 2017, ACMIEEE INT CONF HUM, P1, DOI 10.1145/2909824.3020258
   Broadbent E, 2017, ANNU REV PSYCHOL, V68, P627, DOI 10.1146/annurev-psych-010416-043958
   Bryman A., 1994, ANAL QUALITATIVE DAT, DOI DOI 10.4324/9780203413081
   Bucholtz M, 2016, SOCIOLINGUISTICS: THEORETICAL DEBATES, P173
   Burgoon J. K., 2015, The International Encyclopedia of Interpersonal Communication, P1, DOI DOI 10.1002/9781118540190.WBEIC102
   Burgoon J. K., 1978, Hum. Commun. Res, V4, P129, DOI [10.1111/j.1468-2958.1978.tb00603.x, DOI 10.1111/J.1468-2958.1978.TB00603.X, 10.1111/j.1468-2958.1978]
   Burgoon JK, 2016, INT J HUM-COMPUT ST, V91, P24, DOI 10.1016/j.ijhcs.2016.02.002
   Burleigh TJ, 2015, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01488
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Castro-González A, 2016, INT J HUM-COMPUT ST, V90, P27, DOI 10.1016/j.ijhcs.2016.02.004
   Chang RCS, 2018, COMPUT HUM BEHAV, V84, P194, DOI 10.1016/j.chb.2018.02.025
   Coursey K., 2019, AI LOVE YOU, P77, DOI [10.1007/978-3-030-19734-6_4, DOI 10.1007/978-3-030-19734-6_4]
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   Danaher J, 2017, ROBOT SEX: SOCIAL AND ETHICAL IMPLICATIONS, P3
   Dennett D. C., 1971, Journal of Philosophy, V68, DOI 10.2307/2025382
   Dey I., 1993, QUALITATIVE DATA ANA
   Elkins AC, 2013, GROUP DECIS NEGOT, V22, P897, DOI 10.1007/s10726-012-9339-x
   Goy H, 2016, J ACOUST SOC AM, V139, P1648, DOI 10.1121/1.4945094
   Graziano WG, 2007, J PERS SOC PSYCHOL, V93, P565, DOI 10.1037/0022-3514.93.4.565
   Greenwald AG, 1998, J PERS SOC PSYCHOL, V74, P1464, DOI 10.1037/0022-3514.74.6.1464
   Hannuschke M, 2020, J PERS, V88, P217, DOI 10.1111/jopy.12480
   Harris R. A., 2004, VOICE INTERACTION DE
   Hayamizu Y, 2018, IEEE INT WORK TECH, P13
   Hinterleitner F., 2017, QUALITY SYNTHETIC SP, DOI [10.1007/978-981-10-3734-4_5, DOI 10.1007/978-981-10-3734-4_5]
   Hirai S., 2015, 14 INFORM SCI TECHNO, V2, P289
   Hodari Zack, 2019, 10 ISCA WORKSH SPEEC, P239
   Jaiswal J., 2019, AS C PATT REC, P580, DOI [10.1007/978-3-030-41299-9_45, DOI 10.1007/978-3-030-41299-9_45]
   Kätsyri J, 2019, PERCEPTION, V48, P968, DOI 10.1177/0301006619869134
   Kayte SN, 2016, ADV INTELL SYST, V439, P253, DOI 10.1007/978-981-10-0755-2_27
   Kenny D. A., 1994, Interpersonal perception: A social relations analysis
   Kim SY, 2019, MARKET LETT, V30, P1, DOI 10.1007/s11002-019-09485-9
   Kraus M, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P112
   Kuratate T., 2009, AVSP, P65
   Leiner D. J., 2018, SoSci Survey (Version 2.5.00-i1142
   Li Gong, 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P158, DOI 10.1145/365024.365090
   Liscombe J., 2003, P INT 2013 EUR
   MacDorman K. F., 2006, ICCS COGSCI 2006 LON, P26, DOI DOI 10.1093/SCAN/NSR025
   MacDorman KF, 2015, INTERACT STUD, V16, P141, DOI 10.1075/is.16.2.01mac
   Marchesi S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00450
   Massaro D. W., 2014, SPEECH PERCEPTION EA, DOI [10.4324/9781315808253, DOI 10.4324/9781315808253]
   Massaro D. W., 1987, SPEECH PERCEPTION EA
   Mayring P., 2014, Handbuch Methoden der empirischen Sozialforschung, P543
   McCrae RR, 1997, AM PSYCHOL, V52, P509, DOI 10.1037/0003-066X.52.5.509
   McCrae RR, 2005, J PERS SOC PSYCHOL, V88, P547, DOI 10.1037/0022-3514.88.3.547
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   Mendelson J, 2017, INTERSPEECH, P249, DOI 10.21437/Interspeech.2017-1438
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Nomura T, 2008, IEEE T ROBOT, V24, P442, DOI 10.1109/TRO.2007.914004
   Polkosky M. D., 2003, International Journal of Speech Technology, V6, P161, DOI 10.1023/A:1022390615396
   Rammstedt B, 2007, J RES PERS, V41, P203, DOI 10.1016/j.jrp.2006.02.001
   Rodero E, 2017, COMPUT HUM BEHAV, V77, P336, DOI 10.1016/j.chb.2017.08.044
   Romportl Jan, 2014, Text, Speech and Dialogue. 17th International Conference, TSD 2014. Proceedings: LNCS 8655, P595, DOI 10.1007/978-3-319-10816-2_72
   Rosenthal-von der Pütten AM, 2019, J NEUROSCI, V39, P6555, DOI 10.1523/JNEUROSCI.2956-18.2019
   Salza PL, 1996, ACUSTICA, V82, P650
   Schmidt-Nielsen A., 1995, P 1995 IEEE WORKSH S, P5, DOI [10.1109/SCFT.1995.658104, DOI 10.1109/SCFT.1995.658104]
   Schuller B, 2013, COMPUT SPEECH LANG, V27, P4, DOI 10.1016/j.csl.2012.02.005
   Scott K, 2020, REFERRING EXPRESSIONS, PRAGMATICS AND STYLE: REFERENCE AND BEYOND, P1
   Seymour M., 2017, HAW INT C SYST SCI 2
   Sims Valerie K., 2009, P HUMAN FACTORS ERGO, V53, P1418, DOI 10.1177/154193120905301853
   Stern SE, 1999, HUM FACTORS, V41, P588, DOI 10.1518/001872099779656680
   Strait M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3593, DOI 10.1145/2702123.2702415
   Thepsoonthorn C, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-25314-x
   Thomas David R, 2003, A general inductive approach for qualitative data analysis
   Torre I, 2018, PROCEEDINGS OF THE TECHNOLOGY, MIND, AND SOCIETY CONFERENCE (TECHMINDSOCIETY'18), DOI 10.1145/3183654.3183691
   Tschöpe N, 2017, ACMIEEE INT CONF HUM, P307, DOI 10.1145/3029798.3038319
   Tsiourti C, 2019, INT J SOC ROBOT, V11, P555, DOI 10.1007/s12369-019-00524-z
   Tyagi S., 2019, ARXIV191200955
   Uhrig S, 2017, QUAL USER EXP, V2, P10, DOI [DOI 10.1007/S41233-017-0011-8, 10.1007/s41233-017-0011-8]
   Vallee M, 2017, BODY SOC, V23, P83, DOI 10.1177/1357034X17697366
   Velner E, 2020, ACMIEEE INT CONF HUM, P569, DOI 10.1145/3319502.3374801
   Wasala K, 2020, PROCEEDINGS OF THE 31ST AUSTRALIAN CONFERENCE ON HUMAN-COMPUTER-INTERACTION (OZCHI'19), P503, DOI 10.1145/3369457.3369542
   Westlund JMK, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00295
   Westlund JMK, 2016, IEEE ROMAN, P688, DOI 10.1109/ROMAN.2016.7745193
   Winquist LA, 1998, J RES PERS, V32, P370, DOI 10.1006/jrpe.1998.2221
   Wood D, 2010, J PERS SOC PSYCHOL, V99, P174, DOI 10.1037/a0019390
   Zhou Y, 2019, I COMP CONF WAVELET, P170, DOI 10.1109/ICCWAMTIP47768.2019.9067532
   Zotowski J. A., 2018, GEMINOID STUDIES, P163, DOI [10.1007/978-981-10-8702-8_10, DOI 10.1007/978-981-10-8702-8_10]
NR 96
TC 25
Z9 28
U1 6
U2 54
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 1662-5218
J9 FRONT NEUROROBOTICS
JI Front. Neurorobotics
PD DEC 16
PY 2020
VL 14
AR 593732
DI 10.3389/fnbot.2020.593732
PG 15
WC Computer Science, Artificial Intelligence; Robotics; Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Robotics; Neurosciences & Neurology
GA PL8OO
UT WOS:000603374200001
PM 33390923
OA gold, Green Published
DA 2024-01-09
ER

PT J
AU Merritt, B
   Bent, T
AF Merritt, Brandon
   Bent, Tessa
TI Perceptual Evaluation of Speech Naturalness in Speakers of Varying
   Gender Identities
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
ID SPEAKING FUNDAMENTAL-FREQUENCY; VOWEL FORMANT FREQUENCIES; MAGNITUDE
   ESTIMATION; VOICE PROBLEMS; FEMALE; SEX; IDENTIFICATION; TESTOSTERONE;
   INTONATION; THERAPY
AB Purpose: The purpose of this study was to investigate how speech naturalness relates to masculinity-femininity and gender identification (accuracy and reaction time) for cisgender male and female speakers as well as transmasculine and transfeminine speakers.
   Method: Stimuli included spontaneous speech samples from 20 speakers who are transgender (10 transmasculine and 10 transfeminine) and 20 speakers who are cisgender (10 male and 10 female). Fifty-two listeners completed three tasks: a two-alternative forced-choice gender identification task, a speech naturalness rating task, and a masculinity/femininity rating task.
   Results: Transfeminine and transmasculine speakers were rated as significantly less natural sounding than cisgender speakers. Speakers rated as less natural took longer to identify and were identified less accurately in the gender identification task; furthermore, they were rated as less prototypically masculine/feminine.
   Conclusions: Perceptual speech naturalness for both transfeminine and transmasculine speakers is strongly associated with gender cues in spontaneous speech. Training to align a speaker's voice with their gender identity may concurrently improve perceptual speech naturalness.
C1 [Merritt, Brandon; Bent, Tessa] Indiana Univ, Dept Speech Language & Hearing Sci, Bloomington, IN 47405 USA.
C3 Indiana University System; Indiana University Bloomington
RP Merritt, B (corresponding author), Indiana Univ, Dept Speech Language & Hearing Sci, Bloomington, IN 47405 USA.
EM bmmerrit@iu.edu
OI Merritt, Brandon/0000-0002-6166-0198
FU Department of Speech, Language, and Hearing Sciences at Indiana
   University
FX We would like to thank Karly Jones for assistance with data collection.
   We are also grateful to the Department of Speech, Language, and Hearing
   Sciences at Indiana University for providing funding in support of this
   research.
CR American National Standards Institute, 2010, S362010 ANSI
   Anand S, 2015, J SPEECH LANG HEAR R, V58, P1134, DOI 10.1044/2015_JSLHR-S-14-0243
   [Anonymous], 1993, Early Development and Parenting, DOI [DOI 10.1111/1467-9280.02435, DOI 10.1002/EDP.2430020405]
   [Anonymous], 2012, Voice and communication therapy for the transgender/transsexual client: A comprehensive clinical guide
   [Anonymous], WORLD ENGLISHES
   Azul D, 2019, J SPEECH LANG HEAR R, V62, P3320, DOI 10.1044/2019_JSLHR-S-19-0063
   Azul D, 2018, J SPEECH LANG HEAR R, V61, P25, DOI 10.1044/2017_JSLHR-S-16-0410
   Azul D, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.05.005
   Azur D., 2015, Perspectives on Voice and Voice Disorders, V25, P75, DOI [DOI 10.1044/VVD25.2.75, 10.1044/vvd25.2.75]
   Babel M, 2015, COGNITIVE SCI, V39, P766, DOI 10.1111/cogs.12179
   Boersma P., 2018, PRAAT DOING PHONETIC
   Braun V, 2012, RES DESIGNS QUANTITA, V2, P57, DOI 10.1037/13620-004
   Byrne L. A., 2007, THESIS
   Carew L, 2007, J VOICE, V21, P591, DOI 10.1016/j.jvoice.2006.05.005
   Cosyns M, 2014, LARYNGOSCOPE, V124, P1409, DOI 10.1002/lary.24480
   Coughlin-Woods S, 2005, PERCEPT MOTOR SKILL, V100, P295
   Couper-Kuhlen Elizabeth., 2015, HDB DISCOURSE ANAL, P82
   Davidson L, 2019, PHONETICA, V76, P235, DOI 10.1159/000490948
   Davies S, 2015, INT J TRANSGENDERISM, V16, P117, DOI 10.1080/15532739.2015.1075931
   Davies S, 2006, INT J TRANSGENDERISM, V9, P167, DOI 10.1300/J485v09n03_08
   Davis SA, 2014, INT J SEX HEALTH, V26, P113, DOI 10.1080/19317611.2013.833152
   de Jong NH, 2009, BEHAV RES METHODS, V41, P385, DOI 10.3758/BRM.41.2.385
   Descloux P, 2012, Rev Laryngol Otol Rhinol (Bord), V133, P41
   Eadie TL, 2002, J SPEECH LANG HEAR R, V45, P1088, DOI 10.1044/1092-4388(2002/087)
   Fant G., 1960, ACOUSTIC THEORY SPEE
   Gallena SJK, 2018, J VOICE, V32, P592, DOI 10.1016/j.jvoice.2017.06.023
   Gelfer MP, 2013, J VOICE, V27, P556, DOI 10.1016/j.jvoice.2012.11.008
   Gelfer MP, 2013, J VOICE, V27, P321, DOI 10.1016/j.jvoice.2012.07.008
   Gelfer MP, 2013, J VOICE, V27, P335, DOI 10.1016/j.jvoice.2012.07.009
   Gelfer MP, 2005, J VOICE, V19, P544, DOI 10.1016/j.jvoice.2004.10.006
   Gelfer MP, 2000, J VOICE, V14, P22, DOI 10.1016/S0892-1997(00)80092-2
   Gelfer MP, 1999, AM J SPEECH-LANG PAT, V8, P201, DOI 10.1044/1058-0360.0803.201
   Goldinger SD, 1998, PSYCHOL REV, V105, P251, DOI 10.1037/0033-295X.105.2.251
   Gooren LJ, 2015, CULT HEALTH SEX, V17, P92, DOI 10.1080/13691058.2014.950982
   Gorton RN, 2017, PSYCHIAT CLIN N AM, V40, P79, DOI 10.1016/j.psc.2016.10.005
   Hancock A, 2014, J VOICE, V28, P203, DOI 10.1016/j.jvoice.2013.08.009
   Hancock AB, 2017, J SPEECH LANG HEAR R, V60, P2472, DOI 10.1044/2017_JSLHR-S-16-0320
   Hancock AB, 2017, J LANG SOC PSYCHOL, V36, P599, DOI 10.1177/0261927X17704460
   Hancock AB, 2011, J VOICE, V25, P553, DOI 10.1016/j.jvoice.2010.07.013
   Hardy TLD, 2020, J VOICE, V34, DOI 10.1016/j.jvoice.2018.10.002
   Hardy TLD, 2016, AM J SPEECH-LANG PAT, V25, DOI 10.1044/2015_AJSLP-14-0098
   HENTON CG, 1989, LANG COMMUN, V9, P299, DOI 10.1016/0271-5309(89)90026-8
   HOLLIEN H, 1972, J SPEECH HEAR RES, V15, P155, DOI 10.1044/jshr.1501.155
   Holmberg EB, 2010, J VOICE, V24, P511, DOI 10.1016/j.jvoice.2009.02.002
   Houle N., 2019, JOURNAL OF VOICE ADV, DOI 10.1016/j.jvoice.2019.10.011
   INGHAM RJ, 1978, J SPEECH HEAR RES, V21, P63, DOI 10.1044/jshr.2101.63
   Irwig MS, 2017, ANDROLOGY-US, V5, P107, DOI 10.1111/andr.12278
   Junger J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111672
   Junger J, 2013, NEUROIMAGE, V79, P275, DOI 10.1016/j.neuroimage.2013.04.105
   Kawitzky D, 2020, J VOICE, V34, P53, DOI 10.1016/j.jvoice.2018.07.017
   Kent RD, 2018, J COMMUN DISORD, V74, P74, DOI 10.1016/j.jcomdis.2018.05.004
   King RS, 2012, INT J TRANSGENDERISM, V13, P117, DOI 10.1080/15532739.2011.664464
   Kleinschmidt DF, 2015, PSYCHOL REV, V122, P148, DOI 10.1037/a0038695
   Klopfenstein M, 2015, CLIN LINGUIST PHONET, V29, P938, DOI 10.3109/02699206.2015.1081293
   LANE HL, 1961, J ACOUST SOC AM, V33, P160, DOI 10.1121/1.1908608
   LASS NJ, 1976, J ACOUST SOC AM, V59, P675, DOI 10.1121/1.380917
   LASS NJ, 1980, J PHONETICS, V8, P101, DOI 10.1016/S0095-4470(19)31445-7
   Leung Y, 2018, J SPEECH LANG HEAR R, V61, P266, DOI 10.1044/2017_JSLHR-S-17-0067
   LIEBERMAN P, 1985, J ACOUST SOC AM, V77, P649, DOI 10.1121/1.391883
   Lotfian R., 2017, T AFFECTIVE COMPUTIN, V10, P471, DOI 10.1109/TAFFC.2017.
   Mackey LS, 1997, J SPEECH LANG HEAR R, V40, P349, DOI 10.1044/jslhr.4002.349
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   MILLER CL, 1982, INFANT BEHAV DEV, V5, P143, DOI 10.1016/S0163-6383(82)80024-6
   MOUNT KH, 1988, J COMMUN DISORD, V21, P229, DOI 10.1016/0021-9924(88)90031-7
   Munson B, 2007, LANG SPEECH, V50, P125, DOI 10.1177/00238309070500010601
   Nakamura M, 2008, COMPUT SPEECH LANG, V22, P171, DOI 10.1016/j.csl.2007.07.003
   Nygren U, 2016, J VOICE, V30, DOI 10.1016/j.jvoice.2015.10.016
   Oates J., 2015, Perspectives on Voice and Voice Disorders, V25, P48, DOI [DOI 10.1044/VVD25.2.48, https://doi.org/10.1044/vvd25.2.48, 10.1044/vvd25.2, DOI 10.1044/VVD25.2]
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   PETERSON GE, 1952, J ACOUST SOC AM, V24, P175, DOI 10.1121/1.1906875
   Pettit JM, 2004, MIT ENCY COMMUNICATI, P223
   Pierrehumbert JB, 2016, ANNU REV LINGUIST, V2, P33, DOI 10.1146/annurev-linguistics-030514-125050
   Remez R. E., 1985, J ACOUST SOC AM, V77, pS38, DOI DOI 10.1121/1.2022306
   Roenfeldt K., 2018, BETTER AVERAGE CALCU, P1
   Scheidt D., 2004, 26 WORLD C IALP BRIS
   Schilt K, 2009, GENDER SOC, V23, P440, DOI 10.1177/0891243209340034
   SCHWARTZ MF, 1968, J ACOUST SOC AM, V44, P1736, DOI 10.1121/1.1911324
   SCHWARTZ MF, 1968, J ACOUST SOC AM, V43, P1178, DOI 10.1121/1.1910954
   Schwarz K, 2018, J VOICE, V32, P602, DOI 10.1016/j.jvoice.2017.07.003
   Skuk VG, 2014, J SPEECH LANG HEAR R, V57, P285, DOI 10.1044/1092-4388(2013/12-0314)
   Smith E, 2018, HORM BEHAV, V105, P11, DOI 10.1016/j.yhbeh.2018.07.001
   Snow MP, 1998, HUM FACTORS, V40, P386, DOI 10.1518/001872098779591395
   Southwood M., 1996, J MED SPEECH-LANG PA, V4, P13
   Stevens S. S., 1975, Psychophysics
   STOICHEFF ML, 1981, J SPEECH HEAR RES, V24, P437, DOI 10.1044/jshr.2403.437
   Strand Elizabeth A., 2000, THESIS
   TALAAT M, 1987, ANN OTO RHINOL LARYN, V96, P468, DOI 10.1177/000348948709600423
   Van Borsel J, 2000, INT J LANG COMM DIS, V35, P427
   Van Borsel J, 2001, J VOICE, V15, P570, DOI 10.1016/S0892-1997(01)00059-5
   Van Borsel J, 2013, J VOICE, V27, DOI 10.1016/j.jvoice.2013.04.008
   van Son RJJH, 2005, ACTA ACUST UNITED AC, V91, P771
   Whelan R, 2008, PSYCHOL REC, V58, P475, DOI 10.1007/BF03395630
   WOLFE VI, 1990, J SPEECH HEAR DISORD, V55, P43, DOI 10.1044/jshd.5501.43
   Wong P, 2017, J SOCIOLING, V21, P603, DOI 10.1111/josl.12264
   Yoho SE, 2019, ATTEN PERCEPT PSYCHO, V81, P558, DOI 10.3758/s13414-018-1635-3
NR 95
TC 5
Z9 8
U1 1
U2 7
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 2200 RESEARCH BLVD, #271, ROCKVILLE, MD 20850-3289 USA
SN 1092-4388
EI 1558-9102
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD JUL
PY 2020
VL 63
IS 7
BP 2054
EP 2069
DI 10.1044/2020_JSLHR-19-00337
PG 16
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA MR0JC
UT WOS:000553279000002
PM 32598195
DA 2024-01-09
ER

PT J
AU Aylett, MP
   Vinciarelli, A
   Wester, M
AF Aylett, Matthew P.
   Vinciarelli, Alessandro
   Wester, Mirjam
TI Speech Synthesis for the Generation of Artificial Personality
SO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING
LA English
DT Article
DE Speech; Speech synthesis; Robots; Psychology; Hidden Markov models;
   Digital signal processing; Computational modeling; Personality;
   automatic personality perception; automatic personality recognition;
   automatic personality synthesis
ID VOICE QUALITY; PERCEPTION; BEHAVIOR
AB A synthetic voice personifies the system using it. In this work we examine the impact text content, voice quality and synthesis system have on the perceived personality of two synthetic voices. Subjects rated synthetic utterances based on the Big-Five personality traits and naturalness. The naturalness rating of synthesis output did not correlate significantly with any Big-Five characteristic except for a marginal correlation with openness. Although text content is dominant in personality judgments, results showed that voice quality change implemented using a unit selection synthesis system significantly affected the perception of the Big-Five, for example tense voice being associated with being disagreeable and lax voice with lower conscientiousness. In addition a comparison between a parametric implementation and unit selection implementation of the same voices showed that parametric voices were rated as significantly less neurotic than both the text alone and the unit selection system, while the unit selection was rated as more open than both the text alone and the parametric system. The results have implications for synthesis voice and system type selection for applications such as personal assistants and embodied conversational agents where developing an emotional relationship with the user, or developing a branding experience is important.
C1 [Aylett, Matthew P.; Wester, Mirjam] Univ Edinburgh, Sch Informat, Edinburgh EH8 9AB, Midlothian, Scotland.
   [Vinciarelli, Alessandro] Univ Glasgow, Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
C3 University of Edinburgh; University of Glasgow
RP Aylett, MP (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh EH8 9AB, Midlothian, Scotland.
EM matthewaylett@gmail.com; vincia@dcs.gla.ac.uk; mwester@inf.ed.ac.uk
RI Vinciarelli, Alessandro/HZI-8274-2023; Vinciarelli,
   Alessandro/C-1651-2012
OI Vinciarelli, Alessandro/0000-0002-9048-0524
FU Royal Society through a Royal Society Industrial Fellowship; European
   Union [645378]; EPSRC project [EP/N035305/1]
FX This research was funded by the Royal Society through a Royal Society
   Industrial Fellowship, and by the European Union's Horizon 2020 research
   and innovation programme under grant agreement No 645378 (Aria VALUSPA).
   The work of A.Vinciarelli was supported by the EPSRC project
   EP/N035305/1.
CR Aylett M., 2008, U.K. Patent, Patent No. [GB2447263A, 2447263]
   Aylett M., 2013, SSW8, P133
   Aylett M. P., 2007, AISB, P174
   Bennett C. L., 2006, P BLIZZ CHALL
   BURKE PJ, 1967, SOCIOMETRY, V30, P379, DOI 10.2307/2786183
   Cabral JP, 2014, IEEE J-STSP, V8, P195, DOI 10.1109/JSTSP.2014.2307274
   Chen LZ, 2015, IEEE-ACM T AUDIO SPE, V23, P605, DOI 10.1109/TASLP.2014.2385478
   Chin J., 1996, P C COMP HUM FACT CO, P248
   Clark R. A., 2004, P 5 ISCA WORKSH SPEE, P147
   Corr PJ, 2009, CAMBRIDGE HANDBOOK OF PERSONALITY PSYCHOLOGY, P1, DOI 10.1017/CBO9780511596544
   de Sevin E, 2010, LECT NOTES ARTIF INT, V6356, P187, DOI 10.1007/978-3-642-15892-6_20
   Deary IJ, 2009, CAMBRIDGE HANDBOOK OF PERSONALITY PSYCHOLOGY, P89
   EKMAN P, 1980, J PERS SOC PSYCHOL, V38, P270, DOI 10.1037/0022-3514.38.2.270
   Funder DC, 2001, ANNU REV PSYCHOL, V52, P197, DOI 10.1146/annurev.psych.52.1.197
   Gobl C, 2003, SPEECH COMMUN, V40, P189, DOI 10.1016/S0167-6393(02)00082-1
   Gordon M, 2001, J PHONETICS, V29, P383, DOI 10.1006/jpho.2001.0147
   HAYTER AJ, 1986, J AM STAT ASSOC, V81, P1000
   Hofer G.O., 2005, PROC 9 EUR C SPEECH, P501
   Howell D.C., 2012, Statistical methods for psychology
   Hu Q., 2013, 8 ISCA WORKSH SPEECH, P155
   Hunt A., 1996, P AC SPEECH SIGN PRO, P192
   Imai S., 1983, Proceedings of ICASSP 83. IEEE International Conference on Acoustics, Speech and Signal Processing, P93
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   KENNY DA, 1994, PSYCHOL BULL, V116, P245, DOI 10.1037/0033-2909.116.2.245
   Kenny DA, 2004, PERS SOC PSYCHOL REV, V8, P265, DOI 10.1207/s15327957pspr0803_3
   King S, 2014, LOQUENS, V1, DOI 10.3989/loquens.2014.006
   KLATT DH, 1990, J ACOUST SOC AM, V87, P820, DOI 10.1121/1.398894
   Kominek J., 2005, P ANN C INT SPEECH C, P85
   Krahmer E. J., 2003, PRIVATIZATION SECURI, P7
   Laver J., 1980, Cambridge Studies in Linguistics London, V31, P1
   LEVIN JR, 1994, PSYCHOL BULL, V115, P153, DOI 10.1037/0033-2909.115.1.153
   Ling ZH, 2013, IEEE T AUDIO SPEECH, V21, P2129, DOI 10.1109/TASL.2013.2269291
   Matthews G., 2009, PERSONALITY TRAITS
   McCrae RR, 2009, CAMBRIDGE HANDBOOK OF PERSONALITY PSYCHOLOGY, P148
   McRorie M, 2012, IEEE T AFFECT COMPUT, V3, P311, DOI 10.1109/T-AFFC.2011.38
   Mustafa K, 2006, IEEE T AUDIO SPEECH, V14, P435, DOI 10.1109/TSA.2005.855840
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Nettle D., 2009, PERSONALITY WHAT MAK
   Ohtani Y, 2016, INTERSPEECH, P2258, DOI 10.21437/Interspeech.2016-290
   Ozer DJ, 2006, ANNU REV PSYCHOL, V57, P401, DOI 10.1146/annurev.psych.57.102904.190127
   Partan S, 1999, SCIENCE, V283, P1272, DOI 10.1126/science.283.5406.1272
   Partan SR, 2005, AM NAT, V166, P231, DOI 10.1086/431246
   Potard B, 2016, INTERSPEECH, P2293, DOI 10.21437/Interspeech.2016-1188
   Raitio T, 2011, IEEE T AUDIO SPEECH, V19, P153, DOI 10.1109/TASL.2010.2045239
   Reeves B., 1996, The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Pla, DOI DOI 10.1007/S42452-020-2192-7
   SCHERER KR, 1978, EUR J SOC PSYCHOL, V8, P467, DOI 10.1002/ejsp.2420080405
   Scherer KR., 1979, SOCIAL MARKERS SPEEC, P147
   Schmitz M, 2007, 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P313
   Scrhoder M., 2003, P 15 INT C PHON SCI, P2589
   Slater PE, 1955, AM SOCIOL REV, V20, P300, DOI 10.2307/2087389
   Stevens C, 2005, COMPUT SPEECH LANG, V19, P129, DOI 10.1016/j.csl.2004.03.003
   Tapus A., 2008, AAAI SPRING S EMOTIO, P133
   Tapus A, 2008, INTEL SERV ROBOT, V1, P169, DOI 10.1007/s11370-008-0017-4
   Tausczik YR, 2010, J LANG SOC PSYCHOL, V29, P24, DOI 10.1177/0261927X09351676
   Trouvain J., 2006, P SPEECH PROS DRESD
   Uleman JS, 2008, ANNU REV PSYCHOL, V59, P329, DOI 10.1146/annurev.psych.59.103006.093707
   van den Oord A., 2016, 9 ISCA SPEECH SYNTH
   Vinciarelli A, 2014, IEEE T AFFECT COMPUT, V5, P273, DOI 10.1109/TAFFC.2014.2330816
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Watts O, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2217
   Wester M, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3365
   Wiggins J. S., 1996, The five-factor model of personality, P21
   Woods S, 2005, IEEE-RAS INT C HUMAN, P375
   Wu Z., 2016, SSW, P202, DOI 10.21437/SSW.2016-33
   Yamagishi Junichi, 2007, SSW, P294
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
   Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004
NR 68
TC 14
Z9 14
U1 10
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1949-3045
J9 IEEE T AFFECT COMPUT
JI IEEE Trans. Affect. Comput.
PD APR-JUN
PY 2020
VL 11
IS 2
BP 361
EP 372
DI 10.1109/TAFFC.2017.2763134
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA MB9ZP
UT WOS:000542957100015
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Hardy, TLD
   Rieger, JM
   Wells, K
   Boliek, CA
AF Hardy, Teresa L. D.
   Rieger, Jana M.
   Wells, Kristopher
   Boliek, Carol A.
TI Acoustic Predictors of Gender Attribution, Masculinity -Femininity, and
   Vocal Naturalness Ratings Amongst Transgender and Cisgender Speakers
SO JOURNAL OF VOICE
LA English
DT Article
DE Transgender; Acoustics; Gender; Femininity; Naturalness; Voice
ID SPEAKING FUNDAMENTAL-FREQUENCY; TO-FEMALE TRANSSEXUALS; VOICE QUALITY;
   FORMANT FREQUENCIES; INTERSPEAKER VARIATION; PERCEPTUAL EVALUATION;
   LISTENER PERCEPTIONS; SPEECH RATE; PARAMETERS; IDENTIFICATION
AB Purpose. This study aimed to identify the most salient set of acoustic predictors of (1) gender attribution; (2) perceived masculinity-femininity; and (3) perceived vocal naturalness amongst a group of transgender and cisgender speakers to inform voice and communication feminization training programs. This study used a unique set of acoustic variables and included a third, androgynous, choice for gender attribution ratings.
   Method. Data were collected across two phases and involved two separate groups of participants: communicators and raters. In the first phase, audio recordings were captured of communicators (n = 40) during cartoon retell, sustained vowel, and carrier phrase tasks. Acoustic measures were obtained from these recordings. In the second phase, raters (n = 20) provided ratings of gender attribution, perceived masculinity-femininity, and vocal naturalness based on a sample of the cartoon description recording.
   Results. Results of a multinomial logistic regression analysis identified mean fundamental frequency (f(o)) as the sole acoustic measure that changed the odds of being attributed as a woman or ambiguous in gender rather than as a man. Multiple linear regression analyses identified mean f(o), average formant frequency of /i/, and mean sound pressure level as predictors of masculinity-femininity ratings and mean f(o), average formant frequency, and rate of speech as predictors of vocal naturalness ratings.
   Conclusion. The results of this study support the continued targeting of f o and vocal tract resonance in voice and communication feminization/masculinization training programs and provide preliminary evidence for more emphasis being placed on vocal intensity and rate of speech. Modification of these voice parameters may help clients to achieve a natural-sounding voice that satisfactorily represents their affirmed gender.
C1 [Hardy, Teresa L. D.; Rieger, Jana M.; Wells, Kristopher; Boliek, Carol A.] Univ Alberta, Fac Rehabil Med, 3-48 Corbett Hall, Edmonton, AB T6G 2G4, Canada.
   [Hardy, Teresa L. D.] Glenrose Rehabil Hosp, Alberta Hlth Serv, Edmonton, AB, Canada.
   [Wells, Kristopher] MacEwan Univ, Fac Hlth & Community Studies, Dept Child & Youth Care, Edmonton, AB, Canada.
C3 University of Alberta; Alberta Health Services (AHS)
RP Hardy, TLD (corresponding author), Univ Alberta, Fac Rehabil Med, 3-48 Corbett Hall, Edmonton, AB T6G 2G4, Canada.
EM teresa.hardy@ualberta.ca; jana.rieger@ualberta.ca;
   Kristopher.Wells@Macewan.ca; carol.boliek@ualberta.ca
OI Hardy, Teresa/0000-0003-0768-6909; Rieger, Jana/0000-0002-3906-316X;
   Boliek, Carol/0000-0002-7141-2809
FU Social Sciences and Humanities Research Council of Canada (SSHRC);
   Alberta Innovates
FX The authors would like to thank the research participants for their
   contribution to the research. We also thank the team at Webzao
   Innovations for their tireless work in developing Gender Finder; Ming Ye
   for guidance with statistical analyses; Simone Emery, Jessica Frankel,
   Brianne Haeusler, and Riley Steel for assistance with acoustic analyses;
   and Matthew Kelley for creating PRAAT scripts for the acoustic analyses.
   This research was supported by the Social Sciences and Humanities
   Research Council of Canada (SSHRC) and Alberta Innovates.
CR Andrews ML, 1997, J VOICE, V11, P307, DOI 10.1016/S0892-1997(97)80009-4
   [Anonymous], TF32 COMPUTER PROGRA
   [Anonymous], 2006, VOICE COMMUNICATION
   [Anonymous], 2012, Voice and Communication Therapy for the Transgender/Transsexual Client: A Comprehensive Clinical Guide
   Baken R. J., 2000, Clinical measurement of speech and voice
   Bauer G., 2015, Transgender people in Ontario, Canada: statistics from the Trans PULSE Project to Inform Human Rights Policy
   Bhuta T, 2004, J VOICE, V18, P299, DOI 10.1016/j.jvoice.2003.12.004
   Boersma  P., 2017, PRAAT DOING PHONETIC
   Boonin J, 2012, VOICE COMMUNICATION, P237
   Byrne L. A., 2007, THESIS
   Carew L, 2007, J VOICE, V21, P591, DOI 10.1016/j.jvoice.2006.05.005
   Coleman E, 2012, INT J TRANSGENDERISM, V13, P165, DOI 10.1080/15532739.2011.700873
   COLEMAN RO, 1976, J SPEECH HEAR RES, V19, P168, DOI 10.1044/jshr.1901.168
   Dacakis G, 2012, TRANSSEXUAL VOICE QU
   Dacakis G, 2017, INT J LANG COMM DIS, V52, P831, DOI 10.1111/1460-6984.12319
   Dacakis G, 2012, CURR OPIN OTOLARYNGO, V20, P165, DOI 10.1097/MOO.0b013e3283530f85
   Dahl K., 2018, THESIS
   Davies S, 2015, INT J TRANSGENDERISM, V16, P117, DOI 10.1080/15532739.2015.1075931
   Davis A., 1969, PINK NIGHT
   Fitzsimons M, 2001, BRAIN LANG, V78, P94, DOI 10.1006/brln.2000.2448
   Fox CM, 2012, J SPEECH LANG HEAR R, V55, P930, DOI 10.1044/1092-4388(2011/10-0235)
   Martins RHG, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.06.012
   Gelfer MP, 2013, J VOICE, V27, P556, DOI 10.1016/j.jvoice.2012.11.008
   Gelfer MP, 2013, J VOICE, V27, P321, DOI 10.1016/j.jvoice.2012.07.008
   Gelfer MP, 2013, J VOICE, V27, P335, DOI 10.1016/j.jvoice.2012.07.009
   Gelfer MP, 2005, J VOICE, V19, P544, DOI 10.1016/j.jvoice.2004.10.006
   Gelfer MP, 2000, J VOICE, V14, P22, DOI 10.1016/S0892-1997(00)80092-2
   Gorham-Rowan M, 2006, J VOICE, V20, P251, DOI 10.1016/j.jvoice.2005.03.004
   Grant J.M., 2011, INJUSTICE EVERY TURN
   GUNZBURGER D, 1995, ARCH SEX BEHAV, V24, P339, DOI 10.1007/BF01541604
   GUNZBURGER D, 1989, CLIN LINGUIST PHONET, V3, P163, DOI 10.3109/02699208908985279
   GUNZBURGER D, 1993, EUR J DISORDER COMM, V28, P13
   Hancock A, 2014, J VOICE, V28, P203, DOI 10.1016/j.jvoice.2013.08.009
   Hancock AB, 2017, J LANG SOC PSYCHOL, V36, P599, DOI 10.1177/0261927X17704460
   Hancock AB, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.03.013
   Hancock AB, 2013, INT J LANG COMM DIS, V48, P54, DOI 10.1111/j.1460-6984.2012.00185.x
   Hardy TLD, 2016, AM J SPEECH-LANG PAT, V25, DOI 10.1044/2015_AJSLP-14-0098
   Hardy TLD, 2017, GENDER FINDER COMPUT
   Hillenbrand JM, 2009, ATTEN PERCEPT PSYCHO, V71, P1150, DOI 10.3758/APP.71.5.1150
   Hirsch S., 2017, PERSPECTIVES ASHA SP, V2, P74, DOI DOI 10.1044/PERSP2.SIG10.74
   Holmberg EB, 2010, J VOICE, V24, P511, DOI 10.1016/j.jvoice.2009.02.002
   Hosmer DW, 2013, WILEY SER PROBAB ST, P89
   HUNT KW, 1965, ENGL J, V54, P300, DOI 10.2307/811114
   James SE, 2016, The Report of the 2015 U.S. Transgender Survey
   KayPentax, 2008, MULT SOFTWARETM MOD
   Kelley M., 2016, BOLIEK LAB SEGMENTAL
   Kelley M., 2017, FO SCRIPT PRAAT SCRI
   King RS, 2012, INT J TRANSGENDERISM, V13, P117, DOI 10.1080/15532739.2011.664464
   KLATT DH, 1990, J ACOUST SOC AM, V87, P820, DOI 10.1121/1.398894
   Koo TK, 2016, J CHIROPR MED, V15, P155, DOI 10.1016/j.jcm.2016.02.012
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   Leung Y, 2018, J SPEECH LANG HEAR R, V61, P266, DOI 10.1044/2017_JSLHR-S-17-0067
   McNeill EJM, 2008, J VOICE, V22, P727, DOI 10.1016/j.jvoice.2006.12.010
   Mendoza E, 1996, J VOICE, V10, P59, DOI 10.1016/S0892-1997(96)80019-1
   MORDAUNT M, 2006, VOICE COMMUNICATION, P168
   MOUNT KH, 1988, J COMMUN DISORD, V21, P229, DOI 10.1016/0021-9924(88)90031-7
   Oates J, 1997, VENEREOLOGY, V10, P178
   Oates J., 2015, Perspectives on Voice and Voice Disorders, V25, P48, DOI [DOI 10.1044/VVD25.2.48, https://doi.org/10.1044/vvd25.2.48, 10.1044/vvd25.2, DOI 10.1044/VVD25.2]
   OATES JM, 1983, BRIT J DISORD COMMUN, V18, P139
   Owen K, 2010, INT J TRANSGENDERISM, V12, P272, DOI 10.1080/15532739.2010.550767
   Pasricha N, 2008, LOGOP PHONIATR VOCO, V33, P25, DOI 10.1080/14015430701514500
   Porter C. C., 2012, THESIS
   Ratcliff A., 2002, AUGMENT ALTERN COMM, V18, P11, DOI DOI 10.1080/714043393
   Robb MP, 2004, CLIN LINGUIST PHONET, V18, P1, DOI 10.1080/0269920031000105336
   Skuk VG, 2014, J SPEECH LANG HEAR R, V57, P285, DOI 10.1044/1092-4388(2013/12-0314)
   Snow MP, 1998, HUM FACTORS ERGON SO, V40, P1
   SODERSTEN M, 1995, J VOICE, V9, P182, DOI 10.1016/S0892-1997(05)80252-8
   SPENCER LE, 1988, FOLIA PHONIATR, V40, P31, DOI 10.1159/000265881
   Stryker S, 2008, RADICAL HIST REV, P145, DOI 10.1215/01636545-2007-026
   Tabachnick BarbaraG., 2013, Using Multivariate Statistics
   Tsao YC, 1997, J SPEECH LANG HEAR R, V40, P858, DOI 10.1044/jslhr.4004.858
   Tsao YC, 2006, J SPEECH LANG HEAR R, V49, P1156, DOI 10.1044/1092-4388(2006/083)
   Van Borsel J, 2001, J VOICE, V15, P570, DOI 10.1016/S0892-1997(01)00059-5
   Van Borsel J, 2008, J FLUENCY DISORD, V33, P241, DOI 10.1016/j.jfludis.2008.06.004
   Van Borsel J, 2008, CLIN LINGUIST PHONET, V22, P679, DOI 10.1080/02699200801976695
   Van Borsel J, 2009, J VOICE, V23, P291, DOI 10.1016/j.jvoice.2007.08.002
   Verhoeven J, 2004, LANG SPEECH, V47, P297, DOI 10.1177/00238309040470030401
   WENDAHL RW, 1966, FOLIA PHONIATR, V18, P26, DOI 10.1159/000263081
   WENDAHL RW, 1966, FOLIA PHONIATR, V18, P98, DOI 10.1159/000263059
   Whitehill TL, 2002, J SPEECH LANG HEAR R, V45, P80, DOI 10.1044/1092-4388(2002/006)
   WOLFE VI, 1990, J SPEECH HEAR DISORD, V55, P43, DOI 10.1044/jshd.5501.43
NR 81
TC 20
Z9 23
U1 3
U2 17
PU MOSBY-ELSEVIER
PI NEW YORK
PA 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN 0892-1997
EI 1873-4588
J9 J VOICE
JI J. Voice
PD MAR
PY 2020
VL 34
IS 2
DI 10.1016/j.jvoice.2018.10.002
PG 16
WC Audiology & Speech-Language Pathology; Otorhinolaryngology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Otorhinolaryngology
GA KW6NR
UT WOS:000521282000017
PM 30503396
DA 2024-01-09
ER

PT J
AU Chappell, W
   Nix, J
   Parrott, M
AF Chappell, Whitney
   Nix, John
   Parrott, Mackenzie
TI Social and Stylistic Correlates of Vocal Fry in a <i>cappella</i>
   Performances
SO JOURNAL OF VOICE
LA English
DT Article
DE Vocal fry; Matched guise; Social correlates; Stylistic correlates; a
   cappella
ID CREAKY VOICE
AB Objective. To determine the social and stylistic correlates of vocal fry in a cappella performances. Study Design. A matched-guise experiment was used to measure listener evaluations of fry and non-fry guises.
   Methods. Four singers, two male and two female, sang "The Star-Spangled Banner" with onset vocal fry. These recordings were used to create the two guises: (i) an unmodified recording with onset vocal fry on vowel-initial words and (ii) a recording in which the fry had been removed. In total, 253 participants listened to the recordings and evaluated the singers' social and stylistic attributes along a Likert scale, e.g., how confident, sexy, and sincere each singer sounded. A factor analysis was used to conflate correlated variables, and mixed effects linear regression models (n = 1,012) were fitted to each lone or joint factor to determine whether vocal fry significantly influenced listeners' responses to the singers.
   Results. Vocal fry significantly altered listener evaluations of the singers' sincerity/commitment, maturity/sophistication, naturalness, and confidence (P < 0.05). Unlike male singers, who were rated as significantly less sincere/committed with vocal fry, female singers were seen as more sincere/committed with vocal fry and younger listeners also found them less natural, suggesting vocal fry is associated with emotional intensity in female voices. Younger listeners perceived singers with fry as less mature/sophisticated, suggesting an association with youth. Finally, listeners with more musical training rated singers with fry as less confident, while less trained listeners did not exhibit this difference.
   Conclusions. Listeners are highly attuned to vocal fry in music but respond to it differently based upon their age, musical training, and the singer's sex. Vocal fry is evaluated more positively among younger, less musically trained listeners, and it is better received in women's voices, suggesting that the use of fry strategically targets a specific audience, i.e., younger and less trained listeners, who interpret fry as a marker of youth and emotional earnestness. These findings show that a single stylistic feature like vocal fry can be imbued with multiple meanings depending on the singer and audience, and its use can serve to include or exclude particular listener groups.
C1 [Chappell, Whitney; Nix, John] Univ Texas San Antonio, San Antonio, TX 78249 USA.
C3 University of Texas System; University of Texas at San Antonio (UTSA)
RP Chappell, W (corresponding author), Univ Texas San Antonio, Dept Modern Languages & Literatures, 1 UTSA Circle, San Antonio, TX 78249 USA.
EM whitney.chappell@utsa.edu
RI Chappell, Whitney/AAU-4169-2021; Nix, John/JVN-6840-2024
OI Chappell, Whitney/0000-0001-7600-7549
CR Abdelli-Beruh NB, 2014, J VOICE, V28, P185, DOI 10.1016/j.jvoice.2013.08.011
   Anderson RC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0097506
   [Anonymous], 1996, DISCOVER YOUR VOICE
   [Anonymous], R LANG ENV STAT COMP
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bates D, 2013, J STAT SOFTW, V52, P1, DOI 10.18637/jss.v052.i05
   Chapman JL, 2011, SINGING TEACHING HOL, P284
   Gibson TA, 2017, J VOICE, V31, P62, DOI 10.1016/j.jvoice.2016.02.005
   HOLLIEN H, 1968, J ACOUST SOC AM, V43, P506, DOI 10.1121/1.1910858
   HOLLIEN H, 1968, J SPEECH HEAR RES, V11, P600, DOI 10.1044/jshr.1103.600
   LAMBERT WE, 1960, J ABNORM SOC PSYCH, V60, P44, DOI 10.1037/h0044430
   Lefkowitz D, 106 M AM ANTHR ASS W
   Lunte R., VOCAL FRY ONSETS SIN
   Mendoza-Denton N, 2011, J LINGUIST ANTHROPOL, V21, P261, DOI 10.1111/j.1548-1395.2011.01110.x
   Miller R., 1996, STRUCTURE SINGING, P125
   Nix J, 2005, J SINGING, V62, P53
   Parrott ML, P M ACOUST, DOI [DOI 10.1121/2.0000237, 10.1121/2.0000237.]
   Pecknold D, 2016, ROUTL STUD POP MUSIC, P77
   Podesva R.J., 2013, P ANN M BERKELEY LIN, V37, P427
   Riggs S., 1998, SINGING STARS COMPLE, P58
   Riggs S., 1998, SINGING STARS AUDIO, P7
   Thompson Marie, 2016, N PARADOXA, V37, P5
   Titze I., 1994, Principles of Voice Production
   Titze IR, 2012, VOCOLOGY SCI PRACTIC, p[268, 273]
   Vanek C., 2006, SurveyGizmo
   Vennard W., 1967, SINGING MECH TECHNIC
   Wolk L, 2012, J VOICE, V26, pE111, DOI 10.1016/j.jvoice.2011.04.007
   Yuasa IP, 2010, AM SPEECH, V85, P315, DOI 10.1215/00031283-2010-018
NR 28
TC 2
Z9 2
U1 1
U2 4
PU MOSBY-ELSEVIER
PI NEW YORK
PA 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN 0892-1997
EI 1873-4588
J9 J VOICE
JI J. Voice
PD JAN
PY 2020
VL 34
IS 1
DI 10.1016/j.jvoice.2018.06.004
PG 9
WC Audiology & Speech-Language Pathology; Otorhinolaryngology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Audiology & Speech-Language Pathology; Otorhinolaryngology
GA KE4ZO
UT WOS:000508565600026
PM 30131202
DA 2024-01-09
ER

PT J
AU Klopfenstein, M
   Bernard, K
   Heyman, C
AF Klopfenstein, Marie
   Bernard, Kelsey
   Heyman, Claire
TI The study of speech naturalness in communication disorders: A systematic
   review of the literature
SO CLINICAL LINGUISTICS & PHONETICS
LA English
DT Review
DE Speech naturalness; speech disorders; hearing disorders
ID PROSODIC DISTURBANCE; CLEFT-PALATE; RATINGS; INTELLIGIBILITY; CHILDREN;
   APRAXIA; PATTERNS
AB The concept of speech naturalness is used widely in clinic and research applications. Unfortunately, the lack of consistency in research methods means that comparing findings between studies is difficult at best. In order to better understand the state of research on speech naturalness in communication disorders and quantify these impressions, this study looks at publications from the last 18 years in a systematic manner. A literature search for the exact phrase "speech naturalness" of the PubMed/MEDLINE, EBSCO, and ASHAWire databases was conducted. Articles included in the review were studies of communication and communication disorders published between 1990 and the end of 2014, in English, and in a peer-reviewed journal. 63 articles were selected and coded using a coding sheet adapted from a prior systematic review on intelligibility and cleft palate. Speech naturalness is an object of study in many subfields of communication disorders. Several concerns were raised as a result of the review, including the reliability and validity of measures, inadequate definitions of terminology, lack of detail in method descriptions, and the need to address relationships between naturalness and other variables included in the studies. Future studies should more carefully report methods and operational definitions used and more studies examining the relationship between naturalness and other speech variables in a variety of communication disorders are greatly needed.
C1 [Klopfenstein, Marie] Southern Illinois Univ, Dept Appl Hlth, Edwardsville, IL 62026 USA.
   [Bernard, Kelsey] Univ Arizona, Physiol Sci Program, Tucson, AZ USA.
   [Heyman, Claire] Carle Fdn Hosp, In Patient Rehab Dept, Urbana, IL USA.
C3 Southern Illinois University System; Southern Illinois University
   Edwardsville; University of Arizona
RP Klopfenstein, M (corresponding author), Southern Illinois Univ, Dept Appl Hlth, Edwardsville, IL 62026 USA.
EM maklopf@siue.edu
RI Klopfenstein, Marie/T-5448-2019
OI Klopfenstein, Marie/0000-0002-2229-8050
CR Andrews C, 2012, LANG SPEECH HEAR SER, V43, P359, DOI 10.1044/0161-1461(2012/11-0038)
   [Anonymous], 2000, NOISE REDUCTION SCHE
   [Anonymous], J MED SPEECH LANGUAG
   [Anonymous], 1984, Apraxia of speech in adults: The disorder and its management
   BELLAIRE K, 1986, J COMMUN DISORD, V19, P271, DOI 10.1016/0021-9924(86)90033-X
   COOPER WE, 1984, LANG SPEECH, V27, P17, DOI 10.1177/002383098402700102
   Coughlin-Woods S, 2005, PERCEPT MOTOR SKILL, V100, P295
   Craig A, 1996, J SPEECH HEAR RES, V39, P808, DOI 10.1044/jshr.3904.808
   DAntonio L. L., 1995, Cleft palate speech management: A multidisciplinary approach to the management of cleft palate, P176
   DARLEY FL, 1969, J SPEECH HEAR RES, V12, P246, DOI 10.1044/jshr.1202.246
   Darley FL, 1975, Motor speech disorders, V3rd
   Eadie TL, 2008, J VOICE, V22, P43, DOI 10.1016/j.jvoice.2006.08.008
   Eadie TL, 2002, J SPEECH LANG HEAR R, V45, P1088, DOI 10.1044/1092-4388(2002/087)
   Ingham RJ, 2006, J SPEECH LANG HEAR R, V49, P660, DOI 10.1044/1092-4388(2006/048)
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P217, DOI 10.1044/jshd.5002.217
   INGHAM RJ, 1978, J SPEECH HEAR RES, V21, P63, DOI 10.1044/jshr.2101.63
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P261, DOI 10.1044/jshd.5003.261
   KENT RD, 1982, BRAIN LANG, V15, P259, DOI 10.1016/0093-934X(82)90060-8
   KENT RD, 1983, J SPEECH HEAR RES, V26, P231, DOI 10.1044/jshr.2602.231
   Klopfenstein M, 2016, J INTERACT RES COM D, V7, P123, DOI 10.1558/jircd.v7i1.27932
   Lenden JM, 2007, J COMMUN DISORD, V40, P66, DOI 10.1016/j.jcomdis.2006.04.004
   LINEBAUGH CW, 1984, DYSARTHRIAS PHYSL AC, P197
   Manning W. H, 2001, Clinical decision making in fluency disorders, V2nd
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   McLeod S, 2006, AM J SPEECH-LANG PAT, V15, P192, DOI 10.1044/1058-0360(2006/018)
   MCNEIL MR, 1990, BRAIN LANG, V38, P135, DOI 10.1016/0093-934X(90)90106-Q
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   ODELL K, 1990, J SPEECH HEAR DISORD, V55, P345, DOI 10.1044/jshd.5502.345
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P994, DOI 10.1044/jshr.3505.994
   ONSLOW M, 1987, J SPEECH HEAR DISORD, V52, P2, DOI 10.1044/jshd.5201.02
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P274, DOI 10.1044/jshr.3502.274
   OSBERGER MJ, 1987, J SPEECH HEAR RES, V30, P241, DOI 10.1044/jshr.3002.241
   PARKHURST BG, 1978, J COMMUN DISORD, V11, P249, DOI 10.1016/0021-9924(78)90017-5
   Ratcliff A., 2002, AUGMENT ALTERN COMM, V18, P11, DOI DOI 10.1080/714043393
   RUNYAN CM, 1982, J FLUENCY DISORD, V7, P71, DOI 10.1016/0094-730X(82)90040-7
   RUNYAN CM, 1979, J FLUENCY DISORD, V4, P29, DOI 10.1016/0094-730X(79)90029-9
   RUNYAN CM, 1978, J FLUENCY DISORD, V3, P25, DOI 10.1016/0094-730X(78)90004-9
   RYALLS JH, 1981, NEUROPSYCHOLOGIA, V19, P365, DOI 10.1016/0028-3932(81)90066-X
   Sacco P. R., 1992, SPEECH NATURALNESS N
   Schiavetti N, 1998, J SPEECH LANG HEAR R, V41, P5, DOI 10.1044/jslhr.4101.05
   Shikani AH, 2012, INT FORUM ALLERGY RH, V2, P348, DOI 10.1002/alr.21018
   SIMMONS N, 1983, CLIN DYSARTHRIA, P283
   Spencer KA, 2009, J MED SPEECH-LANG PA, V17, P125
   Stocks R, 2009, BRAIN INJURY, V23, P820, DOI 10.1080/02699050902997888
   Strand EA, 1996, J SPEECH HEAR RES, V39, P1018, DOI 10.1044/jshr.3905.1018
   Tamplin J, 2008, NEUROREHABILITATION, V23, P207
   Tjaden K, 2000, CLIN LINGUIST PHONET, V14, P619, DOI 10.1080/026992000750048143
   Tse ACY, 2013, J SPEECH LANG HEAR R, V56, P906, DOI 10.1044/1092-4388(2012/12-0051)
   Whitehill T, 2002, INVESTIGATIONS IN CLINICAL PHONETICS AND LINGUISTICS, P405
   Whitehill TL, 2002, CLEFT PALATE-CRAN J, V39, P50, DOI 10.1597/1545-1569(2002)039<0050:AIISWC>2.0.CO;2
   Witzel M.A., 1995, CLEFT PALATE SPEECH, P137
   Wyatt R, 1996, BRIT J PLAST SURG, V49, P143, DOI 10.1016/S0007-1226(96)90216-7
   Yorkston K.M., 1999, Management of motor speech disorders in children and adults
   Yorkston K. M., 1984, DYSARTHRIAS PHYSL AC, P197
   YORKSTON KM, 1990, J SPEECH HEAR DISORD, V55, P550, DOI 10.1044/jshd.5503.550
   YORKSTON KM, 2010, MANAGEMENT MOTOR SPE
   Youmans G, 2011, AM J SPEECH-LANG PAT, V20, P23, DOI 10.1044/1058-0360(2010/09-0085)
NR 57
TC 11
Z9 11
U1 4
U2 9
PU TAYLOR & FRANCIS INC
PI PHILADELPHIA
PA 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
SN 0269-9206
EI 1464-5076
J9 CLIN LINGUIST PHONET
JI Clin. Linguist. Phon.
PD APR 2
PY 2020
VL 34
IS 4
BP 327
EP 338
DI 10.1080/02699206.2019.1652692
EA AUG 2019
PG 12
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA KR4EG
UT WOS:000484032900001
PM 31455101
DA 2024-01-09
ER

PT J
AU Vogel, AP
   Stoll, LH
   Oettinger, A
   Rommel, N
   Kraus, EM
   Timmann, D
   Scott, D
   Atay, C
   Storey, E
   Schöls, L
   Synofzik, M
AF Vogel, Adam P.
   Stoll, Lisa H.
   Oettinger, Andreas
   Rommel, Natalie
   Kraus, Eva-Maria
   Timmann, Dagmar
   Scott, Dion
   Atay, Christina
   Storey, Elsdon
   Schoels, Ludger
   Synofzik, Matthis
TI Speech treatment improves dysarthria in multisystemic ataxia: a
   rater-blinded, controlled pilot-study in ARSACS
SO JOURNAL OF NEUROLOGY
LA English
DT Article
DE Speech; Dysarthria; Rehabilitation; Acoustics; Ataxic neuropathy; Voice
ID PARKINSONS-DISEASE; INTELLIGIBILITY; ADULTS; CLEAR
AB We aimed to provide proof-of-principle evidence that intensive home-based speech treatment can improve dysarthria in complex multisystemic degenerative ataxias, exemplified by autosomal recessive spastic ataxia Charlevoix-Saguenay (ARSACS). Feasibility and piloting efficacy of speech training specifically tailored to cerebellar dysarthria was examined through a 4-week program in seven patients with rater-blinded assessment of intelligibility (primary outcome) and naturalness and acoustic measures of speech (secondary outcomes) performed 4 weeks before, immediately prior to, and directly after training (intraindividual control design). Speech intelligibility and naturalness improved post treatment. This provides piloting evidence that ataxia-tailored speech treatment might be effective in degenerative cerebellar disease.
C1 [Vogel, Adam P.; Stoll, Lisa H.; Rommel, Natalie; Kraus, Eva-Maria; Schoels, Ludger; Synofzik, Matthis] Hertie Inst Clin Brain Res, Dept Neurodegenerat, Tubingen, Germany.
   [Vogel, Adam P.; Stoll, Lisa H.; Rommel, Natalie; Kraus, Eva-Maria; Schoels, Ludger; Synofzik, Matthis] Univ Tubingen, Univ Hosp Tubingen, Ctr Neurol, Tubingen, Germany.
   [Vogel, Adam P.] Univ Melbourne, Ctr Neurosci Speech, 550 Swanston St, Melbourne, Vic 3010, Australia.
   [Vogel, Adam P.] RedenLab, Melbourne, Vic, Australia.
   [Stoll, Lisa H.; Rommel, Natalie] Univ Hosp Tubingen, Therapiezentrum, Tubingen, Germany.
   [Oettinger, Andreas] Kliniken Schmieder, Neurol & Rehabil, Gailingen, Germany.
   [Timmann, Dagmar] Univ Duisburg Essen, Essen Univ Hosp, Dept Neurol, Essen, Germany.
   [Scott, Dion; Atay, Christina] Univ Queensland, St Lucia, Qld, Australia.
   [Storey, Elsdon] Monash Univ, Dept Med, Melbourne, Vic, Australia.
   [Schoels, Ludger; Synofzik, Matthis] Ctr Neurodegenerat Dis DZNE, Tubingen, Germany.
C3 Eberhard Karls University of Tubingen; Eberhard Karls University
   Hospital; Eberhard Karls University of Tubingen; Eberhard Karls
   University Hospital; University of Melbourne; Eberhard Karls University
   of Tubingen; Eberhard Karls University Hospital; University of Duisburg
   Essen; University of Queensland; Monash University; Helmholtz
   Association; German Center for Neurodegenerative Diseases (DZNE)
RP Vogel, AP (corresponding author), Hertie Inst Clin Brain Res, Dept Neurodegenerat, Tubingen, Germany.; Vogel, AP (corresponding author), Univ Tubingen, Univ Hosp Tubingen, Ctr Neurol, Tubingen, Germany.; Vogel, AP (corresponding author), Univ Melbourne, Ctr Neurosci Speech, 550 Swanston St, Melbourne, Vic 3010, Australia.; Vogel, AP (corresponding author), RedenLab, Melbourne, Vic, Australia.
EM vogela@unimelb.edu.au
RI Vogel, Adam/ABD-7685-2020; Schöls, Ludger/ABB-2482-2021
OI Vogel, Adam/0000-0002-3505-2631; Schöls, Ludger/0000-0001-7774-5025;
   Synofzik, Matthis/0000-0002-2280-7273
FU Ataxia Charlevoix-Saguenay Foundation; European Union [643578]; BMBF
   [01GM1607]; Alexander von Humboldt Foundation; IZKF Promotionskolleg
   Tubingen [IZKF 2016-1-07]
FX This study was supported by the Ataxia Charlevoix-Saguenay Foundation
   and from the European Union's Horizon 2020 research and innovation
   program under the ERA-NET Cofund action No 643578. It was supported by
   the BMBF (01GM1607 to M.S.), under the frame of the E-Rare-3 network
   PREPARE (to M.S.). A.P.V. received salaried support from the National
   Health and Medical Research Council, Australia (Career Development
   Fellowship ID 1082910), and received funding from the Alexander von
   Humboldt Foundation. This study was supported by the IZKF
   Promotionskolleg Tubingen (IZKF 2016-1-07) to M.S. and E.K.
CR Ballard KJ, 2010, J SPEECH
   Constantinescu G, 2011, INT J LANG COMM DIS, V46, P1, DOI 10.3109/13682822.2010.484848
   Hargrove PM, 2013, CLIN LINGUIST PHONET, V27, P647, DOI 10.3109/02699206.2013.777121
   KATZ S, 1983, J AM GERIATR SOC, V31, P721, DOI 10.1111/j.1532-5415.1983.tb03391.x
   Nasreddine ZS, 2005, J AM GERIATR SOC, V53, P695, DOI 10.1111/j.1532-5415.2005.53221.x
   Park S, 2016, AM J SPEECH-LANG PAT, V25, P97, DOI 10.1044/2015_AJSLP-14-0113
   SCHIAVETTI N, 1981, J SPEECH HEAR RES, V24, P441, DOI 10.1044/jshr.2403.441
   Schmitz-Hübsch T, 2006, NEUROLOGY, V66, P1717, DOI 10.1212/01.wnl.0000219042.60538.92
   Synofzik M, 2013, ORPHANET J RARE DIS, V8, DOI 10.1186/1750-1172-8-41
   Tamplin J, 2008, NEUROREHABILITATION, V23, P207
   Tjaden K, 2014, J SPEECH LANG HEAR R, V57, P779, DOI 10.1044/2014_JSLHR-S-12-0372
   Vogel AP, 2018, J NEUROL, V265, P2060, DOI 10.1007/s00415-018-8950-4
   Vogel AP, 2017, MITOCHONDRION, V37, P1, DOI 10.1016/j.mito.2017.06.002
   Vogel AP, 2011, J VOICE, V25, P137, DOI 10.1016/j.jvoice.2009.09.003
   Vogel AP, 2014, COCHRANE DATABASE SY, P10
   Weismer G, 2002, J SPEECH LANG HEAR R, V45, P421, DOI 10.1044/1092-4388(2002/033)
NR 16
TC 17
Z9 18
U1 0
U2 8
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 0340-5354
EI 1432-1459
J9 J NEUROL
JI J. Neurol.
PD MAY
PY 2019
VL 266
IS 5
BP 1260
EP 1266
DI 10.1007/s00415-019-09258-4
PG 7
WC Clinical Neurology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Neurosciences & Neurology
GA HT8RT
UT WOS:000464833000024
PM 30840144
DA 2024-01-09
ER

PT J
AU Czaplicki, B
AF Czaplicki, Bartlomiej
TI Measuring the phonological (un)naturalness of selected alternation
   patterns in Polish
SO LANGUAGE SCIENCES
LA English
DT Article
DE Naturalness; Morphology-phonology interface; Cophonology theory;
   Consonant mutation; Palatalization; Progressive voice assimilation
ID MORPHOLOGY; REGULARITIES; PERSPECTIVE; CONSTRAINTS; INFERENCE; ENGLISH;
   RULES
AB In most generative research phonological naturalness/markedness has served as a synchronic bias that can explain the predominance of certain patterns in the world's languages. In this paper, on the basis of one language, Polish, it is shown that unnatural patterns are far from rare and, therefore, phonological theories need to accommodate them. The two patterns under scrutiny, consonant mutations and progressive devoicing, are to a large degree unnatural but fully productive. Consonant mutations are subjected to a thorough examination using data from 604 outputs of the concatenation of 27 mutation triggering suffixes, both vowel- and consonant-initial, in order to assess the role of various predictors of palatality. As there is no observable effect of naturalness but a strong influence of specific suffix-initial segments, base-final consonants and individual suffixes on the palatality of the output, the data provide support for a framework that does not incorporate phonological naturalness as an active bias in models of grammar. The morphophonological patterns, formalized as source-oriented schemas, are morpheme specific. This discussion provides evidence for a model of grammar comprising multiple morpheme-specific cophonologies. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Czaplicki, Bartlomiej] Univ Warsaw, Inst English Studies, Ul Hoza 69, PL-00681 Warsaw, Poland.
C3 University of Warsaw
RP Czaplicki, B (corresponding author), Univ Warsaw, Inst English Studies, Ul Hoza 69, PL-00681 Warsaw, Poland.
EM bczaplicki@uw.edu.pl
RI Czaplicki, Bartlomiej/AAM-9366-2021
CR Albright A, 2003, COGNITION, V90, P119, DOI 10.1016/S0010-0277(03)00146-X
   Albright A, 2002, LANGUAGE, V78, P684, DOI 10.1353/lan.2003.0002
   ANDERSON SR, 1981, LINGUIST INQ, V12, P493
   Anderson SR, 2008, WORD STRUCT, V1, P109, DOI 10.3366/E1750124508000184
   [Anonymous], 2012, DIRECT INTERFACE ONE
   [Anonymous], 2006, THESIS
   [Anonymous], UCLA WORKING PAPERS
   [Anonymous], 2000, PHONOLOGY
   [Anonymous], 2010, CONSTRUCTION MORPHOL
   [Anonymous], P BERK LING SOC
   [Anonymous], 1976, AUTOSEGMENTAL PHONOL
   Aronoff Mark, 1994, Morphology by itself
   Baayen R.H., 2003, MORPHOLOGICAL STRUCT, P355, DOI [DOI 10.1515/9783110910186, 10.1515/9783110910186]
   Bank M., 2003, INDEKS TERGO UNIWERS
   Barnes J, 2006, STRENGTH WEAKNESS IN
   Becker M, 2016, LINGUIST INQ, V47, P391, DOI 10.1162/ling_a_00217
   Berent I, 2007, COGNITION, V104, P591, DOI 10.1016/j.cognition.2006.05.015
   Blevins J, 2006, THEOR LINGUIST, V32, P117, DOI 10.1515/TL.2006.009
   Blevins Juliette, 2004, Evolutionary Phonology: The Emergence of Sound Patterns
   Booij G., 1995, PHONOLOGY DUTCH
   Booij G, 2017, COGNITIVE SCI, V41, P277, DOI 10.1111/cogs.12323
   Booij G, 2009, CURR STUD LINGUIST, V47, P487
   Booij Geert, 2018, PHONOLOGY FIELDWORK, P13
   Brown J, 2006, THEOR LINGUIST, V32, P175, DOI 10.1515/TL.2006.011
   Bybee J. L., 2001, PHONOLOGY LANGUAGE U
   Cameron-Faulkner T, 2000, NAT LANG LINGUIST TH, V18, P813, DOI 10.1023/A:1006496821412
   CARSTAIRS A, 1990, TREND LIN S, V49, P17
   Carstairs Andrew, 1988, YB MORPHOLOGY 1988, V1988, P68
   Chambers KE, 2003, COGNITION, V87, pB69, DOI 10.1016/S0010-0277(02)00233-0
   Chomsky Noam., 1968, The Sound Pattern of English
   Clements G. N., 1985, Phonology Yearbook, V2, P225, DOI [10.1017/S0952675700000440., 10.1017/S0952675700000440, DOI 10.1017/S0952675700000440]
   Czaplicki B, 2014, POZ STUD CONTEMP LIN, V50, P419, DOI 10.1515/psicl-2014-0022
   Czaplicki B, 2013, LINGUA, V123, P31, DOI 10.1016/j.lingua.2012.10.002
   Czaplicki B, 2010, J SLAV LINGUIST, V18, P259, DOI 10.1353/is1.2010.0002
   Czaplicki B, 2010, POZ STUD CONTEMP LIN, V46, P177, DOI 10.2478/v10010-010-0009-3
   Czaplicki Bartlomiej, 2016, PHONOLOGY ITS FACES, P19
   Czaplicki Bartlomiej, 2014, LEXICON BASED PHONOL
   Czaplicki Bartlomiej, 2018, PHONOLOGY FIELDWORK, P185
   Czaplicki Bartlomiej, 2016, STUDIES LEXICOGRAMMA, P261, DOI [10.1075/hcp.54.14cza, DOI 10.1075/HCP.54.14CZA]
   Dabrowska E, 2004, LANG COGNITIVE PROC, V19, P225, DOI 10.1080/01690960344000170
   Dabrowska Ewa., 2004, LANGUAGE MIND BRAIN
   de Courtenay Baudouin, 1972, BAUDOUIN COURTENAY A
   De Lacy P, 2006, THEOR LINGUIST, V32, P185
   de Lacy P, 2013, NAT LANG LINGUIST TH, V31, P287, DOI 10.1007/s11049-013-9191-y
   de Lacy Paul, 2002, THESIS
   Embick David, 2010, LINGUISTIC INQUIRY M, V60
   Gouskova M, 2015, LINGUA, V167, P41, DOI 10.1016/j.lingua.2015.08.014
   Grzegorczykowa Renata, 1999, The Grammar of Contemporary Polish. Morphology, P389
   Guion Susan G., 1996, THESIS
   Gussmann E., 1992, PHONOLOGICAL INVESTI, P5
   Gussmann E., 2007, The Phonology of Polish
   Hale Mark, 2008, The Phonological Enterprise
   Hamann S., 2002, OTS YB 2002, P105
   Hansson Gunnar O., 2014, HDB PHONOLOGICAL THE, P319
   Haspelmath M, 2006, J LINGUIST, V42, P25, DOI 10.1017/S0022226705003683
   Hayes B, 2013, LINGUIST INQ, V44, P45, DOI 10.1162/LING_a_00119
   Hayes B, 2009, LANGUAGE, V85, P822
   Hayes Bruce, 2004, PHONETICALLY BASED P, DOI DOI 10.1017/CBO9780511486401
   Henderson Eugenie, 1992, MON KHMER STUDIES, V18-19, P61
   Hothorn T, 2006, J COMPUT GRAPH STAT, V15, P651, DOI 10.1198/106186006X133933
   Hyman L.M., 1975, Phonology: Theory and analysis
   Hyman L. M., 2001, ROLE SPEECH PERCEPTI, P141
   Inkelas Sharon, 2014, INTERPLAY MORPHOLOGY
   JACKENDOFF R, 1975, LANGUAGE, V51, P639, DOI 10.2307/412891
   Jackendoff R., 2002, FDN LANGUAGE
   Kiparsky P, 2006, THEOR LINGUIST, V32, P217, DOI 10.1515/TL.2006.015
   Kiparsky Paul, 2008, LINGUISTIC UNIVERSAL, P25, DOI [10.1093/acprof:oso/9780199298495.003.0002, DOI 10.1093/ACPROF:OSO/9780199298495.003.0002]
   Kochetov vanOostendorp., 2011, BLACKWELL COMPANION, V3, P1666, DOI DOI 10.1002/9781444335262.WBCTP0071
   Kutsch Lojenga Constance, 1994, NGITI CENTRAL SUDANI
   Langacker Ronald W., 1987, Foundations of Cognitive Grammar: Theoretical prerequisites, VI
   Langacker Ronald W., 2000, Usage -based models of language, P1
   Lindblom B., 1990, NATO ASI SERIES, V55
   Marcus G., 1992, MONOGRAPHS SOC RES C, V57
   Mielke Jeffrey, 2008, The emergence of distinctive features
   Ohala J. J., 1994, Sound Symbolism, DOI DOI 10.1017/CBO9780511751806.022
   Ohala John, 1983, The production of Speech, P189, DOI [10.1007/978-1-4613-8202-7_9, DOI 10.1007/978-1-4613-8202-7_9]
   Ohala John J., 1981, PAPERS PARASESSION L, P178, DOI DOI 10.1075/CILT.323.05OHA
   Onishi KH, 2002, COGNITION, V83, pB13, DOI 10.1016/S0010-0277(01)00165-2
   Peperkamp Sharon, 2007, Laboratory Phonology 9, V9, P315
   Pierrehumbert, 2006, LAB PHONOLOGY, P81, DOI 10.1515/9783110197211.1.81
   Prince, 2004, OPTIMALITY THEORY CO, DOI 10.1002/97804
   Pycha A., 2003, WCCFL, P101
   Rubach J, 1996, LINGUIST INQ, V27, P69
   Rubach J, 2007, LINGUIST INQ, V38, P85, DOI 10.1162/ling.2007.38.1.85
   Rubach Jerzy, 1984, Cyclic and lexical phonology. The structure of Polish
   Sagey Elizabeth, 1986, THESIS
   Stockwell R., 1972, LINGUISTIC CHANGE GE, P1
   Svantesson Jan-Olof., 2005, PHONOLOGY MONGOLIAN
   Tomasello M., 2003, Constructing a Language: The usage-based theory of language acquisition
   Wilson C, 2006, COGNITIVE SCI, V30, P945, DOI 10.1207/s15516709cog0000_89
   Wilson Colin, 2003, WCCFL, P533
   Yu Alan C. L., 2014, HDB PHONOLOGICAL THE, P291
   Zsiga Elizabeth C., 1993, THESIS
NR 93
TC 3
Z9 3
U1 0
U2 2
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0388-0001
EI 1873-5746
J9 LANG SCI
JI Lang. Sci.
PD MAR
PY 2019
VL 72
BP 160
EP 187
DI 10.1016/j.langsci.2018.10.002
PG 28
WC Linguistics; Language & Linguistics
WE Social Science Citation Index (SSCI); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Linguistics
GA HQ0UR
UT WOS:000462111000011
DA 2024-01-09
ER

PT J
AU Venkatraman, A
   Sivasankar, MP
AF Venkatraman, Anumitha
   Sivasankar, M. Preeti
TI Continuous Vocal Fry Simulated in Laboratory Subjects: A Preliminary
   Report on Voice Production and Listener Ratings
SO AMERICAN JOURNAL OF SPEECH-LANGUAGE PATHOLOGY
LA English
DT Article
ID PHONATION THRESHOLD PRESSURE; YOUNG; QUALITY
AB Purpose: Vocal fry is prevalent in everyday speech. However, whether the use of vocal fry is detrimental to voice production is unclear. This preliminary study assessed the effects of using continuous vocal fry on voice production measures and listener ratings.
   Method: Ten healthy individuals (equal male and female, mean age = 22.4 years) completed 2 counterbalanced sessions. In each session, participants read in continuous vocal fry or habitual voice quality for 30 min at a comfortable intensity. Continuous vocal fry was simulated. Phonation threshold pressure (PTP10 and PTP20), cepstral peak prominence, and vocal effort ratings were obtained before and after the production of each voice quality. Next, 10 inexperienced listeners (equal male and female, mean age = 24.1 years) used visual analog scales to rate paired samples of continuous vocal fry and habitual voice quality for naturalness, employability, and amount of listener concentration.
   Results: PTP10 and vocal effort ratings increased after 30 min of continuous vocal fry. Inexperienced listeners rated continuous vocal fry more negatively than the habitual voice quality.
   Conclusions: Thirty minutes of simulated, continuous vocal fry worsened some voice measures when compared with a habitual voice quality. Samples of continuous vocal fry were rated as significantly less employable, less natural, and requiring greater listener concentration as compared with samples of habitual voice quality. Future studies should include habitual users of vocal fry to investigate speech stimulability and adaptation with cueing to further understand pathogenesis of vocal fry.
C1 [Venkatraman, Anumitha; Sivasankar, M. Preeti] Purdue Univ, Dept Speech Language & Hearing Sci, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University West Lafayette Campus;
   Purdue University
RP Sivasankar, MP (corresponding author), Purdue Univ, Dept Speech Language & Hearing Sci, W Lafayette, IN 47907 USA.
EM msivasan@purdue.edu
FU National Institutes of Health T32 Training Grant [2T32DC000030-26]
FX Funding was provided by a National Institutes of Health T32 Training
   Grant 2T32DC000030-26 to the Department of Speech, Language, & Hearing
   Sciences at Purdue University. This work was based on a thesis by the
   first author submitted in partial fulfillment of the MS-SLP degree from
   the Department of Speech, Language, & Hearing Sciences at Purdue
   University. The authors thank the members of the MS-thesis committee
   Barbara Solomon, Georgia Malandraki, and Christine Weber for their
   insightful comments. Bruce Craig and Ryan Murphy at the Purdue
   University Statistical Consulting Services assisted with the statistical
   analysis. The authors also thank Robert Fujiki, Abigail Chapleau, and
   Sara Loerch for their contributions to the data analysis.
CR Abdelli-Beruh NB, 2014, J VOICE, V28, P185, DOI 10.1016/j.jvoice.2013.08.011
   Anderson RC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0097506
   [Anonymous], 1980, The Phonetic Description of Voice Quality
   Awan SN, 2010, CLIN LINGUIST PHONET, V24, P742, DOI 10.3109/02699206.2010.492446
   Baldner EF, 2015, J VOICE, V29, P530, DOI 10.1016/j.jvoice.2014.08.017
   Blomgren M, 1998, J ACOUST SOC AM, V103, P2649, DOI 10.1121/1.422785
   Borrie SA, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.12.005
   Cantor-Cutiva L. C, 2017, LOGOP PHONIATR VOCO, V43, P1
   Chen Y, 2002, J SPEECH LANG HEAR R, V45, P821, DOI 10.1044/1092-4388(2002/066)
   CHILDERS DG, 1991, J ACOUST SOC AM, V90, P2394, DOI 10.1121/1.402044
   Erickson E, 2010, J SPEECH LANG HEAR R, V53, P75, DOI 10.1044/1092-4388(2009/09-0024)
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   Fisher KV, 1997, J SPEECH LANG HEAR R, V40, P1122, DOI 10.1044/jslhr.4005.1122
   Fujiki RB, 2017, J VOICE, V31, P211, DOI 10.1016/j.jvoice.2016.07.005
   Guzman M, 2013, INT J SPEECH-LANG PA, V15, P127, DOI 10.3109/17549507.2012.702283
   Heman-Ackah YD, 2014, J VOICE, V28, DOI 10.1016/j.jvoice.2014.05.005
   Kempster GB, 2009, AM J SPEECH-LANG PAT, V18, P124, DOI 10.1044/1058-0360(2008/08-0017)
   Laukkanen AM, 2004, FOLIA PHONIATR LOGO, V56, P335, DOI 10.1159/000081081
   MCGLONE RE, 1971, J SPEECH HEAR RES, V14, P769, DOI 10.1044/jshr.1404.769
   Nagle KF, 2012, J COMMUN DISORD, V45, P235, DOI 10.1016/j.jcomdis.2012.01.001
   Oliveira G, 2016, J VOICE, V30, P684, DOI 10.1016/j.jvoice.2015.08.015
   Oppenheimer DM, 2009, J EXP SOC PSYCHOL, V45, P867, DOI 10.1016/j.jesp.2009.03.009
   Parker MA, 2018, J VOICE, V32, P538, DOI 10.1016/j.jvoice.2017.08.002
   Plexico LW, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.10.004
   Raj A, 2010, J VOICE, V24, P363, DOI 10.1016/j.jvoice.2008.10.005
   Remacle A, 2012, J VOICE, V26, pE177, DOI 10.1016/j.jvoice.2011.07.016
   Rosen CA, 2000, J VOICE, V14, P619, DOI 10.1016/S0892-1997(00)80017-X
   Roy N, 2003, J VOICE, V17, P331, DOI 10.1067/S0892-1997(03)00078-X
   Solomon NP, 2007, J VOICE, V21, P541, DOI 10.1016/j.jvoice.2006.04.002
   STEMPLE JC, 1994, J VOICE, V8, P271, DOI 10.1016/S0892-1997(05)80299-1
   VERDOLINIMARSTON K, 1990, J VOICE, V4, P142, DOI 10.1016/S0892-1997(05)80139-0
   Wolk L, 2012, J VOICE, V26, pE111, DOI 10.1016/j.jvoice.2011.04.007
   Yuasa IP, 2010, AM SPEECH, V85, P315, DOI 10.1215/00031283-2010-018
NR 33
TC 2
Z9 2
U1 0
U2 2
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 2200 RESEARCH BLVD, #271, ROCKVILLE, MD 20850-3289 USA
SN 1058-0360
EI 1558-9110
J9 AM J SPEECH-LANG PAT
JI Am. J. Speech-Lang. Pathol.
PD NOV
PY 2018
VL 27
IS 4
BP 1539
EP 1545
DI 10.1044/2018_AJSLP-17-0212
PG 7
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA HB6TX
UT WOS:000451206200017
PM 30178028
OA Green Published
DA 2024-01-09
ER

PT J
AU Rao, MVA
   Victory, JS
   Ghosh, PK
AF Rao, Achuth M., V
   Victory, Shiny J.
   Ghosh, Prasanta Kumar
TI Effect of source filter interaction on isolated vowel-consonant-vowel
   perception
SO JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA
LA English
DT Article
ID QUALITY SPEECH SYNTHESIS; PHONATION; F0
AB Source-filter interaction explains the drop in pitch in voiced consonant due to constriction in the vocal tract during vowel-consonant-vowel (VCV) production. In this work, a perceptual study is conducted where the pitch contour in the voiced consonant region is modified to four different levels and a listening test is performed to assess the naturalness of the VCVs synthesized with the modified pitch contour. The listening test with 30 listeners shows no statistically significant difference between the naturalness of the original and synthesized VCVs with modified pitch indicating that pitch drop due to source-filter interaction may not be critical for the perceived naturalness of VCVs. (C) 2018 Acoustical Society of America
C1 [Rao, Achuth M., V; Ghosh, Prasanta Kumar] Indian Inst Sci, Bangalore, Karnataka, India.
   [Victory, Shiny J.] Mepco Schlenk Engn Coll, Sivakasi, India.
C3 Indian Institute of Science (IISC) - Bangalore; Mepco Schlenk
   Engineering College
RP Rao, MVA (corresponding author), Indian Inst Sci, Bangalore, Karnataka, India.
EM achuthr@iisc.ac.in; jshinyv96@gmail.com; prasantg@iisc.ac.in
CR [Anonymous], 1971, ACOUSTIC THEORY SPEE
   [Anonymous], 2010, IEICE T INFORM SYSTE
   Bavegard M., 2009, WORKING PAPERS LINGU, V43, P38
   Chi XM, 2007, J ACOUST SOC AM, V122, P1735, DOI 10.1121/1.2756793
   EWAN WG, 1979, J ACOUST SOC AM, V66, P358, DOI 10.1121/1.383669
   Fant G, 1979, STL QPSR, V1, P79
   Gelman A, 2005, ANN STAT, V33, P1, DOI 10.1214/009053604000001048
   ITU, 2003, 15341 ITU
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Lucero JC, 2012, J ACOUST SOC AM, V132, P403, DOI 10.1121/1.4728170
   Mittal VK, 2014, J ACOUST SOC AM, V136, P1932, DOI 10.1121/1.4894789
   Morise M, 2016, SPEECH COMMUN, V84, P57, DOI 10.1016/j.specom.2016.09.001
   Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457
   Nord L, 1984, STL QPSR, V25, P25
   Ohala J. J, 1987, EXPLAINING INTRINSIC, P207
   Stevens K. N., 2000, Acoustic Phonetics, V30
   STEVENS KN, 1971, J ACOUST SOC AM, V50, P1180, DOI 10.1121/1.1912751
   Taylor Paul, 2009, TEXT TO SPEECH SYNTH
   Titze I.R., 2006, The Myoelastic Aerodynamic Theory of Phonation
   Titze I, 2008, J ACOUST SOC AM, V123, P1902, DOI 10.1121/1.2832339
   Titze IR, 2008, J ACOUST SOC AM, V123, P2733, DOI 10.1121/1.2832337
   Titze IR, 2016, IEEE-ACM T AUDIO SPE, V24, P2507, DOI [10.1109/taslp.2016.2616543, 10.1109/TASLP.2016.2616543]
   VILKMAN E, 1991, SPEECH COMMUN, V10, P325, DOI 10.1016/0167-6393(91)90001-A
   Yoshimura T., 1999, T I ELECT INF COMMUN, V83, P2099
NR 24
TC 3
Z9 3
U1 0
U2 1
PU ACOUSTICAL SOC AMER AMER INST PHYSICS
PI MELVILLE
PA STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA
SN 0001-4966
EI 1520-8524
J9 J ACOUST SOC AM
JI J. Acoust. Soc. Am.
PD AUG
PY 2018
VL 144
IS 2
BP EL95
EP EL99
DI 10.1121/1.5049510
PG 5
WC Acoustics; Audiology & Speech-Language Pathology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Acoustics; Audiology & Speech-Language Pathology
GA GS4MV
UT WOS:000443620700003
PM 30180658
OA Bronze
DA 2024-01-09
ER

PT J
AU Baird, A
   Jorgensen, SH
   Parada-Cabaleiro, E
   Cummins, N
   Hantke, S
   Schuller, B
AF Baird, Alice
   Jorgensen, Stina Hasse
   Parada-Cabaleiro, Emilia
   Cummins, Nicholas
   Hantke, Simone
   Schuller, Bjoern
TI The Perception of Vocal Traits in Synthesized Voices: Age, Gender, and
   Human Likeness
SO JOURNAL OF THE AUDIO ENGINEERING SOCIETY
LA English
DT Article
ID EMOTION
AB The paralinguistics of the voice are the perceived states and traits that make that voice unique to the human body from which it resonates. In many cases the synthesized voice is produced by concatenated segments of recorded human speech, a complex process that can result in an arguably lifeless voice, which lacks the ability for free-expression among other human qualities. In recent years technology-based companies are developing their own synthesized voice identities, yet seemingly paying little attention to the stereotypical traits being heard. Do such synthetic voice traits differ from the human traits they are modelled on? To explore this, the presented perception study performed by 18 listeners evaluated the paralinguistic traits of gender, age, and human likeness in the IBM voice library. Results herein have shown a similar trend to a previous study by the authors with no voice achieving complete human likeness, no voice being perceived within a single age frequency band, and none tied solidly to their given binary gender-a novel finding as commercially available synthesized voices are typically developed to operate within binary identification structures.
C1 [Baird, Alice; Parada-Cabaleiro, Emilia; Cummins, Nicholas; Hantke, Simone; Schuller, Bjoern] Univ Augsburg, ZDB Chair Embedded Intelligence Hlth Care & Wellb, Augsburg, Germany.
   [Jorgensen, Stina Hasse] Univ Copenhagen, Dept Arts & Cultural Studies, Copenhagen, Denmark.
   [Hantke, Simone] MKK Tech Univ Munchen, MISP Grp, Munich, Germany.
   [Schuller, Bjoern] Imperial Coll London, GLAM, London, England.
C3 University of Augsburg; University of Copenhagen; Technical University
   of Munich; Imperial College London
RP Baird, A (corresponding author), Univ Augsburg, ZDB Chair Embedded Intelligence Hlth Care & Wellb, Augsburg, Germany.
EM alice.baird@informatik.uni-augsburg.de
RI Cummins, Nicholas/AAC-6431-2019; Baird, Alice/AAA-5559-2021; Schuller,
   Björn Wolfgang/D-3241-2011; Parada-Cabaleiro, Emilia/GXF-2079-2022
OI Cummins, Nicholas/0000-0002-1178-917X; Baird, Alice/0000-0002-7003-5650;
   Schuller, Björn Wolfgang/0000-0002-6478-8699; Parada-Cabaleiro,
   Emilia/0000-0003-1843-3632
FU Bavarian State Ministry of Education, Science and the Arts; European
   Union [338164]
FX This work is funded by the Bavarian State Ministry of Education, Science
   and the Arts in the framework of the Centre Digitisation. Bavaria
   (ZD.B), and the European Union's Seventh Framework and Horizon 2020
   Programmes under grant agreement No. 338164 (ERC StG iHEARu).
CR Alku P, 1999, CLIN NEUROPHYSIOL, V110, P1329, DOI 10.1016/S1388-2457(99)00088-7
   [Anonymous], 2006, P LREC
   Arora S.J., 2012, INT J COMPUTER APPL, V60, P34
   Baird Alice, 2017, P 12 INT AUDIO MOSTL, DOI [10.1145/3123514.3123528, DOI 10.1145/3123514.3123528]
   Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008
   Butler J., 1990, GENDER TROUBLE
   Butler J., 2011, BODIES MATTER DISCUR
   COLEMAN RO, 1976, J SPEECH HEAR RES, V19, P168, DOI 10.1044/jshr.1901.168
   Dudley H., 1955, J AUDIO ENG SOC, V3, P170
   FRANCK K, 1949, J CONSULT PSYCHOL, V13, P247
   Hantke S, 2015, INT CONF AFFECT, P891, DOI 10.1109/ACII.2015.7344680
   Hill KT, 2010, CEREB CORTEX, V20, P583, DOI 10.1093/cercor/bhp124
   IBM &REG;, 2017, TEXT TO SPEECH
   IBM Watson Developer Cloud, 2017, SCI SERV
   Johnson K, 1999, J PHONETICS, V27, P359, DOI 10.1006/jpho.1999.0100
   JONES Amelia, 2012, SEEING DIFFERENTLY
   KASHIMA Y, 1995, J PERS SOC PSYCHOL, V69, P925, DOI 10.1037/0022-3514.69.5.925
   KLATT DH, 1990, J ACOUST SOC AM, V87, P820, DOI 10.1121/1.398894
   Latinus M, 2011, CURR BIOL, V21, pR143, DOI 10.1016/j.cub.2010.12.033
   Lattner S, 2005, HUM BRAIN MAPP, V24, P11, DOI 10.1002/hbm.20065
   Laukka P., 2004, THESIS
   Lee EJ, 2003, INT J HUM-COMPUT ST, V58, P347, DOI 10.1016/S1071-5819(03)00009-0
   Lee Eun Ju, 2000, P CHI 00 HUM FACT CO, P289, DOI [10.1145/633292.633461, DOI 10.1145/633292.633461]
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Munoz J. E., 1998, DISIDENTIFCATIONS QU
   Nass C, 1997, J APPL SOC PSYCHOL, V27, P864, DOI 10.1111/j.1559-1816.1997.tb00275.x
   Nass C., 2001, EFFECTS EMOTION VOIC
   Nass C. I., 2010, The Man Who Lied to His Laptop: What Machines Teach Us About Human Relationships
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   OLSON HF, 1966, J AUDIO ENG SOC, V14, P233
   Oord A. V. D., 2016, 160903499 ARXIV, V1609
   Oye O. J., 2016, HCI A GROWING FIELD
   Phan T., 2017, TRANSFORMATIONS, P23
   Pitrelli JF, 2006, IEEE T AUDIO SPEECH, V14, P1099, DOI 10.1109/TASL.2006.876123
   Robertson A., 2016, GOOGLES DEEPMIND AI
   Scherer KR, 2013, COMPUT SPEECH LANG, V27, P40, DOI 10.1016/j.csl.2011.11.003
   Scherer KR, 2001, J CROSS CULT PSYCHOL, V32, P76, DOI 10.1177/0022022101032001009
   Schuller B., 2013, Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing
   Schuller B, 2013, INTERSPEECH, P148
   Schuller B, 2015, COMPUT SPEECH LANG, V29, P100, DOI 10.1016/j.csl.2014.08.003
   Schuller B, 2011, SPEECH COMMUN, V53, P1062, DOI 10.1016/j.specom.2011.01.011
   Sedgwick Eve Kosofsky, 1993, TENDENCIES
   Seppala T. J., 2017, AMAZONS REDESIGNED E
   Sutikno T., 2010, J TELECOMM COMPUTING, V9, P201
   Sycamore Mattilda Bernstein, 2006, Nobody Passes: Rejecting the Rules of Gender and Conformity
   Tamagawa R, 2011, INT J SOC ROBOT, V3, P253, DOI 10.1007/s12369-011-0100-4
   Tokuda K, 2013, P IEEE, V101, P1234, DOI 10.1109/JPROC.2013.2251852
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
NR 49
TC 11
Z9 11
U1 4
U2 13
PU AUDIO ENGINEERING SOC
PI NEW YORK
PA 60 E 42ND ST, NEW YORK, NY 10165-2520 USA
SN 1549-4950
J9 J AUDIO ENG SOC
JI J. Audio Eng. Soc.
PD APR
PY 2018
VL 66
IS 4
BP 277
EP 285
DI 10.17743/jaes.2018.0023
PG 9
WC Acoustics; Engineering, Multidisciplinary
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Acoustics; Engineering
GA GW9GC
UT WOS:000447289400009
DA 2024-01-09
ER

PT J
AU Hubbard, DJ
   Faso, DJ
   Assmann, PF
   Sasson, NJ
AF Hubbard, Daniel J.
   Faso, Daniel J.
   Assmann, Peter F.
   Sasson, Noah J.
TI Production and perception of emotional prosody by adults with autism
   spectrum disorder
SO AUTISM RESEARCH
LA English
DT Article
DE Autism spectrum disorder; emotion; affective prosody; expressive speech;
   vocal affect; speech production; speech perception
ID CHILDREN; COMMUNICATION; EXPRESSION; SPEECH; VOICE; ADOLESCENTS;
   INTONATION; SPEAKERS; MODEL; MIND
AB This study examined production and perception of affective prosody by adults with autism spectrum disorder (ASD). Previous research has reported increased pitch variability in talkers with ASD compared to typically developing (TD) controls in grammatical speaking tasks (e.g., comparing interrogative vs. declarative sentences), but it is unclear whether this pattern extends to emotional speech. In this study, speech recordings in five emotion contexts (angry, happy, interested, sad, and neutral) were obtained from 15 adult males with ASD and 15 controls (Experiment 1), and were later presented to 52 listeners (22 with ASD) who were asked to identify the emotion expressed and rate the level of naturalness of the emotion in each recording (Experiment 2). Compared to the TD group, talkers with ASD produced phrases with greater intensity, longer durations, and increased pitch range for all emotions except neutral, suggesting that their greater pitch variability was specific to emotional contexts. When asked to identify emotion from speech, both groups of listeners were more accurate at identifying the emotion context from speech produced by ASD speakers compared to TD speakers, but rated ASD emotional speech as sounding less natural. Collectively, these results reveal differences in emotional speech production in talkers with ASD that provide an acoustic basis for reported perceptions of oddness in the speech presentation of adults with ASD. Autism Res2017, 10: 1991-2001. (c) 2017 International Society for Autism Research, Wiley Periodicals, Inc.
   Lay SummaryThis study examined emotional speech communication produced and perceived by adults with autism spectrum disorder (ASD) and typically-developing (TD) controls. Compared to the TD group, talkers with ASD produced emotional phrases that were louder, longer, and more variable in pitch. Both ASD and TD listeners were more accurate at identifying emotion in speech produced by ASD speakers compared to TD speakers, but rated ASD emotional speech as sounding less natural.
C1 [Hubbard, Daniel J.; Faso, Daniel J.; Assmann, Peter F.; Sasson, Noah J.] Univ Texas Dallas, Sch Behav & Brain Sci, GR41,800 West Campbell Rd, Dallas, TX 75080 USA.
C3 University of Texas System; University of Texas Dallas
RP Hubbard, DJ (corresponding author), Univ Texas Dallas, Sch Behav & Brain Sci, GR41,800 West Campbell Rd, Dallas, TX 75080 USA.
EM dhubbard@utdallas.edu
OI Sasson, Noah/0000-0002-3676-1253
FU National Institute of Mental Health at National Institutes of Health
   [R15 MH1015945]; National Science Foundation [1124479]; Division Of
   Behavioral and Cognitive Sci; Direct For Social, Behav & Economic Scie
   [1124479] Funding Source: National Science Foundation
FX The authors wish to thank the individuals who participated in the study,
   and the Nonpareil Institute of Plano, TX for helping with participant
   recruitment. We thank the reviewers for their comments on a previous
   version of the manuscript. This research was funded in part by a grant
   from the National Institute of Mental Health at National Institutes of
   Health (R15 MH1015945, PI Sasson). This work is based on a doctoral
   dissertation by DH, who was supported in part by a grant from the
   National Science Foundation (1124479, PI Assmann).
CR Bänziger T, 2012, EMOTION, V12, P1161, DOI 10.1037/a0025827
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Begeer S, 2008, DEV REV, V28, P342, DOI 10.1016/j.dr.2007.09.001
   Boersma P., 2014, PRAAT DOING PHONETIC
   Diehl JJ, 2012, RES AUTISM SPECT DIS, V6, P123, DOI 10.1016/j.rasd.2011.03.012
   Fairbanks G, 1939, SPEECH MONOGR, V6, P87, DOI 10.1080/03637753909374863
   Faso DJ, 2015, J AUTISM DEV DISORD, V45, P75, DOI 10.1007/s10803-014-2194-7
   Fosnot S.M., 1999, P 14 INT C PHON SCI, P1925
   Golan O, 2007, J AUTISM DEV DISORD, V37, P1096, DOI 10.1007/s10803-006-0252-5
   Green H., 2009, INT J SPEECH LANGUAG, V11, P308, DOI DOI 10.1080/17549500903003060
   Grossman RB, 2012, RES AUTISM SPECT DIS, V6, P1150, DOI 10.1016/j.rasd.2012.03.006
   Hubbard K, 2007, J PSYCHOLINGUIST RES, V36, P159, DOI 10.1007/s10936-006-9037-4
   Hudenko WJ, 2012, AUTISM, V16, P641, DOI 10.1177/1362361311402856
   Hurley RSE, 2007, J AUTISM DEV DISORD, V37, P1679, DOI 10.1007/s10803-006-0299-3
   Hus V, 2014, J AUTISM DEV DISORD, V44, P1996, DOI 10.1007/s10803-014-2080-3
   Juslin PN, 2003, PSYCHOL BULL, V129, P770, DOI 10.1037/0033-2909.129.5.770
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   Lord C, 2000, J AUTISM DEV DISORD, V30, P205, DOI 10.1023/A:1005592401947
   Lyons M, 2014, AUTISM RES, V7, P181, DOI 10.1002/aur.1355
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   Nadig A, 2012, J AUTISM DEV DISORD, V42, P499, DOI 10.1007/s10803-011-1264-3
   Paul R, 2005, J AUTISM DEV DISORD, V35, P205, DOI 10.1007/s10803-004-1999-1
   Paul R, 2008, RES AUTISM SPECT DIS, V2, P110, DOI 10.1016/j.rasd.2007.04.001
   Peppé S, 2003, CLIN LINGUIST PHONET, V17, P345, DOI 10.1080/0269920031000079994
   Peppé S, 2007, J SPEECH LANG HEAR R, V50, P1015, DOI 10.1044/1092-4388(2007/071)
   R Core Team, 2018, R: A Language and Environment for Statistical Computing
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Russell JA, 2003, PSYCHOL REV, V110, P145, DOI 10.1037/0033-295X.110.1.145
   Rutherford MD, 2002, J AUTISM DEV DISORD, V32, P189, DOI 10.1023/A:1015497629971
   Sasson NJ, 2017, SCI REP-UK, V7, DOI 10.1038/srep40700
   Sasson NJ, 2013, AUTISM RES, V6, P134, DOI 10.1002/aur.1272
   Sasson NJ, 2011, J NEURODEV DISORD, V3, P87, DOI 10.1007/s11689-010-9068-x
   SCHERER KR, 1986, PSYCHOL BULL, V99, P143, DOI 10.1037/0033-2909.99.2.143
   Scherer KR, 2003, SPEECH COMMUN, V40, P227, DOI 10.1016/S0167-6393(02)00084-5
   SCHERER KR, 1979, EMOTIONS PERSONALITY, P495
   Shriberg LD, 2011, J AUTISM DEV DISORD, V41, P405, DOI 10.1007/s10803-010-1117-5
   Stewart ME, 2013, AUTISM, V17, P6, DOI 10.1177/1362361311424572
   Uljarevic M, 2013, J AUTISM DEV DISORD, V43, P1517, DOI 10.1007/s10803-012-1695-5
   Van Bourgondien M. E., 1992, HIGH FUNCTIONING IND, P227, DOI DOI 10.1007/978-1-4899-2456-8_12
   Wang AT, 2007, ARCH GEN PSYCHIAT, V64, P698, DOI 10.1001/archpsyc.64.6.698
   Wechsler D, 1999, WECHSLER ABBREVIATED
   WILLIAMS CE, 1972, J ACOUST SOC AM, V52, P1238, DOI 10.1121/1.1913238
   Wundt W., 1909, GRUNDRISS PSYCHOL AC
NR 44
TC 34
Z9 40
U1 1
U2 41
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1939-3792
EI 1939-3806
J9 AUTISM RES
JI Autism Res.
PD DEC
PY 2017
VL 10
IS 12
BP 1991
EP 2001
DI 10.1002/aur.1847
PG 11
WC Behavioral Sciences; Psychology, Developmental
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Behavioral Sciences; Psychology
GA FQ2ED
UT WOS:000418168800008
PM 28815940
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Rodero, E
AF Rodero, Emma
TI Effectiveness, attention, and recall of human and artificial voices in
   an advertising story. Prosody influence and functions of voices
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Human and artificial voices; Prosody; Effectiveness; Attention; Recall
ID COMPUTER-SYNTHESIZED SPEECH; NATURAL SPEECH; PERCEPTION;
   INTELLIGIBILITY; PERSONALITY; CONSISTENCY; INTONATION; LISTENERS;
   ATTITUDES; EMOTION
AB Many users are exposed every day to artificial voices in their different devices. Because of this, there is a growing interest both in improving the quality of these voices and in analyzing how they are perceived and processed. However, very little research has been conducted to examine nonverbal elements such as prosody. Accordingly, the first purpose of this study is to determine how artificial voices compared to human voices are processed in a narrative advertising story modifying prosody regarding effectiveness, attention, concentration, and recall. The second objective is to evaluate their functions for different applications, advertising among them. The results show that human voices are assessed as more effective and achieved a better level of effectiveness, attention, and recall with less concentration. Concerning the functions, the more important and complex a function is, the more a human voice is preferred over an artificial one. (C) 2017 Elsevier Ltd. All rights reserved.
C1 [Rodero, Emma] Pompeu Fabra Univ UPF, Dept Commun, Roc Boronat 138, Barcelona 08108, Spain.
C3 Pompeu Fabra University
RP Rodero, E (corresponding author), Pompeu Fabra Univ UPF, Dept Commun, Roc Boronat 138, Barcelona 08108, Spain.
EM emma.rodero@upf.edu
RI Rodero, Emma/G-9235-2015
OI Rodero, Emma/0000-0003-0948-3400
CR [Anonymous], 10 IND U SPEECH RES
   [Anonymous], 1997, AUGMENT ALTERN COMM, DOI [DOI 10.1080/07434619712331277878, 10.1080/07434619712331277878]
   [Anonymous], 1997, HDB PHONET SCI
   Boersma  P., 2017, PRAAT DOING PHONETIC
   Brave S, 2005, INT J HUM-COMPUT ST, V62, P161, DOI 10.1016/j.ijhcs.2004.11.002
   Cabral J., 2006, P SPECOM JUN 25 29 S, P536
   Chen F., 2006, Designing human interface in speech technology
   Cohen J, 1988, STAT POWER ANAL BEHA
   Crowell CR, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3735, DOI 10.1109/IROS.2009.5354204
   Delogu C, 1998, SPEECH COMMUN, V24, P153, DOI 10.1016/S0167-6393(98)00009-0
   DUFFY SA, 1992, LANG SPEECH, V35, P351, DOI 10.1177/002383099203500401
   Fleming J. H., 2007, Human sigma: Managing the employee-customer encounter
   GelinasChebat C, 1996, PERCEPT MOTOR SKILL, V83, P243, DOI 10.2466/pms.1996.83.1.243
   GILES H, 1973, SPEECH MONOGR, V40, P330
   Gong L., 2003, INT J SPEECH TECHNOL, V6, P123
   Grice M., 1991, EUROSPEECH 91. 2nd European Conference on Speech Communication and Technology Proceedings, P879
   Gussenhoven C., 2004, The Phonology of Tone and Intonation
   Hennig S., 2012, IEEE RO MAN
   Hinterleitner E, 2014, QUALITY EXPERIENCE A
   Hinterleitner F., 2012, SPOK LANG TECHN WORK
   Hirschberg J, 2005, HDB PRAGMATICS, P515
   Hirst D. J., 2007, P 16 INT C PHONETIC, P1233
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   JENKINS JJ, 1982, B PSYCHONOMIC SOC, V20, P203
   Kamm C., 1997, NSF WORKSH HUM CENTR
   Lai J., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P206, DOI 10.1145/365024.365100
   Lai J., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P321, DOI 10.1145/332040.332451
   Levi S. V., 2007, PSYCHOLINGUISTIC PHE
   Luce P. A., 1981, 7 IND U SPEECH RES L
   LUCE PA, 1983, HUM FACTORS, V25, P17, DOI 10.1177/001872088302500102
   MARICS MA, 1988, HUM FACTORS, V30, P719, DOI 10.1177/001872088803000608
   Mayo C, 2011, SPEECH COMMUN, V53, P311, DOI 10.1016/j.specom.2010.10.003
   Mayor O., 2009, AES 35 INT C LOND UK, P11
   MIRENDA P, 1989, J SPEECH HEAR RES, V32, P175, DOI 10.1044/jshr.3201.175
   Moody T., 1986, P VOIC INP OUT SOC A
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass Clliford, 2005, WIRED FOR SPEECH
   Niebuhr O, 2016, COMPUT HUM BEHAV, V64, P366, DOI 10.1016/j.chb.2016.06.059
   Nusbaum H. C., 1995, International Journal of Speech Technology, V1, P7, DOI 10.1007/BF02277176
   Nye P. W., 1975, SR41 HASK LAB
   PARIS CR, 1995, HUM FACTORS, V37, P335, DOI 10.1518/001872095779064609
   Paris CR, 2000, HUM FACTORS, V42, P421, DOI 10.1518/001872000779698132
   Pauletto S, 2013, LOGOP PHONIATR VOCO, V38, P115, DOI 10.3109/14015439.2013.810303
   Pisoni D. B., 1980, ICASSP 80 Proceedings. IEEE International Conference on Acoustics, Speech and Signal Processing, P572
   Pisoni David B, 1987, Comput Speech Lang, V2, P303, DOI 10.1016/0885-2308(87)90014-3
   Pisoni DB., 1997, Progress in Speech Synthesis, P541, DOI [10.1007/978-1-4612-1894-4_43, DOI 10.1007/978-1-4612-1894-4_43]
   Pittam Jeff, 1994, Voice in social interaction
   Potter RF, 2006, MEDIA PSYCHOL, V8, P395, DOI 10.1207/s1532785xmep0804_4
   Rao K.S., 2012, PREDICTING PROSODY T
   Reeves B., 1996, The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Pla, DOI DOI 10.1007/S42452-020-2192-7
   Rodero E., 2007, ESTUDIOS MENSAJE PER, P523
   Rodero E., 2006, EXLING 2006
   Rodero E, 2016, MEDIA PSYCHOL, V19, P224, DOI 10.1080/15213269.2014.1002942
   Rodero E, 2015, J NONVERBAL BEHAV, V39, P79, DOI 10.1007/s10919-014-0201-5
   Rodero E, 2013, SEX ROLES, V68, P349, DOI 10.1007/s11199-012-0247-y
   Rodero Emma, 2014, J APPL LINGUISTICS P, V11, P89, DOI [10.1558/japl.32411, DOI 10.1558/JAPL.32411]
   Roring RW, 2007, HUM FACTORS, V49, P25, DOI 10.1518/001872007779598055
   Rosenberg A, 2009, SPEECH COMMUN, V51, P640, DOI 10.1016/j.specom.2008.11.001
   Rosenthal-von der Pütten AM, 2013, INT J SOC ROBOT, V5, P17, DOI 10.1007/s12369-012-0173-8
   Salza P. L, 1993, SPEECH TECHNOLOGY AS
   Sanderman AA, 1997, LANG SPEECH, V40, P391, DOI 10.1177/002383099704000405
   Schutz S., 2006, PERCEPTION ANAL SYNT, V47
   Shank DB, 2013, COMPUT HUM BEHAV, V29, P715, DOI 10.1016/j.chb.2012.11.006
   Signorello R., 2012, P 7 GSCP INT C SPEEC, P343
   SLOWIACZEK LM, 1985, HUM FACTORS, V27, P701, DOI 10.1177/001872088502700609
   Stern SE, 2014, REHABIL PSYCHOL, V59, P289, DOI 10.1037/a0036663
   Syrdal A. K., 1994, APPL SPEECH TECHNOLO
   Taake K., 2009, THESIS WASHINGTON U
   TERKEN J, 1988, J PHONETICS, V16, P453, DOI 10.1016/S0095-4470(19)30521-2
   Vainio M., 2002, P IEEE 2002 WORKSH S
   van Wissen A, 2012, COMPUT HUM BEHAV, V28, P23, DOI 10.1016/j.chb.2011.08.006
   Winters S. J., 2004, PROGR REPORT RES SPO, V26
   Wolters MK, 2015, J AM MED INFORM ASSN, V22, P35, DOI 10.1136/amiajnl-2014-002820
NR 74
TC 7
Z9 9
U1 3
U2 20
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD DEC
PY 2017
VL 77
BP 336
EP 346
DI 10.1016/j.chb.2017.08.044
PG 11
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA FM2NB
UT WOS:000414820400035
DA 2024-01-09
ER

PT J
AU Yamasaki, R
   Montagnoli, A
   Murano, EZ
   Gebrim, E
   Hachiya, A
   da Silva, JVL
   Behlau, M
   Tsuji, D
AF Yamasaki, Rosiane
   Montagnoli, Arlindo
   Murano, Emi Z.
   Gebrim, Eloisa
   Hachiya, Adriana
   Lopes da Silva, Jorge Vicente
   Behlau, Mara
   Tsuji, Domingos
TI Perturbation Measurements on the Degree of Naturalness of Synthesized
   Vowels
SO JOURNAL OF VOICE
LA English
DT Article
DE Synthesized voices; Acoustical measurements; Auditory; perceptual
   evaluation; Naturalness perception; Vocal tract model
ID VOICE QUALITY; PERCEPTION; SHIMMER; JITTER; EXPERIENCE; SPEECH
AB Objective. To determine the impact of jitter and shimmer on the degree of naturalness perception of synthesized vowels produced by acoustical simulation with glottal pulses (GP) and with solid model of the vocal tract (SMVT).
   Study Design. Prospective study.
   Methods. Synthesized vowels were produced in three steps: 1. Eighty GP were developed (20 with jitter, 20 with shimmer, 20 with jitter+shimmer, 20 without perturbation); 2. A SMVT was produced based on magnetic resonance imaging (MRI) from a woman during phonation-/epsilon/ and using rapid prototyping technology; 3. Acoustic simulations were performed to obtain eighty synthesized vowels-/epsilon/. Two experiments were performed. First Experiment: three judges rated 120 vowels (20 humans+80 synthesized+20% repetition) as "human" or "synthesized". Second Experiment: twenty PowerPoint slide sequences were created. Each slide had 4 synthesized vowels produced with the four perturbation condition. Evaluators were asked to rate the vowels from the most natural to the most artificial.
   Results. First Experiment: all the human vowels were classified as human; 27 out of eighty synthesized vowels were rated as human, 15 of those were produced with jitter+shimmer, 10 with jitter, 2 without perturbation and none with shimmer. Second Experiment: Vowels produced with jitter+shimmer were considered as the most natural. Vowels with shimmer and without perturbation were considered as the most artificial.
   Conclusions. The association of jitter and shimmer increased the degree of naturalness of synthesized vowels. Acoustic simulations performed with GP and using SMVT demonstrated a possible method to test the effect of the perturbation measurements on synthesized voices.
C1 [Yamasaki, Rosiane; Murano, Emi Z.; Hachiya, Adriana; Tsuji, Domingos] Univ Sao Paulo, Fac Med, Hosp Clin, Dept Otorhinolaryngol, Sao Paulo, Brazil.
   [Montagnoli, Arlindo] Univ Sao Paulo Sao Carlos, Dept Elect Engn, Sao Paulo, Brazil.
   [Gebrim, Eloisa] Univ Sao Paulo, Fac Med, Hosp Clin, Dept Radiol,InRad, Sao Paulo, Brazil.
   [Lopes da Silva, Jorge Vicente] Ctr Tecnol Informacao Renato Archer, Div Tridimens Technol, Campinas, SP, Brazil.
   [Behlau, Mara] Univ Fed Sao Paulo UNIFESP, Dept Speech Language Pathol & Audiol, Sao Paulo, Brazil.
   [Behlau, Mara] CEV, Sao Paulo, Brazil.
C3 Universidade de Sao Paulo; Universidade de Sao Paulo; Universidade de
   Sao Paulo; Universidade Federal de Sao Paulo (UNIFESP)
RP Yamasaki, R (corresponding author), Rua Oscar Freire,2250 5o Andar 502, BR-05409011 Sao Paulo, SP, Brazil.
EM r.yamasaki@uol.com.br
RI Montagnoli, Arlindo Neto/C-3802-2012; YAMASAKI, ROSIANE/ABB-5110-2021;
   da Silva, Jorge Vicente Lopes/C-8502-2012; Behlau, Mara
   Suzana/AAN-1054-2021
OI Montagnoli, Arlindo Neto/0000-0002-4095-3602; da Silva, Jorge Vicente
   Lopes/0000-0002-2347-5215; Behlau, Mara Suzana/0000-0003-4663-4546;
   Tsuji, Domingos/0000-0002-8219-7550; Gebrim, Eloisa Maria M
   Santiago/0000-0002-6514-3825; YAMASAKI, ROSIANE/0000-0002-7960-4143
FU Fundacao de Amparo a Pesquisa do Estado de Sao Paulo [FAPESP:
   2012/17390-3]
FX The authors thank the Fundacao de Amparo a Pesquisa do Estado de Sao
   Paulo (FAPESP: 2012/17390-3) for financial support.
CR Amorim P, 2015, ADV VISUAL COMPUTING, P14
   Behlau M., 2001, VOICE BOOK SPECIALIS, P1
   Brockmann M, 2008, J SPEECH LANG HEAR R, V51, P1152, DOI 10.1044/1092-4388(2008/06-0208)
   Brockmann M, 2011, J VOICE, V25, P44, DOI 10.1016/j.jvoice.2009.07.002
   Dang JW, 1997, J ACOUST SOC AM, V101, P456, DOI 10.1121/1.417990
   Englert M, 2016, J VOICE, V30, DOI 10.1016/j.jvoice.2015.07.017
   Fraj S, 2012, J ACOUST SOC AM, V132, P2603, DOI 10.1121/1.4751536
   Fujita S, 2005, ACOUST SCI TECH, V26
   Gerratt BR, 2001, J ACOUST SOC AM, V110, P2560, DOI 10.1121/1.1409969
   Gonçalves Maria Inês Rebelo, 2009, Braz. j. otorhinolaryngol., V75, P680
   HILLENBRAND J, 1988, J ACOUST SOC AM, V83, P2361, DOI 10.1121/1.396367
   Honda K., 2008, J ACOUST SOC AM, V123, P3731
   KERSTA LG, 1960, J ACOUST SOC AM, V32, P1502, DOI 10.1121/1.1935196
   Kisenwether JS, 2015, J VOICE, V29, P548, DOI 10.1016/j.jvoice.2014.11.006
   Kreiman J, 2005, J ACOUST SOC AM, V117, P2201, DOI 10.1121/1.1858351
   Kreiman J, 2015, J ACOUST SOC AM, V138, P1, DOI 10.1121/1.4922174
   Lopes LW, 2014, CODAS, V26, P382, DOI 10.1590/2317-1782/20142013033
   Mattioli F, 2015, J VOICE, V29, P455, DOI 10.1016/j.jvoice.2014.09.027
   Montagnoli AN., 2015, VOICE ANAL PROGRAM 1
   Murphy PJ, 2000, J ACOUST SOC AM, V107, P978, DOI 10.1121/1.428272
   Nusbaum H. C., 1995, International Journal of Speech Technology, V1, P7, DOI 10.1007/BF02277176
   Oppenheim A. V., 2010, Discrete-time signal processing, V3rd
   Petrovic-Lazic M, 2015, J VOICE, V29, P241, DOI 10.1016/j.jvoice.2014.07.009
   ROSENBERG AE, 1971, J ACOUST SOC AM, V49, P583, DOI 10.1121/1.1912389
   ROZSYPAL AJ, 1979, J PHONETICS, V7, P343, DOI 10.1016/S0095-4470(19)31069-1
   ROZSYPAL AJ, 1975, J ACOUST SOC AM, V58, pS23, DOI 10.1121/1.2002025
   Smruti S, 2015, ADV INTELL SYST, V328, P367, DOI 10.1007/978-3-319-12012-6_40
   Sofranko JL, 2014, J VOICE, V28, P24, DOI 10.1016/j.jvoice.2013.06.001
   Sorensen MK, 2015, J VOICE, V30
   Titze IR, 2000, PRINCIPLES VOICE PRO, P313
   TOPALOGLU I, 2014, OTOLARYNGOL HEAD NEC, V151, P1003
   Yiu EML, 2002, J ACOUST SOC AM, V112, P1091, DOI 10.1121/1.1500753
   Yu ZW, 2014, J VOICE, V28, DOI 10.1016/j.jvoice.2014.03.014
   Zhang Y, 2005, J VOICE, V19, P519, DOI 10.1016/j.jvoice.2004.11.005
NR 34
TC 2
Z9 2
U1 1
U2 6
PU MOSBY-ELSEVIER
PI NEW YORK
PA 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN 0892-1997
EI 1873-4588
J9 J VOICE
JI J. Voice
PD MAY
PY 2017
VL 31
IS 3
AR 389.e1
DI 10.1016/j.jvoice.2016.09.020
PG 8
WC Audiology & Speech-Language Pathology; Otorhinolaryngology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Audiology & Speech-Language Pathology; Otorhinolaryngology
GA EX9AK
UT WOS:000403543600053
PM 27777057
DA 2024-01-09
ER

PT J
AU Birkholz, P
   Martin, L
   Xu, Y
   Scherbaum, S
   Neuschaefer-Rube, C
AF Birkholz, Peter
   Martin, Lucia
   Xu, Yi
   Scherbaum, Stefan
   Neuschaefer-Rube, Christiane
TI Manipulation of the prosodic features of vocal tract length, nasality
   and articulatory precision using articulatory synthesis
SO COMPUTER SPEECH AND LANGUAGE
LA English
DT Article
DE Prosody; Feature manipulation; Articulatory synthesis
ID SPEECH SYNTHESIS SYSTEM; VOWEL REDUCTION; VOICE QUALITY; EMOTIONS;
   EXPRESSION; PERSONALITY; SPEAKING; STRESS
AB Vocal emotions, as well as different speaking styles and speaker traits, are characterized by a complex interplay of multiple prosodic features. Natural sounding speech synthesis with the ability to control such paralinguistic aspects requires the manipulation of the corresponding prosodic features. With traditional concatenative speech synthesis it is easy to manipulate the "primary" prosodic features pitch, duration, and intensity, but it is very hard to individually control "secondary" prosodic features like phonation type, vocal tract length, articulatory precision and nasality. These secondary features can be controlled more directly with parametric synthesis methods. In the present study we analyze the ability of articulatory speech synthesis to control secondary prosodic features by rule. To this end, nine German words were re-synthesized with the software VocalTractLab 2.1 and then manipulated in different ways at the articulatory level to vary vocal tract length, articulatory precision and degree of nasality. Listening tests showed that most of the intended prosodic manipulations could be reliably identified with recognition rates between 77% and 96%. Only the manipulations to increase articulatory precision were hardly recognized. The results suggest that rule-based manipulations in articulatory synthesis are generally sufficient for the convincing synthesis of secondary prosodic features at the word level. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Birkholz, Peter] Tech Univ Dresden, Inst Acoust & Speech Commun, D-01062 Dresden, Germany.
   [Martin, Lucia; Neuschaefer-Rube, Christiane] Univ Hosp Aachen, Dept Phoniatr Pedaudiol & Commun Disorders, Pauwelsstr 30, D-52074 Aachen, Germany.
   [Martin, Lucia; Neuschaefer-Rube, Christiane] Rhein Westfal TH Aachen, Pauwelsstr 30, D-52074 Aachen, Germany.
   [Xu, Yi] UCL, Dept Speech Hearing & Phonet Sci, Chandler House,2 Wakefield St, London, England.
   [Scherbaum, Stefan] Tech Univ Dresden, Dept Psychol, D-01062 Dresden, Germany.
C3 Technische Universitat Dresden; RWTH Aachen University; RWTH Aachen
   University Hospital; RWTH Aachen University; University of London;
   University College London; Technische Universitat Dresden
RP Birkholz, P (corresponding author), Tech Univ Dresden, Inst Acoust & Speech Commun, D-01062 Dresden, Germany.
EM peter.birkholz@tu-dresden.de
RI Xu, Yi/M-9738-2019; Xu, Yi/C-4013-2008
OI Xu, Yi/0000-0002-8541-2658; Xu, Yi/0000-0002-8541-2658; Scherbaum,
   Stefan/0000-0002-4408-6016
CR Airas M, 2006, PHONETICA, V63, P26, DOI 10.1159/000091405
   [Anonymous], 2011, 1 INT WORKSHOP PERFO
   [Anonymous], 2005, 3D ARTIKULATORISCHE
   Aryal S, 2016, COMPUT SPEECH LANG, V36, P260, DOI 10.1016/j.csl.2015.02.003
   Beller G., 2008, 4 INT C SPEECH PROS
   Birkholz P., 2011, P INT 2011 FLOR IT, P2681
   Birkholz P., 2004, P INTERSPEECH 2004, P1125, DOI [10.21437/Interspeech.2004-409, DOI 10.21437/INTERSPEECH.2004-409]
   Birkholz P., 2007, 8 ANN C INT SPEECH C, P2865
   Birkholz P, 2015, J ACOUST SOC AM, V137, P1503, DOI 10.1121/1.4906836
   Birkholz P, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060603
   Birkholz P, 2011, IEEE T AUDIO SPEECH, V19, P1422, DOI 10.1109/TASL.2010.2091632
   Black A. W., 2003, EUROSPEECH, P1649
   Boersma P., 2014, PRAAT DOING PHONETIC
   Burkhardt F., 2000, ISCA TUT RES WORKSH
   Burkhardt F, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P2627
   Campbell N., 2003, P 15 INT C PHON SCI, P2417
   Cheng CE, 2013, J ACOUST SOC AM, V134, P4481, DOI 10.1121/1.4824930
   Chuenwattanapranithi S, 2008, PHONETICA, V65, P210, DOI 10.1159/000192793
   Clark R.A.J., 2007, P BLIZZ CHALL WORKSH
   Fant G., 1975, STL QPSR, V16, P1
   FOURAKIS M, 1991, J ACOUST SOC AM, V90, P1816, DOI 10.1121/1.401662
   GIBBS RW, 1986, J EXP PSYCHOL GEN, V115, P3, DOI 10.1037/0096-3445.115.1.3
   Gobl C, 2003, SPEECH COMMUN, V40, P189, DOI 10.1016/S0167-6393(02)00082-1
   Grichkovtsova I., 2009, ROLE PROSODY AFFECTI
   Hunt AJ, 1996, INT CONF ACOUST SPEE, P373, DOI 10.1109/ICASSP.1996.541110
   Iida A, 2003, SPEECH COMMUN, V40, P161, DOI 10.1016/S0167-6393(02)00081-X
   Kane J, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P184
   KLATT DH, 1980, J ACOUST SOC AM, V67, P971, DOI 10.1121/1.383940
   Ladd DR, 2008, CAMB STUD LINGUIST, V79, P1
   LINDBLOM B, 1963, J ACOUST SOC AM, V35, P1773, DOI 10.1121/1.1918816
   LINDBLOM B, 1990, SPEECH PRODUCTION SP, P413, DOI DOI 10.1007/978-94-009-2037-8
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   MURRAY IR, 1995, SPEECH COMMUN, V16, P369, DOI 10.1016/0167-6393(95)00005-9
   Patel S, 2011, BIOL PSYCHOL, V87, P93, DOI 10.1016/j.biopsycho.2011.02.010
   Pfitzinger H.R, 2006, SPEECH PROSODY, P6
   Picart B, 2014, COMPUT SPEECH LANG, V28, P687, DOI 10.1016/j.csl.2013.04.008
   Prom-on S, 2009, J ACOUST SOC AM, V125, P405, DOI 10.1121/1.3037222
   Scherer KR, 2015, COMPUT SPEECH LANG, V29, P218, DOI 10.1016/j.csl.2013.10.002
   Scherer KR, 2003, SER AFFECTIVE SCI, P433
   SCHERER KR, 1978, EUR J SOC PSYCHOL, V8, P467, DOI 10.1002/ejsp.2420080405
   Schroder M., 2010, BLUEPRINT AFFECTIVE, P222
   Schroder Marc, 2001, 7 EUR C SPEECH COMM
   Schuller B, 2015, COMPUT SPEECH LANG, V29, P100, DOI 10.1016/j.csl.2014.08.003
   Sendlmeier W.F., 1998, FORUM PHONETICUM, V66, P1
   Simpson AP, 2001, J ACOUST SOC AM, V109, P2153, DOI 10.1121/1.1356020
   Stevens K., 1998, Acoustic phonetics
   van den Doel K., 2006, 7 INT SEM SPEECH PRO
   VANBERGEM DR, 1993, SPEECH COMMUN, V12, P1, DOI 10.1016/0167-6393(93)90015-D
   Waaramaa T, 2008, FOLIA PHONIATR LOGO, V60, P249, DOI 10.1159/000151762
   Xu Y., 2013, PROSODY ICONICITY, P33, DOI DOI 10.1075/ILL.13.02XU
   Yamagishi J, 2005, IEICE T INF SYST, VE88D, P502, DOI 10.1093/ietisy/e88-d.3.502
   Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004
NR 52
TC 15
Z9 15
U1 0
U2 24
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0885-2308
EI 1095-8363
J9 COMPUT SPEECH LANG
JI Comput. Speech Lang.
PD JAN
PY 2017
VL 41
BP 116
EP 127
DI 10.1016/j.csl.2016.06.004
PG 12
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DY1OI
UT WOS:000384863900006
OA Green Submitted
DA 2024-01-09
ER

PT J
AU Kuriki, S
   Tamura, Y
   Igarashi, M
   Kato, N
   Nakano, T
AF Kuriki, Shinji
   Tamura, Yuri
   Igarashi, Miki
   Kato, Nobumasa
   Nakano, Tamami
TI Similar impressions of humanness for human and artificial singing voices
   in autism spectrum disorders
SO COGNITION
LA English
DT Article
DE Autism; Artificial voice; Humanness; Music
ID MIND PERCEPTION; CHILDREN; MUSIC; ALEXITHYMIA; INSULA; INDIVIDUALS;
   EMOTIONS; ROBOTS
AB People with autism spectrum disorder (ASD) exhibit impairments in the perception of and orientation to social information related to humans, and some people with ASD show higher preference toward human-like robots than other humans. We speculated that this behavioural bias in people with ASD is caused by a weakness in their perception of humanness. To address this issue, we investigated whether people with ASD detect a subtle difference between the same song sung by human and artificial voices even when the lyrics, melody and rhythm are identical. People without ASD answered that the songs sung by a human voice evoked more impressions of humanness (human-likeness, animateness, naturalness, emotion) and more positive feelings (warmth, familiarity, comfort) than those sung by an artificial voice. In contrast, people with ASD had similar impressions of humanness and positive feelings for the songs sung by the human and artificial voices. The evaluations of musical characteristics (complexity, regularity, brightness) did not differ between people with and without ASD. These results suggest that people with ASD are weak in their ability to perceive psychological attributes of humanness. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Kuriki, Shinji; Tamura, Yuri; Nakano, Tamami] Osaka Univ, Sch Med, Dept Brain Physiol, 2-2 Yamadaoka, Suita, Osaka 5650871, Japan.
   [Igarashi, Miki; Kato, Nobumasa] Showa Univ, Sch Med, Dept Psychiat, Tokyo, Japan.
   [Nakano, Tamami] Osaka Univ, Grad Sch Frontiers Biosci, 1-3 Yamadaoka, Suita, Osaka 5650871, Japan.
C3 Osaka University; Showa University; Osaka University
RP Nakano, T (corresponding author), Osaka Univ, Grad Sch Frontiers Biosci, 1-3 Yamadaoka, Suita, Osaka 5650871, Japan.
EM tamami_nakano@fbs.osaka-u.ac.jp
FU Ministry of Education, Culture, Sports, Science and Technology, Japan
   [251195040]; Grants-in-Aid for Scientific Research [15K12620] Funding
   Source: KAKEN
FX This work was supported by the Grant-in-Aid for Scientific Research on
   Innovative Areas 251195040 "Constructive Developmental Science" from the
   Ministry of Education, Culture, Sports, Science and Technology, Japan to
   T.N.
CR Alcántara JI, 2004, J CHILD PSYCHOL PSYC, V45, P1107, DOI 10.1111/j.1469-7610.2004.t01-1-00303.x
   Allen R, 2013, J AUTISM DEV DISORD, V43, P432, DOI 10.1007/s10803-012-1587-8
   Bamiou DE, 2003, BRAIN RES REV, V42, P143, DOI 10.1016/S0165-0173(03)00172-3
   Baron-Cohen S, 2001, J AUTISM DEV DISORD, V31, P5, DOI 10.1023/A:1005653411471
   Baron-Cohen S, 2006, PROG NEURO-PSYCHOPH, V30, P865, DOI 10.1016/j.pnpbp.2006.01.010
   Bird G, 2013, TRANSL PSYCHIAT, V3, DOI 10.1038/tp.2013.61
   Bird G, 2010, BRAIN, V133, P1515, DOI 10.1093/brain/awq060
   Broadbent E, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0072589
   Dawson G, 1998, J AUTISM DEV DISORD, V28, P479, DOI 10.1023/A:1026043926488
   Diehl JJ, 2012, RES AUTISM SPECT DIS, V6, P249, DOI 10.1016/j.rasd.2011.05.006
   Gebauer L, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00192
   Gray HM, 2007, SCIENCE, V315, P619, DOI 10.1126/science.1134475
   Gray K, 2012, COGNITION, V125, P125, DOI 10.1016/j.cognition.2012.06.007
   Gray K, 2011, P NATL ACAD SCI USA, V108, P477, DOI 10.1073/pnas.1015493108
   Haslam N, 2006, PERS SOC PSYCHOL REV, V10, P252, DOI 10.1207/s15327957pspr1003_4
   Heaton P, 1999, PSYCHOL MED, V29, P1405, DOI 10.1017/S0033291799001221
   Heaton P, 2008, BRIT J DEV PSYCHOL, V26, P171, DOI 10.1348/026151007X206776
   Jemel B, 2006, J AUTISM DEV DISORD, V36, P91, DOI 10.1007/s10803-005-0050-5
   Molnar-Szakacs I, 2012, ANN NY ACAD SCI, V1252, P318, DOI 10.1111/j.1749-6632.2012.06465.x
   Molnar-Szakacs Istvan, 2009, Mcgill J Med, V12, P87
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Nakano T, 2010, P ROY SOC B-BIOL SCI, V277, P2935, DOI 10.1098/rspb.2010.0587
   Pierno AC, 2008, NEUROPSYCHOLOGIA, V46, P448, DOI 10.1016/j.neuropsychologia.2007.08.020
   Remedios R, 2009, J NEUROSCI, V29, P1034, DOI 10.1523/JNEUROSCI.4089-08.2009
   Tamura Y, 2015, SCI REP-UK, V5, DOI 10.1038/srep08799
   Thompson JC, 2011, PERCEPTION, V40, P695, DOI 10.1068/p6900
NR 26
TC 12
Z9 13
U1 1
U2 43
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0010-0277
EI 1873-7838
J9 COGNITION
JI Cognition
PD AUG
PY 2016
VL 153
BP 1
EP 5
DI 10.1016/j.cognition.2016.04.004
PG 5
WC Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA DQ9VZ
UT WOS:000379558700001
PM 27107740
DA 2024-01-09
ER

PT J
AU Crumpton, J
   Bethel, CL
AF Crumpton, Joe
   Bethel, Cindy L.
TI A Survey of Using Vocal Prosody to Convey Emotion in Robot Speech
SO INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS
LA English
DT Article
DE Synthesized speech; Emotional robot speech; Human-robot interaction;
   Vocal prosody
ID VOICE; CONVERSION; EXPRESSION; MODEL; PITCH
AB The use of speech for robots to communicate with their human users has been facilitated by improvements in speech synthesis technology. Now that the intelligibility of synthetic speech has advanced to the point that speech synthesizers are a widely accepted and used technology, what are other aspects of speech synthesis that can be used to improve the quality of human-robot interaction? The communication of emotions through changes in vocal prosody is one way to make synthesized speech sound more natural. This article reviews the use of vocal prosody to convey emotions between humans, the use of vocal prosody by agents and avatars to convey emotions to their human users, and previous work within the human-robot interaction (HRI) community addressing the use of vocal prosody in robot speech. The goals of this article are (1) to highlight the ability and importance of using vocal prosody to convey emotions within robot speech and (2) to identify experimental design issues when using emotional robot speech in user studies.
C1 [Crumpton, Joe] Mississippi State Univ, Distributed Analyt & Secur Inst, Starkville, MS 39762 USA.
   [Bethel, Cindy L.] Mississippi State Univ, Social Therapeut & Robot Syst Lab, Dept Comp Sci & Engn, Starkville, MS USA.
C3 Mississippi State University; Mississippi State University
RP Crumpton, J (corresponding author), Mississippi State Univ, Distributed Analyt & Secur Inst, Starkville, MS 39762 USA.
EM crumpton@dasi.msstate.edu; cbethel@cse.msstate.edu
OI Crumpton, Joe/0000-0002-5932-2480
CR Aihara R., 2012, Amer. J. Signal Process., V2, P134, DOI DOI 10.5923/J.AJSP.20120205.06
   Alm CO, 2005, P C HUM LANG TECHN E, P579, DOI [10.3115/1220575.1220648, DOI 10.3115/1220575.1220648]
   Amir N., 2009, PROC IEEE 3 INT C AF, P1
   [Anonymous], 1992, Proceedings of the 1992 International Conference on Spoken Language Processing
   [Anonymous], 2004, EMOTION EVOLUTION RA
   Ashimura K, 2013, VOCABULARIES EMOTION
   Bachorowski J.A., 2008, Handbook of Emotions, V3rd ed., P196
   Baggia P, 2013, EMOTION MARKUP LANGU
   Baggia P., 2010, Speech synthesis markup language (SSML) version 1.1
   Bainbridge WA, 2011, INT J SOC ROBOT, V3, P41, DOI 10.1007/s12369-010-0082-7
   Barrett LF, 2006, PERS SOC PSYCHOL REV, V10, P20, DOI 10.1207/s15327957pspr1001_2
   BATES J, 1994, COMMUN ACM, V37, P122, DOI 10.1145/176789.176803
   Beale R, 2009, INT J HUM-COMPUT ST, V67, P755, DOI 10.1016/j.ijhcs.2009.05.001
   Beckman M., 1994, GUIDELINES TOBI LABE
   Benoit C, 1996, SPEECH COMMUN, V18, P381, DOI 10.1016/0167-6393(96)00026-X
   Bethel CL, 2006, P 2006 AAAI FALL S S
   Black A. W., CMU ARCTIC SPEECH SY
   Boersma P., 2021, Glot International
   Breazeal C., 1999, Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289), P858, DOI 10.1109/IROS.1999.812787
   Breazeal C, 2002, AUTON ROBOT, V12, P83, DOI 10.1023/A:1013215010749
   Brooks DJ, 2012, P WORKSH 26 AAAI C A
   Burkhardt F, 2000, P ISCA TUT RES WORKS
   Cahn J. E., 1990, Journal of the American Voice I/O Society, V8, P1
   Cowie R, 2003, SPEECH COMMUN, V40, P5, DOI 10.1016/S0167-6393(02)00071-7
   Crumpton J, 2015, P 2015 INT C COLL TE
   Crumpton J, 2014, 23 IEEE INT S ROB HU
   Dautenhahn K, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1488
   Dmello S., 2013, ACM T INTERACT INTEL, V2, P1, DOI 10.1145/2395123.2395128
   EKMAN P, 1969, SCIENCE, V164, P86, DOI 10.1126/science.164.3875.86
   Erickson D, 2005, ACOUST SCI TECHNOL, V26, P317, DOI 10.1250/ast.26.317
   Fairbanks G, 1939, SPEECH MONOGR, V6, P87, DOI 10.1080/03637753909374863
   FRICK RW, 1985, PSYCHOL BULL, V97, P412, DOI 10.1037/0033-2909.97.3.412
   Greasley P, 2000, LANG SPEECH, V43, P355, DOI 10.1177/00238309000430040201
   Harnmerschmidt K, 2007, J VOICE, V21, P531, DOI 10.1016/j.jvoice.2006.03.002
   Hennig S., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P589, DOI 10.1109/ROMAN.2012.6343815
   HUTTAR GL, 1968, J SPEECH HEAR RES, V11, P481, DOI 10.1044/jshr.1103.481
   IIDA A, 2000, P ISCA WORKSH SPEECH, V1, P167
   Jung Y, 2004, Proceedings: 7th Annual International Workshop on Presence, Valencia, Spain, P80, DOI DOI 10.1145/1349822.1349866
   Khan Z., 1998, TRITANAP9821 KTH
   Kidd C. D., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P3559
   Kim E. S., 2009, 2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI), P23
   Laukka P, 2004, THESIS UPPSALA U
   Laukka P, 2011, COMPUT SPEECH LANG, V25, P84, DOI 10.1016/j.csl.2010.03.004
   Leyzberg D., 2012, P ANN M COGNITIVE SC, V34
   Leyzberg D, 2011, ACMIEEE INT CONF HUM, P347, DOI 10.1145/1957656.1957789
   Li XY, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P5009, DOI 10.1109/IROS.2009.5354007
   Ling ZH, 2015, IEEE SIGNAL PROC MAG, V32, P35, DOI 10.1109/MSP.2014.2359987
   MASSARO DW, 1989, BEHAV BRAIN SCI, V12, P778, DOI 10.1017/S0140525X0002584X
   Massaro DW, 1996, PSYCHON B REV, V3, P215, DOI 10.3758/BF03212421
   Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918
   Microsoft, 2015, PROS EL
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Nass Clifford, 2005, P HUM FACT COMP SYST, P1973, DOI [DOI 10.1145/1056808.1057070, 10.1145/1056808.1057070]
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Niu HN, 2003, INT CONF ACOUST SPEE, P125
   Nuance Communications, 2015, DRAG MOB SDK REF
   Pearson JC, 2000, An Introduction to Human Communication: Understanding and Sharing, V8th
   Pierre-Yves O, 2003, INT J HUM-COMPUT ST, V59, P157, DOI 10.1016/S1071-5819(03)00141-6
   Pitrelli JF, 2006, IEEE T AUDIO SPEECH, V14, P1099, DOI 10.1109/TASL.2006.876123
   Pittarn J., 1993, HDB EMOTIONS, P185
   Powers A., 2007, 2007 2nd Annual Conference on Human-Robot Interaction (HRI), P145
   Prasad R, 2004, ADV ROBOTICS, V18, P533, DOI 10.1163/156855304774195064
   Rani P, 2004, IEEE-RAS INT C HUMAN, P149
   Ray C, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3816, DOI 10.1109/IROS.2008.4650714
   Read R, 2012, P 2012 HRI PION WORK
   Read R, 2014, ACMIEEE INT CONF HUM, P276, DOI 10.1145/2559636.2559836
   Read R, 2013, ACMIEEE INT CONF HUM, P209, DOI 10.1109/HRI.2013.6483575
   Read R, 2012, ACMIEEE INT CONF HUM, P219
   Read Robin G., 2010, Proceedings: 3rd International Workshop on Affective Interaction in Natural Environments (AFFINE), Firenze, Italy, P65
   Roehling S, 2006, P 11 AUSTR INT C SPE
   Rogalla O, 2002, IEEE ROMAN 2002, PROCEEDINGS, P454, DOI 10.1109/ROMAN.2002.1045664
   Russell JA, 1999, J PERS SOC PSYCHOL, V76, P805, DOI 10.1037/0022-3514.76.5.805
   Sander D., 2009, OXFORD COMPANION EMO
   Scherer Klaus, 2000, NEUROPSYCHOLOGY EMOT, P138
   SCHERER KR, 1986, PSYCHOL BULL, V99, P143, DOI 10.1037/0033-2909.99.2.143
   SCHERER KR, 1991, MOTIV EMOTION, V15, P123, DOI 10.1007/BF00995674
   Schroder Marc, 2001, INTERSPEECH, P87
   Schröder M, 2011, LECT NOTES COMPUT SC, V6974, P316, DOI 10.1007/978-3-642-24600-5_35
   Schuller B, 2013, COMPUT SPEECH LANG, V27, P4, DOI 10.1016/j.csl.2012.02.005
   Sobin C, 1999, J PSYCHOLINGUIST RES, V28, P347, DOI 10.1023/A:1023237014909
   Tao JH, 2006, IEEE T AUDIO SPEECH, V14, P1145, DOI 10.1109/TASL.2006.876113
   Tielman M, 2014, ACMIEEE INT CONF HUM, P407, DOI 10.1145/2559636.2559663
   Tokuda K, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON SPEECH SYNTHESIS, P227
   Türk O, 2010, IEEE T AUDIO SPEECH, V18, P965, DOI 10.1109/TASL.2010.2041113
   Veilleux N., 2006, 6 911 TRANSCRIBING P
   Vinciarelli A., 2008, P 16 ACM INT C MULT, P1061, DOI DOI 10.1145/1459359.1459573
   W3C, 2004, SPEECH SYNTH MARK LA
   Wainer Joshua, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P872
   Walker MR, 2001, INT CONF ACOUST SPEE, P965, DOI 10.1109/ICASSP.2001.941077
   Weaver CH, 1964, FUNDAMENTALS SPEECH, P283
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
   Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004
NR 93
TC 53
Z9 56
U1 3
U2 44
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1875-4791
EI 1875-4805
J9 INT J SOC ROBOT
JI Int. J. Soc. Robot.
PD APR
PY 2016
VL 8
IS 2
BP 271
EP 285
DI 10.1007/s12369-015-0329-4
PG 15
WC Robotics
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Robotics
GA DJ7KO
UT WOS:000374390500008
DA 2024-01-09
ER

PT J
AU Goy, H
   Pichora-Fuller, MK
   van Lieshout, P
AF Goy, Huiwen
   Pichora-Fuller, M. Kathleen
   van Lieshout, Pascal
TI Effects of age on speech and voice quality ratings
SO JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA
LA English
DT Article
ID VOCAL AGE; OLDER-ADULTS; PERCEPTION; FEATURES; SOUND; YOUNG;
   INTELLIGIBILITY; COMMUNICATION; IMPRESSIONS; ELDERSPEAK
AB The quality of communication may be affected by listeners' perception of talkers' characteristics. This study examined if there were effects of talker and listener age on the perception of speech and voice qualities. Younger and older listeners judged younger and older talkers' gender and age, then rated speech samples on pleasantness, naturalness, clarity, ease of understanding, loudness, and the talker's suitability to be an audiobook reader. For the same talkers, listeners also rated voice samples on pleasantness, roughness, and power. Younger and older talkers were perceived to be similar on most qualities except age. Younger and older listeners rated talkers similarly, except that younger listeners perceived younger voices to be more pleasant and less rough than older voices. For vowel samples, younger listeners were more accurate than older listeners at age estimation, while older listeners were more accurate than younger listeners at gender identification, suggesting that younger and older listeners differ in their evaluation of specific talker characteristics. Thus, the perception of quality was generally more affected by the age of the listener than the age of the talker, and age-related differences between listeners depended on whether voice or speech samples were used and the rating being made. (C) 2016 Acoustical Society of America.
C1 [Goy, Huiwen; Pichora-Fuller, M. Kathleen] Univ Toronto, Dept Psychol, 3359 Mississauga Rd, Mississauga, ON L5L 1C6, Canada.
   [van Lieshout, Pascal] Univ Toronto, Dept Speech Language Pathol, 500 Univ Ave, Toronto, ON M5G 1V7, Canada.
C3 University of Toronto; University Toronto Mississauga; University of
   Toronto
RP Goy, H (corresponding author), Univ Toronto, Dept Psychol, 3359 Mississauga Rd, Mississauga, ON L5L 1C6, Canada.
EM huiwen.goy@utoronto.ca
RI van Lieshout, Pascal HHM/A-1371-2008
OI van Lieshout, Pascal HHM/0000-0001-8139-8900; Goy,
   Huiwen/0000-0003-4767-7863
FU Natural Sciences and Engineering Research Council of Canada [138472,
   312308]; Canada Research Chair grant [950-213162]; University of Toronto
   Mississauga Work-Study Program
FX The authors wish to thank James Qi for technical assistance and Sumaiya
   Farooq and Ivian Tchakarova for their assistance in stimuli preparation
   and data collection. This project was funded by grants from the Natural
   Sciences and Engineering Research Council of Canada awarded to M. K.
   P.-F. (No. 138472) and P. v. L. (No. 312308), a Canada Research Chair
   grant awarded to P. v. L. (No. 950-213162), and the University of
   Toronto Mississauga Work-Study Program.
CR AMERMAN JD, 1990, BRIT J DISORD COMMUN, V25, P35
   [Anonymous], 1988, J VOICE, DOI [DOI 10.1016/S0892-1997(88)80024-9, 10.1016/S0892-1997(88)80024-9]
   [Anonymous], 1987, Journal of Voice, DOI DOI 10.1016/S0892-1997(87)80024-3
   Bele IV, 2005, J VOICE, V19, P555, DOI 10.1016/j.jvoice.2004.08.008
   Boersma P., 2014, PRAAT DOING PHONETIC
   Bruckert L, 2006, P ROY SOC B-BIOL SCI, V273, P83, DOI 10.1098/rspb.2005.3265
   Bunton K, 2007, J SPEECH LANG HEAR R, V50, P1481, DOI 10.1044/1092-4388(2007/102)
   COHEN G, 1986, LANG COMMUN, V6, P91, DOI 10.1016/0271-5309(86)90008-X
   Dilley LC, 2013, J SPEECH LANG HEAR R, V56, P159, DOI 10.1044/1092-4388(2012/11-0199)
   DUCHIN SW, 1987, J COMMUN DISORD, V20, P245, DOI 10.1016/0021-9924(87)90022-0
   Eadie TL, 2002, J ACOUST SOC AM, V112, P3014, DOI 10.1121/1.1518983
   Eisenberg LS, 1998, J SPEECH LANG HEAR R, V41, P327, DOI 10.1044/jslhr.4102.327
   FAGEL WPF, 1983, SPEECH COMMUN, V2, P315, DOI 10.1016/0167-6393(83)90048-1
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   GABRIELSSON A, 1979, Scandinavian Audiology, V8, P159, DOI 10.3109/01050397909076317
   Gatehouse S, 2006, INT J AUDIOL, V45, pS120, DOI 10.1080/14992020600783103
   Goy H, 2013, J VOICE, V27, P545, DOI 10.1016/j.jvoice.2013.03.002
   Hamsberger JD, 2008, J VOICE, V22, P58, DOI 10.1016/j.jvoice.2006.07.004
   Harnsberger JD, 2010, J VOICE, V24, P523, DOI 10.1016/j.jvoice.2009.01.003
   HARTMAN DE, 1976, J ACOUST SOC AM, V59, P713, DOI 10.1121/1.380894
   HILLENBRAND J, 1988, J ACOUST SOC AM, V83, P2361, DOI 10.1121/1.396367
   HOLLIEN H, 1991, J COMMUN DISORD, V24, P157, DOI 10.1016/0021-9924(91)90019-F
   HOWELL P, 1991, SPEECH COMMUN, V10, P163, DOI 10.1016/0167-6393(91)90039-V
   Hummert ML, 1999, J NONVERBAL BEHAV, V23, P111
   JACQUES RD, 1990, FOLIA PHONIATR, V42, P118, DOI 10.1159/000266055
   KATES JM, 1994, J ACOUST SOC AM, V95, P3586, DOI 10.1121/1.409976
   Kemper S, 1998, AGING NEUROPSYCHOL C, V5, P43, DOI 10.1076/anec.5.1.43.22
   Klofstad CA, 2012, P ROY SOC B-BIOL SCI, V279, P2698, DOI 10.1098/rspb.2012.0311
   Laan GPM, 1997, SPEECH COMMUN, V22, P43, DOI 10.1016/S0167-6393(97)00012-5
   Linville SE, 1996, J VOICE, V10, P190, DOI 10.1016/S0892-1997(96)80046-4
   LINVILLE SE, 1986, J ACOUST SOC AM, V80, P692, DOI 10.1121/1.394013
   LINVILLE SE, 1985, J ACOUST SOC AM, V78, P40, DOI 10.1121/1.392452
   Maryn Y, 2015, LOGOP PHONIATR VOCO, V40, P122, DOI 10.3109/14015439.2014.915981
   McAleer P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0090779
   Medrado R, 2005, J VOICE, V19, P340, DOI 10.1016/j.jvoice.2004.04.008
   Montepare JM, 2014, J LANG SOC PSYCHOL, V33, P241, DOI 10.1177/0261927X13519080
   Mueller Peter B., 1997, Seminars in Speech and Language, V18, P159, DOI 10.1055/s-2008-1064070
   Mulac A, 1996, HEALTH COMMUN, V8, P199, DOI 10.1207/s15327027hc0803_2
   MUNRO MJ, 1995, LANG LEARN, V45, P73, DOI 10.1111/j.1467-1770.1995.tb00963.x
   PREMINGER JE, 1995, J SPEECH HEAR RES, V38, P714, DOI 10.1044/jshr.3803.714
   RAMIG LA, 1986, LANG COMMUN, V6, P25, DOI 10.1016/0271-5309(86)90003-0
   Rothman HB, 2001, J VOICE, V15, P25, DOI 10.1016/S0892-1997(01)00004-2
   RYAN EB, 1978, J GERONTOL, V33, P98, DOI 10.1093/geronj/33.1.98
   RYAN EB, 1990, PSYCHOL AGING, V5, P514, DOI 10.1037/0882-7974.5.4.514
   RYAN EB, 1986, LANG COMMUN, V6, P1, DOI 10.1016/0271-5309(86)90002-9
   RYAN EB, 1991, PSYCHOL AGING, V6, P442
   SCHERER KR, 1995, J VOICE, V9, P235, DOI 10.1016/S0892-1997(05)80231-0
   SCHWARTZ MF, 1968, J ACOUST SOC AM, V44, P1736, DOI 10.1121/1.1911324
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   SINGH S, 1978, J ACOUST SOC AM, V64, P81, DOI 10.1121/1.381958
   Vongpoisal T, 2007, J SPEECH LANG HEAR R, V50, P1139, DOI 10.1044/1092-4388(2007/079)
   Zäske R, 2011, HEARING RES, V282, P283, DOI 10.1016/j.heares.2011.06.008
NR 52
TC 19
Z9 26
U1 1
U2 16
PU ACOUSTICAL SOC AMER AMER INST PHYSICS
PI MELVILLE
PA STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA
SN 0001-4966
EI 1520-8524
J9 J ACOUST SOC AM
JI J. Acoust. Soc. Am.
PD APR
PY 2016
VL 139
IS 4
BP 1648
EP 1659
DI 10.1121/1.4945094
PG 12
WC Acoustics; Audiology & Speech-Language Pathology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Acoustics; Audiology & Speech-Language Pathology
GA DK5QN
UT WOS:000374974900018
PM 27106312
DA 2024-01-09
ER

PT J
AU Klopfenstein, M
AF Klopfenstein, Marie
TI Relationship between acoustic measures and speech naturalness ratings in
   Parkinson's disease: A within-speaker approach
SO CLINICAL LINGUISTICS & PHONETICS
LA English
DT Article
DE Fundamental frequency; intensity; phrase-final syllable lengthening;
   pitch accent; prosody; speech naturalness; speech rate; vocalic nucleus
ID INTENSIVE VOICE TREATMENT; TREATMENT LSVT(R); INTELLIGIBILITY;
   DYSARTHRIA; LOUDNESS
AB This study investigated the acoustic basis of across-utterance, within-speaker variation in speech naturalness for four speakers with dysarthria secondary to Parkinson's disease (PD). Speakers read sentences and produced spontaneous speech. Acoustic measures of fundamental frequency, phrase-final syllable lengthening, intensity and speech rate were obtained. A group of listeners judged speech naturalness using a nine-point Likert scale. Relationships between judgements of speech naturalness and acoustic measures were determined for individual speakers with PD. Relationships among acoustic measures also were quantified. Despite variability between speakers, measures of mean F0, intensity range, articulation rate, average syllable duration, duration of final syllables, vocalic nucleus length of final unstressed syllables and pitch accent of final syllables emerged as possible acoustic variables contributing to within-speaker variations in speech naturalness. Results suggest that acoustic measures correlate with speech naturalness, but in dysarthric speech they depend on the speaker due to the within-speaker variation in speech impairment.
C1 [Klopfenstein, Marie] So Illinois Univ, Dept Special Educ & Commun Disorders, Edwardsville, IL 62026 USA.
C3 Southern Illinois University System; Southern Illinois University
   Edwardsville
RP Klopfenstein, M (corresponding author), So Illinois Univ, Dept Special Educ & Commun Disorders, Campus Box 1147, Edwardsville, IL 62026 USA.
EM maklopf@siue.edu
RI Klopfenstein, Marie/T-5448-2019
OI Klopfenstein, Marie/0000-0002-2229-8050
CR [Anonymous], J MED SPEECH LANGUAG
   [Anonymous], DYSARTHRIAS
   Awan SN, 2001, VOICE DIAGNOSTIC PRO
   Boersma P., 2014, PRAAT DOING PHONETIC
   CANTER GJ, 1963, J SPEECH HEAR DISORD, V28, P221, DOI 10.1044/jshd.2803.221
   Cohen J, 1988, STAT POWER ANAL BEHA
   Dagenais PA, 2002, INVESTIGATIONS IN CLINICAL PHONETICS AND LINGUISTICS, P363
   DARLEY FL, 1969, J SPEECH HEAR RES, V12, P462, DOI 10.1044/jshr.1203.462
   DARLEY FL, 1969, J SPEECH HEAR RES, V12, P246, DOI 10.1044/jshr.1202.246
   DROMEY C, 1995, J SPEECH HEAR RES, V38, P751, DOI 10.1044/jshr.3804.751
   Duffy J.R., 1995, Motor speech disorders: Substrates, differential diagnosis, and management
   DUNN OJ, 1964, TECHNOMETRICS, V6, P241, DOI 10.2307/1266041
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   Feenaughty L, 2014, CLIN LINGUIST PHONET, V28, P857, DOI 10.3109/02699206.2014.921839
   Flipsen P, 2003, J SPEECH LANG HEAR R, V46, P724, DOI 10.1044/1092-4388(2003/058)
   FORREST K, 1989, J ACOUST SOC AM, V85, P2608, DOI 10.1121/1.397755
   George D, 2002, SPSS for Windows Step by Step: A Simple Guide and Reference, 11.0 Update, V4th
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P217, DOI 10.1044/jshd.5002.217
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P261, DOI 10.1044/jshd.5003.261
   KENT RD, 1982, BRAIN LANG, V15, P259, DOI 10.1016/0093-934X(82)90060-8
   Kim Y, 2011, J SPEECH LANG HEAR R, V54, P417, DOI 10.1044/1092-4388(2010/10-0020)
   Klopfenstein M., 2012, THESIS
   LINEBAUGH CW, 1984, DYSARTHRIAS PHYSL AC, P197
   LUDLOW CL, 1987, BRAIN LANG, V32, P195, DOI 10.1016/0093-934X(87)90124-6
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P994, DOI 10.1044/jshr.3505.994
   ONSLOW M, 1987, J SPEECH HEAR DISORD, V52, P2, DOI 10.1044/jshd.5201.02
   Ramig LO, 2001, MOVEMENT DISORD, V16, P79, DOI 10.1002/1531-8257(200101)16:1<79::AID-MDS1013>3.0.CO;2-H
   Ramig LO, 2001, J NEUROL NEUROSUR PS, V71, P493, DOI 10.1136/jnnp.71.4.493
   RAMIG LO, 1995, J SPEECH HEAR RES, V38, P1232, DOI 10.1044/jshr.3806.1232
   RUNYAN CM, 1982, J FLUENCY DISORD, V7, P71, DOI 10.1016/0094-730X(82)90040-7
   Sacco P. R., 1992, ANN M AM SPEECH LANG
   Sapir S, 2002, FOLIA PHONIATR LOGO, V54, P296, DOI 10.1159/000066148
   Schiavetti N, 1998, J SPEECH LANG HEAR R, V41, P5, DOI 10.1044/jslhr.4101.05
   Skodda S, 2011, J NEUROL SCI, V310, P231, DOI 10.1016/j.jns.2011.07.020
   Snow D, 1998, J SPEECH LANG HEAR R, V41, P1158, DOI 10.1044/jslhr.4105.1158
   Tjaden K, 2011, J COMMUN DISORD, V44, P655, DOI 10.1016/j.jcomdis.2011.06.003
   TURNER GS, 1993, J SPEECH HEAR RES, V36, P1134, DOI 10.1044/jshr.3606.1134
   UZIEL A, 1975, FOLIA PHONIATR, V27, P166, DOI 10.1159/000263984
   WEISMER G, 1985, Journal of the Acoustical Society of America, V78, pS55, DOI 10.1121/1.2022878
   Weismer G, 2001, FOLIA PHONIATR LOGO, V53, P1, DOI 10.1159/000052649
   Weismer G., 1984, DYSARTHRIAS PHYSL AC, P101
   Whitehill T, 2002, INVESTIGATIONS IN CLINICAL PHONETICS AND LINGUISTICS, P405
   Yorkston K. M., 1984, DYSARTHRIAS PHYSL AC, P197
   YORKSTON KM, 2010, MANAGEMENT MOTOR SPE
   Yunusova Y, 2005, J SPEECH LANG HEAR R, V48, P1294, DOI 10.1044/1092-4388(2005/090)
   Yunusova Y, 2012, FOLIA PHONIATR LOGO, V64, P94, DOI 10.1159/000336890
NR 48
TC 7
Z9 7
U1 4
U2 28
PU TAYLOR & FRANCIS INC
PI PHILADELPHIA
PA 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
SN 0269-9206
EI 1464-5076
J9 CLIN LINGUIST PHONET
JI Clin. Linguist. Phon.
PD DEC 2
PY 2015
VL 29
IS 12
BP 938
EP 954
DI 10.3109/02699206.2015.1081293
PG 17
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA CX3KB
UT WOS:000365596600004
PM 26403503
OA Green Submitted
DA 2024-01-09
ER

PT J
AU Lorenzo-Trueba, J
   Barra-Chicote, R
   San-Segundo, R
   Ferreiros, J
   Yamagishi, J
   Montero, JM
AF Lorenzo-Trueba, Jaime
   Barra-Chicote, Roberto
   San-Segundo, Ruben
   Ferreiros, Javier
   Yamagishi, Junichi
   Montero, Juan M.
TI Emotion transplantation through adaptation in HMM-based speech synthesis
SO COMPUTER SPEECH AND LANGUAGE
LA English
DT Article
DE Statistical parametric speech synthesis; Expressive speech synthesis;
   Cascade adaptation; Emotion transplantation
AB This paper proposes an emotion transplantation method capable of modifying a synthetic speech model through the use of CSMAPLR adaptation in order to incorporate emotional information learned from a different speaker model while maintaining the identity of the original speaker as much as possible. The proposed method relies on learning both emotional and speaker identity information by means of their adaptation function from an average voice model, and combining them into a single cascade transform capable of imbuing the desired emotion into the target speaker. This method is then applied to the task of transplanting four emotions (anger, happiness, sadness and surprise) into 3 male speakers and 3 female speakers and evaluated in a number of perceptual tests. The results of the evaluations show how the perceived naturalness for emotional text significantly favors the use of the proposed transplanted emotional speech synthesis when compared to traditional neutral speech synthesis, evidenced by a big increase in the perceived emotional strength of the synthesized utterances at a slight cost in speech quality. A final evaluation with a robotic laboratory assistant application shows how by using emotional speech we can significantly increase the students' satisfaction with the dialog system, proving how the proposed emotion transplantation system provides benefits in real applications. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Lorenzo-Trueba, Jaime; Barra-Chicote, Roberto; San-Segundo, Ruben; Ferreiros, Javier; Montero, Juan M.] Univ Politecn Madrid, ETSI Telecomunicac, Speech Technol Grp, E-28040 Madrid, Spain.
   [Yamagishi, Junichi] Univ Edinburgh, CSTR, Informat Forum, Edinburgh EH8 9AB, Midlothian, Scotland.
C3 Universidad Politecnica de Madrid; University of Edinburgh
RP Lorenzo-Trueba, J (corresponding author), Univ Politecn Madrid, ETSI Telecomunicac, Speech Technol Grp, Ave Complutense 30,Ciudad Univ, E-28040 Madrid, Spain.
EM jaime.lorenzo@die.upm.es; barra@die.upm.es; lapiz@die.upm.es;
   jfl@die.upm.es; yjamagis@inf.ed.ac.uk; juancho@die.upm.es
RI Barra-Chicote, Roberto/L-4963-2014; San-Segundo, Rubén/J-6027-2017;
   Montero, Juan M/K-2381-2014
OI Barra-Chicote, Roberto/0000-0003-0844-7037; San-Segundo,
   Rubén/0000-0001-9659-5464; Montero, Juan M/0000-0002-7908-5400
FU European Union [287678]; TIMPANO project [TIN2011-28169-C05-03]; INAPRA
   (MICINN) project [DPI2010-21247-C02-02]; MA2VICMR (Comunidad Autonoma de
   Madrid) project [S2009/TIC-1542]; Universidad Politecnica de Madrid
   under grant SBUPM-QTKTZHB; Grants-in-Aid for Scientific Research
   [15K12071] Funding Source: KAKEN
FX The work leading to these results has received funding from the European
   Union under grant agreement 287678. It has also been supported by
   TIMPANO (TIN2011-28169-C05-03), INAPRA (MICINN, DPI2010-21247-C02-02),
   and MA2VICMR (Comunidad Autonoma de Madrid, S2009/TIC-1542) projects.
   Jaime Lorenzo has been funded by Universidad Politecnica de Madrid under
   grant SBUPM-QTKTZHB. The authors want to thank the other members of the
   Speech Technology Group, ARABOT and Simple4All projects for the
   continuous and fruitful discussion on these topics. The authors also
   want to thank the Multimedia Technology Group from Universidad de Vigo
   and the Speech Processing Group (VEU) from Universidad Politecnica de
   Cataluna for sharing some of the databases used in this research.
CR Adell J, 2012, SPEECH COMMUN, V54, P459, DOI 10.1016/j.specom.2011.10.010
   Anastasakos T, 1997, INT CONF ACOUST SPEE, P1043, DOI 10.1109/ICASSP.1997.596119
   Andersson S., 2010, SPEECH PROSODY
   Andersson S, 2012, SPEECH COMMUN, V54, P175, DOI 10.1016/j.specom.2011.08.001
   [Anonymous], P 12 ANN C INT SPEEC
   [Anonymous], P ISCA SPEECH PROS C
   [Anonymous], INTERSPEECH
   Banga C.M., 2010, TECHNICAL REPORT GRU
   Barra-Chicote R., 2008, P LREC
   Barra-Chicote R., 2011, THESIS ETSIT UPM
   Barra-Chicote R, 2010, SPEECH COMMUN, V52, P394, DOI 10.1016/j.specom.2009.12.007
   Bonafonte A., 2008, DOCUMENTATION UPC ES, P2781
   Chen L., 2012, INT 2012 13 ANN C IN
   Chesta C., 1999, EUROSPEECH
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Erro D, 2010, IEEE T AUDIO SPEECH, V18, P974, DOI 10.1109/TASL.2009.2038658
   Gales MJF, 1998, COMPUT SPEECH LANG, V12, P75, DOI 10.1006/csla.1998.0043
   Gales MJF, 2000, IEEE T SPEECH AUDI P, V8, P417, DOI 10.1109/89.848223
   Gao L., 2005, LATIN SQURES EXPT DE
   Hsu CY, 2012, EURASIP J AUDIO SPEE, P1, DOI 10.1186/1687-4722-2012-21
   Kawahara H., 2001, P MAVEBA, P13
   King S., 2008, BLIZZARD CHALLENGE, P2008
   Lorenzo-Trueba J., 2013, P WORKSH TECN ACC 4
   Lorenzo-Trueba J., 2012, INT 2012 13 ANN C IN, P9
   Lorenzo-Trueba J., 2013, 8 ISCA SPEECH SYNTH
   Lutfi SL, 2013, SENSORS-BASEL, V13, P10519, DOI 10.3390/s130810519
   Mendez Pazo F., 2010, P 6 JORN TECN HABL 2
   Nose T, 2013, SPEECH COMMUN, V55, P347, DOI 10.1016/j.specom.2012.09.003
   Obin N., 2011, INTERSPEECH
   Qin L, 2006, LECT NOTES COMPUT SC, V4274, P233
   Raitio T., 2013, COMPUT SPEECH LANG
   Rodriguez-Losada D., 2008, ADV SERVICE ROBOTICS, P229, DOI DOI 10.5772/5950
   Schuller B, 2010, IEEE T AFFECT COMPUT, V1, P119, DOI 10.1109/T-AFFC.2010.8
   Seltzer M.L., 2012, INTERSPEECH
   Shinoda K, 1997, 1997 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING, PROCEEDINGS, P381, DOI 10.1109/ASRU.1997.659114
   Takeda S., 2013, INT J AFFECTIVE ENG, V12, P79
   Yamagishi J, 2005, IEICE T INF SYST, VE88D, P502, DOI 10.1093/ietisy/e88-d.3.502
   Yamagishi J, 2003, IEICE T FUND ELECTR, VE86A, P1956
   Yamagishi J, 2009, IEEE T AUDIO SPEECH, V17, P66, DOI 10.1109/TASL.2008.2006647
   Yanagisawa K., 2013, ORDER, V5, P10
   Zovato E., 2004, 5 ISCA WORKSH SPEECH
NR 41
TC 22
Z9 25
U1 0
U2 24
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0885-2308
EI 1095-8363
J9 COMPUT SPEECH LANG
JI Comput. Speech Lang.
PD NOV
PY 2015
VL 34
IS 1
SI SI
BP 292
EP 307
DI 10.1016/j.csl.2015.03.008
PG 16
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CL8JX
UT WOS:000357222100015
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Anand, S
   Stepp, CE
AF Anand, Supraja
   Stepp, Cara E.
TI Listener Perception of Monopitch, Naturalness, and Intelligibility for
   Speakers With Parkinson's Disease
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
ID INTENSIVE VOICE TREATMENT; DYSARTHRIC SPEECH; SENTENCE INTELLIGIBILITY;
   FUNDAMENTAL-FREQUENCY; TREATMENT LSVT(R); INDIVIDUALS; ACCEPTABILITY;
   COMMUNICATION; IMPRESSIONS; PROGRESSION
AB Purpose: Given the potential significance of speech naturalness to functional and social rehabilitation outcomes, the objective of this study was to examine the effect of listener perceptions of monopitch on speech naturalness and intelligibility in individuals with Parkinson's disease (PD).
   Method: Two short utterances were extracted from monologue samples of 16 speakers with PD and 5 age-matched adults without PD. Sixteen listeners evaluated these stimuli for monopitch, speech naturalness and intelligibility using the visual sort and rate method.
   Results: Naive listeners can reliably judge monopitch, speech naturalness, and intelligibility with minimal familiarization. While monopitch and speech intelligibility were only moderately correlated, monopitch and speech naturalness were highly correlated.
   Conclusions: A great deal of attention is currently being paid to improvement of vocal loudness and thus speech intelligibility in PD. Our findings suggest that prosodic characteristics such as monopitch should be explored as adjuncts to this treatment of dysarthria in PD. Development of such prosodic treatments may enhance speech naturalness and thus improve quality of life.
C1 [Anand, Supraja] West Chester Univ, W Chester, PA 19383 USA.
   [Stepp, Cara E.] Boston Univ, Boston, MA 02215 USA.
C3 Pennsylvania State System of Higher Education (PASSHE); West Chester
   University of Pennsylvania; Boston University
RP Anand, S (corresponding author), West Chester Univ, W Chester, PA 19383 USA.
EM sanand@wcupa.edu
RI Stepp, Cara/AAQ-6425-2020
FU National Institute on Deafness and Other Communication Disorders
   [DC012651]; Eunice Kennedy Shriver National Institute of Child Health
   and Human Development [HD065688]
FX This work was supported by Grant DC012651 from the National Institute on
   Deafness and Other Communication Disorders (awarded to C. E. Stepp), and
   the Boston Rehabilitation Outcomes Center was supported by Grant
   HD065688 from the Eunice Kennedy Shriver National Institute of Child
   Health and Human Development (awarded to A. M. Jette). The authors wish
   to thank Jessica Malloy for assistance with participant recruitment and
   the four listeners who provided pilot ratings of the three percepts.
CR Adams S. G., 1998, CANADIAN ACOUSTICS, V26, P86
   ADAMS SG, 1992, EUR J DISORDER COMM, V27, P121
   [Anonymous], J MED SPEECH LANGUAG
   [Anonymous], DYSARTHRIAS
   Aronson A. E., 1990, Clinical Voice Disorders, An Interdisciplinary Approach
   Baumgartner CA, 2001, J VOICE, V15, P105, DOI 10.1016/S0892-1997(01)00010-8
   Berry, 1983, CLIN DYSARTHRIA, P231
   Bowen L. K., 2014, J MED SPEECH-LANG PA, V21, P235
   Bunton K, 2001, CLIN LINGUIST PHONET, V15, P181
   Bunton K, 2007, J SPEECH LANG HEAR R, V50, P1481, DOI 10.1044/1092-4388(2007/102)
   CAEKEBEKE JFV, 1991, J NEUROL NEUROSUR PS, V54, P145, DOI 10.1136/jnnp.54.2.145
   Cannito MP, 2012, J VOICE, V26, P214, DOI 10.1016/j.jvoice.2011.08.014
   Cheang HS, 2007, J NEUROLINGUIST, V20, P221, DOI 10.1016/j.jneuroling.2006.07.001
   Dagenais PA, 2006, CLIN LINGUIST PHONET, V20, P141, DOI 10.1080/02699200400026843
   Dagenais PA, 1999, J MED SPEECH-LANG PA, V7, P91
   DARLEY FL, 1969, J SPEECH HEAR RES, V12, P246, DOI 10.1044/jshr.1202.246
   De Bodt MS, 2002, J COMMUN DISORD, V35, P283, DOI 10.1016/S0021-9924(02)00065-5
   Duffy J.R., 1995, Motor speech disorders: Substrates, differential diagnosis, and management
   Eadie TL, 2006, AM J SPEECH-LANG PAT, V15, P307, DOI 10.1044/1058-0360(2006/030)
   Eadie TL, 2002, J SPEECH LANG HEAR R, V45, P1088, DOI 10.1044/1092-4388(2002/087)
   Fahn S, 2003, ANN NY ACAD SCI, V991, P1, DOI 10.1111/j.1749-6632.2003.tb07458.x
   Fox Cynthia M., 2006, Seminars in Speech and Language, V27, P283, DOI 10.1055/s-2006-955118
   Gamboa J, 1997, J VOICE, V11, P314, DOI 10.1016/S0892-1997(97)80010-0
   Goberman A, 2002, J COMMUN DISORD, V35, P217, DOI 10.1016/S0021-9924(01)00072-7
   Goberman AM, 2005, J COMMUN DISORD, V38, P215, DOI 10.1016/j.jcomdis.2004.10.001
   Granqvist Svante, 2003, Logoped Phoniatr Vocol, V28, P109, DOI 10.1080/14015430310015255
   GREENE MCL, 1968, FOLIA PHONIATR, V20, P250, DOI 10.1159/000263203
   Ho AK, 1999, NEUROPSYCHOLOGIA, V37, P1453, DOI 10.1016/S0028-3932(99)00067-6
   Holmes RJ, 2000, INT J LANG COMM DIS, V35, P407
   Jaywant A, 2010, J INT NEUROPSYCH SOC, V16, P49, DOI 10.1017/S1355617709990919
   Jimenez-Jimenez F J, 1997, Parkinsonism Relat Disord, V3, P111, DOI 10.1016/S1353-8020(97)00007-2
   KENT RD, 1989, J SPEECH HEAR DISORD, V54, P482, DOI 10.1044/jshd.5404.482
   KENT RD, 1982, BRAIN LANG, V15, P259, DOI 10.1016/0093-934X(82)90060-8
   Laures JS, 1999, J SPEECH LANG HEAR R, V42, P1148, DOI 10.1044/jslhr.4205.1148
   Liss J. M., 2007, MOTOR SPEECH DISORDE, P187
   LOGEMANN JA, 1978, J SPEECH HEAR DISORD, V43, P47, DOI 10.1044/jshd.4301.47
   Lowit A, 2010, INT J SPEECH-LANG PA, V12, P426, DOI 10.3109/17549507.2010.497559
   MacPherson MK, 2011, J SPEECH LANG HEAR R, V54, P19, DOI 10.1044/1092-4388(2010/09-0079)
   Meltzner GS, 2005, J SPEECH LANG HEAR R, V48, P766, DOI 10.1044/1092-4388(2005/053)
   METTER EJ, 1986, J COMMUN DISORD, V19, P347, DOI 10.1016/0021-9924(86)90026-2
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   Miller N, 2006, AGE AGEING, V35, P235, DOI 10.1093/ageing/afj053
   Miller N, 2008, CLIN REHABIL, V22, P14, DOI 10.1177/0269215507079096
   Nagle KF, 2012, J COMMUN DISORD, V45, P235, DOI 10.1016/j.jcomdis.2012.01.001
   Patel R, 2013, FOLIA PHONIATR LOGO, V65, P109, DOI 10.1159/000354422
   Pell MD, 2006, BRAIN LANG, V97, P123, DOI 10.1016/j.bandl.2005.08.010
   PITCAIRN TK, 1990, BRIT J DISORD COMMUN, V25, P85
   Plowman-Prine EK, 2009, NEUROREHABILITATION, V24, P131, DOI 10.3233/NRE-2009-0462
   Ramig LO, 2001, J NEUROL NEUROSUR PS, V71, P493, DOI 10.1136/jnnp.71.4.493
   RAMIG LO, 1995, J SPEECH HEAR RES, V38, P1232, DOI 10.1044/jshr.3806.1232
   Ramig LO., 1994, J MED SPEECH-LANG PA, V2, P191
   Ramig Lorraine O, 2008, Expert Rev Neurother, V8, P297, DOI 10.1586/14737175.8.2.297
   Ramig Lorraine Olson, 2004, Seminars in Speech and Language, V25, P169
   RUBOW R, 1985, J SPEECH HEAR DISORD, V50, P178, DOI 10.1044/jshd.5002.178
   Sapir S, 2007, J SPEECH LANG HEAR R, V50, P899, DOI 10.1044/1092-4388(2007/064)
   SCOTT S, 1983, J NEUROL NEUROSUR PS, V46, P140, DOI 10.1136/jnnp.46.2.140
   Skodda S, 2011, J VOICE, V25, pE199, DOI 10.1016/j.jvoice.2010.04.007
   Skodda S, 2009, MOVEMENT DISORD, V24, P716, DOI 10.1002/mds.22430
   Spielman J, 2007, AM J SPEECH-LANG PAT, V16, P95, DOI 10.1044/1058-0360(2007/014)
   Stathopoulos ET, 2014, J COMMUN DISORD, V48, P1, DOI 10.1016/j.jcomdis.2013.12.001
   Teshima S, 2010, J FLUENCY DISORD, V35, P44, DOI 10.1016/j.jfludis.2010.01.001
   Tjaden K, 2004, J SPEECH LANG HEAR R, V47, P766, DOI 10.1044/1092-4388(2004/058)
   Trail M, 2005, NEUROREHABILITATION, V20, P205
   Weismer G, 2002, J SPEECH LANG HEAR R, V45, P421, DOI 10.1044/1092-4388(2002/033)
   Whitehill TL, 2004, J MED SPEECH-LANG PA, V12, P229
   World Health Organization, 2006, Neurological Disorders
   Yorkston K.M., 1999, Management of motor speech disorders in children and adults
   Yorkston KM., 2004, Management of Speech and Swallowing in Degenerative Diseases
NR 68
TC 41
Z9 57
U1 5
U2 16
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 2200 RESEARCH BLVD, #271, ROCKVILLE, MD 20850-3289 USA
SN 1092-4388
EI 1558-9102
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD AUG
PY 2015
VL 58
IS 4
BP 1134
EP 1144
DI 10.1044/2015_JSLHR-S-14-0243
PG 11
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA DX6XJ
UT WOS:000384529000003
PM 26102242
OA Green Published
DA 2024-01-09
ER

PT J
AU Tamura, Y
   Kuriki, S
   Nakano, T
AF Tamura, Yuri
   Kuriki, Shinji
   Nakano, Tamami
TI Involvement of the left insula in the ecological validity of the human
   voice
SO SCIENTIFIC REPORTS
LA English
DT Article
ID LYRICS; SOUNDS; CORTEX; LOBE
AB A subtle difference between a real human and an artificial object that resembles a human evokes an impression of a large qualitative difference between them. This suggests the existence of a neural mechanism that processes the sense of humanness. To examine the presence of such a mechanism, we compared the behavioral and brain responses of participants who listened to human and artificial singing voices created from vocal fragments of a real human voice. The behavioral experiment showed that the song sung by human voices more often elicited positive feelings and feelings of humanness than the same song sung by artificial voices, although the lyrics, melody, and rhythm were identical. Functional magnetic resonance imaging revealed significantly higher activation in the left posterior insula in response to human voices than in response to artificial voices. Insular activation was not merely evoked by differences in acoustic features between the voices. Therefore, these results suggest that the left insula participates in the neural processing of the ecological quality of the human voice.
C1 [Tamura, Yuri; Kuriki, Shinji; Nakano, Tamami] Osaka Univ, Grad Sch Frontiers Biosci, Dynam Brain Network Lab, Osaka, Japan.
   [Nakano, Tamami] Natl Inst Informat & Commun Technol, Ctr Informat & Neural Networks CiNet, Osaka, Japan.
   [Nakano, Tamami] Osaka Univ, Osaka, Japan.
C3 Osaka University; National Institute of Information & Communications
   Technology (NICT) - Japan; Osaka University
RP Nakano, T (corresponding author), Osaka Univ, Grad Sch Frontiers Biosci, Dynam Brain Network Lab, Osaka, Japan.
EM tamami_nakano@fbs.osaka-u.ac.jp
FU Ministry of Education, Culture, Sports, Science and Technology, Japan
   [251195040]; Grants-in-Aid for Scientific Research [25560429, 25700014,
   25119504] Funding Source: KAKEN
FX We are grateful to Shigeru Kitazawa for his comments on the manuscript
   and Hideki Kawahara for his valuable advice on the acoustic analysis.
   This work was supported by Grant-in-Aid for Scientific Research on
   Innovative Areas 251195040 "Constructive Developmental Science'' from
   the Ministry of Education, Culture, Sports, Science and Technology,
   Japan to T. N.
CR Augustine JR, 1996, BRAIN RES REV, V22, P229, DOI 10.1016/S0165-0173(96)00011-2
   Bamiou DE, 2003, BRAIN RES REV, V42, P143, DOI 10.1016/S0165-0173(03)00172-3
   Belin P, 2002, COGNITIVE BRAIN RES, V13, P17, DOI 10.1016/S0926-6410(01)00084-2
   Brattico E, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00308
   Chaminade T, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0011577
   Fecteau S, 2004, NEUROIMAGE, V23, P840, DOI 10.1016/j.neuroimage.2004.09.019
   Kanda T, 2008, IEEE T ROBOT, V24, P725, DOI 10.1109/TRO.2008.921566
   Lattner S, 2003, HUM BRAIN MAPP, V20, P13, DOI 10.1002/hbm.10118
   Leino T, 2009, J VOICE, V23, P671, DOI 10.1016/j.jvoice.2008.03.008
   Lieberman MD, 2009, SOC COGN AFFECT NEUR, V4, P423, DOI 10.1093/scan/nsp052
   MacDorman KF, 2006, INTERACT STUD, V7, P297, DOI 10.1075/is.7.3.03mac
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Remedios R, 2009, J NEUROSCI, V29, P1034, DOI 10.1523/JNEUROSCI.4089-08.2009
   Sammler D, 2010, J NEUROSCI, V30, P3572, DOI 10.1523/JNEUROSCI.2751-09.2010
   Saygin AP, 2012, SOC COGN AFFECT NEUR, V7, P413, DOI 10.1093/scan/nsr025
NR 15
TC 6
Z9 7
U1 0
U2 2
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD MAR 5
PY 2015
VL 5
AR 8799
DI 10.1038/srep08799
PG 7
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA CC6DE
UT WOS:000350454100001
PM 25739519
OA Green Published, gold
DA 2024-01-09
ER

PT J
AU Ilves, M
   Surakka, V
AF Ilves, Mirja
   Surakka, Veikko
TI Subjective responses to synthesised speech with lexical emotional
   content: the effect of the naturalness of the synthetic voice
SO BEHAVIOUR & INFORMATION TECHNOLOGY
LA English
DT Article
DE emotion; synthesised speech; naturalness
ID AROUSAL; VALENCE; SYSTEM; HABITUATION; PICTURES; RATINGS; REFLEX; AGENTS
AB This study aimed to investigate how the degree of naturalness and lexical emotional content of synthesised speech affects the subjective ratings of emotional experiences and how the naturalness of the voice affects the ratings of voice quality. Twenty-four participants listened to a set of affective words produced by three different speech synthesis techniques: formant synthesis, diphone synthesis and unit selection synthesis. The participants' task was to rate their experiences evoked by the speech samples using three emotion-related bipolar scales for valence, arousal and approachability. The pleasantness, naturalness and clarity of the voices were also rated. The results showed that the affective words produced by the synthesisers evoked congruent emotion-related ratings in the participants. The ratings of the experienced valence and approachability were statistically significantly stronger when the affective words were produced by the more humanlike voices as compared to the more machinelike voice. The more humanlike voices were also rated as statistically significantly more natural, pleasant and clear than the less humanlike voice. Thus, our findings suggest that even machinelike voices can be used to communicate affective messages but that increasing the level of naturalness enhances positive feelings about synthetic voices and strengthens emotional communication between computers and humans.
C1 [Ilves, Mirja; Surakka, Veikko] Univ Tampere, Res Grp Emot Social & Comp, Tampere Unit Comp Human Interact TAUCHI, Sch Informat Sci, FI-33014 Tampere, Finland.
C3 Tampere University
RP Ilves, M (corresponding author), Univ Tampere, Res Grp Emot Social & Comp, Tampere Unit Comp Human Interact TAUCHI, Sch Informat Sci, Kanslerinrinne 1, FI-33014 Tampere, Finland.
EM mirja.ilves@sis.uta.fi
OI Ilves, Mirja/0000-0002-7763-3741; Surakka, Veikko/0000-0003-3986-0713
FU Doctoral Program in User-Centred Information Technology (UCIT);
   University of Tampere
FX The authors would like to thank all the participants of the study. This
   research was supported by the Doctoral Program in User-Centred
   Information Technology (UCIT) and a grant from the University of
   Tampere.
CR Alais D, 2006, P R SOC B, V273, P1339, DOI 10.1098/rspb.2005.3420
   [Anonymous], 1995, AFFECTIVE COMPUTING
   [Anonymous], 1999, TECHNICAL REPORT C 1
   [Anonymous], 1998, Attention
   Anttonen J., 2005, P SIGCHI C HUM FACT, P491, DOI [10.1145/1054972.1055040, DOI 10.1145/1054972.1055040]
   Aula A, 2002, BCS CONF SERIES, P337
   Bartneck C, 2001, USER MODEL USER-ADAP, V11, P279, DOI 10.1023/A:1011811315582
   Beale R, 2009, INT J HUM-COMPUT ST, V67, P755, DOI 10.1016/j.ijhcs.2009.05.001
   Bertels J, 2009, PSYCHOL BELG, V49, P19, DOI 10.5334/pb-49-1-19
   Beskow J., 2000, PROC INT C SPOKEN LA
   BRADLEY MM, 1993, BEHAV NEUROSCI, V107, P970, DOI 10.1037/0735-7044.107.6.970
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Brave S, 2005, INT J HUM-COMPUT ST, V62, P161, DOI 10.1016/j.ijhcs.2004.11.002
   Breiter HC, 1996, NEURON, V17, P875, DOI 10.1016/S0896-6273(00)80219-6
   Buchanan TW, 2006, INT J PSYCHOPHYSIOL, V61, P26, DOI 10.1016/j.ijpsycho.2005.10.022
   Cahn J. E., 1990, Journal of the American Voice I/O Society, V8, P1
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Canli T, 1998, NEUROREPORT, V9, P3233, DOI 10.1097/00001756-199810050-00019
   Christie IC, 2004, INT J PSYCHOPHYSIOL, V51, P143, DOI 10.1016/j.ijpsycho.2003.08.002
   Conati C, 2009, USER MODEL USER-ADAP, V19, P267, DOI 10.1007/s11257-009-9062-8
   Cowie R, 2003, SPEECH COMMUN, V40, P5, DOI 10.1016/S0167-6393(02)00071-7
   Dai L, 2005, BEHAV INFORM TECHNOL, V24, P219, DOI 10.1080/01449290412331328563
   Dehn DM, 2000, INT J HUM-COMPUT ST, V52, P1, DOI 10.1006/ijhc.1999.0325
   Dutoit T., 1997, AN INTRODUCTION TO T
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Elliot AJ, 1999, EDUC PSYCHOL-US, V34, P169, DOI 10.1207/s15326985ep3403_3
   ESTRADA CA, 1994, MOTIV EMOTION, V18, P285, DOI 10.1007/BF02856470
   Fredrickson BL, 2005, COGNITION EMOTION, V19, P313, DOI 10.1080/02699930441000238
   FRIJDA NH, 1988, AM PSYCHOL, V43, P349, DOI 10.1037/0003-066X.43.5.349
   Grühn D, 2008, BEHAV RES METHODS, V40, P512, DOI 10.3758/BRM.40.2.512
   Guthrie S., 1993, Faces in the Clouds: A New Theory of Religion
   Harmon-Jones E, 2001, J PERS SOC PSYCHOL, V80, P797, DOI 10.1037/0022-3514.80.5.797
   Hassenzahl M, 2006, BEHAV INFORM TECHNOL, V25, P91, DOI 10.1080/01449290500330331
   Iida A, 2003, SPEECH COMMUN, V40, P161, DOI 10.1016/S0167-6393(02)00081-X
   Ilves M., 2004, P 17 INT C COMP AN S, P19
   Ilves M., 2009, EMOTIONS HUMAN VOICE, P137
   Ilves M, 2011, LECT NOTES COMPUT SC, V6974, P588, DOI 10.1007/978-3-642-24600-5_62
   ISEN AM, 1987, J PERS SOC PSYCHOL, V52, P1122, DOI 10.1037/0022-3514.52.6.1122
   Kappas A., 1991, FUNDAMENTALS OF NONV, P201
   Klein J, 2002, INTERACT COMPUT, V14, P119, DOI 10.1016/S0953-5438(01)00053-4
   Knoll M.A., 2011, BEHAV INFORM TECHNOL
   Ku J, 2005, CYBERPSYCHOL BEHAV, V8, P493, DOI 10.1089/cpb.2005.8.493
   LAIRD JD, 1982, J PERS SOC PSYCHOL, V42, P646, DOI 10.1037/0022-3514.42.4.646
   LANG PJ, 1993, PSYCHOPHYSIOLOGY, V30, P261, DOI 10.1111/j.1469-8986.1993.tb03352.x
   LANG PJ, 1992, PSYCHOL SCI, V3, P44, DOI 10.1111/j.1467-9280.1992.tb00255.x
   Larsen JT, 2003, PSYCHOPHYSIOLOGY, V40, P776, DOI 10.1111/1469-8986.00078
   Law ELC, 2010, INTERACT COMPUT, V22, P313, DOI 10.1016/j.intcom.2010.04.006
   Mauss I, 2009, COGNITION EMOTION, V23, P209, DOI 10.1080/02699930802204677
   MILLER JD, 1974, J ACOUST SOC AM, V56, P729, DOI 10.1121/1.1903322
   Minnema MT, 2008, EMOTION, V8, P643, DOI 10.1037/a0013441
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   MURRAY IR, 1995, SPEECH COMMUN, V16, P369, DOI 10.1016/0167-6393(95)00005-9
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass C., 2005, WIRED FOR SPEECH HOW
   Nowak K.L., 2004, J COMPUTER MEDIATED, P9
   Nowak KL., 2005, J COMPUT-MEDIAT COMM, V11, P153, DOI [DOI 10.1111/J.1083-6101.2006.TB00308.X, 10.1111/j.1083-6101.2006.tb00308, 10.1111/j.1083-6101.2006.tb00308.x]
   Nowak KL, 2008, COMPUT HUM BEHAV, V24, P1473, DOI 10.1016/j.chb.2007.05.005
   Partala T, 2004, INTERACT COMPUT, V16, P295, DOI 10.1016/j.intcom.2003.12.001
   Prendinger H, 2005, INT J HUM-COMPUT ST, V62, P231, DOI 10.1016/j.ijhcs.2004.11.009
   Reeves B., 1998, MEDIA EQUATION PEOPL
   Saarni T., 2010, THESIS
   Schneider Walter, 2002, E-Prime user's guide
   Schroder M., 2009, CULTURE PERCEPTION, VIII, P307
   Shi Y., 2009, BEHAV INFORM TECHNOL
   Shneiderman Ben, 1997, interactions, V4, P42, DOI [DOI 10.1145/267505.267514, 10.1145/267505.267514]
   Spering M, 2005, COGNITION EMOTION, V19, P1252, DOI 10.1080/02699930500304886
   Surakka V, 1998, INT J PSYCHOPHYSIOL, V29, P23, DOI 10.1016/S0167-8760(97)00088-3
   Viswanathan M, 2005, COMPUT SPEECH LANG, V19, P55, DOI 10.1016/j.csl.2003.12.001
   Waaramaa-Maki-Kulmala T., 2009, THESIS
   Wambacq IJA, 2004, NEUROREPORT, V15, P555, DOI 10.1097/00001756-200403010-00034
   Werner NS, 2009, INT J PSYCHOPHYSIOL, V74, P259, DOI 10.1016/j.ijpsycho.2009.09.010
   Weyers P, 2006, PSYCHOPHYSIOLOGY, V43, P450, DOI 10.1111/j.1469-8986.2006.00451.x
   Wickens C.D., 2002, Theoretical issues in ergonomics science, V3, P159, DOI [10.1080/14639220210123806, DOI 10.1080/14639220210123806]
   ZAJONC RB, 1980, AM PSYCHOL, V35, P151, DOI 10.1037/0003-066X.35.2.151
NR 76
TC 8
Z9 9
U1 1
U2 19
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 0144-929X
EI 1362-3001
J9 BEHAV INFORM TECHNOL
JI Behav. Inf. Technol.
PD FEB 1
PY 2013
VL 32
IS 2
BP 117
EP 131
DI 10.1080/0144929X.2012.702285
PG 15
WC Computer Science, Cybernetics; Ergonomics
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Engineering
GA 109QU
UT WOS:000316384200003
DA 2024-01-09
ER

PT J
AU Tamagawa, R
   Watson, CI
   Kuo, IH
   MacDonald, BA
   Broadbent, E
AF Tamagawa, Rie
   Watson, Catherine I.
   Kuo, I. Han
   MacDonald, Bruce A.
   Broadbent, Elizabeth
TI The Effects of Synthesized Voice Accents on User Perceptions of Robots
SO INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS
LA English
DT Article
DE Human-robot interaction; Acceptability; Robot; Voice; Perception
ID LANGUAGE ATTITUDES; NEGATIVE AFFECT; SPEECH; SPEAKERS
AB Human voice accents have been shown to affect people's perceptions of the speaker, but little research has looked at how synthesized voice accents affect perceptions of robots. This research investigated people's perceptions of three synthesized voice accents. Three male robot voices were generated: British (UK), American (US), and New Zealand (NZ). In study one, twenty adults listened through headphones to a recorded script repeated in the three different accents, rated the nationality, roboticness, and overall impression of each voice, and chose their preferred accent. Study two used these voices on a healthcare robot to investigate the influence of accent on user perceptions of the robot. Ninety-one individuals were randomized to one of three conditions. In each condition they interacted with a healthcare robot that assisted with blood pressure measurement but the conditions differed in the accent the robot spoke with. In study one, each accent was correctly identified. There was no difference in impression ratings of each voice, but the US accent was rated as more robotic than the NZ accent, and the UK accent was preferred to the US accent. Study two showed that people randomized to the NZ accent had more positive feelings towards the robot and rated the robot's overall performance as higher compared to the robot with the US voice. These results suggest that the employment of a less robotic voice with a local accent may positively affect user perceptions of robots.
C1 [Tamagawa, Rie; Broadbent, Elizabeth] Univ Auckland, Dept Psychol Med, Auckland 1, New Zealand.
   [Watson, Catherine I.; Kuo, I. Han; MacDonald, Bruce A.] Univ Auckland, Dept Elect & Comp Engn, Auckland 1, New Zealand.
C3 University of Auckland; University of Auckland
RP Broadbent, E (corresponding author), Univ Auckland, Dept Psychol Med, Auckland 1, New Zealand.
EM e.broadbent@auckland.ac.nz
RI Watson, Catherine/JAD-1936-2023
OI Broadbent, Elizabeth/0000-0003-3626-9100; Watson,
   Catherine/0000-0001-9010-5188
FU University of Auckland
FX This research was funded by a University of Auckland Grant to Elizabeth
   Broadbent.
CR Alamsaputra DM, 2006, AUGMENT ALTERN COMM, V22, P258, DOI 10.1080/00498250600718555
   [Anonymous], 2006, P AUSTRALASIAN INT C
   [Anonymous], NZ LING SOC C PALM N
   [Anonymous], FESTIVAL SPEECH SYNT
   ARONOVITCH CD, 1976, J SOC PSYCHOL, V99, P207, DOI 10.1080/00224545.1976.9924774
   Arras K. O., 2005, TECHNICAL REPORT
   Atrash A, 2009, INT J SOC ROBOT, V1, P345, DOI 10.1007/s12369-009-0032-4
   Ball Peter, 1983, LANG SCI, V5, P163, DOI [10.1016/S0388-0001(83)80021-7, DOI 10.1016/S0388-0001(83)80021-7]
   BAYARD D, 1999, NZ ENGLISH, P297
   Bayard D., 2001, Journal of Sociolinguistics, V5, P22, DOI DOI 10.1111/1467-9481.00136
   Bayard Donn., 1995, KIWITALK SOCIOLINGUI
   Bennewitz Maren, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P1072
   Berry DS, 1996, J PERS SOC PSYCHOL, V71, P796, DOI 10.1037/0022-3514.71.4.796
   BLACK AW, 2007, BUILDING SYNTHETIC V
   Breazeal C, 2001, IROS 2001: PROCEEDINGS OF THE 2001 IEEE/RJS INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1388, DOI 10.1109/IROS.2001.977175
   Broadbent E., 2007, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, P3703, DOI 10.1109/IROS.2007.4398982
   Cargile AC, 1997, LANG COMMUN, V17, P195, DOI 10.1016/S0271-5309(97)00016-5
   Cesta A, 2007, PSYCHNOLOGY J, V5, P229
   FITT Susan, 2000, TECHNICAL REPORT
   GILES H, 1970, EDUC REV, V22, P211, DOI 10.1080/0013191700220301
   GILES H, 1995, LANG COMMUN, V15, P107, DOI 10.1016/0271-5309(94)00019-9
   Goetz J, 2003, RO-MAN 2003: 12TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P55
   HALL JA, 1981, J HEALTH SOC BEHAV, V22, P18, DOI 10.2307/2136365
   Huygens I., 1983, J MULTILING MULTICUL, V4, P207, DOI DOI 10.1080/01434632.1983.9994112
   Igic Aleksandar, 2009, Australas. Lang. Technol. Assoc. Work, P109
   Kuo I. H., 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P214, DOI 10.1109/ROMAN.2009.5326292
   LEBARON S, 1985, J FAM PRACTICE, V21, P56
   Li X, 2009, AUSTR C ROB AUT SYDN
   LUHMAN R, 1990, LANG SOC, V19, P331, DOI 10.1017/S0047404500014548
   Mayer RE, 2003, J EDUC PSYCHOL, V95, P419, DOI 10.1037/0022-0663.95.2.419
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Nass C, 2000, COMMUN ACM, V43, P36, DOI 10.1145/348941.348976
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Niculescu Andreea, 2008, P 5 NORDIC C HUMANCO, P523
   Oestreicher Lars, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P558
   Pucher M, 2009, LECT NOTES ARTIF INT, V5398, P216
   Robins B, 2004, RO-MAN 2004: 13TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P277, DOI 10.1109/ROMAN.2004.1374773
   Stern SE, 2008, J LANG SOC PSYCHOL, V27, P254, DOI 10.1177/0261927X08318035
   Tusing KJ, 2000, HUM COMMUN RES, V26, P148, DOI 10.1111/j.1468-2958.2000.tb00754.x
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
NR 42
TC 60
Z9 67
U1 5
U2 16
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1875-4791
EI 1875-4805
J9 INT J SOC ROBOT
JI Int. J. Soc. Robot.
PD AUG
PY 2011
VL 3
IS 3
BP 253
EP 262
DI 10.1007/s12369-011-0100-4
PG 10
WC Robotics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Robotics
GA V31OX
UT WOS:000208894000006
DA 2024-01-09
ER

PT J
AU Mayo, C
   Clark, RAJ
   King, S
AF Mayo, Catherine
   Clark, Robert A. J.
   King, Simon
TI Listeners' weighting of acoustic cues to synthetic speech naturalness: A
   multidimensional scaling analysis
SO SPEECH COMMUNICATION
LA English
DT Article
DE Speech synthesis; Evaluation; Speech perception; Acoustic cue weighting;
   Multidimensional scaling
ID VOICE QUALITY ASSESSMENT; FINAL STOP CONSONANTS; COMPLEX SOUNDS;
   CHILDREN; ADULTS; PERCEPTION; CATEGORIZATION; ENGLISH; RATINGS
AB The quality of current commercial speech synthesis systems is now so high that system improvements are being made at subtle sub- and supra-segmental levels. Human perceptual evaluation of such subtle improvements requires a highly sophisticated level of perceptual attention to specific acoustic characteristics or cues. However, it is not well understood what acoustic cues listeners attend to by default when asked to evaluate synthetic speech. It may, therefore, be potentially quite difficult to design an evaluation method that allows listeners to concentrate on only one dimension of the signal, while ignoring others that are perceptually more important to them.
   The aim of the current study was to determine which acoustic characteristics of unit-selection synthetic speech are most salient to listeners when evaluating the naturalness of such speech. This study made use of multidimensional scaling techniques to analyse listeners' pairwise comparisons of synthetic speech sentences. Results indicate that listeners place a great deal of perceptual importance on the presence of artifacts and discontinuities in the speech, somewhat less importance on aspects of segmental quality, and very little importance on stress/intonation appropriateness. These relative differences in importance will impact on listeners' ability to attend to these different acoustic characteristics of synthetic speech, and should therefore be taken into account when designing appropriate methods of synthetic speech evaluation. (C) 2010 Elsevier B.V. All rights reserved.
C1 [Mayo, Catherine; Clark, Robert A. J.; King, Simon] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9AB, Midlothian, Scotland.
C3 University of Edinburgh
RP Mayo, C (corresponding author), Univ Edinburgh, Ctr Speech Technol Res, 10 Crichton St, Edinburgh EH8 9AB, Midlothian, Scotland.
EM catherin@ling.ed.ac.uk; robert@cstr.ed.ac.uk; simon.king@ed.ac.uk
OI King, Simon/0000-0002-2694-2843
FU Engineering and Physical Sciences Research Council [EP/C53042X/1]
   Funding Source: researchfish
CR Allen P, 1997, J ACOUST SOC AM, V102, P2255, DOI 10.1121/1.419637
   Allen P, 2002, J ACOUST SOC AM, V112, P211, DOI 10.1121/1.1482075
   [Anonymous], P ICSLP 98 SYND
   BAILLY G, 2003, ISCA SPEC SESS HOT T
   BEST CT, 1981, PERCEPT PSYCHOPHYS, V29, P191, DOI 10.3758/BF03207286
   Black A., 1997, FESTIVAL SPEECH SYNT
   Bradlow AR, 1999, PERCEPT PSYCHOPHYS, V61, P206, DOI 10.3758/BF03206883
   CERNAK M, 2009, P ICSVI INT C SOUND
   Cernak M., 2005, EUR C AC, P2725
   CHEN JD, 1999, P EUROSPEECH, P611
   Christensen LA, 1997, J ACOUST SOC AM, V102, P2297, DOI 10.1121/1.419639
   CLARK RAJ, 1999, P EUR 99 6 EUR C SPE, P1623
   CLARK RAJ, 2003, INT C PHON SCI BARCE, P1141
   Clark RAJ, 2007, SPEECH COMMUN, V49, P317, DOI 10.1016/j.specom.2007.01.014
   CUTLER A, 1994, J MEM LANG, V33, P824, DOI 10.1006/jmla.1994.1039
   *EXP ADV GROUP LAN, 1996, EV NAT LANG PROC SYS
   FALK TH, 2008, P BLIZZ WORKSH BRISB
   Fisher C, 1996, SIGNAL TO SYNTAX: BOOTSTRAPPING FROM SPEECH TO GRAMMAR IN EARLY ACQUISITION, P343
   Francis AL, 2008, J ACOUST SOC AM, V124, P1234, DOI 10.1121/1.2945161
   Garofolo J.S., 1988, Tech. rep.
   GORDON PC, 1993, COGNITIVE PSYCHOL, V25, P1, DOI 10.1006/cogp.1993.1001
   Hall JL, 2001, J ACOUST SOC AM, V110, P2167, DOI 10.1121/1.1397322
   Hazan V, 2000, J PHONETICS, V28, P377, DOI 10.1006/jpho.2000.0121
   Hazan V, 1998, SPEECH COMMUN, V24, P211, DOI 10.1016/S0167-6393(98)00011-9
   HAZAN V, 1998, ICSLP SYD AUSTR, P2163
   HIRST D, 1998, P ESCA COCOSDA WORKS
   *ITU T, 1994, P85 ITUT
   Iverson P, 2003, COGNITION, V87, pB47, DOI 10.1016/S0010-0277(02)00198-1
   Iverson P, 2005, J ACOUST SOC AM, V118, P3267, DOI 10.1121/1.2062307
   Jilka M., 2003, P 15 ICPHS BARC, P2549
   JILKA M, 2005, P INT 2005 LISB PORT, P2393
   JUSCZYK P., 1997, The Discovery of Spoken Language
   KLABBERS E, 2001, IEEE T SPEECH AUDIO, V9
   KLABBERS E, 1998, P ICSLP, P1983
   Kreiman J, 1998, J ACOUST SOC AM, V104, P1598, DOI 10.1121/1.424372
   Kreiman J, 2000, J ACOUST SOC AM, V108, P1867, DOI 10.1121/1.1289362
   KREIMAN J, 2004, J ACOUST SOC AM, V115, P2609
   Kreiman J, 2007, J ACOUST SOC AM, V122, P2354, DOI 10.1121/1.2770547
   Kruskal JB., 1978, SAGE U PAPER SERIES, DOI 10.4135/9781412985130
   LAMEL LF, 1989, P SPEECH I O ASS SPE, P2161
   Marozeau J, 2003, J ACOUST SOC AM, V114, P2946, DOI 10.1121/1.1618239
   Mayo C, 2005, J ACOUST SOC AM, V118, P1730, DOI 10.1121/1.1979451
   Mayo C, 2004, J ACOUST SOC AM, V115, P3184, DOI 10.1121/1.1738838
   MAYO C, 2005, P INT 2005 LISB PORT
   MOLLER S, 2009, P NAG DAGA 2009 ROTT, P1168
   Nittrouer S, 2004, J ACOUST SOC AM, V115, P1777, DOI 10.1121/1.1651192
   PLUMPE M, 1998, P ESCA COCOSDA WORKS
   RABINOV CR, 1995, J SPEECH HEAR RES, V38, P26, DOI 10.1044/jshr.3801.26
   Schnieder W., 2002, E PRIME USERS GUIDE
   STYLIANOU Y, 2001, P ICASSP INT C AC SP
   SYRDAL A, 2004, J ACOUST SOC AM, V115, P2543
   SYRDAL AK, 2001, P EUR, P979
   Turk A., 2012, Methods in Empirical Prosody Research, P1, DOI [10.1515/9783110914641, DOI 10.1515/9783110914641.1, 10.1515/9783110914641.1]
   Vainio M., 2002, P IEEE 2002 WORKSH S
   Vepa J., 2004, TEXT SPEECH SYNTHESI
   WARDRIPFRUIN C, 1982, J ACOUST SOC AM, V71, P187, DOI 10.1121/1.387346
   WARDRIPFRUIN C, 1985, J ACOUST SOC AM, V77, P1907, DOI 10.1121/1.391833
   WATSON J, 1997, THESIS QUEEN MARGARE
   WIGHTMAN CW, 1992, J ACOUST SOC AM, V91, P1707, DOI 10.1121/1.402450
   Zen H, 2007, COMPUT SPEECH LANG, V21, P153, DOI 10.1016/j.csl.2006.01.002
NR 60
TC 20
Z9 23
U1 1
U2 12
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-6393
EI 1872-7182
J9 SPEECH COMMUN
JI Speech Commun.
PD MAR
PY 2011
VL 53
IS 3
BP 311
EP 326
DI 10.1016/j.specom.2010.10.003
PG 16
WC Acoustics; Computer Science, Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Acoustics; Computer Science
GA 724FK
UT WOS:000287561300004
OA Green Submitted
DA 2024-01-09
ER

PT J
AU Mitchell, WJ
   Szerszen, KA
   Lu, AS
   Schermerhorn, PW
   Scheutz, M
   MacDorman, KF
AF Mitchell, Wade J.
   Szerszen, Kevin A., Sr.
   Lu, Amy Shirong
   Schermerhorn, Paul W.
   Scheutz, Matthias
   MacDorman, Karl F.
TI A mismatch in the human realism of face and voice produces an uncanny
   valley
SO I-PERCEPTION
LA English
DT Article
DE anthropomorphism; facial-vocal mismatch; human realism; Masahiro Mori;
   social perception
AB The uncanny valley has become synonymous with the uneasy feeling of viewing an animated character or robot that looks imperfectly human. Although previous uncanny valley experiments have focused on relations among a character's visual elements, the current experiment examines whether a mismatch in the human realism of a character's face and voice causes it to be evaluated as eerie. The results support this hypothesis.
C1 [Mitchell, Wade J.; Szerszen, Kevin A., Sr.; Lu, Amy Shirong; MacDorman, Karl F.] Indiana Univ, Sch Informat, Indianapolis, IN 46202 USA.
   [Schermerhorn, Paul W.; Scheutz, Matthias] Indiana Univ, Cognit Sci Program, Indianapolis, IN 46202 USA.
   [Scheutz, Matthias] Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.
C3 Indiana University System; Indiana University-Purdue University
   Indianapolis; Indiana University System; Indiana University-Purdue
   University Indianapolis; Tufts University
RP Mitchell, WJ (corresponding author), Indiana Univ, Sch Informat, 535 West Michigan St, Indianapolis, IN 46202 USA.
EM wamitche@iupui.edu; keszersz@iupui.edu; amylu@iupui.edu;
   pscherme@indiana.edu; mscheutz@cs.tufts.edu; kmacdorm@indiana.edu
RI Lu, Amy Shirong/ITV-5174-2023; MacDorman, Karl/AAH-4483-2020; Lu, Amy
   Shirong/AAQ-7559-2020
OI Lu, Amy Shirong/0000-0002-8230-9049; MacDorman,
   Karl/0000-0003-1093-4184; Lu, Amy Shirong/0000-0002-8230-9049; Scheutz,
   Matthias/0000-0002-0064-2789
CR Ho CC, 2010, COMPUT HUM BEHAV, V26, P1508, DOI 10.1016/j.chb.2010.05.015
   Jentsch Ernst, 1906, Psychiatrisch-Neurologische Wochenschrift, P195
   MacDorman KF, 2006, INTERACT STUD, V7, P297, DOI 10.1075/is.7.3.03mac
   MacDorman KF, 2009, AI SOC, V23, P485, DOI 10.1007/s00146-008-0181-2
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   Misselhorn C, 2009, MIND MACH, V19, P345, DOI 10.1007/s11023-009-9158-2
   Moosa Mandi Muhammad, 2010, Biology Theory, V5, P12, DOI 10.1162/BIOT_a_00016
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Ramey C.H., 2005, P VIEWS UNC VALL WOR, P8
   Seyama J, 2007, PRESENCE-TELEOP VIRT, V16, P337, DOI 10.1162/pres.16.4.337
   Tinwell A, 2010, GAMRES COMPUTING CRE, V2, P3, DOI [10.1386/jgvw.2.1.3_1J, DOI 10.1386/JGVW.2.1.3_1]
NR 11
TC 126
Z9 152
U1 2
U2 41
PU PION LTD
PI LONDON
PA 207 BRONDESBURY PARK, LONDON NW2 5JN, ENGLAND
SN 2041-6695
J9 I-PERCEPTION
JI I-Perception
PY 2011
VL 2
IS 1
BP 10
EP 12
DI 10.1068/i0415
PG 3
WC Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA V32GU
UT WOS:000208940500002
PM 23145223
OA Green Published, gold
DA 2024-01-09
ER

PT J
AU Lee, EJ
AF Lee, Eun-Ju
TI The more humanlike, the better? How speech type and users' cognitive
   style affect social responses to computers
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Anthropomorphism; Computers Are Social Actors (CASA); Experientiality;
   Rationality
ID MODERATING ROLE; GENDER; INTERFACE; ANTHROPOMORPHISM; PERSONALITY;
   COPRESENCE
AB The present experiment investigated if anthropomorphic interfaces facilitate people's tendency to project social expectations onto computers and how such effects might vary depending on users' cognitive style. In a 2 (synthetic vs. recorded speech) x 2 (flattering vs. generic feedback) x 2 (low vs. high rationality) x 2 (low vs. high experientiality) experiment, participants played a trivia game with a computer. Use of recorded speech did not amplify the previously documented flattery effects (Fogg & Nass, 1997), challenging the notion that anthropomorphism will promote social responses to computers. Participants evaluated the human-voiced computer more positively and conformed more to its suggestions than the one using synthetic speech, but such effects were found only among less analytical or more intuition-driven individuals, suggesting dispositional differences in people's susceptibility to anthropomorphic cues embedded in the interface. (C) 2010 Elsevier Ltd. All rights reserved.
C1 Seoul Natl Univ, Dept Commun, Seoul 151742, South Korea.
C3 Seoul National University (SNU)
RP Lee, EJ (corresponding author), Seoul Natl Univ, Dept Commun, San 56-1 Shilim Dong, Seoul 151742, South Korea.
EM eunju0204@snu.ac.kr
RI Lee, Eun-ju/JAN-8749-2023
CR Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2005, PRESENCE-VIRTUAL AUG, V14, P379, DOI 10.1162/105474605774785235
   Byrne D., 1971, The attraction paradigm
   Campbell MC, 2000, J CONSUM RES, V27, P69, DOI 10.1086/314309
   Carli LL, 2001, J SOC ISSUES, V57, P725, DOI 10.1111/0022-4537.00238
   Cassell J, 1999, APPL ARTIF INTELL, V13, P519, DOI 10.1080/088395199117360
   Chen S, 1999, DUAL-PROCESS THEORIES IN SOCIAL PSYCHOLOGY, P73
   Dehn DM, 2000, INT J HUM-COMPUT ST, V52, P1, DOI 10.1006/ijhc.1999.0325
   EAGLY AH, 1975, J PERS SOC PSYCHOL, V32, P136, DOI 10.1037/h0076850
   EPSTEIN S, 1994, AM PSYCHOL, V49, P709, DOI 10.1037/0003-066X.49.8.709
   Epstein S, 1999, DUAL-PROCESS THEORIES IN SOCIAL PSYCHOLOGY, P462
   Fogg B., 2003, PERSUASIVE TECHNOLOG
   Fogg BJ, 1997, INT J HUM-COMPUT ST, V46, P551, DOI 10.1006/ijhc.1996.0104
   HECKMAN CE, 2000, AUTON AGENT MULTI-AG, P435
   Hinds PJ, 2004, HUM-COMPUT INTERACT, V19, P151, DOI 10.1207/s15327051hci1901&2_7
   Lee EJ, 2003, INT J HUM-COMPUT ST, V58, P347, DOI 10.1016/S1071-5819(03)00009-0
   Lee Eun Ju, 2000, P CHI 00 HUM FACT CO, P289, DOI [10.1145/633292.633461, DOI 10.1145/633292.633461]
   Lee EJ, 2008, INT J HUM-COMPUT ST, V66, P789, DOI 10.1016/j.ijhcs.2008.07.009
   Lee EJ, 2008, J COMMUN, V58, P301, DOI 10.1111/j.1460-2466.2008.00386.x
   Lee KM, 2004, HUM COMMUN RES, V30, P182, DOI 10.1093/hcr/30.2.182
   LI G, 2007, HUMAN COMMUNICATION, V33, P163
   MacDorman K. F., 2006, SUBJECTIVE RATINGS R, P25
   Moon Y, 2000, J CONSUM RES, V26, P323, DOI 10.1086/209566
   Moon Y, 1996, COMMUN RES, V23, P651, DOI 10.1177/009365096023006002
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Nass C, 2000, J SOC ISSUES, V56, P81, DOI 10.1111/0022-4537.00153
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Nowak KL, 2005, J COMPUT-MEDIAT COMM, V11
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Pacini R, 1999, J PERS SOC PSYCHOL, V76, P972, DOI 10.1037/0022-3514.76.6.972
   Parise S, 1999, COMPUT HUM BEHAV, V15, P123, DOI 10.1016/S0747-5632(98)00035-1
   Petty R.E., 1986, The Elaboration Likelihood Model of Persuasion, P1, DOI DOI 10.1007/978-1-4612-4964-1_1
   Powers Aaron, 2006, P 1 ACM SIGCHI SIGAR, P218, DOI [10.1145/1121241.1121280, DOI 10.1145/1121241.1121280, DOI 10.1109/MRA.2018.2833157]
   Reeves B., 1998, MEDIA EQUATION PEOPL
   Sproull L, 1996, HUM-COMPUT INTERACT, V11, P97, DOI 10.1207/s15327051hci1102_1
   Sundar SS, 2004, BEHAV INFORM TECHNOL, V23, P107, DOI 10.1080/01449290310001659222
   Yee N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1
NR 37
TC 45
Z9 56
U1 2
U2 43
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD JUL
PY 2010
VL 26
IS 4
BP 665
EP 672
DI 10.1016/j.chb.2010.01.003
PG 8
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA 605LG
UT WOS:000278348500022
DA 2024-01-09
ER

PT J
AU Eadie, TL
   Doyle, TC
   Hansen, K
   Beaudin, TG
AF Eadie, Tanya L.
   Doyle, Tphilip C.
   Hansen, Kerry
   Beaudin, Tpaul G.
TI Influence of speaker gender on listener judgments of tracheoesophageal
   speech
SO JOURNAL OF VOICE
LA English
DT Article; Proceedings Paper
CT 35th Annual Symposium of the Voice-Foundation
CY JUN, 2006
CL Philadelphia, PA
SP Voice Fdn
DE tracheoesophageal; speech-listener; judgments-acoustic measures
ID QUALITY-OF-LIFE; ESOPHAGEAL SPEECH; PERCEPTUAL EVALUATION;
   FREQUENCY-CHARACTERISTICS; ACOUSTIC CHARACTERISTICS; ACCEPTABILITY
   RATINGS; LARYNGEAL-CANCER; VOICE; FEMALE; RESTORATION
AB The objectives of this prospective and exploratory study are to determine: (1) naive listener preference for gender in tracheoesophageal (TE) speech when speech severity is controlled; (2) the accuracy of identifying TE speaker gender; (3) the effects of gender identification on judgments of speech acceptability (ACC) and naturalness (NAT); and (4) the acoustic basis of ACC and NAT judgments. Six male and six female adult TE speakers were matched for speech severity. Twenty naive listeners made auditory-perceptual judgments of speech samples in three listening sessions. First, listeners performed preference judgments using a paired comparison paradigm. Second, listeners made judgments of speaker gender, speech ACC, and NAT using rating scales. Last, listeners made ACC and NAT judgments when speaker gender was provided coincidentally. Duration, frequency, and spectral measures were performed. No significant differences were found for preference of male or female speakers. All male speakers were accurately identified, but only two of six female speakers were accurately identified. Significant interactions were found between gender and listening condition (gender known) for NAT and ACC judgments. Males were judged more natural when gender was known; female speakers were judged less natural and less acceptable when gender was known. Regression analyses revealed that judgments of female speakers were best predicted with duration measures when gender was unknown, but with spectral measures when gender was known; judgments of males were best predicted with spectral measures. Naive listeners have difficulty identifying the gender of female TE speakers. Listeners show no preference for speaker gender, but when gender is known, female speakers are least acceptable and natural. The nature of the perceptual task may affect the acoustic basis of listener judgments.
C1 [Eadie, Tanya L.] Univ Washington, Dept Speech & Hearing Sci, Seattle, WA 98195 USA.
   [Doyle, Tphilip C.; Hansen, Kerry; Beaudin, Tpaul G.] Univ Western Ontario, Doctoral Program Rehabil Sci, London, ON, Canada.
C3 University of Washington; University of Washington Seattle; Western
   University (University of Western Ontario)
RP Eadie, TL (corresponding author), Univ Washington, Dept Speech & Hearing Sci, 1417 NE 42nd St, Seattle, WA 98195 USA.
EM teadie@u.washington.edu
OI Eadie, Tanya/0000-0002-7697-1298
CR [Anonymous], CANC FACTS FIG 2003
   *AV INN INC, 1998, INT VOIC AN SYST IVA
   Avaaz Innovations Inc, 1998, EXP CONTR GEN WIND E
   Bellandese MH, 2001, J SPEECH LANG HEAR R, V44, P1315, DOI 10.1044/1092-4388(2001/102)
   BENNETT S, 1973, J SPEECH HEAR RES, V16, P608, DOI 10.1044/jshr.1604.608
   BERLIN CI, 1965, J SPEECH HEAR DISORD, V30, P174, DOI 10.1044/jshd.3002.174
   Cervera T, 2001, J SPEECH LANG HEAR R, V44, P988, DOI 10.1044/1092-4388(2001/077)
   Doyle P.C., 2005, CONT CONSIDERATIONS, P113
   Doyle P. C., 1994, Foundations of Voice and Speech Rehabilitation Following Laryngeal Cancer
   Eadie TL, 2004, LARYNGOSCOPE, V114, P753, DOI 10.1097/00005537-200404000-00030
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   FILTER MD, 1975, PERCEPT MOTOR SKILL, V40, P63, DOI 10.2466/pms.1975.40.1.63
   Finizia C, 1999, ARCH OTOLARYNGOL, V125, P157, DOI 10.1001/archotol.125.2.157
   Finizia C, 1998, LARYNGOSCOPE, V108, P1566, DOI 10.1097/00005537-199810000-00027
   Gelfer MP, 2000, J VOICE, V14, P22, DOI 10.1016/S0892-1997(00)80092-2
   Graville DJ, 2004, J MED SPEECH-LANG PA, V12, P107
   Hillman RE, 1998, ANN OTO RHINOL LARYN, V107, P2
   Iversen-Thoburn SK, 2000, J MED SPEECH-LANG PA, V8, P85
   *KAY EL CORP, 2000, COMP SPEECH LAB CSL
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   LANPHER A, 1997, SELF HELP LARYNGECTO, P71
   Meltzner GS, 2005, J SPEECH LANG HEAR R, V48, P766, DOI 10.1044/1092-4388(2005/053)
   Most T, 2000, J COMMUN DISORD, V33, P165, DOI 10.1016/S0021-9924(99)00030-1
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Parsa V, 2001, J SPEECH LANG HEAR R, V44, P327, DOI 10.1044/1092-4388(2001/027)
   PINDZOLA RH, 1988, LARYNGOSCOPE, V98, P394
   PINDZOLA RH, 1989, ANN OTO RHINOL LARYN, V98, P960, DOI 10.1177/000348948909801208
   ROBBINS J, 1984, J SPEECH HEAR DISORD, V49, P202, DOI 10.1044/jshd.4902.202
   Searl JP, 2002, J COMMUN DISORD, V35, P407, DOI 10.1016/S0021-9924(02)00092-8
   SHANKS J, 1986, LARYNGECTOMEE REHABI, P269
   SHIPP T, 1967, J SPEECH HEAR RES, V10, P417, DOI 10.1044/jshr.1003.417
   SINGER MI, 1980, ANN OTO RHINOL LARYN, V89, P529, DOI 10.1177/000348948008900608
   SISTY NL, 1972, J SPEECH HEAR RES, V15, P439, DOI 10.1044/jshr.1502.439
   SNIDECOR JC, 1975, LARYNGOSCOPE, V85, P640, DOI 10.1288/00005537-197504000-00005
   SNIDECOR JC, 1959, ANN OTO RHINOL LARYN, V68, P1
   TARDYMITZELL S, 1985, ARCH OTOLARYNGOL, V111, P212
   TRUDEAU MD, 1990, J SPEECH HEAR DISORD, V55, P244, DOI 10.1044/jshd.5502.244
   van As CJ, 1998, J VOICE, V12, P239, DOI 10.1016/S0892-1997(98)80044-1
   Van Riper C., 1978, SPEECH CORRECTION PR
   WEINBERG B, 1972, J SPEECH HEAR RES, V15, P211, DOI 10.1044/jshr.1501.211
   WEINBERG B, 1971, J SPEECH HEAR RES, V14, P391, DOI 10.1044/jshr.1402.391
   WILLIAMS SE, 1985, ARCH OTOLARYNGOL, V111, P216
   Yorkston K. M., 1988, Clinical management of dysarthric speakers
NR 43
TC 11
Z9 15
U1 0
U2 6
PU MOSBY-ELSEVIER
PI NEW YORK
PA 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN 0892-1997
J9 J VOICE
JI J. Voice
PD JAN
PY 2008
VL 22
IS 1
BP 43
EP 57
DI 10.1016/j.jvoice.2006.08.008
PG 15
WC Audiology & Speech-Language Pathology; Otorhinolaryngology
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Audiology & Speech-Language Pathology; Otorhinolaryngology
GA 250HK
UT WOS:000252290400005
PM 17055223
DA 2024-01-09
ER

PT J
AU Gong, L
   Nass, C
AF Gong, Li
   Nass, Clifford
TI When a talking-face computer agent is half-human and half-humanoid:
   Human identity and consistency preference
SO HUMAN COMMUNICATION RESEARCH
LA English
DT Article
ID SELF-DISCLOSURE; EXPERIMENTAL TESTS; COMMUNICATION; INFORMATION;
   REPRESENTATION; INCONSISTENCY; PERSONALITY; ATTENTION; ATTITUDES
AB Computer-generated anthropomorphic characters are a growing type of communicator that is deployed in digital communication environments. An essential theoretical question is how people identify humanlike but clearly artificial, hence humanoid, entities in comparison to natural human ones. This identity categorization inquiry was approached under the framework of consistency and tested through examining inconsistency effects from mismatching categories. Study 1 (N = 80), incorporating a self-disclosure task, tested participants' responses to a talking-face agent, which varied in four combinations of human versus humanoid faces and voices. In line with the literature on inconsistency, the pairing of a human face with a humanoid voice or a humanoid face with a human voice led to longer processing time in making judgment of the agent and less trust than the pairing of a face and a voice from either the human or the humanoid category. Female users particularly showed negative attitudes toward inconsistently paired talking faces. Study 2 (N = 80), using a task that stressed comprehension demand, replicated the inconsistency effects on judging time and females' negative attitudes but not for comprehension-related outcomes. Voice clarity overshadowed the consistency concern for comprehension-related responses. The overall inconsistency effects suggest that people treat humanoid entities in a different category from natural human ones.
C1 Ohio State Univ, Sch Commun, Columbus, OH 43210 USA.
   Stanford Univ, Dept Commun, Stanford, CA 94305 USA.
C3 University System of Ohio; Ohio State University; Stanford University
RP Gong, L (corresponding author), Ohio State Univ, Sch Commun, Columbus, OH 43210 USA.
EM gong.33@osu.edu
CR [Anonymous], P 8 WORLD C ART INT
   [Anonymous], J ABNORMAL SOCIAL PS
   ARGYLE M, 1970, BRIT J SOC CLIN PSYC, V9, P222, DOI 10.1111/j.2044-8260.1970.tb00668.x
   ARGYLE M, 1971, EUR J SOC PSYCHOL, V1, P385, DOI 10.1002/ejsp.2420010307
   Berger C.R., 1974, Human Communication Research, V1, P99, DOI 10.1111/j.1468-2958.1975.tb00258.x
   Berger CR, 2005, J COMMUN, V55, P415, DOI 10.1111/j.1460-2466.2005.tb02680.x
   BERGER CR, 1986, HUM COMMUN RES, V13, P34, DOI 10.1111/j.1468-2958.1986.tb00093.x
   BICKMORE T, 2001, P SIGCHI C HUM FACT, P396, DOI DOI 10.1145/365024.365304
   BURGOON JK, 1993, HUM COMMUN RES, V20, P67, DOI 10.1111/j.1468-2958.1993.tb00316.x
   Burgoon JK, 2000, COMPUT HUM BEHAV, V16, P553, DOI 10.1016/S0747-5632(00)00029-7
   Dindia K, 1997, HUM COMMUN RES, V23, P388, DOI 10.1111/j.1468-2958.1997.tb00402.x
   EKMAN P, 1974, J PERS SOC PSYCHOL, V29, P288, DOI 10.1037/h0036006
   EKMAN P, 1969, PSYCHIATR, V32, P88, DOI 10.1080/00332747.1969.11023575
   FISKE ST, 1990, ADV EXP SOC PSYCHOL, V23, P1, DOI 10.1016/S0065-2601(08)60317-2
   Gibbs JL, 2006, COMMUN RES, V33, P152, DOI 10.1177/0093650205285368
   GOLINKOFF RM, 1976, CHILD DEV, V47, P252, DOI 10.2307/1128308
   Gong L., 2003, INT J SPEECH TECHNOL, V6, P123
   Grabner-Kräuter S, 2003, INT J HUM-COMPUT ST, V58, P783, DOI 10.1016/S1071-5819(03)00043-0
   GRAVES JR, 1976, J COUNS PSYCHOL, V23, P333, DOI 10.1037/0022-0167.23.4.333
   GREEN EJ, 1981, PERCEPT PSYCHOPHYS, V30, P459, DOI 10.3758/BF03204842
   HALL JA, 1978, PSYCHOL BULL, V85, P845, DOI 10.1037/0033-2909.85.4.845
   HAMERS JF, 1972, J VERB LEARN VERB BE, V11, P303, DOI 10.1016/S0022-5371(72)80091-4
   HENDRICK C, 1972, J PERS SOC PSYCHOL, V22, P219, DOI 10.1037/h0032599
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   Kiesler S, 1996, J PERS SOC PSYCHOL, V70, P47, DOI 10.1037/0022-3514.70.1.47
   Lee EJ, 2002, HUM COMMUN RES, V28, P349, DOI 10.1093/hcr/28.3.349
   Lee EJ, 2004, HUM COMMUN RES, V30, P234, DOI 10.1093/hcr/30.2.234
   MACLEOD CM, 1991, PSYCHOL BULL, V109, P163, DOI 10.1037/0033-2909.109.2.163
   MAES P, 1994, COMMUN ACM, V37, P31, DOI 10.1145/176789.176792
   MAZANEC N, 1976, J PSYCHOL, V93, P175, DOI 10.1080/00223980.1976.9915810
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Moon Y, 2000, J CONSUM RES, V26, P323, DOI 10.1086/209566
   Nass C, 2000, COMMUN ACM, V43, P36, DOI 10.1145/348941.348976
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   OLIVE JP, 1997, HALS LEGACY 2001S CO, P101
   Pomerantz J. R., 1986, HDB PERCEPTION HUMAN, V2, P36
   Rosenblum LD, 1997, PERCEPT PSYCHOPHYS, V59, P347, DOI 10.3758/BF03211902
   ROY L, 1990, J GENET PSYCHOL, V151, P515, DOI 10.1080/00221325.1990.9914636
   SCHUL Y, 1983, EUR J SOC PSYCHOL, V13, P143, DOI 10.1002/ejsp.2420130205
   Sproull L, 1996, HUM-COMPUT INTERACT, V11, P97, DOI 10.1207/s15327051hci1102_1
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   Tajfel H., 1986, PSYCHOL INTERGROUP R, P7, DOI DOI 10.4324/9780203505984-16
   VOLKMAR FR, 1979, J CHILD PSYCHOL PSYC, V20, P139, DOI 10.1111/j.1469-7610.1979.tb00494.x
   Wheeless L. R., 1978, Human Communication Research, V4, P143, DOI DOI 10.1111/J.1468-2958.1978.TB00604.X
   Wheeless Lawrence R, 1977, Human Communication Research, V3, P250, DOI [DOI 10.1111/J.1468-2958.1977.TB00523.X, 10.1111/j.1468-2958.1977.tb00523.x]
   ZAHN GL, 1973, J EXP SOC PSYCHOL, V9, P320, DOI 10.1016/0022-1031(73)90069-3
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 54
TC 75
Z9 90
U1 3
U2 32
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 0360-3989
EI 1468-2958
J9 HUM COMMUN RES
JI Hum. Commun. Res.
PD APR
PY 2007
VL 33
IS 2
BP 163
EP 193
DI 10.1111/j.1468-2958.2007.00295.x
PG 31
WC Communication
WE Social Science Citation Index (SSCI)
SC Communication
GA 145SP
UT WOS:000244885100003
DA 2024-01-09
ER

PT J
AU Meltzner, GS
   Hillman, RE
AF Meltzner, GS
   Hillman, RE
TI Impact of aberrant acoustic properties on the perception of sound
   quality in electrolarynx speech
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
DE electrolarynx speech; alaryngeal speech; laryngectomy speech
   rehabilitation; speech quality; artificial larynx speech
ID LARYNGECTOMEE REHABILITATION; COMMUNICATION; MODEL
AB A large percentage of patients who have undergone laryngectomy to treat advanced laryngeal cancer rely on an electrolarynx (EL) to communicate verba Although serviceable, EL speech is plagued by shortcomings in both sound quality This study sought to better quantify the relative contributions of and intelligibility. previously identified acoustic abnormalities to the perception of degraded quality in EL speech. Ten normal listeners evaluated the sound quality of EL speech tokens that had been acoustically enhanced by (a) increased low-frequency energy, (b) EL-noise reduction, and (c) fundamental frequency variation to mimic normal pitch intonation in relation to nonenhanced EL speech, normal speech, and normal monotonous speech (fundamental frequency variation removed). In comparing all possible combinations of token pairs, listeners were asked to identify which one of each pair sounded most like normal natural speech, and then to rate on a visual analog scale how different the chosen token was from normal speech. The results indicate that although EL speech can be most improved by removing the EL noise and providing proper pitch information, the resulting quality is still well below that of normal natural speech or even that of monotonous natural speech. This suggests that, in addition to the widely acknowledged acoustic abnormalities examined in this investigation, there are other attributes that contribute significantly to the unnatural quality of EL speech. Such additional factors need to be clearly identified and remedied before EL speech can be made to more closely approximate the sound quality of normal natural speech.
C1 MIT, Cambridge, MA 02139 USA.
   Massachusetts Gen Hosp, Boston, MA 02114 USA.
   Harvard Univ, Sch Med, Boston, MA 02115 USA.
C3 Massachusetts Institute of Technology (MIT); Harvard University;
   Massachusetts General Hospital; Harvard University; Harvard Medical
   School
RP Meltzner, GS (corresponding author), BAE Syst, 6 New England Execut Pk, Burlington, MA 01803 USA.
EM geoffrey.meltzner@baesystems.com
CR BARNEY HL, 1959, READINGS SPEECH FOLL, P1337
   COLE D, 1997, P IEEE TENCON 97 IEE, V2, P491
   CRYSTAL TH, 1982, J ACOUST SOC AM, V72, P705, DOI 10.1121/1.388251
   DIEDRICH W, 1977, ALARYNGEAL SPEECH
   Edwards A. L., 1983, Techniques of Attitude Scale Construction
   Espy-Wilson CY, 1998, J SPEECH LANG HEAR R, V41, P1253, DOI 10.1044/jslhr.4106.1253
   GATES GA, 1982, AM J OTOLARYNG, V3, P1, DOI 10.1016/S0196-0709(82)80025-2
   GATES GA, 1982, AM J OTOLARYNG, V3, P8, DOI 10.1016/S0196-0709(82)80026-4
   GRAY S, 1976, ARCH PHYS MED REHAB, V57, P140
   Harris R. J., 2001, A primer of multivariate statistics
   HEATON J, 2004, ANN OTOLOGY RHINOLOG, V109, P972
   Hillman R E, 1998, Ann Otol Rhinol Laryngol Suppl, V172, P1
   HOUSE AS, 1958, J SPEECH HEAR RES, V1, P309, DOI 10.1044/jshr.0104.309
   KING PS, 1968, AMER J PHYSICAL MED, V47, P192
   KOMMERS MS, 1979, J COMMUN DISORD, V12, P411, DOI 10.1016/0021-9924(79)90005-4
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   KRUS DJ, 1977, EDUC PSYCHOL MEAS, V37, P189, DOI 10.1177/001316447703700119
   Ma K., 1999, P EUR C SPEECH COMM, P323
   MCCREE AV, 1995, IEEE T SPEECH AUDI P, V3, P242, DOI 10.1109/89.397089
   Meltzner G., 2005, CONT CONSIDERATIONS
   Meltzner GS, 2003, J ACOUST SOC AM, V114, P1035, DOI 10.1121/1.1582440
   MELTZNER GS, 2003, DISS ABSTR INT, V64, P4486
   Mitchell HL, 1996, J SPEECH HEAR RES, V39, P93, DOI 10.1044/jshr.3901.93
   MORRIS HL, 1992, ANN OTO RHINOL LARYN, V101, P503, DOI 10.1177/000348949210100611
   MOSTELLER F, 1951, PSYCHOMETRIKA, V16, P207
   MOULINES E, 1990, SPEECH COMMUN, V9, P453, DOI 10.1016/0167-6393(90)90021-Z
   Poulton E. C., 1989, BIAS QUANTIFYING JUD
   QI YY, 1991, J SPEECH HEAR RES, V34, P1250, DOI 10.1044/jshr.3406.1250
   RICHARDSON J L, 1985, Journal of Psychosocial Oncology, V3, P83
   SISTY NL, 1972, J SPEECH HEAR RES, V15, P439, DOI 10.1044/jshr.1502.439
   Stevens K., 1998, Acoustic phonetics
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   TORGERSON WS, 1957, THEORY METHODS SCALI
   UEMI N, 1994, IEEE WORKSH ROB HUM, P198
   *US DEP DEF, 1999, MILSTD3005 US DEP DE
   WEBSTER PM, 1990, ANN OTO RHINOL LARYN, V99, P197
   WEISS MS, 1979, J ACOUST SOC AM, V65, P1298, DOI 10.1121/1.382697
   WEISS MS, 1985, J SPEECH HEAR RES, V28, P294, DOI 10.1044/jshr.2802.294
   [No title captured]
NR 39
TC 40
Z9 47
U1 0
U2 6
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE, ROCKVILLE, MD 20852-3279 USA
SN 1092-4388
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD AUG
PY 2005
VL 48
IS 4
BP 766
EP 779
DI 10.1044/1092-4388(2005/053)
PG 14
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA 993SI
UT WOS:000233974900005
PM 16378472
DA 2024-01-09
ER

PT J
AU Coughlin-Woods, S
   Lehman, ME
   Cooke, PA
AF Coughlin-Woods, S
   Lehman, ME
   Cooke, PA
TI Ratings of speech naturalness of children ages 8-16 years
SO PERCEPTUAL AND MOTOR SKILLS
LA English
DT Article
ID TREATMENT EFFICACY; STUTTERERS; ADULTS; NONSTUTTERERS; THERAPY
AB The focus of this cross-sectional study was the investigation of Speech Naturalness (speech that Sounds normal or natural to the listener) of 60 normal speaking children and adolescents between the ages of 8 and 16 years. 26 naive adult listeners rated the naturalness of videotaped and computer-presented speech samples, using a 9-point-Likert rating scale (1: highly natural sounding speech and 9: highly unnatural sounding speech). The children and adolescents who participated as speakers were distributed among 5 age groups (8, 10, 12, 14, and 16 yr.) with 6 boys and 6 girls in each group. Each child demonstrated normal articulation, language, voice, and speech fluency skills. Age and sex comparisons indicated boys' and girls' speech was rated comparably; however, 8-yr.-olds' speech was rated as significantly less natural than those of 12-, 14-, and 16-yr.-olds. Preliminary ratings of Speech Naturalness for normal speaking children were presented. Suggestions for the clinical application of the findings as a target criterion in treatment programs with communicatively impaired children were suggested. Replication with a larger and more representative sample is in order.
C1 Cent Michigan Univ, Mt Pleasant, MI 48859 USA.
   Michigan State Univ, E Lansing, MI 48824 USA.
C3 Central Michigan University; Michigan State University
RP Coughlin-Woods, S (corresponding author), Cent Michigan Univ, 2171 Hlth Prof Bldg, Mt Pleasant, MI 48859 USA.
EM Coughlss@cmich.edu
RI Cooke, Philip/GRO-1618-2022
CR CURLEE RF, 1993, J FLUENCY DISORD, V18, P319, DOI 10.1016/0094-730X(93)90012-S
   Dawson LO, 1929, ELEM SCHOOL J, V29, P610, DOI 10.1086/456299
   Dayalu VN, 2002, PERCEPT MOTOR SKILL, V94, P87, DOI 10.2466/PMS.94.1.87-96
   Finn P, 1997, J SPEECH LANG HEAR R, V40, P821, DOI 10.1044/jslhr.4004.821
   Ingham JC, 1998, J SPEECH LANG HEAR R, V41, P753, DOI 10.1044/jslhr.4104.753
   Ingham RJ, 2001, J SPEECH LANG HEAR R, V44, P841, DOI 10.1044/1092-4388(2001/066)
   INGHAM RJ, 1985, J SPEECH HEAR RES, V28, P495, DOI 10.1044/jshr.2804.495
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P261, DOI 10.1044/jshd.5003.261
   JOHNSON L, 1987, THESIS U MINNESOTA
   KALINOWSKI J, 1994, AM J SPEECH-LANG PAT, V3, P61
   KOWAL S, 1975, J PSYCHOLINGUIST RES, V4, P195, DOI 10.1007/BF01066926
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   LOVE LR, 1971, J SPEECH HEAR RES, V14, P229, DOI 10.1044/jshr.1402.229
   MARTIN RR, 1992, J SPEECH HEAR RES, V35, P521, DOI 10.1044/jshr.3503.521
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   NICHOLS AC, 1966, SPEECH MONOGR, V33, P156
   Onslow M, 1996, J SPEECH HEAR RES, V39, P734, DOI 10.1044/jshr.3904.734
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P274, DOI 10.1044/jshr.3502.274
   Parrish WM, 1951, Q J SPEECH, V37, P448, DOI 10.1080/00335635109381697
   Ratcliff A., 2002, AUGMENT ALTERN COMM, V18, P11, DOI DOI 10.1080/714043393
   RUNYAN CM, 1979, J FLUENCY DISORD, V4, P29, DOI 10.1016/0094-730X(79)90029-9
   STARKWEATHER CW, 1985, STUTTERING THERAPY P, V20, P67
   VANRIPER C, 1990, SPEECH CORRECTION
NR 24
TC 6
Z9 7
U1 1
U2 1
PU SAGE PUBLICATIONS INC
PI THOUSAND OAKS
PA 2455 TELLER RD, THOUSAND OAKS, CA 91320 USA
SN 0031-5125
EI 1558-688X
J9 PERCEPT MOTOR SKILL
JI Percept. Mot. Skills
PD APR
PY 2005
VL 100
IS 2
BP 295
EP 304
PG 10
WC Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA 924UG
UT WOS:000229005400003
PM 15974337
DA 2024-01-09
ER

PT J
AU Moore, BCJ
   Tan, CT
AF Moore, BCJ
   Tan, CT
TI Perceived naturalness of spectrally distorted speech and music
SO JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA
LA English
DT Article
ID DIFFERENT FREQUENCY RESPONSES; SOUND QUALITY; LOUDSPEAKER MEASUREMENTS;
   LISTENER PREFERENCES; FIELD; EAR; TRANSFORMATION
AB We determined how the,perceived naturalness of music and speech (male and female talkers) signals was affected by various forms of linear filtering, some of which were intended to mimic the spectral "distortions" introduced by transducers such as microphones, loudspeakers, and earphones. The filters introduced spectral tilts and ripples of various types, variations in upper and lower cutoff frequency, and combinations of these. All of the differently filtered signals (168 conditions) were intermixed in random order within one block of trials. Levels were adjusted to give approximately equal loudness in all conditions. Listeners were required to judge the perceptual quality (naturalness) of the filtered signals on a scale from 1 to 10. For spectral ripples, perceived quality decreased with increasing ripple density up to 0.2 ripple/ERBN and with increasing ripple depth. Spectral tilts also. degraded quality, and the effects were similar for positive and negative tilts. Ripples and/or tilts degraded quality more when they extended over a wide frequency range (87-6981 Hz) than when they extended over subranges. Low- and mid-frequency ranges were roughly equally important for music, but the mid-range was most important for speech. For music, the highest quality was obtained for the broadband signal (55-16854 Hz). Increasing the lower cutoff frequency from 55 Hz resulted in a clear degradation of quality. There was also a distinct degradation as the upper cutoff frequency was decreased from 16 845 Hz. For speech, there was a marked degradation when the lower cutoff frequency was increased from 123 to 208 Hz and when the upper cutoff frequency was decreased from 10 869 Hz. Typical telephone bandwidth (313 to 3547 Hz) gave very poor quality. (C) 2003 Acoustical Society of America.
C1 Univ Cambridge, Dept Expt Psychol, Cambridge CB2 3EB, England.
C3 University of Cambridge
RP Moore, BCJ (corresponding author), Univ Cambridge, Dept Expt Psychol, Downing St, Cambridge CB2 3EB, England.
RI Tan, Chin-Tuan/AAA-9441-2021; Moore, Brian/I-5541-2012
OI Tan, Chin-Tuan/0000-0002-4676-4917; Moore, Brian/0000-0001-7071-0671
CR Aibara R, 2001, HEARING RES, V152, P100, DOI 10.1016/S0378-5955(00)00240-9
   ALLEN JB, 1977, IEEE T ACOUST SPEECH, V25, P235, DOI 10.1109/TASSP.1977.1162950
   Bech S, 2002, J AUDIO ENG SOC, V50, P564
   Bucklein R., 1962, FREQUENZ, V16/1962, P103
   BURKHARD MD, 1975, J ACOUST SOC AM, V58, P214, DOI 10.1121/1.380648
   Cochran W.G., 1967, STAT METHODS, V6th ed.
   FRYER PA, 1977, HI FI NEWS REC, V22, P51
   GABRIELSSON A, 1988, J SPEECH HEAR RES, V31, P166, DOI 10.1044/jshr.3102.166
   GABRIELSSON A, 1991, J ACOUST SOC AM, V90, P707, DOI 10.1121/1.401941
   GABRIELSSON A, 1990, J ACOUST SOC AM, V88, P1359, DOI 10.1121/1.399713
   GABRIELSSON A, 1976, TA83 KAROLINSKA I TE, P1
   GLASBERG BR, 1990, HEARING RES, V47, P103, DOI 10.1016/0378-5955(90)90170-T
   Gockel H, 1997, J ACOUST SOC AM, V102, P2311, DOI 10.1121/1.419640
   GONTCHAROV VP, 1999, AES 106 CONV MUN
   Green D. M., 1988, PROFILE ANAL
   Jacobs J. E., 1964, J. Audio Eng. Soc., V12, P115
   KILLION MC, 1987, J ACOUST SOC AM, V81, pS75, DOI 10.1121/1.2024388
   KUHN GF, 1979, J ACOUST SOC AM, V65, P991, DOI 10.1121/1.382606
   LETOWSKI T, 1975, ACUSTICA, V34, P106
   Moore B. C. J., 2013, An introduction to the psychology of hearing, V6th
   Moore BCJ, 1997, J AUDIO ENG SOC, V45, P224
   MOORE BCJ, 1983, J ACOUST SOC AM, V74, P750, DOI 10.1121/1.389861
   POULTON EC, 1979, PSYCHOL BULL, V86, P777, DOI 10.1037/0033-2909.86.4.777
   Puria S, 1997, J ACOUST SOC AM, V101, P2754, DOI 10.1121/1.418563
   SHAW EAG, 1974, J ACOUST SOC AM, V56, P1848, DOI 10.1121/1.1903522
   TOOLE FE, 1986, J AUDIO ENG SOC, V34, P227
   TOOLE FE, 1988, J AUDIO ENG SOC, V36, P122
   TOOLE FE, 1986, J AUDIO ENG SOC, V34, P323
   ZWICKER E, 1980, J ACOUST SOC AM, V68, P1523, DOI 10.1121/1.385079
NR 29
TC 123
Z9 163
U1 0
U2 9
PU ACOUSTICAL SOC AMER AMER INST PHYSICS
PI MELVILLE
PA STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA
SN 0001-4966
J9 J ACOUST SOC AM
JI J. Acoust. Soc. Am.
PD JUL
PY 2003
VL 114
IS 1
BP 408
EP 419
DI 10.1121/1.1577552
PG 12
WC Acoustics; Audiology & Speech-Language Pathology
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Acoustics; Audiology & Speech-Language Pathology
GA 699ZF
UT WOS:000184087100039
PM 12880052
DA 2024-01-09
ER

PT J
AU Eadie, TL
   Doyle, PC
AF Eadie, TL
   Doyle, PC
TI Direct magnitude estimation and interval scaling of naturalness and
   severity in tracheoesophageal (TE) speakers
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
DE tracheoesophageal speech; perceptual scaling; alaryngeal speech;
   naturalness; severity
ID PERCEPTUAL EVALUATION; SPEECH NATURALNESS; LARYNGEAL-CANCER; VOICE
   QUALITY
AB The purpose of this study was to determine the Psychophysical character and validity of auditory-perceptual ratings of naturalness and overall severity for tracheoesophageal (TE) speech. This was achieved through use of direct magnitude estimation (DME) and equal-appearing interval (EAI) scaling procedures. I Twenty adult listeners judged speech naturalness and overall severity from connected speech samples produced by 20 adult male TE speakers. A comparison of DME- and EAI-scaled judgments yielded a metathetic continuum for naturalness and a prothetic continuum for overall severity. These data provide support for the use of either DME or EAI scales in auditory-perceptual ratings of naturalness, but they provide support only for DME scales in judging overall severity for TE speech. The present results suggest that the nature of perceptual phenomena (prothetic vs. metathetic) for TE speakers is consistent with findings for the same dimensions produced by normal laryngeal speakers. These data also support a need for further study of perceptual dimensions associated with TE voice and speech in order to avoid the inappropriate and invalid use of EAI scales frequently found in diagnosis, assessment, and evaluation of this clinical population.
C1 Univ Western Ontario, Sch Commun Sci & Disorders, Voice Prod Lab, Elborn Coll, London, ON N6G 1H1, Canada.
C3 Western University (University of Western Ontario)
RP Eadie, TL (corresponding author), Univ Western Ontario, Sch Commun Sci & Disorders, Voice Prod Lab, Elborn Coll, London, ON N6G 1H1, Canada.
OI Eadie, Tanya/0000-0002-7697-1298
CR [Anonymous], 1951, Psychometrika
   BARRY SJ, 1981, J SPEECH HEAR RES, V24, P44, DOI 10.1044/jshr.2401.44
   BLOM ED, 1986, ARCH OTOLARYNGOL, V112, P440
   BLOM ED, 1998, TRACHEOESOPHAGEAL VO
   Doyle P. C., 1994, Foundations of Voice and Speech Rehabilitation Following Laryngeal Cancer
   DOYLE PC, IN PRESS CONT CONSID
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   Finizia C, 1999, ARCH OTOLARYNGOL, V125, P157, DOI 10.1001/archotol.125.2.157
   GERRATT BR, 1991, DYSARTHRIA AND APRAXIA OF SPEECH, P77
   GESCHEIDER GA, 1988, ANNU REV PSYCHOL, V39, P169, DOI 10.1146/annurev.ps.39.020188.001125
   Hillman RE, 1998, ANN OTO RHINOL LARYN, V107, P2
   KEITH RL, 1994, LARYNGECTOMEE REHAB
   KEITH RL, 1986, LARYNGECTOMEE REHAB
   Kent R. D., 1996, AM J SPEECH-LANG PAT, V5, P7, DOI [DOI 10.1044/1058-0360.0503.07, 10.1044/1058-0360.0503.07]
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   MOON JB, 1987, J SPEECH HEAR RES, V30, P387, DOI 10.1044/jshr.3003.387
   PINDZOLA RH, 1988, LARYNGOSCOPE, V98, P394
   PINDZOLA RH, 1989, ANN OTO RHINOL LARYN, V98, P960, DOI 10.1177/000348948909801208
   ROBBINS J, 1984, J SPEECH HEAR DISORD, V49, P202, DOI 10.1044/jshd.4902.202
   SCHIAVETTI N, 1983, J SPEECH HEAR RES, V26, P568, DOI 10.1044/jshr.2604.568
   SCHIAVETTI N, 1981, J SPEECH HEAR RES, V24, P441, DOI 10.1044/jshr.2403.441
   SCHIAVETTI N, 1984, ARTICULATION ASSESSM, P237
   Sewall A, 1999, CONT ISSUES COMM SCI, V26, P168, DOI [10.1044/cicsd_26_F_168., DOI 10.1044/CICSD_26_F_168]
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   SINGER MI, 1980, ANN OTO RHINOL LARYN, V89, P529, DOI 10.1177/000348948008900608
   SNIDECOR JC, 1978, SPEECH REAHB LARYNGE
   Stevens SS., 1975, PSYCHOPHYSICS INTRO
   TARDYMITZELL S, 1985, ARCH OTOLARYNGOL, V111, P212
   TONER MA, 1989, J SPEECH HEAR RES, V32, P78, DOI 10.1044/jshr.3201.78
   TRUDEAU MD, 1990, J SPEECH HEAR DISORD, V55, P244, DOI 10.1044/jshd.5502.244
   van As CJ, 1998, J VOICE, V12, P239, DOI 10.1016/S0892-1997(98)80044-1
   Van Riper C., 1978, SPEECH CORRECTION PR
   Whitehill TL, 2002, J SPEECH LANG HEAR R, V45, P80, DOI 10.1044/1092-4388(2002/006)
   WILLIAMS SE, 1985, ARCH OTOLARYNGOL, V111, P216
   Wuyts FL, 2000, J SPEECH LANG HEAR R, V43, P796, DOI 10.1044/jslhr.4303.796
   Yorkston K. M., 1988, Clinical management of dysarthric speakers
   Zraick RI, 2000, J SPEECH LANG HEAR R, V43, P979, DOI 10.1044/jslhr.4304.979
   [No title captured]
NR 40
TC 32
Z9 41
U1 0
U2 2
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE, ROCKVILLE, MD 20852-3279 USA
SN 1092-4388
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD DEC
PY 2002
VL 45
IS 6
BP 1088
EP 1096
DI 10.1044/1092-4388(2002/087)
PG 9
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA 631KM
UT WOS:000180167000002
PM 12546479
DA 2024-01-09
ER

PT J
AU Alku, P
   Tiitinen, H
   Näätänen, R
AF Alku, P
   Tiitinen, H
   Näätänen, R
TI A method for generating natural-sounding speech stimuli for cognitive
   brain research
SO CLINICAL NEUROPHYSIOLOGY
LA English
DT Article
DE speech production; inverse filtering; speech synthesis; speech
   perception; auditory discrimination; mismatch negativity
ID PHONEME REPRESENTATIONS; MISMATCH NEGATIVITY; RESPONSES
AB Objective: In response to the rapidly increasing interest in using human voice in cognitive brain research, a new method, semisynthetic speech generation (SSG), is presented for generation of speech stimuli.
   Methods: The method synthesizes speech stimuli as a combination of purely artificial processes and processes that originate from the natural human speech production mechanism. SSG first estimates the source of speech, the glottal flow, from a natural utterance using an inverse filtering technique. The glottal flow obtained is then used as an excitation to an artificial digital filter that models the formant structure of speech.
   Results: SSG is superior to commercial voice synthesizers because it yields speech stimuli of a highly natural quality due to the contribution of the man-originating glottal excitation.
   Conclusion: The artificial modelling of the vocal tract enables one to adjust the formant frequencies of the stimuli as desired, thus making SSG suitable for cognitive experiments using speech sounds as stimuli. (C) 1999 Elsevier Science Ireland Ltd. All rights reserved.
C1 Aalto Univ, Acoust Lab, FIN-02015 Helsinki, Finland.
   Univ Helsinki, Dept Psychol, Cognit Brain Res Unit, Helsinki, Finland.
C3 Aalto University; University of Helsinki
RP Alku, P (corresponding author), Aalto Univ, Acoust Lab, POB 3000, FIN-02015 Helsinki, Finland.
EM paavo.alku@hut.fi
RI Alku, Paavo/E-2400-2012
OI Alku, Paavo/0000-0002-8173-9418
CR ALKU P, 1992, SPEECH COMMUN, V11, P109, DOI 10.1016/0167-6393(92)90005-R
   Cheour M, 1998, NAT NEUROSCI, V1, P351, DOI 10.1038/1561
   Fant G., 1971, Acoustic Theory of Speech Production: With Calculations Based on X-ray Studies of Russian Articulations, DOI DOI 10.1515/9783110873429
   Flanagan J. L., 1972, Speech Analysis, Synthesis, and Perception
   GOLD B, 1968, IEEE T ACOUST SPEECH, VAU16, P81, DOI 10.1109/TAU.1968.1161954
   Karjalainen M., 1990, IEEE ASSP Magazine, V7, P21, DOI 10.1109/53.53030
   KRAUS N, 1993, ELECTROEN CLIN NEURO, V88, P123, DOI 10.1016/0168-5597(93)90063-U
   KURIKI S, 1995, EXP BRAIN RES, V104, P144
   Maisch B, 1995, EUR HEART J, V16, P1
   Naatanen R, 1997, NATURE, V385, P432, DOI 10.1038/385432a0
   Naatanen R., 1992, Attention and brain function, DOI DOI 10.4324/9780429487354
   Rabiner L.R., 1978, DIGITAL PROCESSING S
   ROGERS RL, 1990, ELECTROEN CLIN NEURO, V77, P237, DOI 10.1016/0168-5597(90)90043-D
   Sams M, 1990, J Cogn Neurosci, V2, P344, DOI 10.1162/jocn.1990.2.4.344
   SCHROGER E, 1994, ELECTROEN CLIN NEURO, V92, P140, DOI 10.1016/0168-5597(94)90054-X
   Sharma A, 1997, EVOKED POTENTIAL, V104, P540, DOI 10.1016/S0168-5597(97)00050-6
   Shtyrov Y, 1998, NEUROSCI LETT, V251, P141, DOI 10.1016/S0304-3940(98)00529-1
   TIITINEN H, 1994, NATURE, V372, P90, DOI 10.1038/372090a0
   WONG DY, 1979, IEEE T ACOUST SPEECH, V27, P350, DOI 10.1109/TASSP.1979.1163260
   ZATORRE RJ, 1992, SCIENCE, V256, P846, DOI 10.1126/science.1589767
NR 20
TC 113
Z9 115
U1 0
U2 1
PU ELSEVIER IRELAND LTD
PI CLARE
PA ELSEVIER HOUSE, BROOKVALE PLAZA, EAST PARK SHANNON, CO, CLARE, 00000,
   IRELAND
SN 1388-2457
EI 1872-8952
J9 CLIN NEUROPHYSIOL
JI Clin. Neurophysiol.
PD AUG
PY 1999
VL 110
IS 8
BP 1329
EP 1333
DI 10.1016/S1388-2457(99)00088-7
PG 5
WC Clinical Neurology; Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Neurosciences & Neurology
GA 220UN
UT WOS:000081686000002
PM 10454267
DA 2024-01-09
ER

PT J
AU Mackey, LS
   Finn, P
   Ingham, RJ
AF Mackey, LS
   Finn, P
   Ingham, RJ
TI Effect of speech dialect on speech naturalness ratings: A systematic
   replication of Martin, Haroldson, and Triden (1984)
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
DE speech naturalness; stuttering; reliability; multicultural; assessment
ID AUDIOVISUAL JUDGMENTS; LANGUAGE ATTITUDES; SYLLABLE STRUCTURE;
   FOREIGN-LANGUAGE; ENGLISH; SPEAKERS; STUTTERERS; ACCENT; NONSTUTTERERS;
   RELIABILITY
AB This study investigated the effect of speech dialect on listeners' speech naturalness ratings by systematically replicating Martin, Haroldson, and Triden's (1984) study using three groups of speaker samples. Two groups consisted of speakers with General American dialect-one with persons who stutter and the other with persons who do not stutter. The third group also consisted of speakers who do not stutter but who spoke non-General American dialect. The results showed that speech naturalness ratings distinguished among the three speaker groups. The variables that appeared to influence speech naturalness ratings were type of dialect, speech fluency, and speaking rate, though they differed across speaker groups. The findings also suggested that strength of speech dialect may be a scaleable dimension that judges can rare with acceptable levels of reliability Dialect may also be an important factor that needs to be incorporated or controlled within systems designed to train speech naturalness ratings. It may also be an important factor in determining the extent to which stuttering treatment produces natural sounding speech.
C1 UNIV NEW MEXICO,DEPT COMMUN DISORDERS,ALBUQUERQUE,NM 87131.
   CLAREMORE REG HOSP,HOME HEALTH,OK.
   UNIV CALIF SANTA BARBARA,SANTA BARBARA,CA 93106.
C3 University of New Mexico; University of California System; University of
   California Santa Barbara
FU NIDCD NIH HHS [5 R01 DC 00060-05] Funding Source: Medline
CR ANDERSONHSIEH J, 1992, LANG LEARN, V42, P529, DOI 10.1111/j.1467-1770.1992.tb01043.x
   [Anonymous], 1986, Webster's third new international dictionary of the English language unabridged
   BATTLE DE, 1993, COMMUNICATION DISORD, pR15
   Bloodstein O., 1995, HDB STUTTERING
   Bouchard Ryan E., 1982, Journal of Language and Social Psychology, V1, P51, DOI DOI 10.1177/0261927X8200100104
   Bradac J. J., 1984, Journal of Language and Social Psychology, V3, P239, DOI [DOI 10.1177/0261927X8400300401, https://doi.org/10.1177/0261927X8400300401]
   BRENNAN EM, 1981, J PSYCHOLINGUIST RES, V10, P487, DOI 10.1007/BF01076735
   BROWN BL, 1985, LANG COMMUN, V5, P207, DOI 10.1016/0271-5309(85)90011-4
   CARGILE AC, 1994, LANG COMMUN, V14, P211, DOI 10.1016/0271-5309(94)90001-9
   Cooper E. B., 1993, COMMUNICATION DISORD, P189
   CORDES AK, 1994, J SPEECH HEAR RES, V37, P264, DOI 10.1044/jshr.3702.264
   CROWTHER CS, 1992, J ACOUST SOC AM, V92, P711, DOI 10.1121/1.403996
   Dictionary O.E, 1989, Oxford English dictionary
   EDWARDS JR, 1989, LANAGUGE DISADVANTAG
   FAYER JM, 1987, LANG LEARN, V37, P313, DOI 10.1111/j.1467-1770.1987.tb00573.x
   FINN P, 1994, J SPEECH HEAR RES, V37, P326, DOI 10.1044/jshr.3702.326
   FLEGE JE, 1987, J PHONETICS, V15, P47, DOI 10.1016/S0095-4470(19)30537-6
   FLEGE JE, 1984, J ACOUST SOC AM, V76, P708, DOI 10.1121/1.391257
   Flege JE., 1989, STUDIES 2 LANGUAGE A, V11, P35, DOI DOI 10.1017/S0272263100007828
   Gallois C., 1989, AUST J LINGUIST, V9, P149, DOI DOI 10.1080/07268608908599415
   Gardner R. C., 1985, Social psychology and second language learning: The role of attitudes and motivation
   GERRATT BR, 1993, J SPEECH HEAR RES, V36, P14, DOI 10.1044/jshr.3601.14
   GILES H, 1983, LANG COMMUN, V3, P305, DOI 10.1016/0271-5309(83)90006-X
   GILES H, 1995, LANG COMMUN, V15, P107, DOI 10.1016/0271-5309(94)00019-9
   Giles H., 1987, SOCIOLINGUISTICS INT, P585
   GOW ML, 1992, J SPEECH HEAR RES, V35, P495, DOI 10.1044/jshr.3503.495
   INGHAM JM, 1986, STUTTERING MEASUREME
   INGHAM RJ, 1985, J SPEECH HEAR RES, V28, P495, DOI 10.1044/jshr.2804.495
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P217, DOI 10.1044/jshd.5002.217
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P261, DOI 10.1044/jshd.5003.261
   JUFFS A, 1990, IRAL-INT REV APPL LI, V28, P99, DOI 10.1515/iral.1990.28.2.99
   KALIN R, 1978, PSYCHOL REP, V43, P1203, DOI 10.2466/pr0.1978.43.3f.1203
   KALINOWSKI J, 1994, AM J SPEECH-LANG PAT, V3, P61
   Kent R. D., 1996, AM J SPEECH-LANG PAT, V5, P7, DOI [DOI 10.1044/1058-0360.0503.07, 10.1044/1058-0360.0503.07]
   KOSTER CJ, 1993, LANG LEARN, V43, P69, DOI 10.1111/j.1467-1770.1993.tb00173.x
   Leith W. R, 1986, ATYPICAL STUTTERER, P9
   LIPPIGREEN R, 1994, LANG SOC, V23, P163, DOI 10.1017/S0047404500017826
   LUDWIG J, 1982, MOD LANG J, V66, P274, DOI 10.2307/326629
   LUHMAN R, 1990, LANG SOC, V19, P331, DOI 10.1017/S0047404500014548
   Martin R., 1981, MAINTENANCE FLUENCY, P1
   MARTIN RR, 1992, J SPEECH HEAR RES, V35, P521, DOI 10.1044/jshr.3503.521
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   MARTIN RR, 1981, J SPEECH HEAR RES, V46, P59
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   MUNRO MJ, 1993, LANG SPEECH, V36, P39, DOI 10.1177/002383099303600103
   Munro MJ., 1994, Language Testing, V11, P253
   NESDALE AR, 1990, AUST J PSYCHOL, V42, P309, DOI 10.1080/00049539008260128
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P994, DOI 10.1044/jshr.3505.994
   ONSLOW M, 1987, J SPEECH HEAR DISORD, V52, P2, DOI 10.1044/jshd.5201.02
   Onslow M, 1996, J SPEECH HEAR RES, V39, P734, DOI 10.1044/jshr.3904.734
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P274, DOI 10.1044/jshr.3502.274
   PITTAM J, 1987, LANG SPEECH, V30, P99, DOI 10.1177/002383098703000201
   PORT RF, 1983, J PHONETICS, V11, P219, DOI 10.1016/S0095-4470(19)30823-X
   Ryan E.B., 1982, Attitudes Towards Language Variation: Social and Applied Contexts
   RYAN EB, 1975, J PERS SOC PSYCHOL, V31, P855, DOI 10.1037/h0076704
   SCHIAVETTI N, 1994, J SPEECH HEAR RES, V37, P46, DOI 10.1044/jshr.3701.46
   SCHIAVETTI N, 1997, NATURE TREATMENT STU, P398
   SHAMES GH, 1989, J FLUENCY DISORD, V14, P67, DOI 10.1016/0094-730X(89)90025-9
   Siegel S., 1956, NONPARAMETRIC STAT B
   TAYLOR O, 1993, COMMUNICATION DISORD, pR12
   Taylor O. L., 1973, LANGUAGE ATTITUDES C, P174
   Van Riper C., 1982, NATURE STUTTERING
   Watson Jennifer Barber, 1994, Seminars in Speech and Language, V15, P149, DOI 10.1055/s-2008-1064140
   Winer BJ, 1991, STATISTICAL PRINCIPL
   YOUNG MA, 1994, J SPEECH HEAR RES, V37, P522, DOI 10.1044/jshr.3703.522
NR 65
TC 20
Z9 25
U1 1
U2 7
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE, ROCKVILLE, MD 20852-3279
SN 1092-4388
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD APR
PY 1997
VL 40
IS 2
BP 349
EP 360
DI 10.1044/jslhr.4002.349
PG 12
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA WV591
UT WOS:A1997WV59100008
PM 9130203
DA 2024-01-09
ER

PT J
AU BURTON, MW
   BLUMSTEIN, SE
AF BURTON, MW
   BLUMSTEIN, SE
TI LEXICAL EFFECTS ON PHONETIC CATEGORIZATION - THE ROLE OF STIMULUS
   NATURALNESS AND STIMULUS QUALITY
SO JOURNAL OF EXPERIMENTAL PSYCHOLOGY-HUMAN PERCEPTION AND PERFORMANCE
LA English
DT Article
ID PERCEPTION; IDENTIFICATION; SPEECH
AB A series of experiments was conducted to determine whether the effects of lexical status on phonetic categorization were influenced by stimulus naturalness (replicating M. W. Burton, S. R. Baum, & S. E. Blumstein, 1989, who manipulated the intrinsic properties of the stimuli) and by stimulus quality (presenting the stimuli in white noise). The experiments compared continua varying in voice onset time (VOT) only to continua covarying VOT and amplitude of the burst and aspiration noise in no-noise and noise conditions. Results overall showed that the emergence of a lexical effect was influenced by stimulus quality but not by stimulus naturalness. Contrary to previous findings, significant lexical effects failed to emerge in the slower reaction time ranges. These results suggest that stimulus quality contributes to lexical effects on phonetic categorization, whereas stimulus naturalness does not.
C1 BROWN UNIV,DEPT COGNIT & LINGUIST SCI,PROVIDENCE,RI.
C3 Brown University
RP BURTON, MW (corresponding author), PENN STATE UNIV,DEPT PSYCHOL,415 MOORE BLDG,UNIVERSITY PK,PA 16802, USA.
FU NIDCD NIH HHS [DC00142] Funding Source: Medline
CR BURTON MW, 1989, J EXP PSYCHOL HUMAN, V15, P567, DOI 10.1037/0096-1523.15.3.567
   CONNINE CM, 1987, J EXP PSYCHOL HUMAN, V13, P291, DOI 10.1037/0096-1523.13.2.291
   CUTLER A, 1987, COGNITIVE PSYCHOL, V19, P141, DOI 10.1016/0010-0285(87)90010-7
   Cutler A., 1979, SENTENCE PROCESSING, P113
   Elman J., 1986, Parallel distributed processing: Explorations in the microstructure of cognition, VII, P58
   Elman JeffreyL., 1986, INVARIANCE VARIABILI, P360
   FOX RA, 1984, J EXP PSYCHOL HUMAN, V10, P526, DOI 10.1037/0096-1523.10.4.526
   GANONG WF, 1980, J EXP PSYCHOL HUMAN, V6, P110, DOI 10.1037/0096-1523.6.1.110
   MCQUEEN JM, 1991, J EXP PSYCHOL HUMAN, V17, P433, DOI 10.1037/0096-1523.17.2.433
   MCQUEEN JM, 1989, EUROSPEECH 89, V2, P581
   MILLER JL, 1988, J EXP PSYCHOL HUMAN, V14, P369, DOI 10.1037/0096-1523.14.3.369
   PITT MA, 1993, J EXP PSYCHOL HUMAN, V19, P699, DOI 10.1037/0096-1523.19.4.699
NR 12
TC 16
Z9 16
U1 0
U2 0
PU AMER PSYCHOLOGICAL ASSOC
PI WASHINGTON
PA 750 FIRST ST NE, WASHINGTON, DC 20002-4242
SN 0096-1523
J9 J EXP PSYCHOL HUMAN
JI J. Exp. Psychol.-Hum. Percept. Perform.
PD OCT
PY 1995
VL 21
IS 5
BP 1230
EP 1235
DI 10.1037/0096-1523.21.5.1230
PG 6
WC Psychology; Psychology, Experimental
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Psychology
GA RY271
UT WOS:A1995RY27100018
PM 7595247
DA 2024-01-09
ER

PT J
AU YORKSTON, KM
   HAMMEN, VL
   BEUKELMAN, DR
   TRAYNOR, CD
AF YORKSTON, KM
   HAMMEN, VL
   BEUKELMAN, DR
   TRAYNOR, CD
TI THE EFFECT OF RATE CONTROL ON THE INTELLIGIBILITY AND NATURALNESS OF
   DYSARTHRIC SPEECH
SO JOURNAL OF SPEECH AND HEARING DISORDERS
LA English
DT Article
C1 UNIV NEBRASKA,LINCOLN,NE 68588.
C3 University of Nebraska System; University of Nebraska Lincoln
RP YORKSTON, KM (corresponding author), UNIV WASHINGTON,DEPT REHABIL MED,RJ-30,SEATTLE,WA 98195, USA.
FU NINDS NIH HHS [5-RO1-NS-19417-03] Funding Source: Medline
CR Berry, 1983, CLIN DYSARTHRIA, P231
   BERRY WR, 1983, CLIN DYSARTHRIA, P253
   Beukelman DR, 1988, PACER TALLY
   CROW E, 1989, RECENT ADVANCES IN CLINICAL DYSARTHRIA, P99
   Darley FL, 1975, Motor speech disorders, V3rd
   FORREST K, 1989, J ACOUST SOC AM, V85, P2606
   HANSON WR, 1980, J SPEECH HEAR DISORD, V45, P268, DOI 10.1044/jshd.4502.268
   HELM NA, 1979, J SPEECH HEAR DISORD, V44, P350, DOI 10.1044/jshd.4403.350
   HYLAND JD, 1988, J SPEECH HEAR DISORD, V53, P271, DOI 10.1044/jshd.5303.271
   MASSEN B, 1986, J SPEECH HEAR RES, V29, P227
   PARKHURST BG, 1978, J COMMUN DISORD, V11, P249, DOI 10.1016/0021-9924(78)90017-5
   Rosenbeck J. C., 1985, Clinical management of neurogenic communication disorders, V2nd, P97
   Yorkston K. M., 1988, Clinical management of dysarthric speakers
   YORKSTON KM, 1981, J SPEECH HEAR DISORD, V46, P398, DOI 10.1044/jshd.4604.398
   YORKSTON KM, 1989, RECENT ADVANCES IN CLINICAL DYSARTHRIA, P85
   YORKSTON KM, 1988, J COMMUN DISORD, V21, P351, DOI 10.1016/0021-9924(88)90038-X
   YORKSTON KM, 1989, ARCH PHYS MED REHAB, V70, P313
   YROKSTON K, 1984, COMPUTERIZED ASSESSM
NR 18
TC 106
Z9 128
U1 0
U2 21
PU AMER SPEECH-LANG-HEARING ASSN
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE RD, ROCKVILLE, MD 20852-3279
SN 0022-4677
J9 J SPEECH HEAR DISORD
PD AUG
PY 1990
VL 55
IS 3
BP 550
EP 560
DI 10.1044/jshd.5503.550
PG 11
WC Linguistics
WE Social Science Citation Index (SSCI)
SC Linguistics
GA DT067
UT WOS:A1990DT06700024
PM 2381196
DA 2024-01-09
ER

PT J
AU MARTIN, RR
   HAROLDSON, SK
   TRIDEN, KA
AF MARTIN, RR
   HAROLDSON, SK
   TRIDEN, KA
TI STUTTERING AND SPEECH NATURALNESS
SO JOURNAL OF SPEECH AND HEARING DISORDERS
LA English
DT Article
RP MARTIN, RR (corresponding author), UNIV MINNESOTA,DEPT COMMUN DISORDERS,MINNEAPOLIS,MN 55455, USA.
CR GOLDIAMOND I, 1965, RES BEHAVIOR MODIFIC
   INGHAM RJ, 1978, J SPEECH HEAR RES, V21, P63, DOI 10.1044/jshr.2101.63
   JONES RJ, 1969, J APPL BEHAV ANAL, V2, P223, DOI 10.1901/jaba.1969.2-223
   Kirk R.E., 2012, EXPT DESIGN PROCEDUR, P302
   MALLARD AR, 1979, J FLUENCY DISORD, V4, P117, DOI 10.1016/0094-730X(79)90010-X
   PERKINS WH, 1974, J SPEECH HEAR DISORD, V39, P416, DOI 10.1044/jshd.3904.416
   RUNYAN CM, 1979, J FLUENCY DISORD, V4, P29, DOI 10.1016/0094-730X(79)90029-9
   RUNYAN CM, 1978, J FLUENCY DISORDERS, V3, P24
   TINSLEY HEA, 1975, J COUNSELING PSYCHOL, V22, P353
   Winer BJ., 1971, STAT PRINCIPLES EXPT
NR 10
TC 171
Z9 188
U1 1
U2 6
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE, ROCKVILLE, MD 20852-3279
SN 0022-4677
J9 J SPEECH HEAR DISORD
PY 1984
VL 49
IS 1
BP 53
EP 58
DI 10.1044/jshd.4901.53
PG 6
WC Linguistics
WE Social Science Citation Index (SSCI); Science Citation Index Expanded (SCI-EXPANDED)
SC Linguistics
GA SG450
UT WOS:A1984SG45000007
PM 6700202
DA 2024-01-09
ER

PT J
AU Bhaya-Grossman, I
   Chang, EF
AF Bhaya-Grossman, Ilina
   Chang, Edward F.
TI Speech Computations of the Human Superior Temporal Gyrus
SO ANNUAL REVIEW OF PSYCHOLOGY
LA English
DT Review
DE superior temporal gyrus; phonological processing; categorization;
   contextual restoration; temporal landmarks
ID HUMAN AUDITORY-CORTEX; VOICE-ONSET TIME; PERCEPTUAL RESTORATION; WORD
   RECOGNITION; GAMMA ACTIVITY; FINE-STRUCTURE; LANGUAGE; ORGANIZATION;
   PHONEME; PITCH
AB Human speech perception results from neural computations that transform external acoustic speech signals into internal representations of words. The superior temporal gyrus (STG) contains the nonprimary auditory cortex and is a critical locus for phonological processing. Here, we describe how speech sound representation in the STG relies on fundamentally nonlinear and dynamical processes, such as categorization, normalization, contextual restoration, and the extraction of temporal structure. A spatial mosaic of local cortical sites on the STG exhibits complex auditory encoding for distinct acoustic-phonetic and prosodic features. We propose that as a population ensemble, these distributed patterns of neural activity give rise to abstract, higher-order phonemic and syllabic representations that support speech perception. This review presents amulti-scale, recurrentmodel of phonological processing in the STG, highlighting the critical interface between auditory and language systems.
C1 [Bhaya-Grossman, Ilina; Chang, Edward F.] Univ Calif San Francisco, Dept Neurol Surg, San Francisco, CA 94143 USA.
   [Bhaya-Grossman, Ilina] Univ Calif Berkeley, Joint Grad Program Bioengn, Berkeley, CA 94720 USA.
   [Bhaya-Grossman, Ilina] Univ Calif San Francisco, Joint Grad Program Bioengn, San Francisco, CA 94720 USA.
RP Chang, EF (corresponding author), Univ Calif San Francisco, Dept Neurol Surg, San Francisco, CA 94143 USA.
EM edward.chang@ucsf.edu
CR Ahissar E, 2001, P NATL ACAD SCI USA, V98, P13367, DOI 10.1073/pnas.201400998
   Allen EJ, 2017, J NEUROSCI, V37, P1284, DOI 10.1523/JNEUROSCI.2336-16.2016
   Anderson LA, 2011, HEARING RES, V274, P48, DOI 10.1016/j.heares.2010.12.016
   Bartlett EL, 2013, BRAIN LANG, V126, P29, DOI 10.1016/j.bandl.2013.03.003
   Benson RR, 2006, NEUROIMAGE, V31, P342, DOI 10.1016/j.neuroimage.2005.11.029
   Benson RR, 2001, BRAIN LANG, V78, P364, DOI 10.1006/brln.2001.2484
   Binder JR, 2000, CEREB CORTEX, V10, P512, DOI 10.1093/cercor/10.5.512
   Blumstein SE, 2005, J COGNITIVE NEUROSCI, V17, P1353, DOI 10.1162/0898929054985473
   Blumstein SE, 2009, LANG LINGUIST COMPAS, V3, DOI 10.1111/j.1749-818x.2009.00136.x
   Boatman D, 2000, BRAIN, V123, P1634, DOI 10.1093/brain/123.8.1634
   Boatman D, 2004, COGNITION, V92, P47, DOI 10.1016/j.cognition.2003.09.010
   Bonte M, 2014, J NEUROSCI, V34, P4548, DOI 10.1523/JNEUROSCI.4339-13.2014
   Chan AM, 2014, CEREB CORTEX, V24, P2679, DOI 10.1093/cercor/bht127
   Chang EF, 2015, NEURON, V86, P68, DOI 10.1016/j.neuron.2015.03.037
   Chang EF, 2010, NAT NEUROSCI, V13, P1428, DOI 10.1038/nn.2641
   CHISTOVICH LA, 1980, LANG SPEECH, V23, P67, DOI 10.1177/002383098002300107
   Crone NE, 2001, CLIN NEUROPHYSIOL, V112, P565, DOI 10.1016/S1388-2457(00)00545-9
   CUTLER A, 1994, J MEM LANG, V33, P824, DOI 10.1006/jmla.1994.1039
   CUTLER A, 1986, J MEM LANG, V25, P385, DOI 10.1016/0749-596X(86)90033-1
   Dehaene-Lambertz G, 2005, NEUROIMAGE, V24, P21, DOI 10.1016/j.neuroimage.2004.09.039
   Diehl RL, 2004, ANNU REV PSYCHOL, V55, P149, DOI 10.1146/annurev.psych.55.090902.142028
   Douglas RJ, 2007, CURR BIOL, V17, pR496, DOI 10.1016/j.cub.2007.04.024
   Drennan DP, 2019, ENEURO, V6, DOI 10.1523/ENEURO.0082-19.2019
   DRULLMAN R, 1995, J ACOUST SOC AM, V97, P585, DOI 10.1121/1.413112
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Feng GY, 2018, CEREB CORTEX, V28, P3241, DOI 10.1093/cercor/bhx195
   Formisano E, 2008, SCIENCE, V322, P970, DOI 10.1126/science.1164318
   Fox N.P., 2017, Journal of the Acoustical Society of America, V141, P3571
   Fox NP, 2020, ELIFE, V9, DOI 10.7554/eLife.53051
   Frazier L, 2006, TRENDS COGN SCI, V10, P244, DOI 10.1016/j.tics.2006.04.002
   Furl N, 2011, NEUROIMAGE, V54, P2267, DOI 10.1016/j.neuroimage.2010.10.038
   Garofolo J.S., 1993, NIST speech disc 1-1.1. NASA STVRecon technical report
   GESCHWIN.N, 1970, SCIENCE, V170, P940, DOI 10.1126/science.170.3961.940
   Goldinger SD, 2003, J PHONETICS, V31, P305, DOI 10.1016/S0095-4470(03)00030-5
   Halle M., 1968, The sound pattern of English
   Hamilton LS, 2020, bioRxiv, DOI [10.1101/2020.06.08.121624, 10.1101/2020.06.08.121624v1, DOI 10.1101/2020.06.08.121624V1, DOI 10.1101/2020.06.08.121624]
   Hamilton LS, 2018, CURR BIOL, V28, P1860, DOI 10.1016/j.cub.2018.04.033
   HEALY AF, 1976, J VERB LEARN VERB BE, V15, P73, DOI 10.1016/S0022-5371(76)90008-6
   Heil P, 2001, J NEUROSCI, V21, P7404, DOI 10.1523/JNEUROSCI.21-18-07404.2001
   Heilbron M, 2018, NEUROSCIENCE, V389, P54, DOI 10.1016/j.neuroscience.2017.07.061
   Hickok G, 2004, COGNITION, V92, P67, DOI 10.1016/j.cognition.2003.10.011
   Hickok G, 2007, NAT REV NEUROSCI, V8, P393, DOI 10.1038/nrn2113
   Hillis AE, 2017, ANN NEUROL, V81, P759, DOI 10.1002/ana.24941
   Holdgraf CR, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13654
   Holt LL, 2010, ATTEN PERCEPT PSYCHO, V72, P1218, DOI 10.3758/APP.72.5.1218
   Hopkins K, 2009, J ACOUST SOC AM, V125, P442, DOI 10.1121/1.3037233
   Howie J.M., 1976, Acoustical Studies of Mandarin Vowels and Tones
   Hullett PW, 2016, J NEUROSCI, V36, P2014, DOI 10.1523/JNEUROSCI.1779-15.2016
   Humphries C, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00406
   Hutchison ER, 2008, NEUROIMAGE, V40, P342, DOI 10.1016/j.neuroimage.2007.10.064
   Jasmin K, 2019, NAT REV NEUROSCI, V20, P425, DOI 10.1038/s41583-019-0160-2
   Johnson K, 2005, BLACKW HBK LINGUIST, P363, DOI 10.1002/9780470757024.ch15
   Jordan M.I., 1997, Adv. Psychol., VVolume 121, P471, DOI [10.1016/s0166-4115(97)80111-2, DOI 10.1016/S0166-4115(97)80111-2, 10.1016/S0166-4115(97)80111-2]
   Khoshkhoo S, 2018, BRAIN LANG, V187, P83, DOI 10.1016/j.bandl.2018.01.007
   KUHL PK, 1992, SCIENCE, V255, P606, DOI 10.1126/science.1736364
   KUHL PK, 1991, PERCEPT PSYCHOPHYS, V50, P93, DOI 10.3758/BF03212211
   Ladd RobertD., 2008, INTONATIONAL PHONOLO
   LADEFOGED P, 1957, J ACOUST SOC AM, V29, P98, DOI 10.1121/1.1908694
   Ladefoged Peter, 2014, COURSE PHONETICS
   Lakertz Y, 2021, NEUROIMAGE, V226, DOI 10.1016/j.neuroimage.2020.117499
   Leech R, 2009, J NEUROSCI, V29, P5234, DOI 10.1523/JNEUROSCI.5758-08.2009
   Leonard MK, 2019, BRAIN LANG, V193, P58, DOI 10.1016/j.bandl.2016.06.001
   Leonard MK, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13619
   Leonard MK, 2015, J NEUROSCI, V35, P7203, DOI 10.1523/JNEUROSCI.4100-14.2015
   LESOGOR L V, 1978, Fiziologiya Cheloveka, V4, P213
   Leszczynski M, 2020, SCI ADV, V6, DOI 10.1126/sciadv.abb0977
   Li YN, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-21430-x
   Liang BS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00495
   LIBERMAN AM, 1957, J EXP PSYCHOL, V54, P358, DOI 10.1037/h0044417
   LIBERMAN AM, 1967, PSYCHOL REV, V74, P431, DOI 10.1037/h0020279
   LIBERMAN AM, 1985, COGNITION, V21, P1, DOI 10.1016/0010-0277(85)90021-6
   LIBERMAN AM, 1958, LANG SPEECH, V1, P153, DOI 10.1177/002383095800100301
   LIBERMAN AM, 1952, AM J PSYCHOL, V65, P497, DOI 10.2307/1418032
   Liebenthal E, 2005, CEREB CORTEX, V15, P1621, DOI 10.1093/cercor/bhi040
   Liebenthal E, 2010, CEREB CORTEX, V20, P2958, DOI 10.1093/cercor/bhq045
   Liégeois-Chauvel C, 2004, CEREB CORTEX, V14, P731, DOI 10.1093/cercor/bhh033
   Lindblom B, 2012, J PHONETICS, V40, P1, DOI 10.1016/j.wocn.2011.09.005
   LISKER L, 1964, WORD, V20, P384, DOI 10.1080/00437956.1964.11659830
   Liu R, 2011, J COGNITIVE NEUROSCI, V23, P683, DOI 10.1162/jocn.2009.21392
   Lorenzi C, 2006, P NATL ACAD SCI USA, V103, P18866, DOI 10.1073/pnas.0607364103
   Lublinskaja V, 2006, DYNAMICS SPEECH PROD, P87
   MANN VA, 1980, PERCEPT PSYCHOPHYS, V28, P407, DOI 10.3758/BF03204884
   MANN VA, 1980, PERCEPT PSYCHOPHYS, V28, P213, DOI 10.3758/BF03204377
   MARSLENWILSON WD, 1978, COGNITIVE PSYCHOL, V10, P29, DOI 10.1016/0010-0285(78)90018-X
   MASSARO DW, 1974, J EXP PSYCHOL, V102, P199, DOI 10.1037/h0035854
   Matsumoto R, 2011, NEUROPSYCHOLOGIA, V49, P1350, DOI 10.1016/j.neuropsychologia.2011.01.023
   MCCLELLAND JL, 1986, COGNITIVE PSYCHOL, V18, P1, DOI 10.1016/0010-0285(86)90015-0
   Mesgarani N, 2014, SCIENCE, V343, P1006, DOI 10.1126/science.1245994
   Moon Il Joon, 2014, Korean J Audiol, V18, P1, DOI 10.7874/kja.2014.18.1.1
   Möttönen R, 2006, NEUROIMAGE, V30, P563, DOI 10.1016/j.neuroimage.2005.10.002
   Nittrouer S, 2000, PERCEPT PSYCHOPHYS, V62, P266, DOI 10.3758/BF03205548
   Norris D, 2000, BEHAV BRAIN SCI, V23, P299, DOI 10.1017/S0140525X00003241
   Nourski KV, 2019, HEARING RES, V371, P53, DOI 10.1016/j.heares.2018.11.009
   Nourski KV, 2009, J NEUROSCI, V29, P15564, DOI 10.1523/JNEUROSCI.3065-09.2009
   Oganian Y, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aay6279
   Ohala J., 1975, AUDITORY ANAL PERCEP, P431, DOI DOI 10.1016/B978-0-12-248550-3.50032-5
   Overath T, 2015, NAT NEUROSCI, V18, P903, DOI 10.1038/nn.4021
   Ozker M, 2017, J COGNITIVE NEUROSCI, V29, P1044, DOI 10.1162/jocn_a_01110
   Parvizi J, 2018, NAT NEUROSCI, V21, P474, DOI 10.1038/s41593-018-0108-2
   Peelle JE, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00320
   PISONI DB, 1973, PERCEPT PSYCHOPHYS, V13, P253, DOI 10.3758/BF03214136
   Rauschecker JP, 2009, NAT NEUROSCI, V12, P718, DOI 10.1038/nn.2331
   Ray S, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1000610
   REMEZ RE, 1981, SCIENCE, V212, P947, DOI 10.1126/science.7233191
   Roux FE, 2015, CORTEX, V71, P398, DOI 10.1016/j.cortex.2015.07.001
   SAMUEL AG, 1987, J MEM LANG, V26, P36, DOI 10.1016/0749-596X(87)90061-1
   Santoro R, 2017, P NATL ACAD SCI USA, V114, P4799, DOI 10.1073/pnas.1617622114
   SAVIN HB, 1970, J VERB LEARN VERB BE, V9, P295, DOI 10.1016/S0022-5371(70)80064-0
   Schönwiesner M, 2009, P NATL ACAD SCI USA, V106, P14611, DOI 10.1073/pnas.0907682106
   SENDLMEIER WF, 1995, PHONETICA, V52, P131, DOI 10.1159/000262128
   SHANNON RV, 1995, SCIENCE, V270, P303, DOI 10.1126/science.270.5234.303
   Sjerps MJ, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-10365-z
   Steinschneider M, 1999, J NEUROPHYSIOL, V82, P2346, DOI 10.1152/jn.1999.82.5.2346
   Steinschneider M, 2008, CEREB CORTEX, V18, P610, DOI 10.1093/cercor/bhm094
   Steinschneider M, 2011, CEREB CORTEX, V21, P2332, DOI 10.1093/cercor/bhr014
   Tang C, 2017, SCIENCE, V357, P797, DOI 10.1126/science.aam8577
   Towle VL, 2008, BRAIN, V131, P2013, DOI 10.1093/brain/awn147
   Tuller B, 2010, STUD COMPUT INTELL, V328, P135
   WARREN RM, 1974, PERCEPT PSYCHOPHYS, V16, P150, DOI 10.3758/BF03203268
   WARREN RM, 1970, SCIENCE, V167, P392, DOI 10.1126/science.167.3917.392
   Wernicke CD., 1874, Der aphasische symptomencomplex
   Yi HG, 2019, NEURON, V102, P1096, DOI 10.1016/j.neuron.2019.04.023
   ZATORRE RJ, 1992, SCIENCE, V256, P846, DOI 10.1126/science.1589767
   Zatorre RJ, 2008, PHILOS T R SOC B, V363, P1087, DOI 10.1098/rstb.2007.2161
   Zatorre RJ, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00544
   Zeng FG, 2005, P NATL ACAD SCI USA, V102, P2293, DOI 10.1073/pnas.0406460102
NR 126
TC 30
Z9 32
PU ANNUAL REVIEWS
PI PALO ALTO
PA 4139 EL CAMINO WAY, PO BOX 10139, PALO ALTO, CA 94303-0139 USA
J9 ANNU REV PSYCHOL
JI Annu. Rev. Psychol.
PY 2022
VL 73
BP 79
EP 102
DI 10.1146/annurev-psych-022321-035256
PG 24
UT WOS:000788647200006
HC Y
HP N
DA 2024-04-26
ER

PT J
AU Yi, HG
   Leonard, MK
   Chang, EF
AF Yi, Han Gyol
   Leonard, Matthew K.
   Chang, Edward F.
TI The Encoding of Speech Sounds in the Superior Temporal Gyrus
SO NEURON
LA English
DT Review
ID HUMAN AUDITORY-CORTEX; VOICE ONSET TIME; PERCEPTUAL RESTORATION;
   LANGUAGE COMPREHENSION; CORTICAL ORGANIZATION; ACOUSTIC LANDMARKS;
   NEURAL PREDICTION; SPOKEN LANGUAGE; DEGRADED SPEECH; GAMMA ACTIVITY
AB The human superior temporal gyrus (STG) is critical for extracting meaningful linguistic features from speech input. Local neural populations are tuned to acoustic-phonetic features of all consonants and vowels and to dynamic cues for intonational pitch. These populations are embedded throughout broader functional zones that are sensitive to amplitude-based temporal cues. Beyond speech features, STG representations are strongly modulated by learned knowledge and perceptual goals. Currently, a major challenge is to understand how these features are integrated across space and time in the brain during natural speech comprehension. We present a theory that temporally recurrent connections within STG generate context-dependent phonological representations, spanning longer temporal sequences relevant for coherent percepts of syllables, words, and phrases.
C1 [Yi, Han Gyol; Leonard, Matthew K.; Chang, Edward F.] Univ Calif San Francisco, Dept Neurol Surg, 675 Nelson Rising Lane, San Francisco, CA 94158 USA.
RP Chang, EF (corresponding author), Univ Calif San Francisco, Dept Neurol Surg, 675 Nelson Rising Lane, San Francisco, CA 94158 USA.
EM edward.chang@ucsf.edu
CR Ahissar E, 2001, P NATL ACAD SCI USA, V98, P13367, DOI 10.1073/pnas.201400998
   [Anonymous], BIORXIV
   [Anonymous], 8604 ICS
   [Anonymous], BRAIN LANG
   Arsenault JS, 2015, J NEUROSCI, V35, P634, DOI 10.1523/JNEUROSCI.2454-14.2015
   Atencio CA, 2016, NEUROSCIENCE, V316, P402, DOI 10.1016/j.neuroscience.2015.12.057
   BADDELEY A, 1992, SCIENCE, V255, P556, DOI 10.1126/science.1736359
   Barbour DL, 2008, J NEUROSCI, V28, P11174, DOI 10.1523/JNEUROSCI.2093-08.2008
   Bates E, 2003, NAT NEUROSCI, V6, P448, DOI 10.1038/nn1050
   Behroozmand R, 2016, J NEUROSCI, V36, P2302, DOI 10.1523/JNEUROSCI.3305-14.2016
   Behroozmand R, 2011, BMC NEUROSCI, V12, DOI 10.1186/1471-2202-12-54
   Berezutskaya J, 2017, J NEUROSCI, V37, P7906, DOI 10.1523/JNEUROSCI.0238-17.2017
   Binder JR, 2000, CEREB CORTEX, V10, P512, DOI 10.1093/cercor/10.5.512
   Bitterman Y, 2008, NATURE, V451, P197, DOI 10.1038/nature06476
   Bizley JK, 2009, J NEUROSCI, V29, P2064, DOI 10.1523/JNEUROSCI.4755-08.2009
   Blank H, 2018, J NEUROSCI, V38, P6076, DOI 10.1523/JNEUROSCI.3258-17.2018
   Blank H, 2016, PLOS BIOL, V14, DOI 10.1371/journal.pbio.1002577
   Blevins Juliette., 1995, HDB PHONOLOGICAL THE, P206, DOI DOI 10.1111/B.9780631201267.1996.00008.X
   BLUMSTEIN SE, 1977, NEUROPSYCHOLOGIA, V15, P19, DOI 10.1016/0028-3932(77)90111-7
   BLUMSTEIN SE, 1981, COGNITION, V10, P25, DOI 10.1016/0010-0277(81)90021-4
   Boatman D, 2004, COGNITION, V92, P47, DOI 10.1016/j.cognition.2003.09.010
   BOATMAN D, 1995, BRAIN LANG, V51, P269, DOI 10.1006/brln.1995.1061
   Brent MR, 1996, COGNITION, V61, P93, DOI 10.1016/S0010-0277(96)00719-6
   Brewer AA, 2016, ANNU REV NEUROSCI, V39, P385, DOI 10.1146/annurev-neuro-070815-014045
   Burgess N, 1999, PSYCHOL REV, V106, P551, DOI 10.1037/0033-295X.106.3.551
   Byrd D, 1996, J PHONETICS, V24, P209, DOI 10.1006/jpho.1996.0012
   Chan AM, 2014, CEREB CORTEX, V24, P2679, DOI 10.1093/cercor/bht127
   Chang EF, 2013, P NATL ACAD SCI USA, V110, P2653, DOI 10.1073/pnas.1216827110
   Chang EF, 2011, J COGNITIVE NEUROSCI, V23, P1437, DOI 10.1162/jocn.2010.21466
   Chang EF, 2010, NAT NEUROSCI, V13, P1428, DOI 10.1038/nn.2641
   CHISTOVICH LA, 1979, HEARING RES, V1, P185, DOI 10.1016/0378-5955(79)90012-1
   Chomsky Noam., 1968, The sound pattern of English
   Christiansen MH, 2016, BEHAV BRAIN SCI, V39, DOI 10.1017/S0140525X1500031X
   Cibelli ES, 2015, BRAIN LANG, V147, P66, DOI 10.1016/j.bandl.2015.05.005
   Clements G. N., 1985, Phonology Yearbook, V2, P225, DOI [10.1017/S0952675700000440., DOI 10.1017/S0952675700000440]
   Cogan GB, 2017, NAT NEUROSCI, V20, P279, DOI 10.1038/nn.4459
   Cope TE, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01958-7
   Corina DP, 2010, BRAIN LANG, V115, P101, DOI 10.1016/j.bandl.2010.04.001
   CREUTZFELDT O, 1989, EXP BRAIN RES, V77, P451, DOI 10.1007/BF00249600
   Crone NE, 2001, CLIN NEUROPHYSIOL, V112, P565, DOI 10.1016/S1388-2457(00)00545-9
   Cutler A, 1997, LANG SPEECH, V40, P141, DOI 10.1177/002383099704000203
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Davis MH, 2005, J EXP PSYCHOL GEN, V134, P222, DOI 10.1037/0096-3445.134.2.222
   de Courtenay JanBaudouin., 1972, A Baudouin de Courtenay anthology: the beginnings of structural linguistics, P144
   de Heer WA, 2017, J NEUROSCI, V37, P6539, DOI 10.1523/JNEUROSCI.3267-16.2017
   de Saussure Ferdinand., 1879, MEMOIRE SYSTEME PRIM
   Dehaene S, 2015, NEURON, V88, P2, DOI 10.1016/j.neuron.2015.09.019
   DELGUTTE B, 1984, J ACOUST SOC AM, V75, P866, DOI 10.1121/1.390596
   DeWitt I, 2012, P NATL ACAD SCI USA, V109, pE505, DOI 10.1073/pnas.1113427109
   Di Liberto GM, 2015, CURR BIOL, V25, P2457, DOI 10.1016/j.cub.2015.08.030
   Diehl RL, 2004, ANNU REV PSYCHOL, V55, P149, DOI 10.1146/annurev.psych.55.090902.142028
   Ding N, 2014, NEUROIMAGE, V88, P41, DOI 10.1016/j.neuroimage.2013.10.054
   Ding N, 2012, P NATL ACAD SCI USA, V109, P11854, DOI 10.1073/pnas.1205381109
   Doelling KB, 2014, NEUROIMAGE, V85, P761, DOI 10.1016/j.neuroimage.2013.06.035
   Douglas RJ, 2007, CURR BIOL, V17, pR496, DOI 10.1016/j.cub.2007.04.024
   DRULLMAN R, 1994, J ACOUST SOC AM, V95, P2670, DOI 10.1121/1.409836
   DRULLMAN R, 1994, J ACOUST SOC AM, V95, P1053, DOI 10.1121/1.408467
   Elliott TM, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000302
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Engel AK, 2005, NAT REV NEUROSCI, V6, P35, DOI 10.1038/nrn1585
   Escabí MA, 2003, J NEUROSCI, V23, P11489
   Evans S, 2015, CEREB CORTEX, V25, P4772, DOI 10.1093/cercor/bhv136
   Fishbach A, 2001, J NEUROPHYSIOL, V85, P2303, DOI 10.1152/jn.2001.85.6.2303
   Formisano E, 2008, SCIENCE, V322, P970, DOI 10.1126/science.1164318
   Francis NA, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-34739-3
   Friston KJ, 2005, PHILOS T R SOC B, V360, P815, DOI 10.1098/rstb.2005.1622
   Furl N, 2011, NEUROIMAGE, V54, P2267, DOI 10.1016/j.neuroimage.2010.10.038
   GESCHWIN.N, 1970, SCIENCE, V170, P940, DOI 10.1126/science.170.3961.940
   Giraud AL, 2012, NAT NEUROSCI, V15, P511, DOI 10.1038/nn.3063
   Golumbic EMZ, 2013, NEURON, V77, P980, DOI 10.1016/j.neuron.2012.12.037
   Griffiths TD, 2010, CURR BIOL, V20, P1128, DOI 10.1016/j.cub.2010.04.044
   Gross J, 2013, PLOS BIOL, V11, DOI 10.1371/journal.pbio.1001752
   Grossberg S, 2011, J ACOUST SOC AM, V130, P440, DOI 10.1121/1.3589258
   Guo W, 2012, J NEUROSCI, V32, P9159, DOI 10.1523/JNEUROSCI.0065-12.2012
   Gwilliams L, 2018, J NEUROSCI, V38, P7585, DOI 10.1523/JNEUROSCI.0065-18.2018
   Hackett TA, 2001, J COMP NEUROL, V441, P197, DOI 10.1002/cne.1407
   Hamilton LS, 2018, CURR BIOL, V28, P1860, DOI 10.1016/j.cub.2018.04.033
   Heil P, 1997, J NEUROPHYSIOL, V77, P2616, DOI 10.1152/jn.1997.77.5.2616
   Heil P, 2004, CURR OPIN NEUROBIOL, V14, P461, DOI 10.1016/j.conb.2004.07.002
   HERMES DJ, 1990, J ACOUST SOC AM, V87, P866, DOI 10.1121/1.398896
   Hickok G, 2007, NAT REV NEUROSCI, V8, P393, DOI 10.1038/nrn2113
   HILLENBRAND J, 1995, J ACOUST SOC AM, V97, P3099, DOI 10.1121/1.411872
   Holdgraf CR, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13654
   Howard MA, 2000, J COMP NEUROL, V416, P79, DOI 10.1002/(SICI)1096-9861(20000103)416:1<79::AID-CNE6>3.0.CO;2-2
   Hullett PW, 2016, J NEUROSCI, V36, P2014, DOI 10.1523/JNEUROSCI.1779-15.2016
   Jakobson Roman, 1951, Preliminaries to speech analysis: The distinctive features and their correlates
   Kaas JH, 2000, P NATL ACAD SCI USA, V97, P11793, DOI 10.1073/pnas.97.22.11793
   Keyser S.J., 1994, PHONOLOGY, V11, P207
   Khoshkhoo S, 2018, BRAIN LANG, V187, P83, DOI 10.1016/j.bandl.2018.01.007
   Kiebel SJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000464
   King AJ, 2009, NAT NEUROSCI, V12, P698, DOI 10.1038/nn.2308
   KLATT DH, 1975, J SPEECH HEAR RES, V18, P686, DOI 10.1044/jshr.1804.686
   Kubanek J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0053398
   Lahiri A, 2010, J PHONETICS, V38, P44, DOI 10.1016/j.wocn.2010.01.002
   Larkum M, 2013, TRENDS NEUROSCI, V36, P141, DOI 10.1016/j.tins.2012.11.006
   Lee YS, 2012, J NEUROSCI, V32, P3942, DOI 10.1523/JNEUROSCI.3814-11.2012
   Leonard MK, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13619
   Leonard MK, 2015, J NEUROSCI, V35, P7203, DOI 10.1523/JNEUROSCI.4100-14.2015
   Leonard MK, 2014, TRENDS COGN SCI, V18, P472, DOI 10.1016/j.tics.2014.05.001
   Li LY, 2014, J NEUROSCI, V34, P13670, DOI 10.1523/JNEUROSCI.1516-14.2014
   Liégeois-Chauvel C, 2004, CEREB CORTEX, V14, P731, DOI 10.1093/cercor/bhh033
   LISKER L, 1986, LANG SPEECH, V29, P3, DOI 10.1177/002383098602900102
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Marr D, 1982, VISION COMPUTATIONAL
   MCNEILL D, 1973, J VERB LEARN VERB BE, V12, P419, DOI 10.1016/S0022-5371(73)80020-9
   McQueen JM, 1998, J MEM LANG, V39, P21, DOI 10.1006/jmla.1998.2568
   Mesgarani N, 2008, J ACOUST SOC AM, V123, P899, DOI 10.1121/1.2816572
   Mesgarani N, 2014, SCIENCE, V343, P1006, DOI 10.1126/science.1245994
   Mesgarani N, 2012, NATURE, V485, P233, DOI 10.1038/nature11020
   Mesulam MM, 2015, BRAIN, V138, P2423, DOI 10.1093/brain/awv154
   MILLER GA, 1955, J ACOUST SOC AM, V27, P338, DOI 10.1121/1.1907526
   MITANI A, 1985, J COMP NEUROL, V235, P430, DOI 10.1002/cne.902350403
   Moerel M, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-41965-w
   Moerel M, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00225
   Nourski KV, 2019, HEARING RES, V371, P53, DOI 10.1016/j.heares.2018.11.009
   Nourski KV, 2017, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00691
   Nourski KV, 2015, INT J PSYCHOPHYSIOL, V95, P191, DOI 10.1016/j.ijpsycho.2014.03.006
   Nourski KV, 2014, CEREB CORTEX, V24, P340, DOI 10.1093/cercor/bhs314
   Nourski KV, 2009, J NEUROSCI, V29, P15564, DOI 10.1523/JNEUROSCI.3065-09.2009
   O'Connell MN, 2014, J NEUROSCI, V34, P16496, DOI 10.1523/JNEUROSCI.2055-14.2014
   Obleser J, 2007, J NEUROSCI, V27, P2283, DOI 10.1523/JNEUROSCI.4663-06.2007
   Obleser J, 2010, CEREB CORTEX, V20, P633, DOI 10.1093/cercor/bhp128
   Overath T, 2012, J NEUROPHYSIOL, V107, P2042, DOI 10.1152/jn.00308.2011
   Ozker M, 2018, FRONT HUM NEUROSCI, V12, DOI 10.3389/fnhum.2018.00141
   Ozker M, 2017, J COGNITIVE NEUROSCI, V29, P1044, DOI 10.1162/jocn_a_01110
   Park H, 2015, CURR BIOL, V25, P1649, DOI 10.1016/j.cub.2015.04.049
   Paton JJ, 2018, NEURON, V98, P687, DOI 10.1016/j.neuron.2018.03.045
   Peelle JE, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00320
   PETERSON GE, 1952, J ACOUST SOC AM, V24, P175, DOI 10.1121/1.1906875
   Petkov CI, 2006, PLOS BIOL, V4, P1213, DOI 10.1371/journal.pbio.0040215
   Phillips WA, 2015, NEUROSCI BIOBEHAV R, V52, P1, DOI 10.1016/j.neubiorev.2015.02.010
   Phillips WA, 1997, BEHAV BRAIN SCI, V20, P657, DOI 10.1017/S0140525X9700160X
   Poeppel D, 2008, PHILOS T R SOC B, V363, P1071, DOI 10.1098/rstb.2007.2160
   Price CJ, 2012, NEUROIMAGE, V62, P816, DOI 10.1016/j.neuroimage.2012.04.062
   Rhone AE, 2016, LANG COGN NEUROSCI, V31, P284, DOI 10.1080/23273798.2015.1101145
   Robson H, 2012, NEUROPSYCHOLOGIA, V50, P276, DOI 10.1016/j.neuropsychologia.2011.11.022
   ROSEN S, 1992, PHILOS T ROY SOC B, V336, P367, DOI 10.1098/rstb.1992.0070
   ROSS AD, 1981, MASS AGR EXP ST RE B, P1
   Roux FE, 2015, CORTEX, V71, P398, DOI 10.1016/j.cortex.2015.07.001
   Saffran JR, 1996, J MEM LANG, V35, P606, DOI 10.1006/jmla.1996.0032
   Sakata S, 2009, NEURON, V64, P404, DOI 10.1016/j.neuron.2009.09.020
   SAMUEL AG, 1987, J MEM LANG, V26, P36, DOI 10.1016/0749-596X(87)90061-1
   Santoro R, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003412
   Sapir E, 1925, LANGUAGE, V1, P37, DOI 10.2307/409004
   Schönwiesner M, 2009, P NATL ACAD SCI USA, V106, P14611, DOI 10.1073/pnas.0907682106
   Scott SK, 2000, BRAIN, V123, P2400, DOI 10.1093/brain/123.12.2400
   See JZ, 2018, ELIFE, V7, DOI 10.7554/eLife.35587
   SHAMMA SA, 1985, J ACOUST SOC AM, V78, P1612, DOI 10.1121/1.392799
   SHANNON RV, 1995, SCIENCE, V270, P303, DOI 10.1126/science.270.5234.303
   Sharpee Tatyana O, 2016, Front Synaptic Neurosci, V8, P26, DOI 10.3389/fnsyn.2016.00026
   ShattuckHufnagel S, 1996, J PSYCHOLINGUIST RES, V25, P193, DOI 10.1007/BF01708572
   Sohoglu E, 2012, J NEUROSCI, V32, P8443, DOI 10.1523/JNEUROSCI.5069-11.2012
   Steinschneider M, 1999, J NEUROPHYSIOL, V82, P2346, DOI 10.1152/jn.1999.82.5.2346
   Steinschneider M, 1998, J ACOUST SOC AM, V104, P2935, DOI 10.1121/1.423877
   Steinschneider M, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00240
   Steinschneider M, 2013, HEARING RES, V305, P57, DOI 10.1016/j.heares.2013.05.013
   Steinschneider M, 2011, CEREB CORTEX, V21, P2332, DOI 10.1093/cercor/bhr014
   Stevens KN, 2002, J ACOUST SOC AM, V111, P1872, DOI 10.1121/1.1458026
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Tang C, 2017, SCIENCE, V357, P797, DOI 10.1126/science.aam8577
   Towle VL, 2008, BRAIN, V131, P2013, DOI 10.1093/brain/awn147
   Versnel H, 1998, J ACOUST SOC AM, V103, P2502, DOI 10.1121/1.422771
   Walker KMM, 2011, J NEUROSCI, V31, P14565, DOI 10.1523/JNEUROSCI.2074-11.2011
   Wang J, 2018, NAT NEUROSCI, V21, P102, DOI 10.1038/s41593-017-0028-6
   WARREN RM, 1970, SCIENCE, V167, P392, DOI 10.1126/science.167.3917.392
   Wernicke C., 1881, Lehrbuch der gehirnkrankheiten, V2
   Wernicke CD., 1874, Der aphasische symptomencomplex
   Winkowski DE, 2013, J NEUROSCI, V33, P1498, DOI 10.1523/JNEUROSCI.3101-12.2013
   Wöstmann M, 2017, LANG COGN NEUROSCI, V32, P855, DOI 10.1080/23273798.2016.1262051
   Wu PY, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31292-x
   Xing DJ, 2012, P NATL ACAD SCI USA, V109, P13871, DOI 10.1073/pnas.1201478109
   Yaron A, 2012, NEURON, V76, P603, DOI 10.1016/j.neuron.2012.08.025
   Yildiz IB, 2016, J NEUROSCI, V36, P12338, DOI 10.1523/JNEUROSCI.4648-15.2016
   Yildiz IB, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003219
   Zec Draga, 1995, Phonology, V12, P85, DOI DOI 10.1017/S0952675700002396
NR 175
TC 154
Z9 168
PU CELL PRESS
PI CAMBRIDGE
PA 50 HAMPSHIRE ST, FLOOR 5, CAMBRIDGE, MA 02139 USA
J9 NEURON
JI Neuron
PD JUN 19
PY 2019
VL 102
IS 6
BP 1096
EP 1110
DI 10.1016/j.neuron.2019.04.023
PG 15
UT WOS:000472017600004
HC Y
HP N
DA 2024-04-26
ER

PT S
AU Hall, JA
   Horgan, TG
   Murphy, NA
AF Hall, Judith A.
   Horgan, Terrence G.
   Murphy, Nora A.
BE Fiske, ST
TI Nonverbal Communication
SO ANNUAL REVIEW OF PSYCHOLOGY, VOL 70
SE Annual Review of Psychology
LA English
DT Review; Book Chapter
DE nonverbal communication; nonverbal behavior; encoding; decoding;
   interpersonal accuracy
ID AUTISM SPECTRUM DISORDER; INFANT-DIRECTED SPEECH; PUPIL SIZE CHANGES;
   LENS MODEL; SOCIAL-RELATIONS; THIN SLICES; INTERPERSONAL SENSITIVITY;
   PITCH CHARACTERISTICS; EMOTION RECOGNITION; SEXUAL ORIENTATION
AB The field of nonverbal communication (NVC) has a long history involving many cue modalities, including face, voice, body, touch, and interpersonal space; different levels of analysis, including normative, group, and individual differences; and many substantive themes that cross from psychology into other disciplines. In this review, we focus on NVC as it pertains to individuals and social interaction. We concentrate specifically on (a) the meanings and correlates of cues that are enacted (sent) by encoders and (b) the perception of nonverbal cues and the accuracy of such perception. Frameworks are presented for conceptualizing and understanding the process of sending and receiving nonverbal cues. Measurement issues are discussed, and theoretical issues and new developments are covered briefly. Although our review is primarily oriented within social and personality psychology, the interdisciplinary nature of NVC is evident in the growing body of research on NVC across many areas of scientific inquiry.
C1 [Hall, Judith A.] Northeastern Univ, Dept Psychol, Boston, MA 02115 USA.
   [Horgan, Terrence G.] Univ Michigan, Dept Psychol, Flint, MI 48502 USA.
   [Murphy, Nora A.] Loyola Marymount Univ, Dept Psychol, Los Angeles, CA 90045 USA.
RP Hall, JA (corresponding author), Northeastern Univ, Dept Psychol, Boston, MA 02115 USA.
EM j.hall@northeastern.edu; thorgan@umflint.edu; nora.murphy@lmu.edu
CR ABEL M., 2002, An Empirical Reflection on the Smile
   ADRIEN JL, 1993, J AM ACAD CHILD PSY, V32, P617, DOI 10.1097/00004583-199305000-00019
   Agnew C.R., 2010, THEN MIRACLE OCCURS
   Ambady N, 2010, PSYCHOL INQ, V21, P271, DOI 10.1080/1047840X.2010.524882
   [Anonymous], IEEE T CIRCUITS SYST
   Apicella CL, 2009, P ROY SOC B-BIOL SCI, V276, P1077, DOI 10.1098/rspb.2008.1542
   Aubanel V, 2010, SPEECH COMMUN, V52, P577, DOI 10.1016/j.specom.2010.02.008
   Babiloni F, 2014, NEUROSCI BIOBEHAV R, V44, P76, DOI 10.1016/j.neubiorev.2012.07.006
   Back M. D., 2016, The Social Psychology of Perceiving Others Accurately, P98, DOI 10.1017/CBO9781316181959.005
   Back MD, 2011, EUR J PERSONALITY, V25, P225, DOI 10.1002/per.790
   Back MD, 2010, SOC PERSONAL PSYCHOL, V4, P855, DOI 10.1111/j.1751-9004.2010.00303.x
   Baeck H, 2011, J VOICE, V25, pE211, DOI 10.1016/j.jvoice.2010.10.019
   Bänziger T, 2014, J NONVERBAL BEHAV, V38, P31, DOI 10.1007/s10919-013-0165-x
   Baron-Cohen S, 2001, J CHILD PSYCHOL PSYC, V42, P241, DOI 10.1111/1469-7610.00715
   Bartlett M, 2011, DYNAMIC FACES INSIGH, P211
   Benson V, 2016, AUTISM RES, V9, P879, DOI 10.1002/aur.1580
   Bernieri FJ, 1996, J PERS SOC PSYCHOL, V71, P110, DOI 10.1037/0022-3514.71.1.110
   Biesanz JC, 2010, MULTIVAR BEHAV RES, V45, P661, DOI 10.1080/00273171.2010.498292
   Biesanz JC, 2010, PSYCHOL SCI, V21, P589, DOI 10.1177/0956797610364121
   Blake R, 2007, ANNU REV PSYCHOL, V58, P47, DOI 10.1146/annurev.psych.57.102904.190152
   Blanch-Hartigan D, 2012, BASIC APPL SOC PSYCH, V34, P483, DOI 10.1080/01973533.2012.728122
   Bond CF, 2006, PERS SOC PSYCHOL REV, V10, P214, DOI 10.1207/s15327957pspr1003_2
   Bonneh YS, 2011, FRONT HUM NEUROSCI, V4, DOI 10.3389/fnhum.2010.00237
   Borkenau P, 2004, J PERS SOC PSYCHOL, V86, P599, DOI 10.1037/0022-3514.86.4.599
   Borkenau P, 2009, J RES PERS, V43, P703, DOI 10.1016/j.jrp.2009.03.007
   Broesch T, 2018, CHILD DEV, V89, pe29, DOI 10.1111/cdev.12768
   BROWN BL, 1976, CAN J BEHAV SCI, V8, P39, DOI 10.1037/h0081933
   Brunswik Egon, 1956, Perception and the representative design of psychological experiments, V3
   Bryant GA, 2009, BIOL LETTERS, V5, P12, DOI 10.1098/rsbl.2008.0507
   Burgoon Judee K., 2016, NONVERBAL COMMUNICAT
   Calvo Rafael A., 2014, The Oxford handbook of affective computing, V1st
   Cartei V, 2012, J NONVERBAL BEHAV, V36, P79, DOI 10.1007/s10919-011-0123-4
   Chaby L, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00548
   Chartrand TL, 2013, ANNU REV PSYCHOL, V64, P285, DOI 10.1146/annurev-psych-113011-143754
   Cheng JT, 2016, J EXP PSYCHOL GEN, V145, P536, DOI 10.1037/xge0000166
   Cho SH, 2004, CLIN BIOMECH, V19, P145, DOI 10.1016/j.clinbiomech.2003.10.003
   Christov-Moore L, 2015, ADV GROUP DECIS NEGO, V7, P1, DOI 10.1007/978-94-017-9963-8_1
   Cohn Jeffrey F, 2007, The handbook of emotion elicitation and assessment. Oxford University Press Series in Affective Science, V1, P203, DOI DOI 10.1007/978-3-540-72348-6_1
   Côte S, 2011, J PERS SOC PSYCHOL, V101, P217, DOI 10.1037/a0023171
   Daily SB, 2017, EMOTIONS AND AFFECT IN HUMAN FACTORS AND HUMAN-COMPUTER INTERACTION, P213, DOI 10.1016/B978-0-12-801851-4.00009-4
   Dasgupta P.B., 2017, International Journal of Computer Trends and Technology, V52, P1, DOI DOI 10.14445/22312803/IJCTT-V52P101
   Dickey CC, 2012, SCHIZOPHR RES, V142, P20, DOI 10.1016/j.schres.2012.09.006
   Doherty-Sneddon G, 2013, RES DEV DISABIL, V34, P616, DOI 10.1016/j.ridd.2012.09.022
   Dore B.P., 2015, APA Handbook of Personality and Social Psychology: Vol, V1, P693, DOI [DOI 10.1037/14341, DOI 10.1037/14341-022]
   Eftekhar A, 2014, COMPUT HUM BEHAV, V37, P162, DOI 10.1016/j.chb.2014.04.048
   Elfenbein HA, 2002, PSYCHOL BULL, V128, P203, DOI 10.1037//0033-2909.128.2.203
   Ellis L, 2008, PERS INDIV DIFFER, V44, P701, DOI 10.1016/j.paid.2007.10.003
   Esposito G, 2011, RES AUTISM SPECT DIS, V5, P1510, DOI 10.1016/j.rasd.2011.02.013
   Fawcett C, 2016, PSYCHOL SCI, V27, P997, DOI 10.1177/0956797616643924
   Fischer J, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0024490
   Fitzgerald CJ, 2016, EVOL HUM BEHAV, V37, P510, DOI 10.1016/j.evolhumbehav.2016.05.004
   Fraccaro P., 2011, Journal of Evolutionary Psychology, V9, P57, DOI DOI 10.1556/JEP.9.2011.33.1
   Funder D.C., 1999, PERSONALITY JUDGMENT
   Gadassi R, 2011, PSYCHOL SCI, V22, P1033, DOI 10.1177/0956797611414728
   Goffman E., 1959, PRESENTATION SELF EV
   Gökçen E, 2016, J AUTISM DEV DISORD, V46, P2072, DOI 10.1007/s10803-016-2735-3
   Goldin-Meadow S, 2013, ANNU REV PSYCHOL, V64, P257, DOI 10.1146/annurev-psych-113011-143802
   Gunaydin G, 2017, SOC PSYCHOL PERS SCI, V8, P36, DOI 10.1177/1948550616662123
   Hall J.A., 2013, NONVERBAL COMMUNICAT
   Hall JA, 2006, J EXP SOC PSYCHOL, V42, P18, DOI 10.1016/j.jesp.2005.01.005
   Hall JA, 2005, PSYCHOL BULL, V131, P898, DOI 10.1037/0033-2909.131.6.898
   Hall JA, 2015, J NONVERBAL BEHAV, V39, P131, DOI 10.1007/s10919-014-0205-1
   Hall JA, 2009, MOTIV EMOTION, V33, P291, DOI 10.1007/s11031-009-9128-2
   Hall JA, 2009, J NONVERBAL BEHAV, V33, P149, DOI 10.1007/s10919-009-0070-5
   Hall JA, 2008, J RES PERS, V42, P1476, DOI 10.1016/j.jrp.2008.06.013
   Harrigan J.A., 2005, NEW HDB METHODS NONV
   Heaver B, 2011, MEMORY, V19, P398, DOI 10.1080/09658211.2011.575788
   Hirschmüller S, 2018, J PERS, V86, P308, DOI 10.1111/jopy.12316
   Hodges S.D., 2015, INTERPERSONAL RELATI, V3, P319, DOI [DOI 10.1037/14344-012, 10.1037/14344-012]
   Horgan TG, 2016, J SOC PERS RELAT, V33, P733, DOI 10.1177/0265407515590279
   Hughes SM, 2004, EVOL HUM BEHAV, V25, P295, DOI 10.1016/j.evolhumbehav.2004.06.001
   Karelaia N, 2008, PSYCHOL BULL, V134, P404, DOI 10.1037/0033-2909.134.3.404
   Karthikeyan S, 2017, J PSYCHOLINGUIST RES, V46, P457, DOI 10.1007/s10936-016-9446-y
   Kaufmann E, 2009, SWISS J PSYCHOL, V68, P99, DOI 10.1024/1421-0185.68.2.99
   Kemper S., 1994, Aging and Cognition, V1, P17, DOI [DOI 10.1080/09289919408251447, 10.1080/09289919408251447]
   Kenny D. A., 2008, First impressions, P129
   Klimecki O., 2013, Cambridge handbook of human affective neuroscience, P533, DOI DOI 10.1017/CBO9780511843716CBO9780511843716
   Knapp M. L., 2013, Nonverbal communication in human interaction, V8
   Ko SJ, 2015, PSYCHOL SCI, V26, P3, DOI 10.1177/0956797614553009
   Ko SU, 2011, J BIOMECH, V44, P1974, DOI 10.1016/j.jbiomech.2011.05.005
   Kostic A, 2015, SOCIAL PSYCHOL NONVE
   Kraus MW, 2009, PSYCHOL SCI, V20, P99, DOI 10.1111/j.1467-9280.2008.02251.x
   Krumhuber EG, 2012, EMOTION, V12, P351, DOI 10.1037/a0026632
   Kuhl PK, 1997, SCIENCE, V277, P684, DOI 10.1126/science.277.5326.684
   Labov W, 1966, The social stratification of English in New York city
   LaFrance M, 2003, PSYCHOL BULL, V129, P305, DOI 10.1037/0033-2909.129.2.305
   Leongómez JD, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179407
   Leongómez JD, 2014, EVOL HUM BEHAV, V35, P489, DOI 10.1016/j.evolhumbehav.2014.06.008
   Linneman TJ, 2013, GENDER SOC, V27, P82, DOI 10.1177/0891243212464905
   Lombardi NJ, 2014, J GERONTOL NURS, V40, P44, DOI 10.3928/00989134-20140407-02
   LOVELAND KA, 1994, DEV PSYCHOPATHOL, V6, P433, DOI 10.1017/S0954579400006039
   Manusov V., 2006, The Sage Handbook of Nonverbal Communication
   Martineau J, 2011, J PSYCHIATR RES, V45, P1077, DOI 10.1016/j.jpsychires.2011.01.008
   Mast MS, 2015, CURR DIR PSYCHOL SCI, V24, P154, DOI 10.1177/0963721414560811
   Matsumoto David., 2016, APA Handbook of Nonverbal Communication. APA Handbook of Nonverbal Communication, DOI [10.1037/14669-000, DOI 10.1037/14669-000]
   McNeill D, 2016, WHY WE GESTURE: THE SURPRISING ROLE OF HAND MOVEMENTS IN COMMUNICATION, P1, DOI 10.1017/CBO9781316480526
   Moriuchi JM, 2017, AM J PSYCHIAT, V174, P26, DOI 10.1176/appi.ajp.2016.15091222
   Murphy NA, 2018, PERSONAL SOC PSYCHOL
   Murphy NA, 2015, PERS SOC PSYCHOL B, V41, P199, DOI 10.1177/0146167214559902
   Murphy NA, 2010, PSYCHOL AGING, V25, P811, DOI 10.1037/a0019888
   Mutic S, 2016, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.01980
   Narayan CR, 2016, J ACOUST SOC AM, V139, P1272, DOI 10.1121/1.4944634
   Naumann LP, 2009, PERS SOC PSYCHOL B, V35, P1661, DOI 10.1177/0146167209346309
   Nestler S, 2013, CURR DIR PSYCHOL SCI, V22, P374, DOI 10.1177/0963721413486148
   Nowicki S, 2001, LEA SER PER CLIN PSY, P183
   O'Connor JJM, 2011, EVOL PSYCHOL-US, V9, P64, DOI 10.1177/147470491100900109
   Otero SC, 2011, PSYCHOPHYSIOLOGY, V48, P1346, DOI 10.1111/j.1469-8986.2011.01217.x
   PATTERSON ML, 1982, PSYCHOL REV, V89, P231, DOI 10.1037/0033-295X.89.3.231
   Patterson ML., 2018, J NONVERBAL BEHAV
   Pernet CR, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00023
   Piazza EA, 2017, CURR BIOL, V27, P3162, DOI 10.1016/j.cub.2017.08.074
   Pine KJ, 2010, J NONVERBAL BEHAV, V34, P169, DOI 10.1007/s10919-010-0089-7
   Plusquellec P, 2018, J NONVERBAL BEHAV, V42, P347, DOI 10.1007/s10919-018-0280-9
   Puts DA, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2015.2830
   Puts DA, 2014, EVOL PSYCHOL-SER, P69, DOI 10.1007/978-1-4939-0314-6_3
   Realo A, 2003, J RES PERS, V37, P420, DOI 10.1016/S0092-6566(03)00021-7
   Reed LI, 2007, J ABNORM PSYCHOL, V116, P804, DOI 10.1037/0021-843X.116.4.804
   Reynolds DJ, 2001, PERS SOC PSYCHOL B, V27, P187, DOI 10.1177/0146167201272005
   Richeson JA, 2005, J NONVERBAL BEHAV, V29, P75, DOI 10.1007/s10919-004-0890-2
   Rodriguez-Lujan I, 2013, KNOWL-BASED SYST, V52, P279, DOI 10.1016/j.knosys.2013.08.002
   Rule NO, 2016, CURR DIR PSYCHOL SCI, V25, P444, DOI 10.1177/0963721416664403
   Rule NO, 2014, LEADERSHIP QUART, V25, P846, DOI 10.1016/j.leaqua.2014.01.001
   RYAN EB, 1995, J LANG SOC PSYCHOL, V14, P144, DOI 10.1177/0261927X95141008
   Saint-Georges Catherine, 2013, PLoS One, V8, pe78103, DOI 10.1371/journal.pone.0078103
   Schlegel K, 2018, COGN EMOT
   Schlegel K, 2017, MOTIV EMOTION, V41, P646, DOI 10.1007/s11031-017-9631-9
   Schlegel K, 2017, J NONVERBAL BEHAV, V41, P103, DOI 10.1007/s10919-017-0249-0
   Schlegel K, 2014, PSYCHOL ASSESSMENT, V26, P666, DOI 10.1037/a0035246
   Schmid Mast M, 2018, CURR DIR PSYCHOL
   Sell A, 2010, P ROY SOC B-BIOL SCI, V277, P3509, DOI 10.1098/rspb.2010.0769
   Sibai FN, 2011, EXPERT SYST APPL, V38, P5940, DOI 10.1016/j.eswa.2010.11.029
   Skorska MN, 2015, ARCH SEX BEHAV, V44, P1377, DOI 10.1007/s10508-014-0454-4
   Smith-Genthôs KR, 2015, J EXP SOC PSYCHOL, V56, P179, DOI 10.1016/j.jesp.2014.09.018
   Sulpizio S, 2018, NEUROSCI RES, V133, P21, DOI 10.1016/j.neures.2017.10.008
   Thimm C, 1998, J APPL COMMUN RES, V26, P66, DOI 10.1080/00909889809365492
   Todorov A., 2017, Face value, DOI DOI 10.2307/J.CTVC7736T
   Todorov A, 2015, ANNU REV PSYCHOL, V66, P519, DOI 10.1146/annurev-psych-113011-143831
   Trevisan DA, 2016, MOL AUTISM, V7, DOI 10.1186/s13229-016-0108-6
   vanBezooijen R, 1995, LANG SPEECH, V38, P253, DOI 10.1177/002383099503800303
   Vazire S, 2008, J RES PERS, V42, P1439, DOI 10.1016/j.jrp.2008.06.007
   Vicaria IM, 2016, J NONVERBAL BEHAV, V40, P335, DOI 10.1007/s10919-016-0238-8
   West TV, 2011, PSYCHOL REV, V118, P357, DOI 10.1037/a0022936
   WIENER M, 1972, PSYCHOL REV, V79, P185, DOI 10.1037/h0032710
   Wieser MJ, 2010, CYBERPSYCH BEH SOC N, V13, P547, DOI 10.1089/cyber.2009.0432
   Williams KN, 2011, BEHAV THER, V42, P42, DOI 10.1016/j.beth.2010.03.003
   Witt PL, 2006, LEA COMMUN SER, P149
   Zaki J, 2013, PERSPECT PSYCHOL SCI, V8, P296, DOI 10.1177/1745691613475454
   Zaki J, 2009, P NATL ACAD SCI USA, V106, P11382, DOI 10.1073/pnas.0902666106
   Zangl R, 2007, INFANCY, V11, P31, DOI 10.1207/s15327078in1101_2
   Zebrowitz L A, 1997, Pers Soc Psychol Rev, V1, P204, DOI 10.1207/s15327957pspr0103_2
   Zhao YJ, 2018, J IMMUNOL RES, V2018, DOI 10.1155/2018/5690258
   Zougkou K, 2017, SOC COGN AFFECT NEUR, V12, P1687, DOI 10.1093/scan/nsx064
NR 152
TC 109
Z9 135
PU ANNUAL REVIEWS
PI PALO ALTO
PA 4139 EL CAMINO WAY, PO BOX 10139, PALO ALTO, CA 94303-0897 USA
J9 ANNU REV PSYCHOL
JI Annu. Rev. Psychol
PY 2019
VL 70
BP 271
EP 294
DI 10.1146/annurev-psych-010418-103145
PG 24
UT WOS:000456388300012
DA 2024-04-26
ER

PT J
AU Schirmer, A
   Adolphs, R
AF Schirmer, Annett
   Adolphs, Ralph
TI Emotion Perception from Face, Voice, and Touch: Comparisons and
   Convergence
SO TRENDS IN COGNITIVE SCIENCES
LA English
DT Review
ID PRIMARY SOMATOSENSORY CORTEX; FACIAL EXPRESSIONS; VOCAL EMOTIONS;
   CULTURALLY UNIVERSAL; SOCIAL-PERCEPTION; TACTILE AFFERENTS;
   VISUAL-ATTENTION; AMYGDALA DAMAGE; HUMAN BRAIN; SELF-TOUCH
AB Historically, research on emotion perception has focused on facial expressions, and findings from this modality have come to dominate our thinking about other modalities. Here we examine emotion perception through a wider lens by comparing facial with vocal and tactile processing. We review stimulus characteristics and ensuing behavioral and brain responses and show that audition and touch do not simply duplicate visual mechanisms. Each modality provides a distinct input channel and engages partly nonoverlapping neuroanatomical systems with different processing specializations (e.g., specific emotions versus affect). Moreover, processing of signals across the different modalities converges, first into multi- and later into amodal representations that enable holistic emotion judgments.
C1 [Schirmer, Annett] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
   [Schirmer, Annett] Max Planck Inst Human Cognit & Brain Sci, Leipzig, Germany.
   [Schirmer, Annett] Natl Univ Singapore, Singapore, Singapore.
   [Adolphs, Ralph] CALTECH, Pasadena, CA 91125 USA.
RP Schirmer, A (corresponding author), Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.; Schirmer, A (corresponding author), Max Planck Inst Human Cognit & Brain Sci, Leipzig, Germany.; Schirmer, A (corresponding author), Natl Univ Singapore, Singapore, Singapore.; Adolphs, R (corresponding author), CALTECH, Pasadena, CA 91125 USA.
EM schirmer@cuhk.edu.hk; radolphs@caltech.edu
CR Ackerley R, 2014, J NEUROSCI, V34, P2879, DOI 10.1523/JNEUROSCI.2847-13.2014
   Ackerley R, 2012, FRONT BEHAV NEUROSCI, V6, DOI 10.3389/fnbeh.2012.00051
   Adolphs R, 2000, J NEUROSCI, V20, P2683
   Adolphs R, 1999, NEUROPSYCHOLOGIA, V37, P1285, DOI 10.1016/S0028-3932(99)00023-8
   Adolphs R, 2002, EMOTION, V2, P23, DOI 10.1037/1528-3542.2.1.23
   Adolphs R, 2010, ANN NY ACAD SCI, V1191, P42, DOI 10.1111/j.1749-6632.2010.05445.x
   Andics A, 2016, SCIENCE, V353, P1030, DOI 10.1126/science.aaf3777
   [Anonymous], 2012, NEURAL BASEMULTISE
   Aubé W, 2015, SOC COGN AFFECT NEUR, V10, P399, DOI 10.1093/scan/nsu067
   Bachorowski JA, 2003, ANN NY ACAD SCI, V1000, P244, DOI 10.1196/annals.1280.012
   Banissy MJ, 2010, J NEUROSCI, V30, P13552, DOI 10.1523/JNEUROSCI.0786-10.2010
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Bänziger T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0136675
   Barber ALA, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0152393
   Bediou B, 2009, NEUROPSYCHOLOGIA, V47, P825, DOI 10.1016/j.neuropsychologia.2008.12.012
   Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008
   Bennett RH, 2014, SOC COGN AFFECT NEUR, V9, P470, DOI 10.1093/scan/nst008
   Bentin S, 1996, J COGNITIVE NEUROSCI, V8, P551, DOI 10.1162/jocn.1996.8.6.551
   Birkholz P, 2015, J ACOUST SOC AM, V137, P1503, DOI 10.1121/1.4906836
   Björnsdotter M, 2010, EXP BRAIN RES, V207, P149, DOI 10.1007/s00221-010-2408-y
   Brauer J, 2016, CEREB CORTEX, V26, P3544, DOI 10.1093/cercor/bhw137
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Brück C, 2011, NEUROIMAGE, V58, P259, DOI 10.1016/j.neuroimage.2011.06.005
   Bryant Gregory A, 2013, Front Psychol, V4, P990, DOI 10.3389/fpsyg.2013.00990
   Calder AJ, 2000, NAT NEUROSCI, V3, P1077, DOI 10.1038/80586
   Calder AJ, 2001, NAT REV NEUROSCI, V2, P352, DOI 10.1038/35072584
   Capilla A, 2013, CEREB CORTEX, V23, P1388, DOI 10.1093/cercor/bhs119
   Case LK, 2016, J NEUROSCI, V36, P5850, DOI 10.1523/JNEUROSCI.1130-15.2016
   Cordaro DT, 2016, EMOTION, V16, P117, DOI 10.1037/emo0000100
   Croy I, 2016, BEHAV BRAIN RES, V297, P37, DOI 10.1016/j.bbr.2015.09.038
   Darwin C., 1872, EXPRESSION EMOTIONS
   De Winter FL, 2015, NEUROIMAGE, V106, P340, DOI 10.1016/j.neuroimage.2014.11.020
   Deschrijver E, 2016, SOC COGN AFFECT NEUR, V11, P1162, DOI 10.1093/scan/nsv081
   Dunbar RIM, 2010, NEUROSCI BIOBEHAV R, V34, P260, DOI 10.1016/j.neubiorev.2008.07.001
   EKMAN P, 1976, ENVIRON PSYCH NONVER, V1, P56, DOI 10.1007/BF01115465
   Enea V, 2016, SOC NEUROSCI-UK, V11, P495, DOI 10.1080/17470919.2015.1114020
   Escoffier N, 2013, HUM BRAIN MAPP, V34, P1796, DOI 10.1002/hbm.22029
   Ethofer T, 2009, CURR BIOL, V19, P1028, DOI 10.1016/j.cub.2009.04.054
   Firestone C., 2015, BEHAV BRAIN SCI, P1
   Frühholz S, 2015, P NATL ACAD SCI USA, V112, P1583, DOI 10.1073/pnas.1411315112
   Gazzola V, 2012, P NATL ACAD SCI USA, V109, pE1657, DOI 10.1073/pnas.1113211109
   Gendron M, 2014, PSYCHOL SCI, V25, P911, DOI 10.1177/0956797613517239
   Gendron M, 2014, EMOTION, V14, P251, DOI 10.1037/a0036052
   Ghazanfar AA, 2014, CURR OPIN NEUROBIOL, V28, P128, DOI 10.1016/j.conb.2014.06.015
   Ghose D, 2014, J NEUROSCI, V34, P4332, DOI 10.1523/JNEUROSCI.3004-13.2014
   Hertenstein MJ, 2009, EMOTION, V9, P566, DOI 10.1037/a0016108
   Hinojosa JA, 2015, NEUROSCI BIOBEHAV R, V55, P498, DOI 10.1016/j.neubiorev.2015.06.002
   Hogendoorn H, 2015, EXP BRAIN RES, V233, P2845, DOI 10.1007/s00221-015-4355-0
   Innes B. R., 2015, LATERALITY
   Jack RE, 2012, P NATL ACAD SCI USA, V109, P7241, DOI 10.1073/pnas.1200155109
   Jiang XM, 2015, CORTEX, V66, P9, DOI 10.1016/j.cortex.2015.02.002
   Kaiser MD, 2016, CEREB CORTEX, V26, P2705, DOI 10.1093/cercor/bhv125
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Kawasaki H, 2001, NAT NEUROSCI, V4, P15, DOI 10.1038/82850
   Keysers C, 2007, TRENDS COGN SCI, V11, P194, DOI 10.1016/j.tics.2007.02.002
   Kim J, 2015, J NEUROSCI, V35, P5655, DOI 10.1523/JNEUROSCI.4059-14.2015
   King AJ, 2009, NAT NEUROSCI, V12, P698, DOI 10.1038/nn.2308
   Klasen M, 2012, REV NEUROSCIENCE, V23, P381, DOI 10.1515/revneuro-2012-0040
   Klasen M, 2011, J NEUROSCI, V31, P13635, DOI 10.1523/JNEUROSCI.2833-11.2011
   Kragel PA, 2015, SOC COGN AFFECT NEUR, V10, P1437, DOI 10.1093/scan/nsv032
   Kraus N, 2015, TRENDS COGN SCI, V19, P642, DOI 10.1016/j.tics.2015.08.017
   Kreifelts B, 2007, NEUROIMAGE, V37, P1445, DOI 10.1016/j.neuroimage.2007.06.020
   Kveraga K, 2007, J NEUROSCI, V27, P13232, DOI 10.1523/JNEUROSCI.3481-07.2007
   Laukka P, 2005, COGNITION EMOTION, V19, P633, DOI 10.1080/02699930441000445
   Löken LS, 2009, NAT NEUROSCI, V12, P547, DOI 10.1038/nn.2312
   McGlone F, 2012, EUR J NEUROSCI, V35, P1782, DOI 10.1111/j.1460-9568.2012.08092.x
   McGugin RW, 2012, P NATL ACAD SCI USA, V109, P17063, DOI 10.1073/pnas.1116333109
   Méndez-Bértolo C, 2016, NAT NEUROSCI, V19, P1041, DOI 10.1038/nn.4324
   Mesulam MM, 1998, BRAIN, V121, P1013, DOI 10.1093/brain/121.6.1013
   Miller RL, 2015, J NEUROSCI, V35, P5213, DOI 10.1523/JNEUROSCI.4771-14.2015
   Mitchell RLC, 2015, NEUROPSYCHOLOGIA, V70, P1, DOI 10.1016/j.neuropsychologia.2015.02.018
   Morimoto Y, 2011, PRIMATES, V52, P279, DOI 10.1007/s10329-011-0249-3
   Morrison I, 2016, HUM BRAIN MAPP, V37, P1308, DOI 10.1002/hbm.23103
   Mothes-Lasch M, 2011, J NEUROSCI, V31, P9594, DOI 10.1523/JNEUROSCI.6665-10.2011
   Müri RM, 2016, J COMP NEUROL, V524, P1578, DOI 10.1002/cne.23908
   Olausson H, 2002, NAT NEUROSCI, V5, P900, DOI 10.1038/nn896
   Palagi E, 2011, J COMP PSYCHOL, V125, P11, DOI 10.1037/a0020869
   Parr LA, 2010, AM J PHYS ANTHROPOL, V143, P625, DOI 10.1002/ajpa.21401
   Parr LA, 2011, PHILOS T R SOC B, V366, P1764, DOI 10.1098/rstb.2010.0358
   Paulmann S, 2008, NEUROREPORT, V19, P209, DOI 10.1097/WNR.0b013e3282f454db
   Peelen MV, 2010, J NEUROSCI, V30, P10127, DOI 10.1523/JNEUROSCI.2161-10.2010
   Pemet C. R., 2015, NEUROIMAGE, V119, P164
   Pessoa L, 2010, NAT REV NEUROSCI, V11, P773, DOI 10.1038/nrn2920
   Pitcher D, 2008, J NEUROSCI, V28, P8929, DOI 10.1523/JNEUROSCI.1450-08.2008
   Pourtois G, 2000, NEUROREPORT, V11, P1329, DOI 10.1097/00001756-200004270-00036
   Rudrauf D, 2008, J NEUROSCI, V28, P2793, DOI 10.1523/JNEUROSCI.3476-07.2008
   Rutishauser U, 2015, TRENDS NEUROSCI, V38, P295, DOI 10.1016/j.tins.2015.03.001
   Schehka S, 2012, EMOTION, V12, P632, DOI 10.1037/a0026893
   Schirmer A, 2006, TRENDS COGN SCI, V10, P24, DOI 10.1016/j.tics.2005.11.009
   Schirmer A, 2016, TRENDS COGN SCI, V20, P760, DOI 10.1016/j.tics.2016.08.002
   Schirmer A, 2015, BRIT J PSYCHOL, V106, P107, DOI 10.1111/bjop.12068
   Schirmer A, 2013, FRONT BEHAV NEUROSCI, V7, DOI 10.3389/fnbeh.2013.00167
   Schirmer A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0074591
   Schirmer A, 2013, COGN AFFECT BEHAV NE, V13, P80, DOI 10.3758/s13415-012-0132-8
   Schirmer A, 2012, NEUROIMAGE, V63, P137, DOI 10.1016/j.neuroimage.2012.06.025
   Schirmer A, 2011, SOC NEUROSCI-UK, V6, P219, DOI 10.1080/17470919.2010.507958
   Scott SK, 1997, NATURE, V385, P254, DOI 10.1038/385254a0
   Sherwood CC, 2005, J HUM EVOL, V48, P45, DOI 10.1016/j.jhevol.2004.10.003
   Skerry AE, 2015, CURR BIOL, V25, P1945, DOI 10.1016/j.cub.2015.06.009
   Staios M, 2013, FRONT HUM NEUROSCI, V7, DOI [10.3389/fnhum.2013.00178, 10.3389/fnhum.2013.00464]
   Suess F, 2015, SOC COGN AFFECT NEUR, V10, P531, DOI 10.1093/scan/nsu088
   Susskind JM, 2008, NAT NEUROSCI, V11, P843, DOI 10.1038/nn.2138
   Talkington WJ, 2012, J NEUROSCI, V32, P8084, DOI 10.1523/JNEUROSCI.1118-12.2012
   Tanaka A, 2010, PSYCHOL SCI, V21, P1259, DOI 10.1177/0956797610380698
   Tsao DY, 2008, ANNU REV NEUROSCI, V31, P411, DOI 10.1146/annurev.neuro.30.051606.094238
   Tse CY, 2013, NEUROIMAGE, V83, P870, DOI 10.1016/j.neuroimage.2013.07.037
   Voos AC, 2013, SOC COGN AFFECT NEUR, V8, P378, DOI 10.1093/scan/nss009
   Vuilleumier P, 2007, NEUROPSYCHOLOGIA, V45, P174, DOI 10.1016/j.neuropsychologia.2006.06.003
   Wang S, 2014, P NATL ACAD SCI USA, V111, pE3110, DOI 10.1073/pnas.1323342111
   Wegrzyn M, 2015, CORTEX, V69, P131, DOI 10.1016/j.cortex.2015.05.003
   Yang DYJ, 2015, NEUROSCI BIOBEHAV R, V51, P263, DOI 10.1016/j.neubiorev.2015.01.020
NR 111
TC 198
Z9 221
PU ELSEVIER SCIENCE LONDON
PI LONDON
PA 84 THEOBALDS RD, LONDON WC1X 8RR, ENGLAND
J9 TRENDS COGN SCI
JI TRENDS COGN. SCI.
PD MAR
PY 2017
VL 21
IS 3
BP 216
EP 228
DI 10.1016/j.tics.2017.01.001
PG 13
UT WOS:000397373700008
DA 2024-04-26
ER

PT J
AU Frühholz, S
   Trost, W
   Kotz, SA
AF Fruhholz, Sascha
   Trost, Wiebke
   Kotz, Sonja A.
TI The sound of emotions-Towards a unifying neural network perspective of
   affective sound processing
SO NEUROSCIENCE AND BIOBEHAVIORAL REVIEWS
LA English
DT Review
DE Affect; Sound; Voice; Music; Limbic system; Auditory cortex; Basal
   ganglia; Cerebellum
ID HUMAN ORBITOFRONTAL CORTEX; AUDITORY-CORTEX; AMYGDALA SUBREGIONS;
   PREFRONTAL CORTEX; VOCAL EXPRESSIONS; AFFECTIVE PROSODY; FUNCTIONAL MRI;
   BASAL GANGLIA; QUANTITATIVE METAANALYSIS; ACOUSTIC PARAMETERS
AB Affective sounds are an integral part of the natural and social environment that shape and influence behavior across a multitude of species. In human primates, these affective sounds span a repertoire of environmental and human sounds when we vocalize or produce music. In terms of neural processing, cortical and subcortical brain areas constitute a distributed network that supports our listening experience to these affective sounds. Taking an exhaustive cross-domain view, we accordingly suggest a common neural network that facilitates the decoding of the emotional meaning from a wide source of sounds rather than a traditional view that postulates distinct neural systems for specific affective sound types. This new integrative neural network view unifies the decoding of affective valence in sounds, and ascribes differential as well as complementary functional roles to specific nodes within a common neural network. It also highlights the importance of an extended brain network beyond the central limbic and auditory brain systems engaged in the processing of affective sounds. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Fruhholz, Sascha] Univ Zurich, Dept Psychol, Binzmuhlestr 14,Box 18, CH-8050 Zurich, Switzerland.
   [Fruhholz, Sascha] Univ Zurich, Neurosci Ctr Zurich, Zurich, Switzerland.
   [Fruhholz, Sascha] ETH, Zurich, Switzerland.
   [Fruhholz, Sascha] Univ Zurich, Ctr Integrat Human Physiol ZIHP, CH-8006 Zurich, Switzerland.
   [Trost, Wiebke] Univ Geneva, Swiss Ctr Affect Sci, Geneva, Switzerland.
   [Kotz, Sonja A.] Maastricht Univ, Dept Neuropsychol & Psychopharmacol, Fac Psychol & Neurosci, Maastricht, Netherlands.
   [Kotz, Sonja A.] Max Planck Inst Human Cognit & Brain Sci, Dept Neuropsychol, Leipzig, Germany.
RP Frühholz, S (corresponding author), Univ Zurich, Dept Psychol, Binzmuhlestr 14,Box 18, CH-8050 Zurich, Switzerland.
EM sascha.fruehholz@uzh.ch
CR Adamaszek M, 2014, CEREBELLUM, V13, P338, DOI 10.1007/s12311-013-0537-0
   Adolphs R, 2002, CURR OPIN NEUROBIOL, V12, P169, DOI 10.1016/S0959-4388(02)00301-X
   Alba-Ferrara L, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0028701
   Amodio DM, 2006, NAT REV NEUROSCI, V7, P268, DOI 10.1038/nrn1884
   Armony JL, 1997, ANN NY ACAD SCI, V821, P259, DOI 10.1111/j.1749-6632.1997.tb48285.x
   Awano H, 2011, LECT NOTES COMPUT SC, V7064, P323, DOI 10.1007/978-3-642-24965-5_36
   Bach DR, 2008, NEUROIMAGE, V42, P919, DOI 10.1016/j.neuroimage.2008.05.034
   Bach DR, 2008, CEREB CORTEX, V18, P145, DOI 10.1093/cercor/bhm040
   Bachorowski JA, 2001, PSYCHOL SCI, V12, P252, DOI 10.1111/1467-9280.00346
   Ball T, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000307
   Bamiou DE, 2003, BRAIN RES REV, V42, P143, DOI 10.1016/S0165-0173(03)00172-3
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Baumgartner T, 2006, BRAIN RES, V1075, P151, DOI 10.1016/j.brainres.2005.12.065
   Beaucousin V, 2007, CEREB CORTEX, V17, P339, DOI 10.1093/cercor/bhj151
   Beaucousin V, 2011, BRAIN RES, V1390, P108, DOI 10.1016/j.brainres.2011.03.043
   Bestelmeyer PEG, 2014, J NEUROSCI, V34, P8098, DOI 10.1523/JNEUROSCI.4820-13.2014
   Blackford JU, 2010, NEUROIMAGE, V50, P1188, DOI 10.1016/j.neuroimage.2009.12.083
   Blood AJ, 1999, NAT NEUROSCI, V2, P382, DOI 10.1038/7299
   Blood AJ, 2001, P NATL ACAD SCI USA, V98, P11818, DOI 10.1073/pnas.191355898
   Boemio A, 2005, NAT NEUROSCI, V8, P389, DOI 10.1038/nn1409
   Brattico E, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00308
   Brown S, 2004, NEUROREPORT, V15, P2033, DOI 10.1097/00001756-200409150-00008
   Buchanan TW, 2000, COGNITIVE BRAIN RES, V9, P227, DOI 10.1016/S0926-6410(99)00060-9
   Büchel C, 1999, J NEUROSCI, V19, P10869
   Calder AJ, 2000, NAT NEUROSCI, V3, P1077, DOI 10.1038/80586
   Cappe C, 2009, CEREB CORTEX, V19, P2025, DOI 10.1093/cercor/bhn228
   Caria A, 2011, CEREB CORTEX, V21, P2838, DOI 10.1093/cercor/bhr084
   Chanda ML, 2013, TRENDS COGN SCI, V17, P179, DOI 10.1016/j.tics.2013.02.007
   Chapin H, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0013812
   Dalgleish T, 2004, NAT REV NEUROSCI, V5, P583, DOI 10.1038/nrn1432
   Dietrich S, 2007, NEUROREPORT, V18, P1891, DOI 10.1097/WNR.0b013e3282f290df
   Dietrich S, 2008, NEUROREPORT, V19, P1751, DOI 10.1097/WNR.0b013e3283193e9e
   Duchaine B, 2015, ANNU REV VIS SCI, V1, P393, DOI 10.1146/annurev-vision-082114-035518
   Eldar E, 2007, CEREB CORTEX, V17, P2828, DOI 10.1093/cercor/bhm011
   Escoffier N, 2013, HUM BRAIN MAPP, V34, P1796, DOI 10.1002/hbm.22029
   Ethofer T, 2006, NEUROIMAGE, V30, P580, DOI 10.1016/j.neuroimage.2005.09.059
   Ethofer T, 2006, NEUROREPORT, V17, P249, DOI 10.1097/01.wnr.0000199466.32036.5d
   Ethofer T, 2007, SOC COGN AFFECT NEUR, V2, P334, DOI 10.1093/scan/nsm028
   Ethofer T, 2012, CEREB CORTEX, V22, P191, DOI 10.1093/cercor/bhr113
   Ethofer T, 2009, J COGNITIVE NEUROSCI, V21, P1255, DOI 10.1162/jocn.2009.21099
   Etkin A, 2011, TRENDS COGN SCI, V15, P85, DOI 10.1016/j.tics.2010.11.004
   Euston DR, 2012, NEURON, V76, P1057, DOI 10.1016/j.neuron.2012.12.002
   Fecteau S, 2005, J NEUROPHYSIOL, V94, P2251, DOI 10.1152/jn.00329.2005
   Fecteau S, 2007, NEUROIMAGE, V36, P480, DOI 10.1016/j.neuroimage.2007.02.043
   Ferrucci R, 2012, COGNITION EMOTION, V26, P786, DOI 10.1080/02699931.2011.619520
   Friederici AD, 2012, TRENDS COGN SCI, V16, P262, DOI 10.1016/j.tics.2012.04.001
   Frühholz S, 2015, NEUROIMAGE, V109, P27, DOI 10.1016/j.neuroimage.2015.01.016
   Frühholz S, 2015, P NATL ACAD SCI USA, V112, P1583, DOI 10.1073/pnas.1411315112
   Frühholz S, 2014, PROG NEUROBIOL, V123, P1, DOI 10.1016/j.pneurobio.2014.09.003
   Frühholz S, 2013, CORTEX, V49, P1394, DOI 10.1016/j.cortex.2012.08.003
   Frühholz S, 2013, NEUROSCI BIOBEHAV R, V37, P24, DOI 10.1016/j.neubiorev.2012.11.002
   Frühholz S, 2012, NEUROIMAGE, V62, P1658, DOI 10.1016/j.neuroimage.2012.06.015
   Frühholz S, 2012, CEREB CORTEX, V22, P1107, DOI 10.1093/cercor/bhr184
   Frühholz S, 2010, NEUROSCI LETT, V469, P260, DOI 10.1016/j.neulet.2009.12.010
   Fruhholz S., 2013, NEUROSCI BIOBEHAV R, V37, P2847
   Fruhholz S., 2016, BEHAV BRAIN SCI, V37, P554
   Fusar-Poli P, 2009, J PSYCHIATR NEUROSCI, V34, P418
   Garcia R, 1999, NATURE, V402, P294, DOI 10.1038/46286
   George MS, 1996, ARCH NEUROL-CHICAGO, V53, P665, DOI 10.1001/archneur.1996.00550070103017
   Gifford GW, 2005, J COGNITIVE NEUROSCI, V17, P1471, DOI 10.1162/0898929054985464
   Glasser MF, 2008, CEREB CORTEX, V18, P2471, DOI 10.1093/cercor/bhn011
   Grahn JA, 2007, J COGNITIVE NEUROSCI, V19, P893, DOI 10.1162/jocn.2007.19.5.893
   Grandjean D, 2005, NAT NEUROSCI, V8, P145, DOI 10.1038/nn1392
   Green AC, 2008, NEUROREPORT, V19, P711, DOI 10.1097/WNR.0b013e3282fd0dd8
   Gripon V, 2011, IEEE T NEURAL NETWOR, V22, P1087, DOI 10.1109/TNN.2011.2146789
   Hass J, 2012, NEURAL COMPUT, V24, P1519, DOI 10.1162/NECO_a_00280
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   Hoekert M, 2010, BMC NEUROSCI, V11, DOI 10.1186/1471-2202-11-93
   Hoekert M, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0002244
   JACOBSON L, 1991, ENDOCR REV, V12, P118, DOI 10.1210/edrv-12-2-118
   Janata P, 2012, J EXP PSYCHOL GEN, V141, P54, DOI 10.1037/a0024208
   Janata P, 2009, CEREB CORTEX, V19, P2579, DOI 10.1093/cercor/bhp008
   Johnstone T, 2006, SOC COGN AFFECT NEUR, V1, P242, DOI 10.1093/scan/nsl027
   Juslin PN, 2003, PSYCHOL BULL, V129, P770, DOI 10.1037/0033-2909.129.5.770
   Khalfa S, 2005, NEUROREPORT, V16, P1981, DOI 10.1097/00001756-200512190-00002
   Knight DC, 2005, NEUROIMAGE, V26, P1193, DOI 10.1016/j.neuroimage.2005.03.020
   Koelsch S, 2006, HUM BRAIN MAPP, V27, P239, DOI 10.1002/hbm.20180
   Koelsch S, 2014, NAT REV NEUROSCI, V15, P170, DOI 10.1038/nrn3666
   Koelsch S, 2013, NEUROIMAGE, V81, P49, DOI 10.1016/j.neuroimage.2013.05.008
   Koelsch S, 2010, TRENDS COGN SCI, V14, P131, DOI 10.1016/j.tics.2010.01.002
   Koelsch S, 2008, NEUROREPORT, V19, P1815, DOI 10.1097/WNR.0b013e32831a8722
   Kotz SA, 2003, BRAIN LANG, V86, P366, DOI 10.1016/S0093-934X(02)00532-1
   Kotz SA, 2013, HUM BRAIN MAPP, V34, P1971, DOI 10.1002/hbm.22041
   Kotz SA, 2010, TRENDS COGN SCI, V14, P392, DOI 10.1016/j.tics.2010.06.005
   Kotz SA, 2009, CORTEX, V45, P982, DOI 10.1016/j.cortex.2009.02.010
   Kreifelts B, 2010, HUM BRAIN MAPP, V31, P979, DOI 10.1002/hbm.20913
   Kriegeskorte N, 2006, P NATL ACAD SCI USA, V103, P3863, DOI 10.1073/pnas.0600244103
   Kringelbach ML, 2005, NAT REV NEUROSCI, V6, P691, DOI 10.1038/nrn1747
   Kringelbach ML, 2004, PROG NEUROBIOL, V72, P341, DOI 10.1016/j.pneurobio.2004.03.006
   Kumar S, 2012, J NEUROSCI, V32, P14184, DOI 10.1523/JNEUROSCI.1759-12.2012
   Laird AR, 2005, NEUROINFORMATICS, V3, P65, DOI 10.1385/NI:3:1:065
   Laricchiuta D, 2015, BRAIN STRUCT FUNCT, V220, P2275, DOI 10.1007/s00429-014-0790-0
   LEDOUX JE, 1990, J NEUROSCI, V10, P1062
   LeDoux J, 2012, NEURON, V73, P653, DOI 10.1016/j.neuron.2012.02.004
   Lee YS, 2011, NEUROIMAGE, V57, P293, DOI 10.1016/j.neuroimage.2011.02.006
   Lehne M, 2014, SOC COGN AFFECT NEUR, V9, P1515, DOI 10.1093/scan/nst141
   Leitman DI, 2011, FRONT HUM NEUROSCI, V5, DOI [10.3389/fnhum.2011.00096, 10.3389/fnhum.2010.00019]
   Mas-Herrero E, 2014, CURR BIOL, V24, P699, DOI 10.1016/j.cub.2014.01.068
   Menon V, 2005, NEUROIMAGE, V28, P175, DOI 10.1016/j.neuroimage.2005.05.053
   Menon V, 2010, BRAIN STRUCT FUNCT, V214, P655, DOI 10.1007/s00429-010-0262-0
   Meyer M, 2005, COGNITIVE BRAIN RES, V24, P291, DOI 10.1016/j.cogbrainres.2005.02.008
   Milesi V, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00275
   Mirz F, 2000, NEUROREPORT, V11, P633, DOI 10.1097/00001756-200002280-00039
   Mitchell RLC, 2007, NEUROIMAGE, V36, P1015, DOI 10.1016/j.neuroimage.2007.03.016
   Mitchell RLC, 2006, EUR J NEUROSCI, V24, P3611, DOI 10.1111/j.1460-9568.2006.05231.x
   Mitchell RLC, 2003, NEUROPSYCHOLOGIA, V41, P1410, DOI 10.1016/S0028-3932(03)00017-4
   Mitterschiffthaler MT, 2007, HUM BRAIN MAPP, V28, P1150, DOI 10.1002/hbm.20337
   Mizuno T, 2007, NEUROREPORT, V18, P1651, DOI 10.1097/WNR.0b013e3282f0b787
   Morris JS, 2001, NEUROIMAGE, V13, P1044, DOI 10.1006/nimg.2000.0721
   Morris JS, 1999, NEUROPSYCHOLOGIA, V37, P1155, DOI 10.1016/S0028-3932(99)00015-9
   Mothes-Lasch M, 2011, J NEUROSCI, V31, P9594, DOI 10.1523/JNEUROSCI.6665-10.2011
   Mueller K, 2011, NEUROIMAGE, V54, P337, DOI 10.1016/j.neuroimage.2010.08.029
   Müller VI, 2011, NEUROIMAGE, V54, P2257, DOI 10.1016/j.neuroimage.2010.10.047
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   Panksepp J, 2002, BEHAV PROCESS, V60, P133, DOI 10.1016/S0376-6357(02)00080-3
   Panksepp J, 1995, MUSIC PERCEPT, V13, P171
   Pannese A, 2015, HEARING RES, V328, P67, DOI 10.1016/j.heares.2015.07.003
   Paradiso S, 2013, PSYCHIAT RES-NEUROIM, V211, P148, DOI 10.1016/j.pscychresns.2012.05.008
   Patel S, 2011, BIOL PSYCHOL, V87, P93, DOI 10.1016/j.biopsycho.2011.02.010
   Paulmann S, 2005, BRAIN LANG, V95, P143, DOI 10.1016/j.bandl.2005.07.079
   Paulmann S, 2008, BRAIN RES, V1217, P171, DOI 10.1016/j.brainres.2008.04.032
   Pell MD, 2003, COGN AFFECT BEHAV NE, V3, P275, DOI 10.3758/CABN.3.4.275
   Pell MD, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0027256
   Pell MD, 2006, BRAIN LANG, V96, P221, DOI 10.1016/j.bandl.2005.04.007
   Péron J, 2011, PROG NEURO-PSYCHOPH, V35, P987, DOI 10.1016/j.pnpbp.2011.01.019
   Pessoa L, 2011, NAT REV NEUROSCI, V12, P425, DOI 10.1038/nrn2920-c2
   Pessoa L, 2010, NAT REV NEUROSCI, V11, P773, DOI 10.1038/nrn2920
   Phillips ML, 1998, P ROY SOC B-BIOL SCI, V265, P1809, DOI 10.1098/rspb.1998.0506
   Pichon S, 2013, J NEUROSCI, V33, P1640, DOI 10.1523/JNEUROSCI.3530-12.2013
   Popescu AT, 2009, NAT NEUROSCI, V12, P801, DOI 10.1038/nn.2305
   Quadflieg S, 2008, BIOL PSYCHOL, V78, P129, DOI 10.1016/j.biopsycho.2008.01.014
   Rauschecker JP, 2009, NAT NEUROSCI, V12, P718, DOI 10.1038/nn.2331
   Romanski LM, 2005, J NEUROPHYSIOL, V93, P734, DOI 10.1152/jn.00675.2004
   Ross ED, 2008, BRAIN LANG, V104, P51, DOI 10.1016/j.bandl.2007.04.007
   Rota G, 2008, EXP BRAIN RES, V186, P401, DOI 10.1007/s00221-007-1242-3
   Salimpoor VN, 2013, SCIENCE, V340, P216, DOI 10.1126/science.1231059
   Salimpoor VN, 2011, NAT NEUROSCI, V14, P257, DOI 10.1038/nn.2726
   Sander D, 2005, NEUROIMAGE, V28, P848, DOI 10.1016/j.neuroimage.2005.06.023
   Sander K, 2005, J COGNITIVE NEUROSCI, V17, P1519, DOI 10.1162/089892905774597227
   Sander K, 2003, BRAIN RES PROTOC, V11, P81, DOI 10.1016/S1385-299X(03)00018-7
   Sander K, 2007, HUM BRAIN MAPP, V28, P1007, DOI 10.1002/hbm.20333
   Saur D, 2008, P NATL ACAD SCI USA, V105, P18035, DOI 10.1073/pnas.0805234105
   Schirmer A, 2006, TRENDS COGN SCI, V10, P24, DOI 10.1016/j.tics.2005.11.009
   Schirmer A, 2004, NEUROIMAGE, V21, P1114, DOI 10.1016/j.neuroimage.2003.10.048
   Schirmer A, 2008, NEUROIMAGE, V40, P1402, DOI 10.1016/j.neuroimage.2008.01.018
   Schmahmann JD, 1998, BRAIN, V121, P561, DOI 10.1093/brain/121.4.561
   Schönwiesner M, 2005, EUR J NEUROSCI, V22, P1521, DOI 10.1111/j.1460-9568.2005.04315.x
   Schubotz RI, 2003, NEUROIMAGE, V20, P173, DOI 10.1016/S1053-8119(03)00218-0
   Schutter DJLG, 2009, J PSYCHIATR NEUROSCI, V34, P60
   Schwartze M, 2013, NEUROSCI BIOBEHAV R, V37, P2587, DOI 10.1016/j.neubiorev.2013.08.005
   Sebeok T.A., 1985, Contributions to the Doctrine of Signs
   Sergerie K, 2008, NEUROSCI BIOBEHAV R, V32, P811, DOI 10.1016/j.neubiorev.2007.12.002
   Sescousse G, 2013, NEUROSCI BIOBEHAV R, V37, P681, DOI 10.1016/j.neubiorev.2013.02.002
   Sescousse G, 2010, J NEUROSCI, V30, P13095, DOI 10.1523/JNEUROSCI.3501-10.2010
   Shabel SJ, 2009, P NATL ACAD SCI USA, V106, P15031, DOI 10.1073/pnas.0905580106
   Strata P, 2011, PHYSIOL RES, V60, pS39, DOI 10.33549/physiolres.932169
   Sundberg J, 2011, IEEE T AFFECT COMPUT, V2, P162, DOI 10.1109/T-AFFC.2011.14
   Suzuki M, 2008, COGN AFFECT BEHAV NE, V8, P126, DOI 10.3758/CABN.8.2.126
   Syka J, 1997, ACOUSTICAL SIGNAL PROCESSING IN THE CENTRAL AUDITORY SYSTEM, P431, DOI 10.1007/978-1-4419-8712-9_39
   Szameitat DP, 2010, NEUROIMAGE, V53, P1264, DOI 10.1016/j.neuroimage.2010.06.028
   Taylor KS, 2009, HUM BRAIN MAPP, V30, P2731, DOI 10.1002/hbm.20705
   Tölgyesi B, 2014, J NEUROL SCI, V343, P76, DOI 10.1016/j.jns.2014.05.036
   Tomlinson SP, 2013, NEUROSCI BIOBEHAV R, V37, P766, DOI 10.1016/j.neubiorev.2013.03.001
   Trost W, 2015, SOC COGN AFFECT NEUR, V10, P1705, DOI 10.1093/scan/nsv060
   Trost W, 2014, NEUROIMAGE, V103, P55, DOI 10.1016/j.neuroimage.2014.09.009
   Trost W, 2012, CEREB CORTEX, V22, P2769, DOI 10.1093/cercor/bhr353
   Viinikainen M, 2012, HUM BRAIN MAPP, V33, P2295, DOI 10.1002/hbm.21362
   Villanueva R, 2012, PSYCHIAT RES, V198, P527, DOI 10.1016/j.psychres.2012.02.023
   Vuilleumier P, 2005, TRENDS COGN SCI, V9, P585, DOI 10.1016/j.tics.2005.10.011
   Vuilleumier P, 2004, NAT NEUROSCI, V7, P1271, DOI 10.1038/nn1341
   Warren JE, 2006, J NEUROSCI, V26, P13067, DOI 10.1523/JNEUROSCI.3907-06.2006
   Weninger F, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00292
   Wenstrup JJ, 1999, J NEUROPHYSIOL, V82, P2528, DOI 10.1152/jn.1999.82.5.2528
   Wiethoff S, 2008, NEUROIMAGE, V39, P885, DOI 10.1016/j.neuroimage.2007.09.028
   Wildgruber D, 2005, NEUROIMAGE, V24, P1233, DOI 10.1016/j.neuroimage.2004.10.034
   Wildgruber D, 2004, CEREB CORTEX, V14, P1384, DOI 10.1093/cercor/bhh099
   Wildgruber D, 2002, NEUROIMAGE, V15, P856, DOI 10.1006/nimg.2001.0998
   Wildgruber D., 2009, INT J SPEECH-LANG PA, V11, P277, DOI DOI 10.1080/17549500902943043
   Wittfoth M, 2010, CEREB CORTEX, V20, P383, DOI 10.1093/cercor/bhp106
   Zald DH, 2002, NEUROIMAGE, V16, P746, DOI 10.1006/nimg.2002.1115
   Zatorre RJ, 2001, CEREB CORTEX, V11, P946, DOI 10.1093/cercor/11.10.946
   Zentner M, 2008, EMOTION, V8, P494, DOI 10.1037/1528-3542.8.4.494
NR 182
TC 125
Z9 139
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
J9 NEUROSCI BIOBEHAV R
JI Neurosci. Biobehav. Rev.
PD SEP
PY 2016
VL 68
BP 96
EP 110
DI 10.1016/j.neubiorev.2016.05.002
PG 15
UT WOS:000383293500006
DA 2024-04-26
ER

PT J
AU Deen, B
   Koldewyn, K
   Kanwisher, N
   Saxe, R
AF Deen, Ben
   Koldewyn, Kami
   Kanwisher, Nancy
   Saxe, Rebecca
TI Functional Organization of Social Perception and Cognition in the
   Superior Temporal Sulcus
SO CEREBRAL CORTEX
LA English
DT Article
DE social cognition; social perception; superior temporal sulcus
ID BIOLOGICAL MOTION PERCEPTION; AUDIOVISUAL INTEGRATION; PHYSIOLOGICAL
   NOISE; LANGUAGE AREAS; BRAIN-REGIONS; 7 T; FMRI; ACTIVATION; MIND;
   CORTEX
AB The superior temporal sulcus (STS) is considered a hub for social perception and cognition, including the perception of faces and human motion, as well as understanding others' actions, mental states, and language. However, the functional organization of the STS remains debated: Is this broad region composed of multiple functionally distinct modules, each specialized for a different process, or are STS subregions multifunctional, contributing to multiple processes? Is the STS spatially organized, and if so, what are the dominant features of this organization? We address these questions by measuring STS responses to a range of social and linguistic stimuli in the same set of human participants, using fMRI. We find a number of STS subregions that respond selectively to certain types of social input, organized along a posterior-to-anterior axis. We also identify regions of overlapping response to multiple contrasts, including regions responsive to both language and theory of mind, faces and voices, and faces and biological motion. Thus, the human STS contains both relatively domain-specific areas, and regions that respond to multiple types of social information.
C1 MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.
   MIT, McGovern Inst Brain Res, Cambridge, MA 02139 USA.
RP Deen, B (corresponding author), MIT, Room 46-4021,43 Vassar St, Cambridge, MA 02139 USA.
EM bdeen@mit.edu
CR Allison T, 2000, TRENDS COGN SCI, V4, P267, DOI 10.1016/S1364-6613(00)01501-1
   AMBADY N, 1992, PSYCHOL BULL, V111, P256, DOI 10.1037/0033-2909.111.2.256
   Apperly IA, 2004, J COGNITIVE NEUROSCI, V16, P1773, DOI 10.1162/0898929042947928
   Apperly IA, 2006, SOC NEUROSCI-UK, V1, P334, DOI 10.1080/17470910601038693
   Barraclough NE, 2005, J COGNITIVE NEUROSCI, V17, P377, DOI 10.1162/0898929053279586
   Beauchamp MS, 2004, NEURON, V41, P809, DOI 10.1016/S0896-6273(04)00070-4
   Behzadi Y, 2007, NEUROIMAGE, V37, P90, DOI 10.1016/j.neuroimage.2007.04.042
   Belin P, 2000, NATURE, V403, P309, DOI 10.1038/35002078
   Binder JR, 2012, NEUROIMAGE, V62, P1086, DOI 10.1016/j.neuroimage.2011.09.026
   Binder JR, 1997, J NEUROSCI, V17, P353, DOI 10.1523/JNEUROSCI.17-01-00353.1997
   Bonda E, 1996, J NEUROSCI, V16, P3737
   Brass M, 2007, CURR BIOL, V17, P2117, DOI 10.1016/j.cub.2007.11.057
   Brett M, 2002, NAT REV NEUROSCI, V3, P243, DOI 10.1038/nrn756
   Bruneau EG, 2012, PHILOS T R SOC B, V367, P717, DOI 10.1098/rstb.2011.0293
   Buckner RL, 2007, TRENDS COGN SCI, V11, P49, DOI 10.1016/j.tics.2006.11.004
   Calvert GA, 2001, NEUROIMAGE, V14, P427, DOI 10.1006/nimg.2001.0812
   Chai XQJ, 2012, NEUROIMAGE, V59, P1420, DOI 10.1016/j.neuroimage.2011.08.048
   Ciaramidaro A, 2007, NEUROPSYCHOLOGIA, V45, P3105, DOI 10.1016/j.neuropsychologia.2007.05.011
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   de Villiers J, 2007, LINGUA, V117, P1858, DOI 10.1016/j.lingua.2006.11.006
   Dodell-Feder D, 2011, NEUROIMAGE, V55, P705, DOI 10.1016/j.neuroimage.2010.12.040
   Engell AD, 2013, NEUROIMAGE, V74, P140, DOI 10.1016/j.neuroimage.2013.02.025
   Fedorenko E, 2012, NEUROPSYCHOLOGIA, V50, P499, DOI 10.1016/j.neuropsychologia.2011.09.014
   Fedorenko E, 2011, P NATL ACAD SCI USA, V108, P16428, DOI 10.1073/pnas.1112937108
   Fedorenko E, 2009, LANG LINGUIST COMPAS, V3, DOI 10.1111/j.1749-818x.2009.00143.x
   FLETCHER PC, 1995, COGNITION, V57, P109, DOI 10.1016/0010-0277(95)00692-R
   Gallagher HL, 2000, NEUROPSYCHOLOGIA, V38, P11, DOI 10.1016/S0028-3932(99)00053-6
   Ghazanfar AA, 2008, J NEUROSCI, V28, P4457, DOI 10.1523/JNEUROSCI.0541-08.2008
   Gobbini MI, 2007, J COGNITIVE NEUROSCI, V19, P1803, DOI 10.1162/jocn.2007.19.11.1803
   Greve DN, 2009, NEUROIMAGE, V48, P63, DOI 10.1016/j.neuroimage.2009.06.060
   Grossman E, 2000, J COGNITIVE NEUROSCI, V12, P711, DOI 10.1162/089892900562417
   Grossman ED, 2002, NEURON, V35, P1167, DOI 10.1016/S0896-6273(02)00897-8
   Gweon H, 2012, CHILD DEV, V83, P1853, DOI 10.1111/j.1467-8624.2012.01829.x
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   Hein G, 2008, J COGNITIVE NEUROSCI, V20, P2125, DOI 10.1162/jocn.2008.20148
   Kanwisher N, 2010, P NATL ACAD SCI USA, V107, P11163, DOI 10.1073/pnas.1005062107
   Kreifelts B, 2009, NEUROPSYCHOLOGIA, V47, P3059, DOI 10.1016/j.neuropsychologia.2009.07.001
   LINDSTROM MJ, 1990, BIOMETRICS, V46, P673, DOI 10.2307/2532087
   Marchini JL, 2000, NEUROIMAGE, V12, P366, DOI 10.1006/nimg.2000.0628
   Morris JP, 2005, J COGNITIVE NEUROSCI, V17, P1744, DOI 10.1162/089892905774589253
   Pelphrey KA, 2005, CEREB CORTEX, V15, P1866, DOI 10.1093/cercor/bhi064
   Pelphrey KA, 2004, J COGNITIVE NEUROSCI, V16, P1706, DOI 10.1162/0898929042947900
   Pelphrey KA, 2004, PSYCHOL SCI, V15, P598, DOI 10.1111/j.0956-7976.2004.00726.x
   Pelphrey KA, 2003, J NEUROSCI, V23, P6819
   Pelphrey KA, 2003, NEUROPSYCHOLOGIA, V41, P156, DOI 10.1016/S0028-3932(02)00146-X
   Perrodin C, 2014, J NEUROSCI, V34, P2524, DOI 10.1523/JNEUROSCI.2805-13.2014
   Pitcher D, 2011, NEUROIMAGE, V56, P2356, DOI 10.1016/j.neuroimage.2011.03.067
   Polimeni JR, 2010, NEUROIMAGE, V52, P1334, DOI 10.1016/j.neuroimage.2010.05.005
   Power JD, 2011, NEURON, V72, P665, DOI 10.1016/j.neuron.2011.09.006
   Puce A, 1996, J NEUROSCI, V16, P5205
   Saxe R, 2003, NEUROIMAGE, V19, P1835, DOI 10.1016/S1053-8119(03)00230-1
   Saxe R, 2006, PSYCHOL SCI, V17, P692, DOI 10.1111/j.1467-9280.2006.01768.x
   Saxe R, 2006, NEUROIMAGE, V30, P1088, DOI 10.1016/j.neuroimage.2005.12.062
   Saxe RR, 2009, CHILD DEV, V80, P1197, DOI 10.1111/j.1467-8624.2009.01325.x
   Shih P, 2011, BIOL PSYCHIAT, V70, P270, DOI 10.1016/j.biopsych.2011.03.040
   Shultz S, 2011, SOC COGN AFFECT NEUR, V6, P602, DOI 10.1093/scan/nsq087
   Smith SM, 2009, P NATL ACAD SCI USA, V106, P13040, DOI 10.1073/pnas.0905267106
   Taylor KI, 2006, P NATL ACAD SCI USA, V103, P8239, DOI 10.1073/pnas.0509704103
   Triantafyllou C, 2005, NEUROIMAGE, V26, P243, DOI 10.1016/j.neuroimage.2005.01.007
   Triantafyllou C, 2006, NEUROIMAGE, V32, P551, DOI 10.1016/j.neuroimage.2006.04.182
   Vanrie J, 2004, BEHAV RES METH INS C, V36, P625, DOI 10.3758/BF03206542
   Vigneau M, 2006, NEUROIMAGE, V30, P1414, DOI 10.1016/j.neuroimage.2005.11.002
   Watson R, 2014, CORTEX, V50, P125, DOI 10.1016/j.cortex.2013.07.011
   Woolrich MW, 2004, NEUROIMAGE, V21, P1732, DOI 10.1016/j.neuroimage.2003.12.023
   Woolrich MW, 2001, NEUROIMAGE, V14, P1370, DOI 10.1006/nimg.2001.0931
   Wright TM, 2003, CEREB CORTEX, V13, P1034, DOI 10.1093/cercor/13.10.1034
   Wyk BCV, 2009, PSYCHOL SCI, V20, P771, DOI 10.1111/j.1467-9280.2009.02359.x
   Yeo BTT, 2011, J NEUROPHYSIOL, V106, P1125, DOI 10.1152/jn.00338.2011
NR 68
TC 221
Z9 248
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
J9 CEREB CORTEX
JI Cereb. Cortex
PD NOV
PY 2015
VL 25
IS 11
BP 4596
EP 4609
DI 10.1093/cercor/bhv111
PG 14
UT WOS:000366463300046
DA 2024-04-26
ER

PT J
AU Moerel, M
   De Martino, F
   Formisano, E
AF Moerel, Michelle
   De Martino, Federico
   Formisano, Elia
TI An anatomical and functional topography of human auditory cortical areas
SO FRONTIERS IN NEUROSCIENCE
LA English
DT Review
DE human auditory cortex; tonotopy; ultra-high field fMRI;
   cytoarchitectonic parcellation; auditory cortical areas
ID HUMAN TEMPORAL-LOBE; TONOTOPIC ORGANIZATION; VOLUME MEASUREMENT;
   EVOKED-POTENTIALS; PITCH PERCEPTION; NATURAL SOUNDS; BELT CORTEX;
   REPRESENTATION; SUBDIVISIONS; VOICE
AB While advances in magnetic resonance imaging (MRI) throughout the last decades have enabled the detailed anatomical and functional inspection of the human brain non-invasively, to date there is no consensus regarding the precise subdivision and topography of the areas forming the human auditory cortex. Here, we propose a topography of the human auditory areas based on insights on the anatomical and functional properties of human auditory areas as revealed by studies of cyto- and myelo-architecture and fMRI investigations at ultra-high magnetic field (7 Tesla). Importantly, we illustrate that whereas a group-based approach to analyze functional (tonotopic) maps is appropriate to highlight the main tonotopic axis the examination of tonotopic maps at single subject level is required to detail the topography of primary and non-primary areas that may be more variable across subjects. Furthermore, we show that considering multiple maps indicative of anatomical (i.e., myelination) as well as of functional properties (e.g., broadness of frequency tuning) is helpful in identifying auditory cortical areas in individual human brains. We propose and discuss a topography of areas that is consistent with old and recent anatomical post-mortem characterizations of the human auditory cortex and that may serve as a working model for neuroscience studies of auditory functions.
C1 [Moerel, Michelle; De Martino, Federico; Formisano, Elia] Maastricht Univ, Fac Psychol & Neurosci, Dept Cognit Neurosci, NL-6229 EV Maastricht, Netherlands.
   [Moerel, Michelle; De Martino, Federico; Formisano, Elia] Maastricht Univ, Maastricht Brain Imaging Ctr, Maastricht, Netherlands.
   [Moerel, Michelle] Univ Minnesota, Ctr Magnet Resonance Res, Dept Radiol, Minneapolis, MN USA.
RP Formisano, E (corresponding author), Maastricht Univ, Fac Psychol & Neurosci, Dept Cognit Neurosci, POB 616,Oxfordlaan 55, NL-6229 EV Maastricht, Netherlands.
EM e.formisano@maastrichtuniversity.nl
CR [Anonymous], 1954, J HIRNFORSCH
   [Anonymous], AUDITORY CORTEX
   Bandyopadhyay S, 2010, NAT NEUROSCI, V13, P361, DOI 10.1038/nn.2490
   Barton B, 2012, P NATL ACAD SCI USA, V109, P20738, DOI 10.1073/pnas.1213381109
   Baumann S, 2013, FRONT SYST NEUROSCI, V7, DOI 10.3389/fnsys.2013.00011
   Baumann S, 2011, NAT NEUROSCI, V14, P423, DOI 10.1038/nn.2771
   Beck E, 1928, J PSYCHOL NEUROL, V36, P1
   Belin P, 2000, NATURE, V403, P309, DOI 10.1038/35002078
   Bendor D, 2005, NATURE, V436, P1161, DOI 10.1038/nature03867
   Bilecen D, 1998, HEARING RES, V126, P19, DOI 10.1016/S0378-5955(98)00139-7
   Bonte M, 2013, NEUROIMAGE, V83, P739, DOI 10.1016/j.neuroimage.2013.07.017
   Brodmann K, 1909, VERGLEICHENDE LOKALI
   CAMPAIN R, 1976, BRAIN LANG, V3, P318, DOI 10.1016/0093-934X(76)90026-2
   CELESIA GG, 1976, BRAIN, V99, P403, DOI 10.1093/brain/99.3.403
   Clarke S, 2012, SPRINGER HANDB AUDIT, V43, P11, DOI 10.1007/978-1-4614-2314-0_2
   Cohen-Adad J, 2012, NEUROIMAGE, V60, P1006, DOI 10.1016/j.neuroimage.2012.01.053
   Da Costa S, 2011, J NEUROSCI, V31, P14067, DOI 10.1523/JNEUROSCI.2000-11.2011
   De Martino F, 2015, CEREB CORTEX, V25, P3394, DOI 10.1093/cercor/bhu150
   De Martino F, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms2379
   Di Salle F, 2003, MAGN RESON IMAGING, V21, P1213, DOI 10.1016/j.mri.2003.08.023
   Dick F, 2012, J NEUROSCI, V32, P16095, DOI 10.1523/JNEUROSCI.1712-12.2012
   ENGEL SA, 1994, NATURE, V369, P525, DOI 10.1038/369525a0
   Engelien A, 2000, HEARING RES, V148, P153, DOI 10.1016/S0378-5955(00)00148-9
   Engelien A, 2002, NEUROIMAGE, V16, P944, DOI 10.1006/nimg.2002.1149
   Formisano E, 2003, NEURON, V40, P859, DOI 10.1016/S0896-6273(03)00669-X
   Formisano E, 2008, SCIENCE, V322, P970, DOI 10.1126/science.1164318
   GALABURDA A, 1980, J COMP NEUROL, V190, P597, DOI 10.1002/cne.901900312
   GALABURDA AM, 1978, SCIENCE, V199, P852, DOI 10.1126/science.341314
   GESCHWIND N, 1968, SCIENCE, V161, P186, DOI 10.1126/science.161.3837.186
   Glasel H, 2011, NEUROIMAGE, V58, P716, DOI 10.1016/j.neuroimage.2011.06.016
   Glasser MF, 2011, J NEUROSCI, V31, P11597, DOI 10.1523/JNEUROSCI.2180-11.2011
   Goebel R, 1998, EUR J NEUROSCI, V10, P1563, DOI 10.1046/j.1460-9568.1998.00181.x
   Griffiths TD, 2012, J NEUROSCI, V32, P13343, DOI 10.1523/JNEUROSCI.3813-12.2012
   Hackett TA, 1998, J COMP NEUROL, V394, P475, DOI 10.1002/(SICI)1096-9861(19980518)394:4<475::AID-CNE6>3.0.CO;2-Z
   Hackett TA, 2001, J COMP NEUROL, V441, P197, DOI 10.1002/cne.1407
   HARI R, 1986, ACTA OTO-LARYNGOL, P26, DOI 10.3109/00016488609108882
   Hasson U, 2003, NEURON, V37, P1027, DOI 10.1016/S0896-6273(03)00144-2
   Herdener M, 2013, CORTEX, V49, P2822, DOI 10.1016/j.cortex.2013.04.003
   Hickok G, 2000, TRENDS COGN SCI, V4, P131, DOI 10.1016/S1364-6613(00)01463-7
   Howard MA, 1996, BRAIN RES, V724, P260, DOI 10.1016/0006-8993(96)00315-0
   Humphries C, 2010, NEUROIMAGE, V50, P1202, DOI 10.1016/j.neuroimage.2010.01.046
   Ide A, 1996, CEREB CORTEX, V6, P717, DOI 10.1093/cercor/6.5.717
   Joly O, 2012, CEREB CORTEX, V22, P838, DOI 10.1093/cercor/bhr150
   Kaas JH, 2000, P NATL ACAD SCI USA, V97, P11793, DOI 10.1073/pnas.97.22.11793
   Kaas JH, 2011, AUDITORY CORTEX, P407, DOI 10.1007/978-1-4419-0074-6_19
   Kajikawa Y, 2005, J NEUROPHYSIOL, V93, P22, DOI 10.1152/jn.00248.2004
   Kim JJ, 2000, NEUROIMAGE, V11, P271, DOI 10.1006/nimg.2000.0543
   Kosaki H, 1997, J COMP NEUROL, V386, P304
   Kusmierek P, 2009, J NEUROPHYSIOL, V102, P1606, DOI 10.1152/jn.00167.2009
   Langers DRM, 2014, HUM BRAIN MAPP, V35, P1544, DOI 10.1002/hbm.22272
   Langers DRM, 2012, CEREB CORTEX, V22, P2024, DOI 10.1093/cercor/bhr282
   Langers DRM, 2007, NEUROIMAGE, V34, P264, DOI 10.1016/j.neuroimage.2006.09.002
   Langers DRM, 2003, NEUROIMAGE, V20, P265, DOI 10.1016/S1053-8119(03)00258-1
   Langner G, 2009, FRONT INTEGR NEUROSC, V3, DOI 10.3389/neuro.07.027.2009
   Liégeois-Chauvel C, 1999, CEREB CORTEX, V9, P484, DOI 10.1093/cercor/9.5.484
   Liégeois-Chauvel C, 2001, ANN NY ACAD SCI, V930, P117, DOI 10.1111/j.1749-6632.2001.tb05728.x
   LIEGEOISCHAUVEL C, 1994, ELECTROEN CLIN NEURO, V92, P204, DOI 10.1016/0168-5597(94)90064-7
   Lütkenhöner B, 2003, NEUROIMAGE, V19, P935, DOI 10.1016/S1053-8119(03)00172-1
   Lutkenhoner B, 1998, AUDIOL NEURO-OTOL, V3, P191, DOI 10.1159/000013790
   Malach R, 2002, TRENDS COGN SCI, V6, P176, DOI 10.1016/S1364-6613(02)01870-3
   MERZENICH MM, 1973, BRAIN RES, V50, P275, DOI 10.1016/0006-8993(73)90731-2
   MERZENICH MM, 1973, BRAIN RES, V63, P343, DOI 10.1016/0006-8993(73)90101-7
   Moerel M, 2013, J NEUROSCI, V33, P11888, DOI 10.1523/JNEUROSCI.5306-12.2013
   Moerel M, 2012, J NEUROSCI, V32, P14205, DOI 10.1523/JNEUROSCI.1388-12.2012
   MOREL A, 1993, J COMP NEUROL, V335, P437, DOI 10.1002/cne.903350312
   Morosan P, 2005, ANAT EMBRYOL, V210, P401, DOI 10.1007/s00429-005-0029-1
   Morosan P, 2001, NEUROIMAGE, V13, P684, DOI 10.1006/nimg.2000.0715
   Nieuwenhuys R, 2013, BRAIN STRUCT FUNCT, V218, P303, DOI 10.1007/s00429-012-0460-z
   Nourski KV, 2014, CEREB CORTEX, V24, P340, DOI 10.1093/cercor/bhs314
   Ochiai T, 2004, NEUROIMAGE, V22, P706, DOI 10.1016/j.neuroimage.2004.01.023
   PANTEV C, 1988, ELECTROEN CLIN NEURO, V69, P160, DOI 10.1016/0013-4694(88)90211-8
   PANTEV C, 1995, ELECTROEN CLIN NEURO, V94, P26, DOI 10.1016/0013-4694(94)00209-4
   Penhune VB, 1996, CEREB CORTEX, V6, P661, DOI 10.1093/cercor/6.5.661
   Petkov CI, 2008, NAT NEUROSCI, V11, P367, DOI 10.1038/nn2043
   Petkov CI, 2006, PLOS BIOL, V4, P1213, DOI 10.1371/journal.pbio.0040215
   Rademacher J, 2001, NEUROIMAGE, V13, P669, DOI 10.1006/nimg.2000.0714
   Rauschecker JP, 2004, J NEUROPHYSIOL, V91, P2578, DOI 10.1152/jn.00834.2003
   RAUSCHECKER JP, 1995, SCIENCE, V268, P111, DOI 10.1126/science.7701330
   REALE RA, 1980, J COMP NEUROL, V192, P265, DOI 10.1002/cne.901920207
   Remedios R, 2009, P NATL ACAD SCI USA, V106, P18010, DOI 10.1073/pnas.0909756106
   Rivier F, 1997, NEUROIMAGE, V6, P288, DOI 10.1006/nimg.1997.0304
   Roberts TPL, 1996, NEUROREPORT, V7, P1138, DOI 10.1097/00001756-199604260-00007
   ROMANI GL, 1982, SCIENCE, V216, P1339, DOI 10.1126/science.7079770
   Rothschild G, 2010, NAT NEUROSCI, V13, P353, DOI 10.1038/nn.2484
   Saenz M, 2014, HEARING RES, V307, P42, DOI 10.1016/j.heares.2013.07.016
   Santoro R, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003412
   Scherg M., 1989, ADV BIOMAGNETISM, P97, DOI DOI 10.1007/978-1-4613-0581-1_11
   Schönwiesner M, 2009, P NATL ACAD SCI USA, V106, P14611, DOI 10.1073/pnas.0907682106
   Schönwiesner M, 2002, NEUROIMAGE, V17, P1144, DOI 10.1006/nimg.2002.1250
   Schreiner CE, 2007, NEURON, V56, P356, DOI 10.1016/j.neuron.2007.10.013
   Scott SK, 2000, BRAIN, V123, P2400, DOI 10.1093/brain/123.12.2400
   Sereno MI, 2013, CEREB CORTEX, V23, P2261, DOI 10.1093/cercor/bhs213
   SERENO MI, 1995, SCIENCE, V268, P889, DOI 10.1126/science.7754376
   Sereno MI, 2001, SCIENCE, V294, P1350, DOI 10.1126/science.1063695
   SHANNON RV, 1995, SCIENCE, V270, P303, DOI 10.1126/science.270.5234.303
   Sigalovsky IS, 2006, NEUROIMAGE, V32, P1524, DOI 10.1016/j.neuroimage.2006.05.023
   STEINMETZ H, 1990, BRAIN LANG, V39, P357, DOI 10.1016/0093-934X(90)90145-7
   Striem-Amit E, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017832
   Sweet RA, 2005, J COMP NEUROL, V491, P270, DOI 10.1002/cne.20702
   Talavage TM, 2012, NEUROIMAGE, V62, P641, DOI 10.1016/j.neuroimage.2012.01.006
   Talavage TM, 2004, J NEUROPHYSIOL, V91, P1282, DOI 10.1152/jn.01125.2002
   Talavage TM, 2000, HEARING RES, V150, P225, DOI 10.1016/S0378-5955(00)00203-3
   Van Essen D.C., 2004, VISUAL NEUROSCIENCES, P507
   von Economo C, 1930, Z GESAMTE NEUROL PSY, V130, P678
   Wallace MN, 2002, EXP BRAIN RES, V143, P499, DOI 10.1007/s00221-002-1014-z
   Wessinger CM, 1997, HUM BRAIN MAPP, V5, P18, DOI 10.1002/(SICI)1097-0193(1997)5:1<18::AID-HBM3>3.0.CO;2-Q
   Wienbruch C, 2006, NEUROIMAGE, V33, P180, DOI 10.1016/j.neuroimage.2006.06.023
   WITELSON SF, 1973, BRAIN, V96, P641, DOI 10.1093/brain/96.3.641
   Woods DL, 2010, FRONT SYST NEUROSCI, V4, DOI 10.3389/fnsys.2010.00155
   Woods DL, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0005183
   ZATORRE RJ, 1988, J ACOUST SOC AM, V84, P566, DOI 10.1121/1.396834
   Zatorre RJ, 2001, CEREB CORTEX, V11, P946, DOI 10.1093/cercor/11.10.946
   Zatorre RJ, 2002, TRENDS COGN SCI, V6, P37, DOI 10.1016/S1364-6613(00)01816-7
   Zilles K, 2002, EUR NEUROPSYCHOPHARM, V12, P587, DOI 10.1016/S0924-977X(02)00108-6
   Zilles K, 2009, CURR OPIN NEUROL, V22, P331, DOI 10.1097/WCO.0b013e32832d95db
NR 115
TC 147
Z9 169
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
J9 FRONT NEUROSCI-SWITZ
JI Front. Neurosci.
PD JUL 29
PY 2014
VL 8
AR 225
DI 10.3389/fnins.2014.00225
PG 14
UT WOS:000346503100002
DA 2024-04-26
ER

PT J
AU Ellis, HD
   Jones, DM
   Mosdell, N
AF Ellis, HD
   Jones, DM
   Mosdell, N
TI Intra- and inter-modal repetition priming of familiar faces and voices
SO BRITISH JOURNAL OF PSYCHOLOGY
LA English
DT Article
ID INTERACTIVE ACTIVATION MODEL; RECOGNITION; FACILITATION; IDENTITY
AB Two experiments explored repetition priming for familiar voices and faces. Expt 1 revealed that, like faces, prior exposure to a voice in a gender judgment task speeds its subsequent classification as familiar or unfamiliar, some minutes later. Faces and voices do not prime one another, however; a result consistent with the notion that evidence is initially accumulated separately for voices and faces. In Expt 2, a prediction derived from the IAC model of Burton, Bruce & Johnston (1990) was explored. The results confirmed that inter-modal repetition priming occurs when the interval between exposures to different personal identification stimuli are separated by a short SOA. This result is consistent with similar ones reported by Calder (1993) and Young, Flude, Hellawell Be Ellis (1994) for face-name combinations.
RP Ellis, HD (corresponding author), UNIV WALES COLL CARDIFF,SCH PSYCHOL,CARDIFF CF1 3YG,S GLAM,WALES.
CR BRUCE V, 1983, PHILOS T ROY SOC B, V302, P423, DOI 10.1098/rstb.1983.0065
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   BRUCE V, 1985, BRIT J PSYCHOL, V76, P373, DOI 10.1111/j.2044-8295.1985.tb01960.x
   BRUCE V, 1992, PHILOS T ROY SOC B, V335, P121, DOI 10.1098/rstb.1992.0015
   BRUCE V, 1986, Q J EXP PSYCHOL-A, V38, P125, DOI 10.1080/14640748608401588
   BURTON AM, 1990, BRIT J PSYCHOL, V81, P361, DOI 10.1111/j.2044-8295.1990.tb02367.x
   CALDER AJ, 1993, THESIS DURHAM U
   CLARKE R, 1983, Q J EXP PSYCHOL-A, V35, P79, DOI 10.1080/14640748308402118
   ELLIS AW, 1992, PHILOS T ROY SOC B, V335, P113, DOI 10.1098/rstb.1992.0014
   ELLIS AW, 1990, Q J EXP PSYCHOL-A, V42, P495, DOI 10.1080/14640749008401234
   ELLIS AW, 1987, Q J EXP PSYCHOL-A, V39, P193, DOI 10.1080/14640748708401784
   ELLIS AW, 1987, MODELLING COGNITION
   Ellis H.D., 1986, NEUROPSYCHOLOGY FACE
   ELLIS HD, 1993, BRIT J PSYCHOL, V84, P101, DOI 10.1111/j.2044-8295.1993.tb02465.x
   ELLIS HD, 1994, R000232841 ESRC
   HANLEY JR, 1988, MEM COGNITION, V16, P545, DOI 10.3758/BF03197056
   Hay D.C., 1982, NORMALITY PATHOLOGY, P173
   LUPKER SJ, 1985, PROGR PSYCHOL LANGUA, V1
   MCCLELLAND JL, 1981, PSYCHOL REV, V88, P375, DOI 10.1037/0033-295X.88.5.375
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   MEYER DE, 1971, J EXP PSYCHOL, V90, P227, DOI 10.1037/h0031564
   POSNER M, 1975, ATTENTION PERFORMANC, V5
   SCHACTER DL, 1987, J EXP PSYCHOL LEARN, V13, P501, DOI 10.1037/0278-7393.13.3.501
   VANDERWART M, 1984, J VERB LEARN VERB BE, V23, P67, DOI 10.1016/S0022-5371(84)90509-7
   YARMEY AD, 1973, MEM COGNITION, V1, P287, DOI 10.3758/BF03198110
   Young A.W., 1989, HDB RES FACE PROCESS
   YOUNG AW, 1992, PHILOS T ROY SOC B, V335, P47, DOI 10.1098/rstb.1992.0006
   YOUNG AW, 1994, BRIT J PSYCHOL, V85, P393, DOI 10.1111/j.2044-8295.1994.tb02531.x
   YOUNG AW, 1988, Q J EXP PSYCHOL-A, V40, P561, DOI 10.1080/02724988843000087
NR 29
TC 110
Z9 120
PU BRITISH PSYCHOLOGICAL SOC
PI LEICESTER
PA ST ANDREWS HOUSE, 48 PRINCESS RD EAST, LEICESTER, LEICS, ENGLAND LE1 7DR
J9 BRIT J PSYCHOL
JI Br. J. Psychol.
PD FEB
PY 1997
VL 88
BP 143
EP 156
DI 10.1111/j.2044-8295.1997.tb02625.x
PN 1
PG 14
UT WOS:A1997WL02300009
DA 2024-04-26
ER

PT J
AU Neuner, F
   Schweinberger, SR
AF Neuner, F
   Schweinberger, SR
TI Neuropsychological impairments in the recognition of faces, voices, and
   personal names
SO BRAIN AND COGNITION
LA English
DT Article
ID RIGHT-HEMISPHERE; FAMILIAR FACES; OBJECT AGNOSIA; BRAIN INJURY;
   PROSOPAGNOSIA; MEMORY; DISCRIMINATION; PERCEPTION; ANOMIA
AB In order to determine the dissociability of face, voice, and personal name recognition, we studied the performance of 36 brain-lesioned patients and 20 control subjects. Participants performed familiarity decisions for portraits, voice samples, and written names of celebrities and unfamiliar people. In those patients who displayed significant impairments in any of these tests, the specificity of these impairments was tested using corresponding object recognition tests (with pictures of objects, environmental sounds, or written common words as stimuli). The results showed that 58% of the patients were significantly impaired in at least one test of person recognition. Moreover, 28% of the patients showed impairments that appeared to be specific for people (i.e., performance was preserved in the corresponding object recognition test). Three patients showed a deficit that appeared to be confined to the recognition of familiar voices, a pattern that was not described previously. Results were generally consistent with the assumption that impairments in face, voice, and name recognition are dissociable from one another. In contrast, there was no clear evidence for a dissociability between deficits in face and voice naming. The results further suggest that (a) impairments in person recognition after brain lesions may be more common than was thought previously and (b) the patterns of impairment that were observed can be interpreted using current cognitive models of person recognition (Bruce & Young, 1986; Burton, Bruce, & Johnston, 1990). (C) 2000 Academic Press.
C1 Univ Glasgow, Dept Psychol, Glasgow G12 8QQ, Lanark, Scotland.
   Neurol Rehabil Zentrum Godeshohe, Bonn, Germany.
   Univ Konstanz, Erlangen, Germany.
   Kliniken Europakanai, Erlangen, Germany.
RP Schweinberger, SR (corresponding author), Univ Glasgow, Dept Psychol, 58 Hillhead St, Glasgow G12 8QQ, Lanark, Scotland.
CR Alexander M.P., 1983, Localization in Neurology, P393
   ASSAL G, 1981, REV NEUROL, V137, P255
   Benton A. L., 1994, Contributions to neuropsychological assessment: A clinical manual
   BENTON AL, 1972, J NEUROL SCI, V15, P167, DOI 10.1016/0022-510X(72)90004-4
   BODAMER J, 1947, Arch Psychiatr Nervenkr Z Gesamte Neurol Psychiatr, V118, P6, DOI 10.1007/BF00352849
   Bornstein B, 1969, Cortex, V5, P164
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   BRUYER R, 1983, BRAIN COGNITION, V2, P257, DOI 10.1016/0278-2626(83)90014-3
   BURTON AM, 1990, BRIT J PSYCHOL, V81, P361, DOI 10.1111/j.2044-8295.1990.tb02367.x
   CARNEY R, 1993, COGNITIVE NEUROPSYCH, V10, P185, DOI 10.1080/02643299308253460
   COLE M, 1964, NEUROPSYCHOLOGIA, V2, P237, DOI 10.1016/0028-3932(64)90008-9
   DAMASIO AR, 1990, ANNU REV NEUROSCI, V13, P89, DOI 10.1146/annurev.neuro.13.1.89
   DAMASIO AR, 1982, NEUROLOGY, V32, P89
   DERENZI E, 1994, NEUROPSYCHOLOGIA, V32, P893, DOI 10.1016/0028-3932(94)90041-8
   DERENZI E, 1991, CORTEX, V27, P213, DOI 10.1016/S0010-9452(13)80125-6
   Ellis HD, 1997, BRIT J PSYCHOL, V88, P143, DOI 10.1111/j.2044-8295.1997.tb02625.x
   FARAH MJ, 1995, NEUROPSYCHOLOGIA, V33, P661, DOI 10.1016/0028-3932(95)00002-K
   FARAH MJ, 1991, COGNITIVE NEUROPSYCH, V8, P1, DOI 10.1080/02643299108253364
   FERY P, 1995, CORTEX, V31, P191, DOI 10.1016/S0010-9452(13)80117-7
   HANLEY JR, 1989, COGNITIVE NEUROPSYCH, V6, P179, DOI 10.1080/02643298908253418
   HANLEY JR, IN PRESS Q J EXPT PS
   HECAEN H, 1962, ARCH NEUROL-CHICAGO, V7, P92, DOI 10.1001/archneur.1962.04210020014002
   Henke K, 1998, CORTEX, V34, P289, DOI 10.1016/S0010-9452(08)70756-1
   LHERMITTE F, 1973, BRAIN, V96, P695, DOI 10.1093/brain/96.4.695
   MALONE DR, 1982, J NEUROL NEUROSUR PS, V45, P820, DOI 10.1136/jnnp.45.9.820
   MCNEIL JE, 1993, Q J EXP PSYCHOL-A, V46, P1, DOI 10.1080/14640749308401064
   Moscovitch M, 1997, J COGNITIVE NEUROSCI, V9, P555, DOI 10.1162/jocn.1997.9.5.555
   NEWCOMBE F, 1989, NEUROPSYCHOLOGIA, V27, P179, DOI 10.1016/0028-3932(89)90170-X
   PARRY FM, 1991, J CLIN EXP NEUROPSYC, V13, P545, DOI 10.1080/01688639108401070
   PERETZ I, 1994, BRAIN, V117, P1283, DOI 10.1093/brain/117.6.1283
   ROSCH E, 1976, COGNITIVE PSYCHOL, V8, P382, DOI 10.1016/0010-0285(76)90013-X
   SASLOVE H, 1980, J APPL PSYCHOL, V65, P111, DOI 10.1037/0021-9010.65.1.111
   SCHWEICH M, 1993, COGN NEUROPSYCHOL, V10, P529, DOI 10.1080/02643299308253472
   SCHWEINBERGER SR, 1991, NEUROPSYCHOLOGIA, V29, P389, DOI 10.1016/0028-3932(91)90027-6
   Schweinberger SR, 1997, J SPEECH LANG HEAR R, V40, P453, DOI 10.1044/jslhr.4002.453
   Schweinberger SR, 1997, Q J EXP PSYCHOL-A, V50, P498, DOI 10.1080/027249897391991
   SCHWEINBERGER SR, 1995, CORTEX, V31, P521
   SCHWEINBERGER SR, 1995, BRAIN COGNITION, V28, P23
   SCHWEINBERGER SR, 1993, GERMAN J PSYCHOL, V17, P240
   Semenza C, 1995, NEUROCASE, V1, P183, DOI 10.1093/neucas/1.2.183
   TOVEE MJ, 1993, COGN NEUROPSYCHOL, V10, P505, DOI 10.1080/02643299308253471
   Valentine T., 1991, EUR J COGN PSYCHOL, V3, P147, DOI DOI 10.1080/09541449108406224
   Van Lancker D R, 1982, Brain Cogn, V1, P185, DOI 10.1016/0278-2626(82)90016-1
   VANLANCKER D, 1990, BRAIN LANG, V39, P511, DOI 10.1016/0093-934X(90)90159-E
   VANLANCKER D, 1987, NEUROPSYCHOLOGIA, V25, P829, DOI 10.1016/0028-3932(87)90120-5
   VANLANCKER D, 1991, BRAIN COGNITION, V17, P64, DOI 10.1016/0278-2626(91)90067-I
   VANLANCKER D, 1991, CLIN APHASIOLOGY, V20, P181
   VANLANCKER DR, 1989, J CLIN EXP NEUROPSYC, V11, P665, DOI 10.1080/01688638908400923
   VANLANCKER DR, 1988, CORTEX, V24, P195, DOI 10.1016/S0010-9452(88)80029-7
   YOUNG AW, 1992, PHILOS T ROY SOC B, V335, P47, DOI 10.1098/rstb.1992.0006
   YOUNG AW, 1993, BRAIN, V116, P941, DOI 10.1093/brain/116.4.941
NR 51
TC 87
Z9 92
PU ACADEMIC PRESS INC
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
J9 BRAIN COGNITION
JI Brain Cogn.
PD DEC
PY 2000
VL 44
IS 3
BP 342
EP 366
DI 10.1006/brcg.1999.1196
PG 25
UT WOS:000165877300003
DA 2024-04-26
ER

PT J
AU Frühholz, S
   Schweinberger, SR
AF Fruhholz, Sascha
   Schweinberger, Stefan R.
TI Nonverbal auditory communication - Evidence for integrated neural
   systems for voice signal production and perception
SO PROGRESS IN NEUROBIOLOGY
LA English
DT Review
DE Communication; Voice; Auditory system; Neural network; Nonverbal
AB While humans have developed a sophisticated and unique system of verbal auditory communication, they also share a more common and evolutionarily important nonverbal channel of voice signaling with many other mammalian and vertebrate species. This nonverbal communication is mediated and modulated by the acoustic properties of a voice signal, and is a powerful - yet often neglected - means of sending and perceiving socially relevant information. From the viewpoint of dyadic (involving a sender and a signal receiver) voice signal communication, we discuss the integrated neural dynamics in primate nonverbal voice signal production and perception. Most previous neurobiological models of voice communication modelled these neural dynamics from the limited perspective of either voice production or perception, largely disregarding the neural and cognitive commonalities of both functions. Taking a dyadic perspective on nonverbal communication, however, it turns out that the neural systems for voice production and perception are surprisingly similar. Based on the interdependence of both production and perception functions in communication, we first propose a re-grouping of the neural mechanisms of communication into auditory, limbic, and paramotor systems, with special consideration for a subsidiary basal-ganglia-centered system. Second, we propose that the similarity in the neural systems involved in voice signal production and perception is the result of the co-evolution of nonverbal voice production and perception systems promoted by their strong interdependence in dyadic interactions.
C1 [Fruhholz, Sascha] Univ Zurich, Dept Psychol, Binzmuhlestr 14,Box 18, CH-8050 Zurich, Switzerland.
   [Fruhholz, Sascha] Univ Oslo, Dept Psychol, N-0373 Oslo, Norway.
   [Fruhholz, Sascha] Univ Zurich, Neurosci Ctr Zurich, CH-8057 Zurich, Switzerland.
   [Fruhholz, Sascha] Swiss Fed Inst Technol, CH-8057 Zurich, Switzerland.
   [Schweinberger, Stefan R.] Friedrich Schiller Univ, Dept Gen Psychol & Cognit Neurosci, D-07743 Jena, Germany.
   [Schweinberger, Stefan R.] Friedrich Schiller Univ, Voice Res Unit, D-07743 Jena, Germany.
   [Schweinberger, Stefan R.] Univ Western Australia, Sch Psychol, ARC Ctr Excellence Cognit & Its Disorders, Nedlands, WA 6009, Australia.
RP Frühholz, S (corresponding author), Univ Zurich, Dept Psychol, Binzmuhlestr 14,Box 18, CH-8050 Zurich, Switzerland.
EM sascha.fruehholz@uzh.ch
CR Abrams DA, 2016, P NATL ACAD SCI USA, V113, P6295, DOI 10.1073/pnas.1602948113
   Ackermann H, 2014, BEHAV BRAIN SCI, V37, DOI 10.1017/S0140525X13003099
   Agamaite JA, 2015, J ACOUST SOC AM, V138, P2906, DOI 10.1121/1.4934268
   Alba-Ferrara L, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0028701
   Alderson-Day B, 2016, SOC COGN AFFECT NEUR, V11, P110, DOI 10.1093/scan/nsv094
   AMARAL DG, 1984, J COMP NEUROL, V230, P465, DOI 10.1002/cne.902300402
   Amodio DM, 2006, NAT REV NEUROSCI, V7, P268, DOI 10.1038/nrn1884
   Andics A, 2014, CURR BIOL, V24, P574, DOI 10.1016/j.cub.2014.01.058
   Anikin A, 2018, Q J EXP PSYCHOL, V71, P622, DOI 10.1080/17470218.2016.1270976
   Anikin A, 2018, J NONVERBAL BEHAV, V42, P53, DOI 10.1007/s10919-017-0267-y
   Anolli L, 1997, J NONVERBAL BEHAV, V21, P259, DOI 10.1023/A:1024916214403
   [Anonymous], 1978, BEHAV ECOLOGY EVOLUT
   Anzellotti S, 2017, CORTEX, V89, P85, DOI 10.1016/j.cortex.2017.01.013
   Argyle M, 1972, NONVERBAL COMMUNICAT, Vxiii, P443
   Arnal LH, 2015, CURR BIOL, V25, P2051, DOI 10.1016/j.cub.2015.06.043
   Arnold C, 2014, NEUROIMAGE-CLIN, V4, P82, DOI 10.1016/j.nicl.2013.10.016
   Ashby FG, 2010, TRENDS COGN SCI, V14, P208, DOI 10.1016/j.tics.2010.02.001
   Averbeck BB, 2004, J NEUROPHYSIOL, V91, P2897, DOI 10.1152/jn.01103.2003
   Babel M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088616
   Bachorowski JA, 2003, ANN NY ACAD SCI, V1000, P244, DOI 10.1196/annals.1280.012
   Baltaxe, 1985, COMMUNICATION PROBLE, DOI [DOI 10.1007/978-1-4757-4806-2_7, 10.1007/978-1-4757-4806-2_7]
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Barsalou LW, 2009, PHILOS T R SOC B, V364, P1281, DOI 10.1098/rstb.2008.0319
   Bauer EE, 2008, J NEUROSCI, V28, P1509, DOI 10.1523/JNEUROSCI.3838-07.2008
   Behroozmand R, 2009, CLIN NEUROPHYSIOL, V120, P1303, DOI 10.1016/j.clinph.2009.04.022
   Belin P, 2003, NEUROREPORT, V14, P2105, DOI 10.1097/00001756-200311140-00019
   Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008
   Belin P, 2000, NATURE, V403, P309, DOI 10.1038/35002078
   Belin P., 2013, INTEGRATING FACE VOI, DOI [10.1007/978-1-4614-3585-3, DOI 10.1007/978-1-4614-3585-3]
   Belin P, 2018, HEARING RES, V366, P65, DOI 10.1016/j.heares.2018.04.010
   Belin P, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0185651
   Benetti S, 2017, P NATL ACAD SCI USA, V114, pE6437, DOI 10.1073/pnas.1618287114
   Bestelmeyer PEG, 2015, CEREB CORTEX, V25, P3953, DOI 10.1093/cercor/bhu282
   Bestelmeyer PEG, 2014, J NEUROSCI, V34, P8098, DOI 10.1523/JNEUROSCI.4820-13.2014
   Bestelmeyer PEG, 2012, CEREB CORTEX, V22, P1263, DOI 10.1093/cercor/bhr204
   Blank H, 2011, J NEUROSCI, V31, P12906, DOI 10.1523/JNEUROSCI.2091-11.2011
   Blasi A, 2011, CURR BIOL, V21, P1220, DOI 10.1016/j.cub.2011.06.009
   Bolhuis JJ, 2006, NAT REV NEUROSCI, V7, P347, DOI 10.1038/nrn1904
   Bolhuis JJ, 2015, NEUROSCI BIOBEHAV R, V50, P41, DOI 10.1016/j.neubiorev.2014.11.019
   Bolhuis JJ, 2010, NAT REV NEUROSCI, V11, P747, DOI 10.1038/nrn2931
   Bolhuis JJ, 2009, NATURE, V458, P832, DOI 10.1038/458832a
   Borkowska B, 2011, ANIM BEHAV, V82, P55, DOI 10.1016/j.anbehav.2011.03.024
   Bornkessel-Schlesewsky I, 2015, TRENDS COGN SCI, V19, P142, DOI 10.1016/j.tics.2014.12.008
   Bourguignon M, 2013, HUM BRAIN MAPP, V34, P314, DOI 10.1002/hbm.21442
   Brainard MS, 2000, NAT REV NEUROSCI, V1, P31, DOI 10.1038/35036205
   Brainard MS, 2000, NATURE, V404, P762, DOI 10.1038/35008083
   Brosch T, 2009, J COGNITIVE NEUROSCI, V21, P1670, DOI 10.1162/jocn.2009.21110
   Bruckert L, 2010, CURR BIOL, V20, P116, DOI 10.1016/j.cub.2009.11.034
   CAEKEBEKE JFV, 1991, J NEUROL NEUROSUR PS, V54, P145, DOI 10.1136/jnnp.54.2.145
   Cäsar C, 2013, BIOL LETTERS, V9, DOI 10.1098/rsbl.2013.0535
   Calvert GA, 1997, SCIENCE, V276, P593, DOI 10.1126/science.276.5312.593
   Capilla A, 2013, CEREB CORTEX, V23, P1388, DOI 10.1093/cercor/bhs119
   Ceravolo L, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00216
   Chandrasekaran C, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002165
   Charest I, 2009, BMC NEUROSCI, V10, DOI 10.1186/1471-2202-10-127
   Charlton BD, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12739
   Cheang HS, 2008, SPEECH COMMUN, V50, P366, DOI 10.1016/j.specom.2007.11.003
   Cheung C, 2016, ELIFE, V5, DOI [10.7554/eLife.12577, 10.7554/eLife.17181]
   Cheung SW, 2005, J NEUROSCI, V25, P2490, DOI 10.1523/JNEUROSCI.5289-04.2005
   Choi JY, 2015, J NEUROPHYSIOL, V114, P274, DOI 10.1152/jn.00228.2015
   Clarke E, 2015, BMC EVOL BIOL, V15, DOI 10.1186/s12862-015-0332-2
   Constantinidis Christos, 2004, Cognitive Affective & Behavioral Neuroscience, V4, P444, DOI 10.3758/CABN.4.4.444
   CREUTZFELDT O, 1989, EXP BRAIN RES, V77, P451, DOI 10.1007/BF00249600
   D'Ausillo A, 2009, CURR BIOL, V19, P381, DOI 10.1016/j.cub.2009.01.017
   de la Torre S, 2009, AM J PRIMATOL, V71, P333, DOI 10.1002/ajp.20657
   De Lucia M, 2010, J NEUROSCI, V30, P11210, DOI 10.1523/JNEUROSCI.2239-10.2010
   Doupe AJ, 1999, ANNU REV NEUROSCI, V22, P567, DOI 10.1146/annurev.neuro.22.1.567
   Dricu M, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16594-w
   Dricu M, 2016, NEUROSCI BIOBEHAV R, V71, P810, DOI 10.1016/j.neubiorev.2016.10.020
   Egnor SER, 2004, TRENDS NEUROSCI, V27, P649, DOI 10.1016/j.tins.2004.08.009
   Ehret Guenter, 2006, P85
   Eliades SJ, 2008, NATURE, V453, P1102, DOI 10.1038/nature06910
   Eliades SJ, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04961-8
   Eliades SJ, 2012, J NEUROSCI, V32, P10737, DOI 10.1523/JNEUROSCI.3448-11.2012
   Ellis HD, 1997, P ROY SOC B-BIOL SCI, V264, P1085, DOI 10.1098/rspb.1997.0150
   Engelberg JWM, 2019, Q J EXP PSYCHOL, V72, P1889, DOI 10.1177/1747021818816307
   Ethofer T, 2012, CEREB CORTEX, V22, P191, DOI 10.1093/cercor/bhr113
   Fichtel C, 2005, ANIM BEHAV, V70, P165, DOI 10.1016/j.anbehav.2004.09.020
   Fitch WTS, 2003, METH NE FRO NEUROSCI, P87
   Ford JM, 2007, AM J PSYCHIAT, V164, P458, DOI 10.1176/appi.ajp.164.3.458
   Ford JM, 2009, SCHIZOPHRENIA BULL, V35, P58, DOI 10.1093/schbul/sbn140
   Fraccaro P., 2011, Journal of Evolutionary Psychology, V9, P57, DOI DOI 10.1556/JEP.9.2011.33.1
   Fraccaro PJ, 2013, ANIM BEHAV, V85, P127, DOI 10.1016/j.anbehav.2012.10.016
   Friederici AD, 2012, TRENDS COGN SCI, V16, P262, DOI 10.1016/j.tics.2012.04.001
   Friederici AD, 2011, PHYSIOL REV, V91, P1357, DOI 10.1152/physrev.00006.2011
   Friston KJ, 2015, CORTEX, V68, P129, DOI 10.1016/j.cortex.2015.03.025
   Frühholz S, 2017, NEUROSCI BIOBEHAV R, V83, P516, DOI 10.1016/j.neubiorev.2017.09.009
   Frühholz S, 2015, CEREB CORTEX, V25, P2752, DOI 10.1093/cercor/bhu074
   Frühholz S, 2015, NEUROIMAGE, V109, P27, DOI 10.1016/j.neuroimage.2015.01.016
   Frühholz S, 2015, P NATL ACAD SCI USA, V112, P1583, DOI 10.1073/pnas.1411315112
   Frühholz S, 2014, BEHAV BRAIN SCI, V37, DOI 10.1017/S0140525X13004020
   Frühholz S, 2014, PROG NEUROBIOL, V123, P1, DOI 10.1016/j.pneurobio.2014.09.003
   Frühholz S, 2013, CORTEX, V49, P1394, DOI 10.1016/j.cortex.2012.08.003
   Frühholz S, 2013, NEUROSCI BIOBEHAV R, V37, P24, DOI 10.1016/j.neubiorev.2012.11.002
   Frühholz S, 2012, NEUROIMAGE, V62, P1658, DOI 10.1016/j.neuroimage.2012.06.015
   Fruhholz S, 2018, OXF HDB VOICE PERCEP, P430, DOI [10.1093/oxfordhb/9780198743187.013.19, DOI 10.1093/OXFORDHB/9780198743187.013.19]
   Frühholz S, 2016, NEUROIMAGE, V142, P602, DOI 10.1016/j.neuroimage.2016.08.023
   Frühholz S, 2016, SOC COGN AFFECT NEUR, V11, P1638, DOI 10.1093/scan/nsw066
   Frühholz S, 2016, NEUROSCI BIOBEHAV R, V68, P96, DOI 10.1016/j.neubiorev.2016.05.002
   Gainotti G, 2008, CORTEX, V44, P238, DOI 10.1016/j.cortex.2006.09.001
   Gazzola V, 2006, CURR BIOL, V16, P1824, DOI 10.1016/j.cub.2006.07.072
   Geiser E, 2012, J NEUROSCI, V32, P6177, DOI 10.1523/JNEUROSCI.5153-11.2012
   Ghazanfar AA, 2005, J NEUROSCI, V25, P5004, DOI 10.1523/JNEUROSCI.0799-05.2005
   Ghazanfar AA, 2008, J NEUROSCI, V28, P4457, DOI 10.1523/JNEUROSCI.0541-08.2008
   Ghazanfar AA, 2008, NAT NEUROSCI, V11, P382, DOI 10.1038/nn0408-382
   Ghazanfar AA, 2007, CURR BIOL, V17, P425, DOI 10.1016/j.cub.2007.01.029
   Goldman AI, 2005, COGNITION, V94, P193, DOI 10.1016/j.cognition.2004.01.005
   Grahn JA, 2007, J COGNITIVE NEUROSCI, V19, P893, DOI 10.1162/jocn.2007.19.5.893
   Grandjean D, 2005, NAT NEUROSCI, V8, P145, DOI 10.1038/nn1392
   Griffiths TD, 2002, TRENDS NEUROSCI, V25, P348, DOI 10.1016/S0166-2236(02)02191-4
   Grossmann T, 2006, DEVELOPMENTAL SCI, V9, P309, DOI 10.1111/j.1467-7687.2006.00494.x
   Grossmann T, 2010, NEURON, V65, P852, DOI 10.1016/j.neuron.2010.03.001
   Groyecka A, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.00778
   Hagan CC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070648
   Hage SR, 2016, TRENDS NEUROSCI, V39, P813, DOI 10.1016/j.tins.2016.10.006
   Hage SR, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3409
   Hage SR, 2013, J COGNITIVE NEUROSCI, V25, P1692, DOI 10.1162/jocn_a_00428
   Hahnloser RHR, 2010, CURR OPIN NEUROBIOL, V20, P332, DOI 10.1016/j.conb.2010.02.011
   Hall ML, 2004, BEHAV ECOL SOCIOBIOL, V55, P415, DOI 10.1007/s00265-003-0741-x
   Harnsberger JD, 2010, J VOICE, V24, P523, DOI 10.1016/j.jvoice.2009.01.003
   Hasan BAS, 2016, SCI REP-UK, V6, DOI 10.1038/srep37494
   Hass J, 2012, NEURAL COMPUT, V24, P1519, DOI 10.1162/NECO_a_00280
   Hauser MD, 2002, SCIENCE, V298, P1569, DOI 10.1126/science.298.5598.1569
   Hellbernd N, 2016, J MEM LANG, V88, P70, DOI 10.1016/j.jml.2016.01.001
   Hickok G, 2007, NAT REV NEUROSCI, V8, P393, DOI 10.1038/nrn2113
   Hickok G, 2012, NAT REV NEUROSCI, V13, P135, DOI 10.1038/nrn3158
   Hickok G, 2011, NEURON, V69, P407, DOI 10.1016/j.neuron.2011.01.019
   HOLLIEN H, 1987, J FORENSIC SCI, V32, P405
   Houde JF, 2015, CURR OPIN NEUROBIOL, V33, P174, DOI 10.1016/j.conb.2015.04.006
   Hugdahl K, 2009, SCAND J PSYCHOL, V50, P553, DOI 10.1111/j.1467-9450.2009.00775.x
   Hughes SM, 2010, J NONVERBAL BEHAV, V34, P155, DOI 10.1007/s10919-010-0087-9
   Jarvis ED, 2005, NAT REV NEUROSCI, V6, P151, DOI 10.1038/nrn1606
   Jarvis ED, 2007, J ORNITHOL, V148, pS35, DOI 10.1007/s10336-007-0243-0
   Jasmin K, 2019, NAT REV NEUROSCI, V20, P425, DOI 10.1038/s41583-019-0160-2
   Jessen S, 2011, NEUROIMAGE, V58, P665, DOI 10.1016/j.neuroimage.2011.06.035
   Jessen S, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0036070
   Jiang XM, 2015, CORTEX, V66, P9, DOI 10.1016/j.cortex.2015.02.002
   Joosten B, 2015, J MULTIMODAL USER IN, V9, P183, DOI 10.1007/s12193-015-0187-2
   JURGENS U, 1970, EXP BRAIN RES, V10, P532
   Jürgens U, 2002, NEUROSCI BIOBEHAV R, V26, P235, DOI 10.1016/S0149-7634(01)00068-9
   Kaplan JT, 2008, SOC COGN AFFECT NEUR, V3, P218, DOI 10.1093/scan/nsn014
   Kersken V, 2017, SCI REP-UK, V7, DOI 10.1038/srep41016
   Keysers C, 2010, NAT REV NEUROSCI, V11, P417, DOI 10.1038/nrn2833
   Klaas HS, 2015, FRONT BEHAV NEUROSCI, V9, DOI 10.3389/fnbeh.2015.00121
   Ko SJ, 2015, PSYCHOL SCI, V26, P3, DOI 10.1177/0956797614553009
   Ko SJ, 2009, PERS SOC PSYCHOL B, V35, P198, DOI 10.1177/0146167208326477
   Kojima S, 2013, P NATL ACAD SCI USA, V110, P4756, DOI 10.1073/pnas.1216308110
   Kokinous J, 2015, SOC COGN AFFECT NEUR, V10, P713, DOI 10.1093/scan/nsu105
   Korb S, 2015, SOC COGN AFFECT NEUR, V10, P1644, DOI 10.1093/scan/nsv051
   Korzyukov O, 2012, INT J PSYCHOPHYSIOL, V83, P71, DOI 10.1016/j.ijpsycho.2011.10.006
   Kotz SA, 2010, TRENDS COGN SCI, V14, P392, DOI 10.1016/j.tics.2010.06.005
   Kotz SA, 2009, CORTEX, V45, P982, DOI 10.1016/j.cortex.2009.02.010
   Kragel PA, 2016, ENEURO, V3, DOI 10.1523/ENEURO.0090-15.2016
   Kreifelts B, 2007, NEUROIMAGE, V37, P1445, DOI 10.1016/j.neuroimage.2007.06.020
   KROODSMA DE, 1984, ANIM BEHAV, V32, P395, DOI 10.1016/S0003-3472(84)80275-4
   Kumar S, 2007, PLOS COMPUT BIOL, V3, P977, DOI 10.1371/journal.pcbi.0030100
   Kumar S, 2016, J NEUROSCI, V36, P4492, DOI 10.1523/JNEUROSCI.4341-14.2016
   Kumar S, 2012, J NEUROSCI, V32, P14184, DOI 10.1523/JNEUROSCI.1759-12.2012
   Latinus M, 2013, CURR BIOL, V23, P1075, DOI 10.1016/j.cub.2013.04.055
   Latinus M, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00175
   Latinus M, 2010, BMC NEUROSCI, V11, DOI 10.1186/1471-2202-11-36
   Laukka P, 2011, COGN AFFECT BEHAV NE, V11, P413, DOI 10.3758/s13415-011-0032-3
   Lauterbach EC, 2013, NEUROSCI BIOBEHAV R, V37, P1893, DOI 10.1016/j.neubiorev.2013.03.002
   Lavan N, 2018, EVOL HUM BEHAV, V39, P139, DOI 10.1016/j.evolhumbehav.2017.11.002
   Leaver AM, 2016, J NEUROSCI, V36, P1416, DOI 10.1523/JNEUROSCI.0226-15.2016
   Leaver AM, 2010, J NEUROSCI, V30, P7604, DOI 10.1523/JNEUROSCI.0296-10.2010
   Leaver AM, 2009, J NEUROSCI, V29, P2477, DOI 10.1523/JNEUROSCI.4921-08.2009
   Lemus L, 2009, P NATL ACAD SCI USA, V106, P14640, DOI 10.1073/pnas.0907505106
   Leongómez JD, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179407
   Levréro F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms8609
   Lindblom B, 1996, J ACOUST SOC AM, V99, P1683, DOI 10.1121/1.414691
   Linville SE, 1996, J VOICE, V10, P190, DOI 10.1016/S0892-1997(96)80046-4
   Maguinness C, 2018, NEUROPSYCHOLOGIA, V116, P179, DOI 10.1016/j.neuropsychologia.2018.03.039
   Mantell JT, 2013, COGNITION, V127, P177, DOI 10.1016/j.cognition.2012.12.008
   Masaki H, 2001, NEUROREPORT, V12, P1851, DOI 10.1097/00001756-200107030-00018
   McAleer P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0090779
   Mechelli A, 2003, J COGNITIVE NEUROSCI, V15, P925, DOI 10.1162/089892903770007317
   Milesi V, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00275
   Mileva M, 2018, J EXP PSYCHOL HUMAN, V44, P128, DOI 10.1037/xhp0000439
   Miller CT, 2015, J NEUROPHYSIOL, V114, P1158, DOI 10.1152/jn.01003.2014
   Miller CT, 2010, FRONT INTEGR NEUROSC, V4, DOI 10.3389/fnint.2010.00128
   Miller CT, 2006, J COMP PHYSIOL A, V192, P27, DOI 10.1007/s00359-005-0043-z
   Mitchell RLC, 2007, NEUROIMAGE, V36, P1015, DOI 10.1016/j.neuroimage.2007.03.016
   Monetta L, 2008, J NEUROPSYCHOL, V2, P415, DOI 10.1348/174866407X216675
   Muñoz M, 2009, CEREB CORTEX, V19, P2114, DOI 10.1093/cercor/bhn236
   Munoz-Lopez MM, 2010, FRONT NEUROANAT, V4, DOI 10.3389/fnana.2010.00129
   Murray MM, 2006, J NEUROSCI, V26, P1293, DOI 10.1523/JNEUROSCI.4511-05.2006
   Nelson LR, 2016, HISPANIC J BEHAV SCI, V38, P166, DOI 10.1177/0739986316632319
   Niedenthal PM, 2007, SCIENCE, V316, P1002, DOI 10.1126/science.1136930
   O'Connor JJM, 2017, EVOL HUM BEHAV, V38, P506, DOI 10.1016/j.evolhumbehav.2017.03.001
   O'Connor JJM, 2011, EVOL PSYCHOL-US, V9, P64, DOI 10.1177/147470491100900109
   Oleszkiewicz A, 2017, PSYCHON B REV, V24, P856, DOI 10.3758/s13423-016-1146-y
   Olson IR, 2007, BRAIN, V130, P1718, DOI 10.1093/brain/awm052
   Oveis C, 2016, J EXP SOC PSYCHOL, V65, P109, DOI 10.1016/j.jesp.2016.04.005
   Panksepp J, 2003, SCIENCE, V302, P237, DOI 10.1126/science.1091062
   Pannese A, 2016, CORTEX, V85, P116, DOI 10.1016/j.cortex.2016.10.013
   Parkinson AL, 2013, NEUROPSYCHOLOGIA, V51, P1471, DOI 10.1016/j.neuropsychologia.2013.05.002
   Parkinson AL, 2012, NEUROIMAGE, V61, P314, DOI 10.1016/j.neuroimage.2012.02.068
   Parsons CE, 2014, SOC COGN AFFECT NEUR, V9, P977, DOI 10.1093/scan/nst076
   Parsons CE, 2012, ACTA PAEDIATR, V101, pE189, DOI 10.1111/j.1651-2227.2011.02554.x
   Pasternak T, 2005, NAT REV NEUROSCI, V6, P97, DOI [10.1038/nrn1603, 10.1038/nrn1637]
   Patel S, 2011, BIOL PSYCHOL, V87, P93, DOI 10.1016/j.biopsycho.2011.02.010
   Paulmann S, 2008, BRAIN LANG, V105, P59, DOI 10.1016/j.bandl.2007.11.005
   Paulmann S, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0017694
   Pell MD, 2006, PROG BRAIN RES, V156, P303, DOI 10.1016/S0079-6123(06)56017-0
   Pell MD, 2007, BRAIN LANG, V101, P64, DOI 10.1016/j.bandl.2006.10.003
   Pell MD, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0027256
   Pérez-Bellido A, 2018, CEREB CORTEX, V28, P3908, DOI 10.1093/cercor/bhx255
   Pernet CR, 2015, NEUROIMAGE, V119, P164, DOI 10.1016/j.neuroimage.2015.06.050
   Pernet CR, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00023
   Peron J, 2016, SOC COGN AFFECT NEUR, V11, P349, DOI 10.1093/scan/nsv118
   Péron J, 2012, MOVEMENT DISORD, V27, P186, DOI 10.1002/mds.24025
   Perrodin C, 2015, TRENDS COGN SCI, V19, P783, DOI 10.1016/j.tics.2015.09.002
   Perrodin C, 2011, CURR BIOL, V21, P1408, DOI 10.1016/j.cub.2011.07.028
   Petkov CI, 2008, NAT NEUROSCI, V11, P367, DOI 10.1038/nn2043
   Petkov Christopher I, 2012, Front Evol Neurosci, V4, P12, DOI 10.3389/fnevo.2012.00012
   PETRIDES M, 1988, J COMP NEUROL, V273, P52, DOI 10.1002/cne.902730106
   Pichon S, 2013, J NEUROSCI, V33, P1640, DOI 10.1523/JNEUROSCI.3530-12.2013
   Pisanski K, 2016, SCI REP-UK, V6, DOI 10.1038/srep34389
   POULSON CL, 1991, J EXP CHILD PSYCHOL, V51, P267, DOI 10.1016/0022-0965(91)90036-R
   Prather JF, 2008, NATURE, V451, P305, DOI 10.1038/nature06492
   Puts DA, 2012, J SEX RES, V49, P227, DOI 10.1080/00224499.2012.658924
   Pye A, 2015, COGNITION, V134, P245, DOI 10.1016/j.cognition.2014.11.001
   González IQ, 2011, BRAIN RES, V1407, P13, DOI 10.1016/j.brainres.2011.03.029
   Rakic T, 2011, BRIT J PSYCHOL, V102, P868, DOI 10.1111/j.2044-8295.2011.02051.x
   Rakic T, 2011, J PERS SOC PSYCHOL, V100, P16, DOI 10.1037/a0021522
   Rathelot JA, 2009, P NATL ACAD SCI USA, V106, P918, DOI 10.1073/pnas.0808362106
   Rauschecker JP, 2018, CURR OPIN BEHAV SCI, V21, P195, DOI 10.1016/j.cobeha.2018.06.003
   Rauschecker Josef P, 2012, Front Evol Neurosci, V4, P7, DOI 10.3389/fnevo.2012.00007
   Rauschecker JP, 2011, HEARING RES, V271, P16, DOI 10.1016/j.heares.2010.09.001
   Rauschecker JP, 2009, NAT NEUROSCI, V12, P718, DOI 10.1038/nn.2331
   Rauschecker JP, 2000, P NATL ACAD SCI USA, V97, P11800, DOI 10.1073/pnas.97.22.11800
   Rigoulot S, 2014, SPEECH COMMUN, V65, P36, DOI 10.1016/j.specom.2014.05.006
   Rigoulot S, 2014, BRAIN RES, V1565, P48, DOI 10.1016/j.brainres.2014.04.022
   Rizzolatti G, 2004, ANNU REV NEUROSCI, V27, P169, DOI 10.1146/annurev.neuro.27.070203.144230
   Rockwell P, 2000, J PSYCHOLINGUIST RES, V29, P483, DOI 10.1023/A:1005120109296
   Egnor SER, 2006, J EXP BIOL, V209, P3652, DOI 10.1242/jeb.02420
   Romanski LM, 1999, NAT NEUROSCI, V2, P1131, DOI 10.1038/16056
   Roy S, 2016, J NEUROSCI, V36, P12168, DOI 10.1523/JNEUROSCI.1646-16.2016
   Roy S, 2011, J EXP BIOL, V214, P3619, DOI 10.1242/jeb.056101
   Sadagopan S, 2015, SCI REP-UK, V5, DOI 10.1038/srep10950
   SCHERER KR, 1985, J PSYCHOLINGUIST RES, V14, P409, DOI 10.1007/BF01067884
   SCHERER KR, 1986, PSYCHOL BULL, V99, P143, DOI 10.1037/0033-2909.99.2.143
   Scheumann M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0091192
   Schirmer A, 2004, NEUROIMAGE, V21, P1114, DOI 10.1016/j.neuroimage.2003.10.048
   Schirmer A, 2017, TRENDS COGN SCI, V21, P216, DOI 10.1016/j.tics.2017.01.001
   Schirmer A, 2013, COGN AFFECT BEHAV NE, V13, P80, DOI 10.3758/s13415-012-0132-8
   Schirmer A, 2010, CLIN NEUROPHYSIOL, V121, P53, DOI 10.1016/j.clinph.2009.09.029
   Schomers MR, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00435
   Schroder C., 2008, Mov. Disord., V23, P824, DOI 10.1002/mds.21940
   Schroeder J, 2015, PSYCHOL SCI, V26, P877, DOI 10.1177/0956797615572906
   Schweinberger SR, 2001, NEUROPSYCHOLOGIA, V39, P921, DOI 10.1016/S0028-3932(01)00023-9
   Schweinberger SR, 2003, CORTEX, V39, P9, DOI 10.1016/S0010-9452(08)70071-6
   Schweinberger SR, 1997, J SPEECH LANG HEAR R, V40, P453, DOI 10.1044/jslhr.4002.453
   Schweinberger SR, 2008, CURR BIOL, V18, P684, DOI 10.1016/j.cub.2008.04.015
   Schweinberger SR, 2017, VIS COGN, V25, P589, DOI 10.1080/13506285.2016.1276110
   Schweinberger SR, 2014, WIRES COGN SCI, V5, P15, DOI 10.1002/wcs.1261
   Schweinberger SR, 2011, CORTEX, V47, P1026, DOI 10.1016/j.cortex.2010.11.011
   Scott BH, 2014, CURR BIOL, V24, P2767, DOI 10.1016/j.cub.2014.10.004
   Scott SK, 2019, SCIENCE, V366, P58, DOI 10.1126/science.aax0288
   SEYFARTH RM, 1980, SCIENCE, V210, P801, DOI 10.1126/science.7433999
   Sidtis DV, 2006, BRAIN LANG, V97, P135, DOI 10.1016/j.bandl.2005.09.001
   Sidtis John J., 2003, Seminars in Speech and Language, V24, P93, DOI 10.1055/s-2003-38901
   Silk JB, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0163978
   Simmonds AJ, 2014, J NEUROPHYSIOL, V112, P792, DOI 10.1152/jn.00901.2013
   Simonyan K, 2011, NEUROSCIENTIST, V17, P197, DOI 10.1177/1073858410386727
   Skuk VG, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0081691
   Sliwa J, 2011, P NATL ACAD SCI USA, V108, P1735, DOI 10.1073/pnas.1008169108
   Smiley JF, 2009, HEARING RES, V258, P37, DOI 10.1016/j.heares.2009.06.019
   Sodoyer D, 2009, J ACOUST SOC AM, V125, P1184, DOI 10.1121/1.3050257
   Solis MM, 2000, P NATL ACAD SCI USA, V97, P11836, DOI 10.1073/pnas.97.22.11836
   Stanley D, 1931, J FRANKL INST, V211, P405, DOI 10.1016/S0016-0032(31)90646-7
   Stephens GJ, 2010, P NATL ACAD SCI USA, V107, P14425, DOI 10.1073/pnas.1008662107
   Suga N., 2004, NAT REV NEUROSCI, V5, P1, DOI [10.1038/nrn1366-c3, DOI 10.1038/NRN1366-C3]
   Sulpizio S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0128882
   Tallal P, 2004, NAT REV NEUROSCI, V5, P721, DOI 10.1038/nrn1499
   Tigue CC, 2012, EVOL HUM BEHAV, V33, P210, DOI 10.1016/j.evolhumbehav.2011.09.004
   Toyomura A, 2007, NEUROSCIENCE, V146, P499, DOI 10.1016/j.neuroscience.2007.02.023
   Tremblay S, 2003, NATURE, V423, P866, DOI 10.1038/nature01710
   Tuomainen O., 2016, J ACOUST SOC AM, V140, P3444, DOI [10.1121/1.4971112, DOI 10.1121/1.4971112]
   VANLANCKER D, 1985, J PHONETICS, V13, P19, DOI 10.1016/S0095-4470(19)30723-5
   von Kriegstein K, 2008, P NATL ACAD SCI USA, V105, P6747, DOI 10.1073/pnas.0710826105
   von Kriegstein K, 2010, J NEUROSCI, V30, P629, DOI 10.1523/JNEUROSCI.2742-09.2010
   VONHOLST E, 1950, NATURWISSENSCHAFTEN, V37, P464, DOI 10.1007/BF00622503
   Walsh B, 2012, MOVEMENT DISORD, V27, P843, DOI 10.1002/mds.24888
   Wambacq IJA, 2004, NEUROREPORT, V15, P555, DOI 10.1097/00001756-200403010-00034
   Warren JE, 2006, J NEUROSCI, V26, P13067, DOI 10.1523/JNEUROSCI.3907-06.2006
   Watson R, 2014, J NEUROSCI, V34, P6813, DOI 10.1523/JNEUROSCI.4478-13.2014
   Wattendorf E, 2013, CEREB CORTEX, V23, P1280, DOI 10.1093/cercor/bhs094
   Weston PSJ, 2015, NEUROIMAGE, V105, P208, DOI 10.1016/j.neuroimage.2014.10.056
   Wild CJ, 2017, NEUROIMAGE, V157, P623, DOI 10.1016/j.neuroimage.2017.06.038
   Wilkins MR, 2013, TRENDS ECOL EVOL, V28, P156, DOI 10.1016/j.tree.2012.10.002
   WILLIAMS H, 1985, SCIENCE, V229, P279, DOI 10.1126/science.4012321
   Yeterian EH, 1998, J COMP NEUROL, V399, P384
   Young A., 2016, FACIAL EXPRESSION RE, P1, DOI [10.4324/9781315715933, DOI 10.4324/9781315715933]
   Yovel G, 2016, TRENDS COGN SCI, V20, P383, DOI 10.1016/j.tics.2016.02.005
   Zäske R, 2014, J NEUROSCI, V34, P10821, DOI 10.1523/JNEUROSCI.0581-14.2014
   Zäske R, 2011, HEARING RES, V282, P283, DOI 10.1016/j.heares.2011.06.008
   Zäske R, 2010, HEARING RES, V268, P38, DOI 10.1016/j.heares.2010.04.011
   Zäske R, 2009, EUR J NEUROSCI, V30, P527, DOI 10.1111/j.1460-9568.2009.06839.x
   Zhang CL, 2008, FORENSIC SCI INT, V175, P118, DOI 10.1016/j.forsciint.2007.05.019
   Zhao LY, 2019, P ROY SOC B-BIOL SCI, V286, DOI 10.1098/rspb.2019.0817
   Ziegler W, 2017, TRENDS NEUROSCI, V40, P458, DOI 10.1016/j.tins.2017.06.005
   Zürcher Y, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0222486
NR 304
TC 20
Z9 21
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
J9 PROG NEUROBIOL
JI Prog. Neurobiol.
PD APR
PY 2021
VL 199
AR 101948
DI 10.1016/j.pneurobio.2020.101948
EA FEB 2021
PG 19
UT WOS:000625009700001
DA 2024-04-26
ER

PT J
AU Gainotti, G
AF Gainotti, Guido
TI What the study of voice recognition in normal subjects and brain-damaged
   patients tells us about models of familiar people recognition
SO NEUROPSYCHOLOGIA
LA English
DT Article
DE Face and voice recognition disorders; Models of familiar people
   recognition; Right temporal lobe lesions;
   'Familiarity-only-experiences'with faces and voices
ID ANTERIOR TEMPORAL-LOBE; FUSIFORM FACE AREA; PROGRESSIVE PROSOPAGNOSIA;
   DEFECTIVE RECOGNITION; ONLY EXPERIENCES; ITALIAN NORMS; FAMOUS FACES;
   KNOWLEDGE; DEFICITS; AGNOSIA
AB In recent years it has been shown that a disorder in recognizing familiar people can be observed in patients with lesions affecting the anterior parts of the temporal lobes and that these disorders can be multi-modal, simultaneously affecting the visual, auditory and linguistic channels that allow person identification. Several authors have also shown that patients with right anterior temporal atrophy are more impaired in assessing familiarity and in retrieving person-specific semantic information from faces than from names, whereas the opposite pattern of performance can be observed in patients with left temporal lobe atrophy. Voice recognition disorders have been studied much less even despite their clinical and theoretical importance. The aim of the present review, therefore, was to compare recognition of familiar faces and voices, taking into account not only results obtained in individual patients with right anterior temporal lesions, but also those of group studies of unselected right- and left brain-damaged patients and results of experimental investigations conducted on face and voice recognition in normal subjects. Results of the review showed that: (1) voice recognition disorders are mainly due to right temporal lesions, similarly to face recognition disorders; (2) famous voice recognition disorders can be dissociated from unfamiliar voice discrimination impairments; (3) although face and voice recognition disorders tend to co-occur, they can also dissociate and in these patients there is a prevalent involvement of the right fusiform gyrus when face recognition disorders are on the foreground, and of the right superior temporal gyrus when voice recognition disorders are prominent; (4) normal subjects have greater difficulty evaluating familiarity and drawing semantic information from the voices than from the faces of celebrities. These data are at variance with models which assume that familiarity feelings may be generated at the level of person identity nodes (PINS) and that the latter may be considered as modality-free gateways to single semantic systems in which information about people is stored in an amodal format. (C) 2011 Elsevier Ltd. All rights reserved.
C1 Univ Cattolica, Policlin Gemelli, Serv Neuropsicol, Ctr Neuropsychol Res,Dept Neurosci, I-00168 Rome, Italy.
RP Gainotti, G (corresponding author), Univ Cattolica, Policlin Gemelli, Serv Neuropsicol, Ctr Neuropsychol Res,Dept Neurosci, Largo A Gemelli 8, I-00168 Rome, Italy.
EM gainotti@rm.unicatt.it
CR Acres K, 2009, NEUROPSYCHOLOGIA, V47, P1836, DOI 10.1016/j.neuropsychologia.2009.02.024
   Barsics C., 2010, CONSCIOUS COGNI 0407
   Belin P, 2000, NATURE, V403, P309, DOI 10.1038/35002078
   Belin P, 2006, PHILOS T R SOC B, V361, P2091, DOI 10.1098/rstb.2006.1933
   Bizzozero I, 2007, NEUROL SCI, V28, P16, DOI 10.1007/s10072-007-0743-y
   Bizzozero I, 2005, NEUROL SCI, V26, P95, DOI 10.1007/s10072-005-0442-5
   BOUDOURESQUES J, 1979, B ACAD NAT MED PARIS, V163, P695
   BREDART S, 1995, Q J EXP PSYCHOL-A, V48, P466, DOI 10.1080/14640749508401400
   Brédart S, 2009, EUR J COGN PSYCHOL, V21, P1013, DOI 10.1080/09541440802591821
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Burton AM, 1999, COGNITIVE SCI, V23, P1
   BURTON AM, 1990, BRIT J PSYCHOL, V81, P361, DOI 10.1111/j.2044-8295.1990.tb02367.x
   Busigny T, 2009, NEUROCASE, V15, P485, DOI 10.1080/13554790902971141
   Butler CR, 2009, COGN BEHAV NEUROL, V22, P73, DOI 10.1097/WNN.0b013e318197925d
   Chan D, 2009, BRAIN, V132, P1287, DOI 10.1093/brain/awp037
   Damasio H, 2004, COGNITION, V92, P179, DOI 10.1016/j.cognition.2002.07.001
   Damjanovic L, 2007, MEM COGNITION, V35, P1205, DOI 10.3758/BF03193594
   De Renzi E., 1986, ASPECTS FACE PROCESS, P243, DOI DOI 10.1007/978-94-009-4420-6
   DERENZI E, 1994, NEUROPSYCHOLOGIA, V32, P893, DOI 10.1016/0028-3932(94)90041-8
   EdwardsLee T, 1997, BRAIN, V120, P1027, DOI 10.1093/brain/120.6.1027
   ELLIS AW, 1989, BRAIN, V112, P1469, DOI 10.1093/brain/112.6.1469
   EVANS JJ, 1995, BRAIN, V118, P1, DOI 10.1093/brain/118.1.1
   Gainotti G, 2003, BRAIN, V126, P792, DOI 10.1093/brain/awg092
   Gainotti G, 2008, CORTEX, V44, P238, DOI 10.1016/j.cortex.2006.09.001
   Gainotti G, 2007, NEUROPSYCHOLOGIA, V45, P1591, DOI 10.1016/j.neuropsychologia.2006.12.013
   Gainotti G, 2010, BRAIN RES, V1307, P103, DOI 10.1016/j.brainres.2009.10.009
   Gauthier I, 2000, J COGNITIVE NEUROSCI, V12, P495, DOI 10.1162/089892900562165
   Gentileschi V, 2001, COGN NEUROPSYCHOL, V18, P439, DOI 10.1080/02643290125835
   Gentileschi V, 1999, NEUROCASE, V5, P407, DOI 10.1093/neucas/5.5.407
   Gorno-Tempini ML, 2004, CORTEX, V40, P631, DOI 10.1016/S0010-9452(08)70159-X
   Hailstone JC, 2010, NEUROPSYCHOLOGIA, V48, P1104, DOI 10.1016/j.neuropsychologia.2009.12.011
   Hanley JR, 2009, MEMORY, V17, P830, DOI 10.1080/09658210903264175
   Hanley JR, 1998, Q J EXP PSYCHOL-A, V51, P179, DOI 10.1080/713755751
   Hanley JR, 2000, Q J EXP PSYCHOL-A, V53, P1105, DOI 10.1080/02724980050156317
   HANLEY JR, 1989, COGNITIVE NEUROPSYCH, V6, P179, DOI 10.1080/02643298908253418
   Haslam C, 2004, CORTEX, V40, P451, DOI 10.1016/S0010-9452(08)70139-4
   Hocking J, 2009, BRAIN LANG, V108, P89, DOI 10.1016/j.bandl.2008.10.005
   Howard D., 1992, PYRAMIS PALM TREES A
   Ikeda M, 2006, NEUROPSYCHOLOGIA, V44, P566, DOI 10.1016/j.neuropsychologia.2005.07.006
   Joubert S, 2003, BRAIN, V126, P2537, DOI 10.1093/brain/awg259
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Lang CJG, 2009, J NEUROL, V256, P1303, DOI 10.1007/s00415-009-5118-2
   Mendez MF, 2001, NEUROLOGY, V57, P519, DOI 10.1212/WNL.57.3.519
   Mion M, 2010, BRAIN, V133, P3256, DOI 10.1093/brain/awq272
   Nakachi R, 2007, PSYCHOGERIATRICS, V7, P155, DOI 10.1111/j.1479-8301.2007.00205.x
   Neuner F, 2000, BRAIN COGNITION, V44, P342, DOI 10.1006/brcg.1999.1196
   Patterson K, 2007, NAT REV NEUROSCI, V8, P976, DOI 10.1038/nrn2277
   Ralph MAL, 2008, ANN NY ACAD SCI, V1124, P61, DOI 10.1196/annals.1440.006
   SERGENT J, 1990, BRAIN, V113, P989, DOI 10.1093/brain/113.4.989
   SERGENT J, 1992, CEREB CORTEX, V2, P375, DOI 10.1093/cercor/2.5.375
   Snowden J. S., BEHAV NEURO IN PRESS
   Snowden JS, 2004, BRAIN, V127, P860, DOI 10.1093/brain/awh099
   Thierry G, 2003, NEURON, V38, P499, DOI 10.1016/S0896-6273(03)00199-5
   Thierry G, 2006, J COGNITIVE NEUROSCI, V18, P1018, DOI 10.1162/jocn.2006.18.6.1018
   Tsukiura T, 2006, NEUROIMAGE, V30, P617, DOI 10.1016/j.neuroimage.2005.09.043
   Tsukiura T, 2002, J COGNITIVE NEUROSCI, V14, P922, DOI 10.1162/089892902760191144
   Tsukiura T, 2008, HUM BRAIN MAPP, V29, P1343, DOI 10.1002/hbm.20469
   Valentine T., 1996, COGNITIVE PSYCHOL PR
   Van Lancker D R, 1982, Brain Cogn, V1, P185, DOI 10.1016/0278-2626(82)90016-1
   VANLANCKER DR, 1989, J CLIN EXP NEUROPSYC, V11, P665, DOI 10.1080/01688638908400923
   VANLANCKER DR, 1988, CORTEX, V24, P195, DOI 10.1016/S0010-9452(88)80029-7
   von Kriegstein K, 2005, J COGNITIVE NEUROSCI, V17, P367, DOI 10.1162/0898929053279577
   Young AW, 1999, COGN NEUROPSYCHOL, V16, P1, DOI 10.1080/026432999380960
NR 63
TC 52
Z9 55
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
J9 NEUROPSYCHOLOGIA
JI Neuropsychologia
PD JUL
PY 2011
VL 49
IS 9
BP 2273
EP 2282
DI 10.1016/j.neuropsychologia.2011.04.027
PG 10
UT WOS:000293611600001
DA 2024-04-26
ER

PT J
AU Young, AW
   Frühholz, S
   Schweinberger, SR
AF Young, Andrew W.
   Fruhholz, Sascha
   Schweinberger, Stefan R.
TI Face and Voice Perception: Understanding Commonalities and Differences
SO TRENDS IN COGNITIVE SCIENCES
LA English
DT Review
ID FUNCTIONAL NEUROIMAGING TELL; SUPERIOR TEMPORAL SULCUS; AUDIOVISUAL
   INTEGRATION; AMYGDALA DAMAGE; EMOTIONAL EXPRESSIONS; PERSON RECOGNITION;
   SPEECH-PERCEPTION; FACIAL IDENTITY; BRAIN NETWORKS; TALKING FACES
AB Faces and voices are of high importance in interpersonal communication, and there are notable parallels between face and voice perception. However, these parallels do not sit entirely comfortably with the full range of available evidence. In this review, we evaluate parallels between the functional and neural organisation of face and voice perception, while locating these in the context of ways in which faces and voices also differ. We take the discussion to the next level by asking why these commonalities and differences exist. A novel synthesis is offered, grounded in the interaction between intrinsic characteristics of faces and voices and the demands of everyday life, showing how the pattern of findings reflects a system that can respond optimally to different everyday demands.
C1 [Young, Andrew W.] Univ York, York, N Yorkshire, England.
   [Fruhholz, Sascha] Univ Zurich, Zurich, Switzerland.
   [Fruhholz, Sascha] Univ Oslo, Oslo, Norway.
   [Schweinberger, Stefan R.] Univ Jena, Jena, Germany.
   [Schweinberger, Stefan R.] Univ Geneva, Swiss Ctr Affect Sci, Geneva, Switzerland.
RP Young, AW (corresponding author), Univ York, York, N Yorkshire, England.; Frühholz, S (corresponding author), Univ Zurich, Zurich, Switzerland.; Frühholz, S (corresponding author), Univ Oslo, Oslo, Norway.; Schweinberger, SR (corresponding author), Univ Jena, Jena, Germany.; Schweinberger, SR (corresponding author), Univ Geneva, Swiss Ctr Affect Sci, Geneva, Switzerland.
EM andy.young@york.ac.uk; sascha.fruehholz@uzh.ch;
   stefan.schweinberger@uni-jena.de
CR Andrews S, 2015, Q J EXP PSYCHOL, V68, P2041, DOI 10.1080/17470218.2014.1003949
   Barrett LF, 2019, PSYCHOL SCI PUBL INT, V20, P1, DOI 10.1177/1529100619832930
   Barton JJS, 2016, CORTEX, V75, P132, DOI 10.1016/j.cortex.2015.11.023
   Beauchamp MS, 2010, J NEUROSCI, V30, P2414, DOI 10.1523/JNEUROSCI.4865-09.2010
   Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008
   Belin P, 2017, VIS COGN, V25, P658, DOI 10.1080/13506285.2017.1339156
   Belin P, 2011, BRIT J PSYCHOL, V102, P711, DOI 10.1111/j.2044-8295.2011.02041.x
   Bernstein M, 2015, NEUROSCI BIOBEHAV R, V55, P536, DOI 10.1016/j.neubiorev.2015.06.010
   Bestelmeyer PEG, 2014, J NEUROSCI, V34, P8098, DOI 10.1523/JNEUROSCI.4820-13.2014
   Biederman I, 2018, NEUROPSYCHOLOGIA, V116, P205, DOI 10.1016/j.neuropsychologia.2018.01.036
   Blank H, 2014, NEUROSCI BIOBEHAV R, V47, P717, DOI 10.1016/j.neubiorev.2014.10.022
   Broks P, 1998, NEUROPSYCHOLOGIA, V36, P59, DOI 10.1016/S0028-3932(97)00105-X
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   BRUCE V, 1994, Q J EXP PSYCHOL-A, V47, P5, DOI 10.1080/14640749408401141
   Bruce V., 2012, FACE PERCEPTION
   Burton AM, 2016, COGNITIVE SCI, V40, P202, DOI 10.1111/cogs.12231
   Burton AM, 2013, Q J EXP PSYCHOL, V66, P1467, DOI 10.1080/17470218.2013.800125
   Burton AM, 1999, COGNITIVE SCI, V23, P1
   BURTON AM, 1990, BRIT J PSYCHOL, V81, P361, DOI 10.1111/j.2044-8295.1990.tb02367.x
   Calder AJ, 2000, NAT NEUROSCI, V3, P1077, DOI 10.1038/80586
   Calder AJ, 2001, NAT REV NEUROSCI, V2, P352, DOI 10.1038/35072584
   Calder AJ, 2005, NAT REV NEUROSCI, V6, P641, DOI 10.1038/nrn1724
   Calvert GA, 2001, NEUROIMAGE, V14, P427, DOI 10.1006/nimg.2001.0812
   Calvert GA, 2001, CEREB CORTEX, V11, P1110, DOI 10.1093/cercor/11.12.1110
   Campanella S, 2007, TRENDS COGN SCI, V11, P535, DOI 10.1016/j.tics.2007.10.001
   Coltheart M, 2006, CORTEX, V42, P323, DOI 10.1016/S0010-9452(08)70358-7
   Connolly HL, 2020, COGNITION, V197, DOI 10.1016/j.cognition.2019.104166
   Cosseddu M, 2018, NEUROPSYCHOLOGY, V32, P920, DOI 10.1037/neu0000480
   Dricu M, 2016, NEUROSCI BIOBEHAV R, V71, P810, DOI 10.1016/j.neubiorev.2016.10.020
   Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111
   EIMAS PD, 1973, COGNITIVE PSYCHOL, V4, P99, DOI 10.1016/0010-0285(73)90006-6
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Ekman P, 1972, NEBRASKA S MOTIVATIO, V19, P207, DOI DOI 10.1037/0022-3514.53.4.712
   ELLIS AW, 1989, BRAIN, V112, P1469, DOI 10.1093/brain/112.6.1469
   Fecteau S, 2007, NEUROIMAGE, V36, P480, DOI 10.1016/j.neuroimage.2007.02.043
   Feinstein JS, 2011, CURR BIOL, V21, P34, DOI 10.1016/j.cub.2010.11.042
   Filippi P, 2017, COGNITION EMOTION, V31, P879, DOI 10.1080/02699931.2016.1177489
   Frühholz S, 2017, NEUROSCI BIOBEHAV R, V83, P516, DOI 10.1016/j.neubiorev.2017.09.009
   Frühholz S, 2015, P NATL ACAD SCI USA, V112, P1583, DOI 10.1073/pnas.1411315112
   Frühholz S, 2014, PROG NEUROBIOL, V123, P1, DOI 10.1016/j.pneurobio.2014.09.003
   Fruhholz S., 2019, OXFORD HDB VOICE PER, P3
   Frühholz S, 2016, NEUROSCI BIOBEHAV R, V68, P96, DOI 10.1016/j.neubiorev.2016.05.002
   Gainotti G, 2014, PSYCHOL BELG, V54, P298, DOI 10.5334/pb.at
   Gao CJ, 2019, CORTEX, V120, P66, DOI 10.1016/j.cortex.2019.05.016
   Gau R, 2016, NEUROIMAGE, V124, P876, DOI 10.1016/j.neuroimage.2015.09.045
   Gobbini MI, 2007, NEUROPSYCHOLOGIA, V45, P32, DOI 10.1016/j.neuropsychologia.2006.04.015
   Gosselin N, 2007, NEUROPSYCHOLOGIA, V45, P236, DOI 10.1016/j.neuropsychologia.2006.07.012
   Gray HA, 2010, NEUROPSYCHOLOGY, V24, P176, DOI 10.1037/a0018104
   Hagan CC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070648
   Hagan CC, 2009, P NATL ACAD SCI USA, V106, P20010, DOI 10.1073/pnas.0905792106
   Hall DA, 2005, J COGNITIVE NEUROSCI, V17, P939, DOI 10.1162/0898929054021175
   Hanley JR, 2009, MEMORY, V17, P830, DOI 10.1080/09658210903264175
   HANLEY JR, 1989, COGNITIVE NEUROPSYCH, V6, P179, DOI 10.1080/02643298908253418
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   Henson R, 2005, Q J EXP PSYCHOL-A, V58, P193, DOI 10.1080/02724980443000502
   Israelashvili J, 2019, EMOTION, V19, P558, DOI 10.1037/emo0000441
   Janak PH, 2015, NATURE, V517, P284, DOI 10.1038/nature14188
   Jenkins R., 2019, P R SOC B, V285
   Jenkins R, 2011, COGNITION, V121, P313, DOI 10.1016/j.cognition.2011.08.001
   Jiang J, 2017, SOC COGN AFFECT NEUR, V12, P319, DOI 10.1093/scan/nsw127
   Kahana-Kalman R, 2001, CHILD DEV, V72, P352, DOI 10.1111/1467-8624.00283
   Kanwisher N, 2017, J NEUROSCI, V37, P1056, DOI 10.1523/JNEUROSCI.1706-16.2016
   Keane J, 2002, NEUROPSYCHOLOGIA, V40, P655, DOI 10.1016/S0028-3932(01)00156-7
   KLEINKE CL, 1986, PSYCHOL BULL, V100, P78, DOI 10.1037/0033-2909.100.1.78
   Kramer RSS, 2018, COGNITION, V172, P46, DOI 10.1016/j.cognition.2017.12.005
   Kramer RSS, 2017, PSYCHOL REV, V124, P115, DOI 10.1037/rev0000048
   KUHL PK, 1982, SCIENCE, V218, P1138, DOI 10.1126/science.7146899
   Kuhn LK, 2017, EMOTION, V17, P912, DOI 10.1037/emo0000282
   Lavan N, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-10295-w
   Lavan N, 2019, PSYCHON B REV, V26, P90, DOI 10.3758/s13423-018-1497-7
   Lavan N, 2018, EVOL HUM BEHAV, V39, P139, DOI 10.1016/j.evolhumbehav.2017.11.002
   Lavan N, 2015, COGNITION EMOTION, V29, P935, DOI 10.1080/02699931.2014.957656
   Layton R, 2012, INT J PRIMATOL, V33, P1215, DOI 10.1007/s10764-012-9634-z
   Lee H, 2014, CURR BIOL, V24, pR309, DOI 10.1016/j.cub.2014.02.007
   Lee Y, 2019, J ACOUST SOC AM, V146, P1568, DOI 10.1121/1.5125134
   Liu RR, 2016, CEREB CORTEX, V26, P1473, DOI 10.1093/cercor/bhu240
   Macaluso E, 2004, NEUROIMAGE, V21, P725, DOI 10.1016/j.neuroimage.2003.09.049
   Magnotti JF, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-36772-8
   Maguinness C, 2018, NEUROPSYCHOLOGIA, V116, P179, DOI 10.1016/j.neuropsychologia.2018.03.039
   Maguinness C, 2017, VIS COGN, V25, P644, DOI 10.1080/13506285.2017.1313347
   Martens U, 2010, J EXP PSYCHOL HUMAN, V36, P103, DOI 10.1037/a0017167
   McAleer P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0090779
   McGettigan C, 2017, NEUROPSYCHOLOGIA, V100, P51, DOI 10.1016/j.neuropsychologia.2017.04.013
   McGettigan C, 2012, TRENDS COGN SCI, V16, P269, DOI 10.1016/j.tics.2012.04.006
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mileva M, 2019, COGNITION, V190, P184, DOI 10.1016/j.cognition.2019.04.027
   Mileva M, 2018, J EXP PSYCHOL HUMAN, V44, P128, DOI 10.1037/xhp0000439
   MILLER GA, 1955, J ACOUST SOC AM, V27, P338, DOI 10.1121/1.1907526
   Mormann F, 2015, NAT NEUROSCI, V18, P1568, DOI 10.1038/nn.4139
   Neuner F, 2000, BRAIN COGNITION, V44, P342, DOI 10.1006/brcg.1999.1196
   NYGAARD LC, 1994, PSYCHOL SCI, V5, P42, DOI 10.1111/j.1467-9280.1994.tb00612.x
   Oatley K, 2014, TRENDS COGN SCI, V18, P134, DOI 10.1016/j.tics.2013.12.004
   Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105
   Page MPA, 2006, CORTEX, V42, P428, DOI 10.1016/S0010-9452(08)70375-7
   Palermo R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068126
   Patterson ML, 2003, DEVELOPMENTAL SCI, V6, P191, DOI 10.1111/1467-7687.00271
   Pernet CR, 2015, NEUROIMAGE, V119, P164, DOI 10.1016/j.neuroimage.2015.06.050
   Perrodin C, 2015, TRENDS COGN SCI, V19, P783, DOI 10.1016/j.tics.2015.09.002
   Philip RCM, 2010, PSYCHOL MED, V40, P1919, DOI 10.1017/S0033291709992364
   Rezlescu C, 2015, J NONVERBAL BEHAV, V39, P355, DOI 10.1007/s10919-015-0214-8
   Riedel P, 2015, CORTEX, V68, P86, DOI 10.1016/j.cortex.2014.11.016
   Roswandowitz C, 2018, BRAIN, V141, P234, DOI 10.1093/brain/awx313
   RUSSELL JA, 1987, J EXP PSYCHOL GEN, V116, P223, DOI 10.1037/0096-3445.116.3.223
   Sander D, 2018, EMOT REV, V10, P219, DOI 10.1177/1754073918765653
   Sauter DA, 2017, EMOT REV, V9, P222, DOI 10.1177/1754073916667236
   Schall S, 2013, NEUROIMAGE, V77, P237, DOI 10.1016/j.neuroimage.2013.03.043
   Schirmer A, 2006, TRENDS COGN SCI, V10, P24, DOI 10.1016/j.tics.2005.11.009
   Schirmer A, 2018, SOC COGN AFFECT NEUR, V13, P1, DOI 10.1093/scan/nsx142
   Schirmer A, 2017, TRENDS COGN SCI, V21, P216, DOI 10.1016/j.tics.2017.01.001
   Schweinberger SR, 2008, CURR BIOL, V18, P684, DOI 10.1016/j.cub.2008.04.015
   Schweinberger SR, 2017, VIS COGN, V25, P589, DOI 10.1080/13506285.2016.1276110
   Schweinberger SR, 2016, CORTEX, V80, P141, DOI 10.1016/j.cortex.2015.11.001
   Schweinberger SR, 2014, WIRES COGN SCI, V5, P15, DOI 10.1002/wcs.1261
   Schweinberger SR, 2011, BRIT J PSYCHOL, V102, P695, DOI 10.1111/j.2044-8295.2011.02070.x
   Schweinberger SR, 2011, CORTEX, V47, P1026, DOI 10.1016/j.cortex.2010.11.011
   Scott GG, 2009, BIOL PSYCHOL, V80, P95, DOI 10.1016/j.biopsycho.2008.03.010
   Scott SK, 1997, NATURE, V385, P254, DOI 10.1038/385254a0
   Sprengelmeyer R, 1999, P ROY SOC B-BIOL SCI, V266, P2451, DOI 10.1098/rspb.1999.0945
   Tsantani M, 2019, NEUROIMAGE, V201, DOI 10.1016/j.neuroimage.2019.07.017
   von Kriegstein K, 2008, P NATL ACAD SCI USA, V105, P6747, DOI 10.1073/pnas.0710826105
   von Kriegstein K, 2006, CEREB CORTEX, V16, P1314, DOI 10.1093/cercor/bhj073
   Vytal K, 2010, J COGNITIVE NEUROSCI, V22, P2864, DOI 10.1162/jocn.2009.21366
   WALKER S, 1995, PERCEPT PSYCHOPHYS, V57, P1124, DOI 10.3758/BF03208369
   Wang S, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14821
   Watson R, 2014, J NEUROSCI, V34, P6813, DOI 10.1523/JNEUROSCI.4478-13.2014
   Webster MA, 2004, NATURE, V428, P557, DOI 10.1038/nature02420
   Wiese H, 2019, PSYCHOL SCI, V30, P261, DOI 10.1177/0956797618813572
   Wright TM, 2003, CEREB CORTEX, V13, P1034, DOI 10.1093/cercor/13.10.1034
   Young AW, 2018, Q J EXP PSYCHOL, V71, P569, DOI 10.1177/1747021817740275
   Young AW, 2018, TRENDS COGN SCI, V22, P100, DOI 10.1016/j.tics.2017.11.007
   Young AW, 2017, CURR DIR PSYCHOL SCI, V26, P212, DOI 10.1177/0963721416688114
   Young AW, 1999, COGN NEUROPSYCHOL, V16, P1, DOI 10.1080/026432999380960
   YOUNG AW, 1993, BRAIN, V116, P941, DOI 10.1093/brain/116.4.941
   Yovel G, 2016, TRENDS COGN SCI, V20, P383, DOI 10.1016/j.tics.2016.02.005
   Yovel G, 2013, TRENDS COGN SCI, V17, P263, DOI 10.1016/j.tics.2013.04.004
NR 135
TC 75
Z9 83
PU CELL PRESS
PI CAMBRIDGE
PA 50 HAMPSHIRE ST, FLOOR 5, CAMBRIDGE, MA 02139 USA
J9 TRENDS COGN SCI
JI TRENDS COGN. SCI.
PD MAY
PY 2020
VL 24
IS 5
BP 398
EP 410
DI 10.1016/j.tics.2020.02.001
PG 13
UT WOS:000526992500008
DA 2024-04-26
ER

PT J
AU Schweinberger, SR
   Kawahara, H
   Simpson, AP
   Skuk, VG
   Zäske, R
AF Schweinberger, Stefan R.
   Kawahara, Hideki
   Simpson, Adrian P.
   Skuk, Verena G.
   Zaeske, Romi
TI Speaker perception
SO WILEY INTERDISCIPLINARY REVIEWS-COGNITIVE SCIENCE
LA English
DT Review
ID CROSS-MODAL INTERACTIONS; FACE-RECOGNITION; INDIVIDUAL-DIFFERENCES;
   WOMENS PREFERENCES; SPEECH-PERCEPTION; VOICE; ADAPTATION;
   IDENTIFICATION; MEMORY; MECHANISMS
AB While humans use their voice mainly for communicating information about the world, paralinguistic cues in the voice signal convey rich dynamic information about a speaker's arousal and emotional state, and extralinguistic cues reflect more stable speaker characteristics including identity, biological sex and social gender, socioeconomic or regional background, and age. Here we review the anatomical and physiological bases for individual differences in the human voice, before discussing how recent methodological progress in voice morphing and voice synthesis has promoted research on current theoretical issues, such as how voices are mentally represented in the human brain. Special attention is dedicated to the distinction between the recognition of familiar and unfamiliar speakers, in everyday situations or in the forensic context, and on the processes and representational changes that accompany the learning of new voices. We describe how specific impairments and individual differences in voice perception could relate to specific brain correlates. Finally, we consider that voices are produced by speakers who are often visible during communication, and review recent evidence that shows how speaker perception involves dynamic face-voice integration. The representation of para- and extralinguistic vocal information plays a major role in person perception and social communication, could be neuronally encoded in a prototype-referenced manner, and is subject to flexible adaptive recalibration as a result of specific perceptual experience. (C) 2013 John Wiley & Sons, Ltd.
C1 [Schweinberger, Stefan R.; Skuk, Verena G.; Zaeske, Romi] Univ Jena, Inst Psychol, Dept Gen Psychol & Cognit Neurosci, Jena, Germany.
   [Schweinberger, Stefan R.; Simpson, Adrian P.; Skuk, Verena G.; Zaeske, Romi] Univ Jena, DFG Res Unit Person Percept, Jena, Germany.
   [Kawahara, Hideki] Wakayama Univ, Fac Syst Engn, Wakayama, Japan.
   [Simpson, Adrian P.] Univ Jena, Inst German Linguist, Dept Speech, Jena, Germany.
RP Schweinberger, SR (corresponding author), Univ Jena, Inst Psychol, Dept Gen Psychol & Cognit Neurosci, Jena, Germany.
EM Stefan.schweinberger@uni-jena.de
CR Andics A, 2010, NEUROIMAGE, V52, P1528, DOI 10.1016/j.neuroimage.2010.05.048
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Baumann O, 2010, PSYCHOL RES-PSYCH FO, V74, P110, DOI 10.1007/s00426-008-0185-z
   Belin P, 2003, NEUROREPORT, V14, P2105, DOI 10.1097/00001756-200311140-00019
   Belin P, 2000, NATURE, V403, P309, DOI 10.1038/35002078
   Belin P., 2013, INTEGRATING FACE VOI
   Belin P, 2011, BRIT J PSYCHOL, V102, P711, DOI 10.1111/j.2044-8295.2011.02041.x
   Blank H, 2011, J NEUROSCI, V31, P12906, DOI 10.1523/JNEUROSCI.2091-11.2011
   Bruckert L, 2010, CURR BIOL, V20, P116, DOI 10.1016/j.cub.2009.11.034
   BURTON AM, 1990, BRIT J PSYCHOL, V81, P361, DOI 10.1111/j.2044-8295.1990.tb02367.x
   Campanella S, 2007, TRENDS COGN SCI, V11, P535, DOI 10.1016/j.tics.2007.10.001
   Casserly ED, 2010, WIRES COGN SCI, V1, P629, DOI 10.1002/wcs.63
   Charest I, 2009, BMC NEUROSCI, V10, DOI 10.1186/1471-2202-10-127
   DECASPER AJ, 1980, SCIENCE, V208, P1174, DOI 10.1126/science.7375928
   Feinberg DR, 2006, HORM BEHAV, V49, P215, DOI 10.1016/j.yhbeh.2005.07.004
   Fitch WT, 1999, J ACOUST SOC AM, V106, P1511, DOI 10.1121/1.427148
   Formisano E, 2008, SCIENCE, V322, P970, DOI 10.1126/science.1164318
   Garrido L, 2009, NEUROPSYCHOLOGIA, V47, P123, DOI 10.1016/j.neuropsychologia.2008.08.003
   Ghazanfar AA, 2006, TRENDS COGN SCI, V10, P278, DOI 10.1016/j.tics.2006.04.008
   GUNZBURGER D, 1987, LANG SPEECH, V30, P47
   Joassin F, 2004, NEUROSCI LETT, V369, P132, DOI 10.1016/j.neulet.2004.07.067
   Joassin F, 2011, CORTEX, V47, P367, DOI 10.1016/j.cortex.2010.03.003
   Joassin F, 2011, NEUROIMAGE, V54, P1654, DOI 10.1016/j.neuroimage.2010.08.073
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Kawahara H, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P256
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   Kawahara H, 2009, P ICASSP, P19
   Kisilevsky BS, 2003, PSYCHOL SCI, V14, P220, DOI 10.1111/1467-9280.02435
   KREIMAN J, 1992, J SPEECH HEAR RES, V35, P512, DOI 10.1044/jshr.3503.512
   Kreiman Jody, 2011, FDN VOICE STUDIES IN, P237
   LASS NJ, 1976, J ACOUST SOC AM, V59, P675, DOI 10.1121/1.380917
   Latinus M, 2011, CEREB CORTEX, V21, P2820, DOI 10.1093/cercor/bhr077
   Latinus M, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00175
   LEGGE GE, 1984, J EXP PSYCHOL LEARN, V10, P298
   Leopold DA, 2001, NAT NEUROSCI, V4, P89, DOI 10.1038/82947
   LUUS CAE, 1994, J APPL PSYCHOL, V79, P714, DOI 10.1037/0021-9010.79.5.714
   MCCONACHIE HR, 1976, CORTEX, V12, P76, DOI 10.1016/S0010-9452(76)80033-0
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mulac A, 1996, HEALTH COMMUN, V8, P199, DOI 10.1207/s15327027hc0803_2
   Munson B, 2007, LANG LINGUIST COMPAS, V1, DOI 10.1111/j.1749-818x.2007.00028.x
   Neuner F, 2000, BRAIN COGNITION, V44, P342, DOI 10.1006/brcg.1999.1196
   Nolan F., 2011, P 17 INT C PHON SCI, P1506, DOI DOI 10.1016/S0926-6410(03)00079-X
   NYGAARD LC, 1994, PSYCHOL SCI, V5, P42, DOI 10.1111/j.1467-9280.1994.tb00612.x
   O'Mahony C, 2012, BRIT J PSYCHOL, V103, P73, DOI 10.1111/j.2044-8295.2011.02044.x
   Owren MJ, 2007, PERCEPT PSYCHOPHYS, V69, P930, DOI 10.3758/BF03193930
   PAPCUN G, 1989, J ACOUST SOC AM, V85, P913, DOI 10.1121/1.397564
   Perrachione TK, 2007, NEUROPSYCHOLOGIA, V45, P1899, DOI 10.1016/j.neuropsychologia.2006.11.015
   Philippon AC, 2007, APPL COGNITIVE PSYCH, V21, P103, DOI 10.1002/acp.1281
   POLLACK I, 1954, J ACOUST SOC AM, V26, P403, DOI 10.1121/1.1907349
   Puts DA, 2005, EVOL HUM BEHAV, V26, P388, DOI 10.1016/j.evolhumbehav.2005.03.001
   Rakic T, 2011, BRIT J PSYCHOL, V102, P868, DOI 10.1111/j.2044-8295.2011.02051.x
   Remez RE, 1997, J EXP PSYCHOL HUMAN, V23, P651, DOI 10.1037/0096-1523.23.3.651
   Robertson DMC, 2010, Q J EXP PSYCHOL, V63, P23, DOI 10.1080/17470210903144376
   Russell R, 2009, PSYCHON B REV, V16, P252, DOI 10.3758/PBR.16.2.252
   SASLOVE H, 1980, J APPL PSYCHOL, V65, P111, DOI 10.1037/0021-9010.65.1.111
   Schirmer A, 2006, TRENDS COGN SCI, V10, P24, DOI 10.1016/j.tics.2005.11.009
   Schweinberger SR, 1997, J SPEECH LANG HEAR R, V40, P453, DOI 10.1044/jslhr.4002.453
   Schweinberger SR, 2008, CURR BIOL, V18, P684, DOI 10.1016/j.cub.2008.04.015
   Schweinberger SR, 2007, Q J EXP PSYCHOL, V60, P1446, DOI 10.1080/17470210601063589
   Schweinberger SR, 2011, BRIT J PSYCHOL, V102, P748, DOI 10.1111/j.2044-8295.2011.02048.x
   Schweinberger SR, 2011, CORTEX, V47, P1026, DOI 10.1016/j.cortex.2010.11.011
   Shah NJ, 2001, BRAIN, V124, P804, DOI 10.1093/brain/124.4.804
   Sheffert SM, 2004, PERCEPT PSYCHOPHYS, V66, P352, DOI 10.3758/BF03194884
   Skuk VG, 2013, HEARING RES, V296, P131, DOI 10.1016/j.heares.2012.11.004
   Stevens K.N, 1998, ACOUSTIC PHONETICS
   Summerfield A.Q., 1989, HDB RES FACE PROCESS, P223
   VALENTINE T, 1991, Q J EXP PSYCHOL-A, V43, P161, DOI 10.1080/14640749108400966
   Van Lancker D R, 1982, Brain Cogn, V1, P185, DOI 10.1016/0278-2626(82)90016-1
   VANLANCKER D, 1987, NEUROPSYCHOLOGIA, V25, P829, DOI 10.1016/0028-3932(87)90120-5
   VANLANCKER D, 1985, J PHONETICS, V13, P19, DOI 10.1016/S0095-4470(19)30723-5
   von Kriegstein K, 2005, J COGNITIVE NEUROSCI, V17, P367, DOI 10.1162/0898929053279577
   von Kriegstein K, 2003, COGNITIVE BRAIN RES, V17, P48, DOI 10.1016/S0926-6410(03)00079-X
   Vukovic J, 2011, BRIT J PSYCHOL, V102, P37, DOI 10.1348/000712610X498750
   Wiese H, 2012, BIOL PSYCHOL, V89, P137, DOI 10.1016/j.biopsycho.2011.10.002
   Zäske R, 2010, HEARING RES, V268, P38, DOI 10.1016/j.heares.2010.04.011
NR 75
TC 74
Z9 77
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
J9 WIRES COGN SCI
JI Wiley Interdiscip. Rev.-Cogn. Sci.
PD JAN
PY 2014
VL 5
IS 1
BP 15
EP 25
DI 10.1002/wcs.1261
PG 11
UT WOS:000328560300002
DA 2024-04-26
ER

PT J
AU Yovel, G
   Belin, P
AF Yovel, Galit
   Belin, Pascal
TI A unified coding strategy for processing faces and voices
SO TRENDS IN COGNITIVE SCIENCES
LA English
DT Review
DE face recognition; voice recognition; neural selectivity; sensory coding;
   visual cortex; auditory cortex
ID HUMAN AUDITORY-CORTEX; HUMAN BRAIN; FACIAL ATTRACTIVENESS; NEURAL
   REPRESENTATION; 2-MONTH-OLD INFANTS; PREFRONTAL CORTEX;
   SPEECH-PERCEPTION; RECOGNITION; ADAPTATION; SOUNDS
AB Both faces and voices are rich in socially-relevant information, which humans are remarkably adept at extracting, including a person's identity, age, gender, affective state, personality, etc. Here, we review accumulating evidence from behavioral, neuropsychological, electrophysiological, and neuroimaging studies which suggest that the cognitive and neural processing mechanisms engaged by perceiving faces or voices are highly similar, despite the very different nature of their sensory input. The similarity between the two mechanisms likely facilitates the multi-modal integration of facial and vocal information during everyday social interactions. These findings emphasize a parsimonious principle of cerebral organization, where similar computational problems in different modalities are solved using similar solutions.
C1 [Yovel, Galit] Tel Aviv Univ, Sch Psychol Sci, IL-69978 Tel Aviv, Israel.
   [Yovel, Galit] Tel Aviv Univ, Sagol Sch Neurosci, IL-69978 Tel Aviv, Israel.
   [Belin, Pascal] Univ Glasgow, Inst Neurosci & Psychol, Glasgow, Lanark, Scotland.
   [Belin, Pascal] Univ Montreal, Dept Psychol, Montreal, PQ H3C 3J7, Canada.
   [Belin, Pascal] CNRS, Inst Neurosci Timone, UMR 7289, F-75700 Paris, France.
   [Belin, Pascal] Univ Aix Marseille, Marseille, France.
RP Yovel, G (corresponding author), Tel Aviv Univ, Sch Psychol Sci, IL-69978 Tel Aviv, Israel.
EM gality@post.tau.ac.il; p.belin@psy.gla.ac.uk
CR Agus TR, 2012, J ACOUST SOC AM, V131, P4124, DOI 10.1121/1.3701865
   Atkinson AP, 2011, PHILOS T R SOC B, V366, P1726, DOI 10.1098/rstb.2010.0349
   Barton JJS, 2008, J NEUROPSYCHOL, V2, P197, DOI 10.1348/174866407X214172
   Beauchemin M, 2011, CEREB CORTEX, V21, P1705, DOI 10.1093/cercor/bhq242
   Bédard C, 2004, BRAIN COGNITION, V55, P247, DOI 10.1016/j.bandc.2004.02.008
   Belin P, 2000, NATURE, V403, P309, DOI 10.1038/35002078
   Belin P., 2012, INTEGRATING FACE AND
   Belin P, 2011, BRIT J PSYCHOL, V102, P711, DOI 10.1111/j.2044-8295.2011.02041.x
   Bestelmeyer PEG, 2011, CURR BIOL, V21, pR838, DOI 10.1016/j.cub.2011.08.046
   Bestelmeyer PEG, 2010, COGNITION, V117, P217, DOI 10.1016/j.cognition.2010.08.008
   Blasi A, 2011, CURR BIOL, V21, P1220, DOI 10.1016/j.cub.2011.06.009
   BLESSER B, 1972, J SPEECH HEAR RES, V15, P5, DOI 10.1044/jshr.1501.05
   Braun C., 2001, PROJEKTABSCHLUSSBERI
   BRUCE V, 1993, CAN J EXP PSYCHOL, V47, P38, DOI 10.1037/h0078773
   Bruckert L, 2010, CURR BIOL, V20, P116, DOI 10.1016/j.cub.2009.11.034
   Capilla A, 2013, CEREB CORTEX, V23, P1388, DOI 10.1093/cercor/bhs119
   Charest I, 2013, CEREB CORTEX, V23, P958, DOI 10.1093/cercor/bhs090
   Charest I, 2009, BMC NEUROSCI, V10, DOI 10.1186/1471-2202-10-127
   de Gelder B, 2000, COGNITION EMOTION, V14, P289, DOI 10.1080/026999300378824
   de Haan M, 2003, INT J PSYCHOPHYSIOL, V51, P45, DOI 10.1016/S0167-8760(03)00152-1
   De Lucia M, 2010, J NEUROSCI, V30, P11210, DOI 10.1523/JNEUROSCI.2239-10.2010
   DeBruine LM, 2007, J EXP PSYCHOL HUMAN, V33, P1420, DOI 10.1037/0096-1523.33.6.1420
   DeGutis J, 2013, COGNITION, V126, P87, DOI 10.1016/j.cognition.2012.09.004
   Dehaene-Lambertz G, 2002, SCIENCE, V298, P2013, DOI 10.1126/science.1077066
   Di Giorgio E, 2012, DEV PSYCHOL, V48, P1083, DOI 10.1037/a0026521
   Engell AD, 2010, PERCEPTION, V39, P931, DOI 10.1068/p6633
   Ethofer T, 2012, CEREB CORTEX, V22, P191, DOI 10.1093/cercor/bhr113
   Feinberg DR, 2008, PERCEPTION, V37, P615, DOI 10.1068/p5514
   Freiwald WA, 2010, SCIENCE, V330, P845, DOI 10.1126/science.1194908
   Gainotti G, 2013, NEUROPSYCHOLOGIA, V51, P1151, DOI 10.1016/j.neuropsychologia.2013.03.009
   Galton FJ, 1878, Nature, V18, P97, DOI [10.1038/018686a0, DOI 10.1038/018686A0, DOI 10.2307/2841021]
   Gao Z., 2012, HUM BRAIN MAPP
   Garrido L, 2009, NEUROPSYCHOLOGIA, V47, P123, DOI 10.1016/j.neuropsychologia.2008.08.003
   Grammer K, 2003, BIOL REV, V78, P385, DOI 10.1017/S1464793102006085
   Grossmann T, 2012, DEVELOPMENTAL SCI, V15, P830, DOI 10.1111/j.1467-7687.2012.01179.x
   Grossmann T, 2010, NEURON, V65, P852, DOI 10.1016/j.neuron.2010.03.001
   Hagan CC, 2009, P NATL ACAD SCI USA, V106, P20010, DOI 10.1073/pnas.0905792106
   Hailstone JC, 2010, NEUROPSYCHOLOGIA, V48, P1104, DOI 10.1016/j.neuropsychologia.2009.12.011
   Halit H, 2003, NEUROIMAGE, V19, P1180, DOI 10.1016/S1053-8119(03)00076-4
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   Hershler O, 2010, J VISION, V10, DOI 10.1167/10.10.21
   Jeffery L, 2011, J EXP PSYCHOL HUMAN, V37, P1824, DOI 10.1037/a0025643
   Johnson MH, 2005, NAT REV NEUROSCI, V6, P766, DOI 10.1038/nrn1766
   Kanwisher N, 2006, PHILOS T R SOC B, V361, P2109, DOI 10.1098/rstb.2006.1934
   Kisilevsky BS, 2003, PSYCHOL SCI, V14, P220, DOI 10.1111/1467-9280.02435
   Kravitz DJ, 2011, NAT REV NEUROSCI, V12, P217, DOI 10.1038/nrn3008
   KUHL PK, 1992, SCIENCE, V255, P606, DOI 10.1126/science.1736364
   LANGLOIS JH, 1990, PSYCHOL SCI, V1, P115, DOI 10.1111/j.1467-9280.1990.tb00079.x
   Langton SRH, 2008, COGNITION, V107, P330, DOI 10.1016/j.cognition.2007.07.012
   Latinus M., CURRENT BIOLOGY IN P
   Latinus M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041384
   Latinus M, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00175
   Lavner Y., 2001, INT J SPEECH TECHNOL, V4, P63, DOI [10.1023/A:1009656816383, DOI 10.1023/A:1009656816383]
   Leaver AM, 2010, J NEUROSCI, V30, P7604, DOI 10.1523/JNEUROSCI.0296-10.2010
   Leopold DA, 2001, NAT NEUROSCI, V4, P89, DOI 10.1038/82947
   Leopold DA, 2006, NATURE, V442, P572, DOI 10.1038/nature04951
   Lewkowicz DJ, 2009, TRENDS COGN SCI, V13, P470, DOI 10.1016/j.tics.2009.08.004
   Little AC, 2011, PHILOS T R SOC B, V366, P1638, DOI 10.1098/rstb.2010.0404
   Loffler G, 2005, NAT NEUROSCI, V8, P1386, DOI 10.1038/nn1538
   Moerel M, 2012, J NEUROSCI, V32, P14205, DOI 10.1523/JNEUROSCI.1388-12.2012
   Natu V, 2011, BRIT J PSYCHOL, V102, P726, DOI 10.1111/j.2044-8295.2011.02053.x
   Nederhouser M, 2007, VISION RES, V47, P2134, DOI 10.1016/j.visres.2007.04.007
   Neuner F, 2000, BRAIN COGNITION, V44, P342, DOI 10.1006/brcg.1999.1196
   O'Neil SF, 2011, VIS COGN, V19, P534, DOI 10.1080/13506285.2011.561262
   Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105
   Pascalis O, 2002, SCIENCE, V296, P1321, DOI 10.1126/science.1070223
   Patterson ML, 2003, DEVELOPMENTAL SCI, V6, P191, DOI 10.1111/1467-7687.00271
   Perrachione TK, 2007, NEUROPSYCHOLOGIA, V45, P1899, DOI 10.1016/j.neuropsychologia.2006.11.015
   Perrachione TK, 2010, COGNITION, V114, P42, DOI 10.1016/j.cognition.2009.08.012
   Perrodin C, 2011, CURR BIOL, V21, P1408, DOI 10.1016/j.cub.2011.07.028
   Petkov CI, 2008, NAT NEUROSCI, V11, P367, DOI 10.1038/nn2043
   Pitcher D, 2009, CURR BIOL, V19, P319, DOI 10.1016/j.cub.2009.01.007
   Pourtois G, 2005, CORTEX, V41, P49, DOI 10.1016/S0010-9452(08)70177-1
   Rauschecker JP, 2009, NAT NEUROSCI, V12, P718, DOI 10.1038/nn.2331
   Rauschecker JP, 2000, P NATL ACAD SCI USA, V97, P11800, DOI 10.1073/pnas.97.22.11800
   Remedios R, 2009, J NEUROSCI, V29, P1034, DOI 10.1523/JNEUROSCI.4089-08.2009
   Rogier O, 2010, INT J PSYCHOPHYSIOL, V75, P44, DOI 10.1016/j.ijpsycho.2009.10.013
   Romanski LM, 2012, P NATL ACAD SCI USA, V109, P10717, DOI 10.1073/pnas.1204335109
   Romanski LM, 2005, J NEUROPHYSIOL, V93, P734, DOI 10.1152/jn.00675.2004
   Rossion B, 2008, NEUROIMAGE, V39, P1959, DOI 10.1016/j.neuroimage.2007.10.011
   Sadeh B, 2011, CURR BIOL, V21, P1894, DOI 10.1016/j.cub.2011.09.030
   Said CP, 2011, PSYCHOL SCI, V22, P1183, DOI 10.1177/0956797611419169
   SCHERER KR, 1972, J PERS, V40, P191, DOI 10.1111/j.1467-6494.1972.tb00998.x
   Schweinberger SR, 2008, CURR BIOL, V18, P684, DOI 10.1016/j.cub.2008.04.015
   Schweinberger SR, 2011, CORTEX, V47, P1026, DOI 10.1016/j.cortex.2010.11.011
   Sidtis D, 2012, INTEGR PSYCHOL BEHAV, V46, P146, DOI 10.1007/s12124-011-9177-4
   Simion F, 2011, PROG BRAIN RES, V189, P173, DOI 10.1016/B978-0-444-53884-0.00024-5
   Smith EL, 2007, CURR BIOL, V17, P1680, DOI 10.1016/j.cub.2007.08.043
   Stevenage SV, 2013, PSYCHOL RES-PSYCH FO, V77, P167, DOI 10.1007/s00426-012-0450-z
   Susilo T., 2012, CURR OPIN NEUROBIOL
   Tillman MA, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00014
   Tsao DY, 2008, P NATL ACAD SCI USA, V105, P19514, DOI 10.1073/pnas.0809662105
   Tzourio-Mazoyer N, 2002, NEUROIMAGE, V15, P454, DOI 10.1006/nimg.2001.0979
   VALENTINE T, 1991, Q J EXP PSYCHOL-A, V43, P161, DOI 10.1080/14640749108400966
   von Kriegstein K, 2005, J COGNITIVE NEUROSCI, V17, P367, DOI 10.1162/0898929053279577
   Vouloumanos A, 2010, CHILD DEV, V81, P517, DOI 10.1111/j.1467-8624.2009.01412.x
   Webster MA, 2011, PHILOS T R SOC B, V366, P1702, DOI 10.1098/rstb.2010.0360
   Willis J, 2006, PSYCHOL SCI, V17, P592, DOI 10.1111/j.1467-9280.2006.01750.x
   Winters SJ, 2008, J ACOUST SOC AM, V123, P4524, DOI 10.1121/1.2913046
   YIN RK, 1969, J EXP PSYCHOL, V81, P141, DOI 10.1037/h0027474
   Young AW, 2011, BRIT J PSYCHOL, V102, P959, DOI 10.1111/j.2044-8295.2011.02045.x
   Young SG, 2012, PERS SOC PSYCHOL REV, V16, P116, DOI 10.1177/1088868311418987
   Yovel G., 2013, F1000PRIME REPORT 5
   Yovel G, 2008, NEUROPSYCHOLOGIA, V46, P3061, DOI 10.1016/j.neuropsychologia.2008.06.017
   Zäske R, 2010, HEARING RES, V268, P38, DOI 10.1016/j.heares.2010.04.011
NR 105
TC 106
Z9 118
PU ELSEVIER SCIENCE LONDON
PI LONDON
PA 84 THEOBALDS RD, LONDON WC1X 8RR, ENGLAND
J9 TRENDS COGN SCI
JI TRENDS COGN. SCI.
PD JUN
PY 2013
VL 17
IS 6
BP 263
EP 271
DI 10.1016/j.tics.2013.04.004
PG 9
UT WOS:000321224500003
DA 2024-04-26
ER

PT J
AU Young, AW
   Bruce, V
AF Young, Andrew W.
   Bruce, Vicki
TI Understanding person perception
SO BRITISH JOURNAL OF PSYCHOLOGY
LA English
DT Article
ID FACE RECOGNITION; IMPAIRED RECOGNITION; FACIAL EXPRESSIONS; NEURAL
   SYSTEMS; HUMAN BRAIN; EMOTION; FAMILIAR; IDENTITY; AMYGDALA; VOICE
AB Bruce and Young's (1986) theoretical framework was actually a synthesis of ideas contributed by several people. Some of its insights have stood the test of time especially the importance of using converging evidence from as wide a range of methods of enquiry as possible, and an emphasis on understanding the demands that are made by particular face perception tasks. But there were also areas where Bruce and Young failed to obey their own edicts ( emotion recognition), and some topics they simply omitted ( gaze perception). We discuss these, and then look at how the field has been transformed by computing developments, finishing with a few thoughts about where things may go over the next few (25?) years.
C1 [Young, Andrew W.] Univ York, Dept Psychol, York YO10 5DD, N Yorkshire, England.
   [Young, Andrew W.] Univ York, York NeuroImaging Ctr, York YO10 5DD, N Yorkshire, England.
   [Bruce, Vicki] Newcastle Univ, Sch Psychol, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
RP Young, AW (corresponding author), Univ York, Dept Psychol, York YO10 5DD, N Yorkshire, England.
EM awy1@york.ac.uk
CR ADOLPHS R, 1994, NATURE, V372, P669, DOI 10.1038/372669a0
   Akiyama T, 2006, NEUROPSYCHOLOGIA, V44, P1804, DOI 10.1016/j.neuropsychologia.2006.03.007
   Anastasi JS, 2005, PSYCHON B REV, V12, P1043, DOI 10.3758/BF03206441
   Andrews TJ, 2010, J NEUROSCI, V30, P3544, DOI 10.1523/JNEUROSCI.4863-09.2010
   [Anonymous], 1982, NORMALITY PATHOLOGY
   BARON RJ, 1981, INT J MAN MACH STUD, V15, P137, DOI 10.1016/S0020-7373(81)80001-6
   Baron-Cohen S., 1995, Mindblindness, DOI DOI 10.7551/MITPRESS/4635.001.0001
   BARONCOHEN S, 1995, BRIT J DEV PSYCHOL, V13, P379, DOI 10.1111/j.2044-835X.1995.tb00687.x
   Behrmann M, 2005, TRENDS COGN SCI, V9, P180, DOI 10.1016/j.tics.2005.02.011
   Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008
   Bruce V, 1999, J EXP PSYCHOL-APPL, V5, P339, DOI 10.1037/1076-898X.5.4.339
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   BRUCE V, 1982, BRIT J PSYCHOL, V73, P105, DOI 10.1111/j.2044-8295.1982.tb01795.x
   BRUCE V, 1985, BRIT J PSYCHOL, V76, P373, DOI 10.1111/j.2044-8295.1985.tb01960.x
   Bruce V., FACE PERCEP IN PRESS
   Bruce V., 1998, In the eye of the beholder: the science of face perception
   Burton AM, 2011, BRIT J PSYCHOL, V102, P943, DOI 10.1111/j.2044-8295.2011.02039.x
   BURTON AM, 1991, COGNITION, V39, P129, DOI 10.1016/0010-0277(91)90041-2
   Burton AM, 1999, COGNITIVE SCI, V23, P1
   Burton AM, 2005, COGNITIVE PSYCHOL, V51, P256, DOI 10.1016/j.cogpsych.2005.06.003
   BURTON AM, 1990, BRIT J PSYCHOL, V81, P361, DOI 10.1111/j.2044-8295.1990.tb02367.x
   Calder AJ, 1996, COGNITIVE NEUROPSYCH, V13, P699, DOI 10.1080/026432996381890
   Calder AJ, 2000, NAT NEUROSCI, V3, P1077, DOI 10.1038/80586
   Calder AJ, 2001, NAT REV NEUROSCI, V2, P352, DOI 10.1038/35072584
   Calder AJ, 2005, NAT REV NEUROSCI, V6, P641, DOI 10.1038/nrn1724
   Calder AJ, 2001, VISION RES, V41, P1179, DOI 10.1016/S0042-6989(01)00002-5
   Calvert GA, 2001, CEREB CORTEX, V11, P1110, DOI 10.1093/cercor/11.12.1110
   Campanella S, 2007, TRENDS COGN SCI, V11, P535, DOI 10.1016/j.tics.2007.10.001
   CAMPBELL R, 1990, NEUROPSYCHOLOGIA, V28, P1123, DOI 10.1016/0028-3932(90)90050-X
   Campbell R, 2011, BRIT J PSYCHOL, V102, P704, DOI 10.1111/j.2044-8295.2011.02021.x
   Coltheart M, 2006, CORTEX, V42, P323, DOI 10.1016/S0010-9452(08)70358-7
   DAMASIO AR, 1982, NEUROLOGY, V32, P331, DOI 10.1212/WNL.32.4.331
   Davies-Thompson J, 2009, NEUROPSYCHOLOGIA, V47, P1627, DOI 10.1016/j.neuropsychologia.2009.01.017
   Ekman P., 1976, Pictures of Facial Affect
   ELLIS AW, 1987, Q J EXP PSYCHOL-A, V39, P193, DOI 10.1080/14640748708401784
   Ewbank MP, 2008, NEUROIMAGE, V40, P1857, DOI 10.1016/j.neuroimage.2008.01.049
   George N, 2001, NEUROIMAGE, V13, P1102, DOI 10.1006/nimg.2001.0769
   Gobbini MI, 2007, NEUROPSYCHOLOGIA, V45, P32, DOI 10.1016/j.neuropsychologia.2006.04.015
   Hagan CC, 2009, P NATL ACAD SCI USA, V106, P20010, DOI 10.1073/pnas.0905792106
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   Haxby JV, 2002, BIOL PSYCHIAT, V51, P59, DOI 10.1016/S0006-3223(01)01330-0
   Hay D.C., 1982, NORMALITY PATHOLOGY
   Heining M, 2003, ANN NY ACAD SCI, V1000, P380, DOI 10.1196/annals.1280.035
   Henson R, 2005, Q J EXP PSYCHOL-A, V58, P193, DOI 10.1080/02724980443000502
   HEYWOOD CA, 1992, PHILOS T ROY SOC B, V335, P31, DOI 10.1098/rstb.1992.0004
   Jenkins R, 2008, SCIENCE, V319, P435, DOI 10.1126/science.1149656
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Keane J, 2002, NEUROPSYCHOLOGIA, V40, P655, DOI 10.1016/S0028-3932(01)00156-7
   Kriegeskorte N, 2007, P NATL ACAD SCI USA, V104, P20600, DOI 10.1073/pnas.0705654104
   Longmore CA, 2008, J EXP PSYCHOL HUMAN, V34, P77, DOI 10.1037/0096-1523.34.1.77
   Marr D., 1982, Vision
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   MEADOWS JC, 1974, J NEUROL NEUROSUR PS, V37, P489, DOI 10.1136/jnnp.37.5.489
   Meissner CA, 2001, PSYCHOL PUBLIC POL L, V7, P3, DOI 10.1037//1076-8971.7.1.3
   MILLER GA, 1955, J ACOUST SOC AM, V27, P338, DOI 10.1121/1.1907526
   Morris JS, 1996, NATURE, V383, P812, DOI 10.1038/383812a0
   Morton J., 1979, PROCESSING VISIBLE L, V1, P259, DOI [DOI 10.1007/978-1-4684-0994-9_15, 10.1007/978-1-4684-0994-9_15]
   PARRY FM, 1991, J CLIN EXP NEUROPSYC, V13, P545, DOI 10.1080/01688639108401070
   PERRETT DI, 1992, PHILOS T ROY SOC B, V335, P23, DOI 10.1098/rstb.1992.0003
   Phillips ML, 1998, P ROY SOC B-BIOL SCI, V265, P1809, DOI 10.1098/rspb.1998.0506
   Phillips ML, 1997, NATURE, V389, P495, DOI 10.1038/39051
   Robertson DMC, 2010, Q J EXP PSYCHOL, V63, P23, DOI 10.1080/17470210903144376
   Russell R, 2009, PSYCHON B REV, V16, P252, DOI 10.3758/PBR.16.2.252
   Schweinberger SR, 1998, J EXP PSYCHOL HUMAN, V24, P1748, DOI 10.1037/0096-1523.24.6.1748
   Schweinberger SR, 1999, PERCEPT PSYCHOPHYS, V61, P1102, DOI 10.3758/BF03207617
   Schweinberger SR, 2007, Q J EXP PSYCHOL, V60, P1446, DOI 10.1080/17470210601063589
   Scott SK, 1997, NATURE, V385, P254, DOI 10.1038/385254a0
   Sprengelmeyer R, 1999, P ROY SOC B-BIOL SCI, V266, P2451, DOI 10.1098/rspb.1999.0945
   Tsao DY, 2008, P NATL ACAD SCI USA, V105, P19514, DOI 10.1073/pnas.0809662105
   von Kriegstein K, 2008, P NATL ACAD SCI USA, V105, P6747, DOI 10.1073/pnas.0710826105
   von Kriegstein K, 2006, CEREB CORTEX, V16, P1314, DOI 10.1093/cercor/bhj073
   Webster MA, 2004, NATURE, V428, P557, DOI 10.1038/nature02420
   Wilmer JB, 2010, P NATL ACAD SCI USA, V107, P5238, DOI 10.1073/pnas.0913053107
   Wollaston William Hyde, 1824, PHILOS T R SOC LOND, V114, DOI DOI 10.1098/RSTL.1824.0016
   YOUNG A W, 1991, European Journal of Cognitive Psychology, V3, P5, DOI 10.1080/09541449108406218
   Young A.W., 2002, Facial Expressions of Emotions: Stimuli and Test (FEEST)
   Young AW, 1999, COGN NEUROPSYCHOL, V16, P1, DOI 10.1080/026432999380960
   Young AW, 1997, COGNITION, V63, P271, DOI 10.1016/S0010-0277(97)00003-6
   YOUNG AW, 1993, BRAIN, V116, P941, DOI 10.1093/brain/116.4.941
   Zhu Q, 2010, CURR BIOL, V20, P137, DOI 10.1016/j.cub.2009.11.067
NR 80
TC 116
Z9 125
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
J9 BRIT J PSYCHOL
JI Br. J. Psychol.
PD NOV
PY 2011
VL 102
SI SI
BP 959
EP 974
DI 10.1111/j.2044-8295.2011.02045.x
PN 4
PG 16
UT WOS:000296085900018
DA 2024-04-26
ER

PT J
AU Belin, P
   Fecteau, S
   Bédard, C
AF Belin, P
   Fecteau, S
   Bédard, C
TI Thinking the voice:: neural correlates of voice perception
SO TRENDS IN COGNITIVE SCIENCES
LA English
DT Review
ID EMOTIONAL PROSODY; VOCAL-TRACT; AUDITORY-PERCEPTION; SPEECH-PERCEPTION;
   SPOKEN LANGUAGE; RECOGNITION; IDENTIFICATION; ORGANIZATION; RESPONSES;
   NEWBORNS
AB The human voice is the carrier of speech, but also an 'auditory face' that conveys important affective and identity information. Little is known about the neural bases of our abilities to perceive such paralinguistic information in voice. Results from recent neuroimaging studies suggest that the different types of vocal information could be processed in partially dissociated functional pathways, and support a neurocognitive model of voice perception largely similar to that proposed for face perception.
C1 Univ Montreal, Dept Psychol, GRENEC, Lab Neurocognit Vocale, Montreal, PQ H3C 3J7, Canada.
RP Univ Montreal, Dept Psychol, GRENEC, Lab Neurocognit Vocale, CP 6128 Succ Ctr Ville, Montreal, PQ H3C 3J7, Canada.
EM pascal.belin@umontreal.ca
CR ASSAL G, 1976, SCHWEIZ ARCH NEUROL, V119, P307
   ASSAL G, 1981, REV NEUROL, V137, P255
   Belin P, 2003, NEUROREPORT, V14, P2105, DOI 10.1097/00001756-200311140-00019
   Belin P, 2002, COGNITIVE BRAIN RES, V13, P17, DOI 10.1016/S0926-6410(01)00084-2
   Belin P, 2000, NATURE, V403, P309, DOI 10.1038/35002078
   Binder JR, 2000, CEREB CORTEX, V10, P512, DOI 10.1093/cercor/10.5.512
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Buchanan TW, 2000, COGNITIVE BRAIN RES, V9, P227, DOI 10.1016/S0926-6410(99)00060-9
   Charrier I, 2001, NATURE, V412, P873, DOI 10.1038/35091136
   Davis MH, 2003, J NEUROSCI, V23, P3423
   DECASPER AJ, 1980, SCIENCE, V208, P1174, DOI 10.1126/science.7375928
   Dolan RJ, 2001, P NATL ACAD SCI USA, V98, P10006, DOI 10.1073/pnas.171288598
   Ellis A.W., 1989, Handbook of research on face processing, P207
   Fant G, 1960, ACOUSTIC THEORY SPEE, DOI 10.1515/9783110873429
   Farah MJ, 1996, BEHAV BRAIN RES, V76, P181, DOI 10.1016/0166-4328(95)00198-0
   FIFER WP, 1994, ACTA PAEDIATR, V83, P86, DOI 10.1111/j.1651-2227.1994.tb13270.x
   Fitch WT, 1999, J ACOUST SOC AM, V106, P1511, DOI 10.1121/1.427148
   Fitch WT, 1997, J ACOUST SOC AM, V102, P1213, DOI 10.1121/1.421048
   Fitch WT, 2000, TRENDS COGN SCI, V4, P258, DOI 10.1016/S1364-6613(00)01494-7
   Gauthier I, 2000, NAT NEUROSCI, V3, P191, DOI 10.1038/72140
   George MS, 1996, ARCH NEUROL-CHICAGO, V53, P665, DOI 10.1001/archneur.1996.00550070103017
   George N, 1996, COGNITIVE BRAIN RES, V4, P65, DOI 10.1016/0926-6410(95)00045-3
   GIRAUD AL, IN PRESS CEREB CORTE
   Gunji A, 2003, NEUROSCI LETT, V348, P13, DOI 10.1016/S0304-3940(03)00640-2
   Hanson HM, 1999, J ACOUST SOC AM, V106, P1064, DOI 10.1121/1.427116
   HARTMAN DE, 1976, J ACOUST SOC AM, V59, P713, DOI 10.1121/1.380894
   Hauser MD., 1996, The Evolution of Communication
   Haxby JV, 2000, TRENDS COGN SCI, V4, P223, DOI 10.1016/S1364-6613(00)01482-0
   HEILMAN KM, 1984, NEUROLOGY, V34, P917, DOI 10.1212/WNL.34.7.917
   Hickok G, 2000, TRENDS COGN SCI, V4, P131, DOI 10.1016/S1364-6613(00)01463-7
   Imaizumi S, 1997, NEUROREPORT, V8, P2809, DOI 10.1097/00001756-199708180-00031
   Insley SJ, 2000, NATURE, V406, P404, DOI 10.1038/35019064
   Kaas JH, 1999, NAT NEUROSCI, V2, P1045, DOI 10.1038/15967
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Kisilevsky BS, 2003, PSYCHOL SCI, V14, P220, DOI 10.1111/1467-9280.02435
   KLATT DH, 1980, J ACOUST SOC AM, V67, P971, DOI 10.1121/1.383940
   KLATT DH, 1990, J ACOUST SOC AM, V87, P820, DOI 10.1121/1.398894
   KLUENDER KR, 1987, SCIENCE, V237, P1195, DOI 10.1126/science.3629235
   Kreiman J., 1997, TALKER VARIABILITY S, P85
   LASS NJ, 1976, J ACOUST SOC AM, V59, P675, DOI 10.1121/1.380917
   Levy DA, 2001, NEUROREPORT, V12, P2653, DOI 10.1097/00001756-200108280-00013
   Levy DA, 2003, PSYCHOPHYSIOLOGY, V40, P291, DOI 10.1111/1469-8986.00031
   Liberman AM, 2000, TRENDS COGN SCI, V4, P187, DOI 10.1016/S1364-6613(00)01471-6
   LIBERMAN AM, 1989, SCIENCE, V243, P489, DOI 10.1126/science.2643163
   Linville SE, 1996, J VOICE, V10, P190, DOI 10.1016/S0892-1997(96)80046-4
   McCarthy G, 1999, CEREB CORTEX, V9, P431, DOI 10.1093/cercor/9.5.431
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Mitchell RLC, 2003, NEUROPSYCHOLOGIA, V41, P1410, DOI 10.1016/S0028-3932(03)00017-4
   Monrad-Krohn G.H., 1963, PROBLEMS DYNAMIC NEU, P101
   Morris JS, 1999, NEUROPSYCHOLOGIA, V37, P1155, DOI 10.1016/S0028-3932(99)00015-9
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Nakamura K, 2001, NEUROPSYCHOLOGIA, V39, P1047, DOI 10.1016/S0028-3932(01)00037-9
   Neuner F, 2000, BRAIN COGNITION, V44, P342, DOI 10.1006/brcg.1999.1196
   OCKLEFORD EM, 1988, EARLY HUM DEV, V18, P27, DOI 10.1016/0378-3782(88)90040-0
   PAPCUN G, 1989, J ACOUST SOC AM, V85, P913, DOI 10.1121/1.397564
   PERETZ I, 1994, BRAIN, V117, P1283, DOI 10.1093/brain/117.6.1283
   PERRETT DI, 1992, PHILOS T ROY SOC B, V335, P23, DOI 10.1098/rstb.1992.0003
   Phillips ML, 1998, P ROY SOC B-BIOL SCI, V265, P1809, DOI 10.1098/rspb.1998.0506
   PUCE A, 1995, J NEUROPHYSIOL, V74, P1192, DOI 10.1152/jn.1995.74.3.1192
   Ramus F, 2000, SCIENCE, V288, P349, DOI 10.1126/science.288.5464.349
   Rauschecker JP, 2000, P NATL ACAD SCI USA, V97, P11800, DOI 10.1073/pnas.97.22.11800
   Remez RE, 1997, J EXP PSYCHOL HUMAN, V23, P651, DOI 10.1037/0096-1523.23.3.651
   REMEZ RE, 1981, SCIENCE, V212, P947, DOI 10.1126/science.7233191
   Rendall D, 1998, J ACOUST SOC AM, V103, P602, DOI 10.1121/1.421104
   Rendall D, 1996, ANIM BEHAV, V51, P1007, DOI 10.1006/anbe.1996.0103
   ROSS ED, 1981, ARCH NEUROL-CHICAGO, V38, P561, DOI 10.1001/archneur.1981.00510090055006
   Sachs Jacqueline., 1973, Language Attitudes: Current Trends and Prospects, P74
   Samson Y, 2001, REV NEUROL-FRANCE, V157, P837
   Sander K, 2001, COGNITIVE BRAIN RES, V12, P181, DOI 10.1016/S0926-6410(01)00045-3
   SCHERER KR, 1995, J VOICE, V9, P235, DOI 10.1016/S0892-1997(05)80231-0
   Schweinberger SR, 1997, J SPEECH LANG HEAR R, V40, P453, DOI 10.1044/jslhr.4002.453
   Scott SK, 2000, BRAIN, V123, P2400, DOI 10.1093/brain/123.12.2400
   Scott SK, 2003, TRENDS NEUROSCI, V26, P100, DOI 10.1016/S0166-2236(02)00037-1
   Sekiyama K, 2003, NEUROSCI RES, V47, P277, DOI 10.1016/S0168-0102(03)00214-1
   SELTZER B, 1989, J COMP NEUROL, V290, P451, DOI 10.1002/cne.902900402
   Shah NJ, 2001, BRAIN, V124, P804, DOI 10.1093/brain/124.4.804
   Tarr MJ, 2003, TRENDS COGN SCI, V7, P23, DOI 10.1016/S1364-6613(02)00010-4
   TITZE IR, 1989, J ACOUST SOC AM, V85, P1699, DOI 10.1121/1.397959
   Ungerleider Leslie G., 1994, Current Opinion in Neurobiology, V4, P157, DOI 10.1016/0959-4388(94)90066-3
   Van Lancker D R, 1982, Brain Cogn, V1, P185, DOI 10.1016/0278-2626(82)90016-1
   VANLANCKER D, 1987, NEUROPSYCHOLOGIA, V25, P829, DOI 10.1016/0028-3932(87)90120-5
   Wildgruber D, 2002, NEUROIMAGE, V15, P856, DOI 10.1006/nimg.2001.0998
   Wright TM, 2003, CEREB CORTEX, V13, P1034, DOI 10.1093/cercor/13.10.1034
   YIN RK, 1969, J EXP PSYCHOL, V81, P141, DOI 10.1037/h0027474
   Zatorre R.J., 2000, Brain mapping: The systems, P365, DOI 10.1016/b978-012692545-6/50014-3
NR 85
TC 577
Z9 649
PU CELL PRESS
PI CAMBRIDGE
PA 50 HAMPSHIRE ST, FLOOR 5, CAMBRIDGE, MA 02139 USA
J9 TRENDS COGN SCI
JI TRENDS COGN. SCI.
PD MAR
PY 2004
VL 8
IS 3
BP 129
EP 135
DI 10.1016/j.tics.2004.01.008
PG 7
UT WOS:000220326900009
DA 2024-04-26
ER

PT J
AU Belin, P
   Bestelmeyer, PEG
   Latinus, M
   Watson, R
AF Belin, Pascal
   Bestelmeyer, Patricia E. G.
   Latinus, Marianne
   Watson, Rebecca
TI Understanding Voice Perception
SO BRITISH JOURNAL OF PSYCHOLOGY
LA English
DT Article
ID CROSS-MODAL INTERACTIONS; EVENT-RELATED FMRI; LONG-TERM-MEMORY;
   SPEECH-PERCEPTION; NEURAL RESPONSES; AUDITORY-CORTEX; HUMAN BRAIN;
   ELECTROPHYSIOLOGICAL EVIDENCE; AUDIOVISUAL INTEGRATION; EMOTIONAL
   VOCALIZATIONS
AB Voices carry large amounts of socially relevant information on persons, much like 'auditory faces'. Following Bruce and Young (1986)'s seminal model of face perception, we propose that the cerebral processing of vocal information is organized in interacting but functionally dissociable pathways for processing the three main types of vocal information: speech, identity, and affect. The predictions of the 'auditory face' model of voice perception are reviewed in the light of recent clinical, psychological, and neuroimaging evidence.
C1 [Belin, Pascal; Bestelmeyer, Patricia E. G.; Latinus, Marianne; Watson, Rebecca] Univ Glasgow, Voice Neurocognit Lab, Inst Neurosci & Psychol, Coll Med Vet & Life Sci, Glasgow G12 8QQ, Lanark, Scotland.
   [Belin, Pascal] Univ Montreal, Int Labs Brain Mus & Sound BRAMS, Montreal, PQ, Canada.
   [Belin, Pascal] McGill Univ, Montreal, PQ, Canada.
RP Belin, P (corresponding author), 58 Hillhead St, Glasgow G12 8QB, Lanark, Scotland.
EM p.belin@psy.gla.ac.uk
CR Andics A, 2010, NEUROIMAGE, V52, P1528, DOI 10.1016/j.neuroimage.2010.05.048
   Baumann O, 2010, PSYCHOL RES-PSYCH FO, V74, P110, DOI 10.1007/s00426-008-0185-z
   Belin P, 2003, NEUROREPORT, V14, P2105, DOI 10.1097/00001756-200311140-00019
   Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008
   Belin P, 2002, COGNITIVE BRAIN RES, V13, P17, DOI 10.1016/S0926-6410(01)00084-2
   Belin P, 2000, NATURE, V403, P309, DOI 10.1038/35002078
   Belin P, 2008, BEHAV RES METHODS, V40, P531, DOI 10.3758/BRM.40.2.531
   Belin P, 2010, NEURON, V65, P733, DOI 10.1016/j.neuron.2010.03.018
   Bestelmeyer PEG, 2010, COGNITION, V117, P217, DOI 10.1016/j.cognition.2010.08.008
   Brédart S, 2009, EUR J COGN PSYCHOL, V21, P1013, DOI 10.1080/09541440802591821
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Bruckert L, 2010, CURR BIOL, V20, P116, DOI 10.1016/j.cub.2009.11.034
   Buchanan TW, 2000, COGNITIVE BRAIN RES, V9, P227, DOI 10.1016/S0926-6410(99)00060-9
   Burton AM, 2011, BRIT J PSYCHOL, V102, P943, DOI 10.1111/j.2044-8295.2011.02039.x
   Burton AM, 2004, PERCEPTION, V33, P747, DOI 10.1068/p3458
   BURTON AM, 1990, BRIT J PSYCHOL, V81, P361, DOI 10.1111/j.2044-8295.1990.tb02367.x
   Calvert GA, 2000, CURR BIOL, V10, P649, DOI 10.1016/S0960-9822(00)00513-3
   Calvert GA, 2001, CEREB CORTEX, V11, P1110, DOI 10.1093/cercor/11.12.1110
   Campanella S, 2007, TRENDS COGN SCI, V11, P535, DOI 10.1016/j.tics.2007.10.001
   Charest I., 2009, HUM BRAIN MAPP C SAN
   Charest I, 2009, BMC NEUROSCI, V10, DOI 10.1186/1471-2202-10-127
   Collignon O, 2008, BRAIN RES, V1242, P126, DOI 10.1016/j.brainres.2008.04.023
   de Gelder B, 2000, COGNITION EMOTION, V14, P289, DOI 10.1080/026999300378824
   De Lucia M, 2010, J NEUROSCI, V30, P11210, DOI 10.1523/JNEUROSCI.2239-10.2010
   Ellis A.W., 1989, Handbook of research on face processing, P207
   Ellis HD, 1997, BRIT J PSYCHOL, V88, P143, DOI 10.1111/j.2044-8295.1997.tb02625.x
   Ethofer T, 2006, NEUROREPORT, V17, P249, DOI 10.1097/01.wnr.0000199466.32036.5d
   Ethofer T, 2006, HUM BRAIN MAPP, V27, P707, DOI 10.1002/hbm.20212
   Ethofer T, 2009, CURR BIOL, V19, P1028, DOI 10.1016/j.cub.2009.04.054
   Fecteau S, 2007, NEUROIMAGE, V36, P480, DOI 10.1016/j.neuroimage.2007.02.043
   Formisano E, 2008, SCIENCE, V322, P970, DOI 10.1126/science.1164318
   Garrido L, 2009, NEUROPSYCHOLOGIA, V47, P123, DOI 10.1016/j.neuropsychologia.2008.08.003
   Gauthier I, 2007, COGNITION, V103, P322, DOI 10.1016/j.cognition.2006.05.003
   Gervais H, 2004, NAT NEUROSCI, V7, P801, DOI 10.1038/nn1291
   Ghazanfar AA, 2001, CURR OPIN NEUROBIOL, V11, P712, DOI 10.1016/S0959-4388(01)00274-4
   Grandjean D, 2005, NAT NEUROSCI, V8, P145, DOI 10.1038/nn1392
   Grill-Spector K, 1999, NEURON, V24, P187, DOI 10.1016/S0896-6273(00)80832-6
   Grossmann T, 2010, NEURON, V65, P852, DOI 10.1016/j.neuron.2010.03.001
   Hailstone JC, 2010, NEUROPSYCHOLOGIA, V48, P1104, DOI 10.1016/j.neuropsychologia.2009.12.011
   HEILMAN KM, 1975, J NEUROL NEUROSUR PS, V38, P69, DOI 10.1136/jnnp.38.1.69
   Hornak J, 1996, NEUROPSYCHOLOGIA, V34, P247, DOI 10.1016/0028-3932(95)00106-9
   Imaizumi S, 1997, NEUROREPORT, V8, P2809, DOI 10.1097/00001756-199708180-00031
   Joassin F, 2004, NEUROSCI LETT, V369, P132, DOI 10.1016/j.neulet.2004.07.067
   Joassin F, 2011, CORTEX, V47, P367, DOI 10.1016/j.cortex.2010.03.003
   Jones JA, 2003, NEUROREPORT, V14, P1129, DOI 10.1097/00001756-200306110-00006
   Kamachi M, 2003, CURR BIOL, V13, P1709, DOI 10.1016/j.cub.2003.09.005
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Kreifelts B, 2007, NEUROIMAGE, V37, P1445, DOI 10.1016/j.neuroimage.2007.06.020
   Kreiman J, 1998, J ACOUST SOC AM, V104, P1598, DOI 10.1121/1.424372
   KREIMAN J, 1992, J SPEECH HEAR RES, V35, P512, DOI 10.1044/jshr.3503.512
   Kriegstein KV, 2004, NEUROIMAGE, V22, P948, DOI 10.1016/j.neuroimage.2004.02.020
   KUHL PK, 1982, SCIENCE, V218, P1138, DOI 10.1126/science.7146899
   Latinus M., 2010, COGN NEUR M MONTR
   Latinus M., 2009, Organization for Human Brain Mapping 2009 Annual Meeting, July 2009, Neuroimage, V47, pS156
   Lattner S, 2003, NEUROSCI LETT, V339, P191, DOI 10.1016/S0304-3940(03)00027-2
   Leaver AM, 2010, J NEUROSCI, V30, P7604, DOI 10.1523/JNEUROSCI.0296-10.2010
   Leitman DI, 2011, FRONT HUM NEUROSCI, V5, DOI [10.3389/fnhum.2011.00096, 10.3389/fnhum.2010.00019]
   Leopold DA, 2001, NAT NEUROSCI, V4, P89, DOI 10.1038/82947
   Levy DA, 2001, NEUROREPORT, V12, P2653, DOI 10.1097/00001756-200108280-00013
   Levy DA, 2003, PSYCHOPHYSIOLOGY, V40, P291, DOI 10.1111/1469-8986.00031
   Linden DEJ, 2011, CEREB CORTEX, V21, P330, DOI 10.1093/cercor/bhq097
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   McKone E, 2007, TRENDS COGN SCI, V11, P8, DOI 10.1016/j.tics.2006.11.002
   Mitchell RLC, 2003, NEUROPSYCHOLOGIA, V41, P1410, DOI 10.1016/S0028-3932(03)00017-4
   Morris JS, 1999, NEUROPSYCHOLOGIA, V37, P1155, DOI 10.1016/S0028-3932(99)00015-9
   MURRY T, 1980, J ACOUST SOC AM, V68, P1294, DOI 10.1121/1.385122
   Nakamura K, 2001, NEUROPSYCHOLOGIA, V39, P1047, DOI 10.1016/S0028-3932(01)00037-9
   Noppeney U, 2010, J NEUROSCI, V30, P7434, DOI 10.1523/JNEUROSCI.0455-10.2010
   Nygaard LC, 1998, PERCEPT PSYCHOPHYS, V60, P355, DOI 10.3758/BF03206860
   PAPCUN G, 1989, J ACOUST SOC AM, V85, P913, DOI 10.1121/1.397564
   Pell MD, 2003, COGN AFFECT BEHAV NE, V3, P275, DOI 10.3758/CABN.3.4.275
   Petkov CI, 2008, NAT NEUROSCI, V11, P367, DOI 10.1038/nn2043
   Phillips ML, 1998, P ROY SOC B-BIOL SCI, V265, P1809, DOI 10.1098/rspb.1998.0506
   PISONI DB, 1993, SPEECH COMMUN, V13, P109, DOI 10.1016/0167-6393(93)90063-Q
   Rämä P, 2001, NEUROIMAGE, V13, P1090, DOI 10.1006/nimg.2001.0777
   Sander K, 2005, J COGNITIVE NEUROSCI, V17, P1519, DOI 10.1162/089892905774597227
   Schirmer A, 2006, TRENDS COGN SCI, V10, P24, DOI 10.1016/j.tics.2005.11.009
   Schweinberger SR, 2001, NEUROPSYCHOLOGIA, V39, P921, DOI 10.1016/S0028-3932(01)00023-9
   Schweinberger SR, 1997, J SPEECH LANG HEAR R, V40, P453, DOI 10.1044/jslhr.4002.453
   Schweinberger SR, 2008, CURR BIOL, V18, P684, DOI 10.1016/j.cub.2008.04.015
   Schweinberger SR, 2007, Q J EXP PSYCHOL, V60, P1446, DOI 10.1080/17470210601063589
   Sestieri C, 2006, NEUROIMAGE, V33, P672, DOI 10.1016/j.neuroimage.2006.06.045
   Stevens AA, 2004, COGNITIVE BRAIN RES, V18, P162, DOI 10.1016/j.cogbrainres.2003.10.008
   Tempini MLG, 1998, BRAIN, V121, P2103, DOI 10.1093/brain/121.11.2103
   Van Lancker D R, 1982, Brain Cogn, V1, P185, DOI 10.1016/0278-2626(82)90016-1
   VANLANCKER D, 1992, J SPEECH HEAR RES, V35, P963, DOI 10.1044/jshr.3505.963
   VANLANCKER D, 1987, NEUROPSYCHOLOGIA, V25, P829, DOI 10.1016/0028-3932(87)90120-5
   VANLANCKER DR, 1988, CORTEX, V24, P195, DOI 10.1016/S0010-9452(88)80029-7
   von Kriegstein K, 2003, COGNITIVE BRAIN RES, V17, P48, DOI 10.1016/S0926-6410(03)00079-X
   von Kriegstein K, 2006, PLOS BIOL, V4, P1809, DOI 10.1371/journal.pbio.0040326
   von Kriegstein K, 2006, CEREB CORTEX, V16, P1314, DOI 10.1093/cercor/bhj073
   Webster MA, 2004, NATURE, V428, P557, DOI 10.1038/nature02420
   Wildgruber D, 2004, CEREB CORTEX, V14, P1384, DOI 10.1093/cercor/bhh099
   Wildgruber D, 2002, NEUROIMAGE, V15, P856, DOI 10.1006/nimg.2001.0998
   Winston JS, 2004, J NEUROPHYSIOL, V92, P1830, DOI 10.1152/jn.00155.2004
   Young AW, 2011, BRIT J PSYCHOL, V102, P959, DOI 10.1111/j.2044-8295.2011.02045.x
   Zäske R, 2010, HEARING RES, V268, P38, DOI 10.1016/j.heares.2010.04.011
NR 97
TC 199
Z9 223
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
J9 BRIT J PSYCHOL
JI Br. J. Psychol.
PD NOV
PY 2011
VL 102
SI SI
BP 711
EP 725
DI 10.1111/j.2044-8295.2011.02041.x
PN 4
PG 15
UT WOS:000296085900003
DA 2024-04-26
ER
FN Clarivate Anlytics Web of Science
VR 1.0
PT C
AU Assmann, PF
    Dembling, S
    Nearey, TM
AF Assmann, Peter F
    Dembling, Sophie
    Nearey, Terrance M
TI Investigating the Uncanny Valley Effect for Prosody
SO INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING
LA English
DE [Uncanny Valley, Naturalness in Speech, Phonetics]
AB In natural speech, there is a moderate correlation between the fundamental frequency and formant frequencies across talkers. The present study used a high-quality vocoder to manipulate these properties and determine their contribution to perceived naturalness and voice gender. The stimuli were re-synthesized sentences spoken by two adult males and two adult females. Scale factors were chosen for each sentence and for each talker to produce frequency-shifted versions with a specified mean fundamental frequency (F0) ranging from 60 Hz to 450 Hz in 10 steps, paired with 10 steps in geometric mean formant frequencies ranging from 850 Hz to 2500 Hz. Listeners judged frequency-shifted sentences as more natural when F0 and formant frequencies followed the co-variation of F0 and formant frequencies in natural voices. Sentences with low F0s and low formant frequencies were perceived as masculine, while sentences with high F0 and high formant frequencies were assigned high ratings of femininity. Sentences with "mismatched" F0 and formant frequencies were assigned ratings near the midpoint of the range, indicating gender ambiguity. Frequency-shifted sentences derived from male talkers received consistently higher ratings of masculinity than those derived from females, while sentences from female talkers received higher ratings of femininity, even when assigned scale factors appropriate for the opposite gender, indicating that factors other than F0 and mean formant frequencies contribute to perceived gender.
CR [Placeholder]
NR [Placeholder]
SN 2308-457X
J9 INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING
JI INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING
PD 2006 SEPT
PY 2006
VL 1-5
IS 19
BP 889
EP 892
DI 10.21437/Interspeech.2006-297
DA 2024-01-09
ER

PT C
AU Baird, A
    Joergensen, S H
    Parada-Cabaleiro, E
    Hantke, S
    Cummins, N
    Schuller, B
AF Baird, Alice
    Joergensen, Stina Hasse
    Parada-Cabaleiro, Emilia
    Hantke, Simone
    Cummins, Nicholas
    Schuller, Bjoern
TI Perception of Paralinguistic Traits in Synthesized Voices
SO Proceedings of AM ’17, London, United Kingdom
LA English
DE Synthesized Voice, Humanisation of Synthesis, Human–Machine Interaction, Paralinguistic Traits, Personification Debate
AB Along with the rise of artificial intelligence and the internet-of-things, synthesized voices are now common in daily--life, providing us with guidance, assistance, and even companionship. From formant to concatenative synthesis, the synthesized voice continues to be defined by the same traits we prescribe to ourselves. When the recorded voice is synthesized, does our perception of its new machine embodiment change, and can we consider an alternative, more inclusive form? To begin evaluating the impact of aesthetic design, this study presents a first--step perception test to explore the paralinguistic traits of the synthesized voice. Using a corpus of 13 synthesized voices, constructed from acoustic concatenative speech synthesis, we assessed the response of 23 listeners from differing cultural backgrounds. To evaluate if perception shifts from the defined traits, we asked listeners to assigned traits of age, gender, accent origin, and human--likeness. Results present a difference in perception for age and human--likeness across voices, and a general agreement across listeners for both gender and accent origin. Connections found between age, gender and human--likeness call for further exploration into a more participatory and inclusive synthesized vocal identity.
CR [Placeholder]
NR [Placeholder]
J9 P AM ’17 London
JI P AM ’17 London
PD 2017 AUG
PY 2017
VL 999
IS 12
BP 23
EP 26
DI 10.1145/3123514.3123528
DA 2024-01-09
ER

PT C
AU Baird, A
    Parada-Cabaleiro, E
    Hantke, S
    Burkhardt, F
    Cummins, N
    Schuller, B
AF Baird, Alice
    Parada-Cabaleiro, Emilia
    Hantke, Simone
    Burkhardt, Felix
    Cummins, Nicholas
    Schuller, Bjoern
TI The Perception and Analysis of the Likeability and Human Likeness of Synthesized Speech
SO INTERSPEECH 2018
LA English
DE synthesized voices, human likeness, likeability
AB The synthesized voice has become an ever present aspect of daily life. Heard through our smart-devices and from public announcements, engineers continue in an endeavour to achieve naturalness in such voices. Yet, the degree to which these methods can produce likeable, human like voices, has not been fully evaluated. With recent advancements in synthetic speech technology suggesting that human like imitation is more obtainable, this study asked 25 listeners to evaluate both the likeability and human likeness of a corpus of 13 German male voices, produced via 5 synthesis approaches (from formant to hybrid unit selection, deep neural network systems), and 1 Human control. Results show that unlike visual artificially intelligent elements – as posed by the concept of the Uncanny Valley – likeability consistently improves along with human likeness for the synthesized voice, with recent methods achieving substantially closer results to human speech than older methods. A small scale acoustic analysis shows that the F0 of hybrid systems correlates less closely to human speech with a higher standard deviation for F0. This analysis suggests that limited variance in F0 is linked to a reduction in human likeness, resulting in lower likeability for conventional synthetic speech methods.
CR [Placeholder]
NR [Placeholder]
SN 2308-457X
J9 INTERSPEECH
JI Interspeech 2018
PD 2018 SEPT 06
PY 2018
BP 2863
EP 2867
DI 10.21437/Interspeech.2018-1093
DA 2024-01-09
ER

PT [Placeholder]
AU Cabral, JP
    Cowan, BR
    Zibrek, K
    McDonnell, R
AF Cabral, João Paulo
    Cowan, Benjamin R
    Zibrek, Katja
    McDonnell, Rachel
TI The Influence of Synthetic Voice on the Evaluation of a Virtual Character
SO INTERSPEECH 2017
LA English
ID expressive speech, synthetic voice evaluation, avatars
AB Graphical realism and the naturalness of the voice used are important aspects to consider when designing a virtual agent or character. In this work, we evaluate how synthetic speech impacts people’s perceptions of a rendered virtual character. Using a controlled experiment, we focus on the role that speech, in particular voice expressiveness in the form of personality, has on the assessment of voice level and character level perceptions. We found that people rated a real human voice as more expressive, understandable and likeable than the expressive synthetic voice we developed. Contrary to our expectations, we found that the voices did not have a significant impact on the character level judgments; people in the voice conditions did not significantly vary on their ratings of appeal, credibility, human-likeness and voice matching the character. The implications this has for character design and how this compares with previous work are discussed.
CR [Placeholder]
NR [Placeholder]
J9 INTERSPEECH 2017
JI Interspeech 2017
PD 2017 AUG
PY 2017
VL 999
IS 2308-457X
BP 229
EP 233
DI 10.21437/Interspeech.2017-325
DA 2024-01-09
ER

PT J
AU Diel, A
    Lewis, M
AF Diel, Alexander
    Lewis, Michael
TI The vocal uncanny valley: Deviation from typical organic voices best explains uncanniness.
SO Research Square
LA English
DE uncanny valley, voice processing, pathological voice, voice distortion, text-to-speech, deviation from familiarity
AB The uncanny valley describes the negative evaluation of near humanlike artificial entities. Previous research with synthetic and real voices failed to find an uncanny valley of voices. This may have been due to the selection of stimuli. In Experiment 1 (n = 50), synthetic, normal, and deviating voices (distorted and pathological) were rated on uncanniness and human likeness and categorized as human or nonhuman. Results showed a non-monotonic function when the uncanniness was plotted against human likeness indicative of an uncanny valley. However, the shape could be divided into two monotonic functions based on voice type (synthetic vs deviating). Categorization ambiguity could not predict voice uncanniness but moderated the effect of realism on uncanniness. Experiment 2 (n = 35) found that perceived organicness of voices significantly moderated the effect of realism on uncanniness, while attribution of mind or animacy did not. Results indicate a vocal uncanny valley re-imagined as monotonic functions of two types of deviations from typical human voices. While voices can fall into an uncanny valley, synthetic voices successfully escape it. Finally, the results support the account that uncanniness is caused by deviations from familiar categories, rather than categorical ambiguity or the misattribution of mind or animacy.
CR [Placeholder]
NR [Placeholder]
J9 Research Square
JI Research Square
PD 2023 APRIL 14
PY 2023
BP 1
EP 23
DI 10.21203/rs.3.rs-2784067/v1
DA 2024-01-09
ER

PT C
AU Ehret, J
    Bönsch, A
    Aspöck, L
    Röhr, C T
    Baumann, S
    Grice, M
    Fels, J
    Kuhlen, T W
AF Ehret, Jonathan
    Bönsch, Andrea
    Aspöck, Lukas
    Röhr, Christine T
    Baumann, Stefan
    Grice, Martine
    Fels, Janina
    Kuhlen, Thorsten W
TI Do Prosody and Embodiment Influence the Perceived Naturalness of Conversational Agents’ Speech?
SO ACM Trans. Appl. Percept.
LA English
ID Computing methodologies, Phonology, morphology, Intelligent agents, Human-centered computing, User studies, Natural language interfaces,
DE Embodied conversational agents (ECAs), virtual acoustics, prosody, accentuation, speech, text-to-speech, audio, embodiment
AB For conversational agents’ speech, either all possible sentences have to be prerecorded by voice actors or the required utterances can be synthesized. While synthesizing speech is more flexible and economic in production, it also potentially reduces the perceived naturalness of the agents among others due to mistakes at various linguistic levels. In our article, we are interested in the impact of adequate and inadequate prosody, here particularly in terms of accent placement, on the perceived naturalness and aliveness of the agents. We compare (1) inadequate prosody, as generated by off-the-shelf text-to-speech (TTS) engines with synthetic output; (2) the same inadequate prosody imitated by trained human speakers; and (3) adequate prosody produced by those speakers. The speech was presented either as audio-only or by embodied, anthropomorphic agents, to investigate the potential masking effect by a simultaneous visual representation of those virtual agents. To this end, we conducted an online study with 40 participants listening to four different dialogues each presented in the three Speech levels and the two Embodiment levels. Results confirmed that adequate prosody in human speech is perceived as more natural (and the agents are perceived as more alive) than inadequate prosody in both human (2) and synthetic speech (1). Thus, it is not sufficient to just use a human voice for an agents’ speech to be perceived as natural—it is decisive whether the prosodic realisation is adequate or not. Furthermore, and surprisingly, we found no masking effect by speaker embodiment, since neither a human voice with inadequate prosody nor a synthetic voice was judged as more natural, when a virtual agent was visible compared to the audio-only condition. On the contrary, the human voice was even judged as less “alive” when accompanied by a virtual agent. In sum, our results emphasize, on the one hand, the importance of adequate prosody for perceived naturalness, especially in terms of accents being placed on important words in the phrase, while showing, on the other hand, that the embodiment of virtual agents plays a minor role in the naturalness ratings of voices.
CR [Placeholder]
NR [Placeholder]
J9 ACM T APPL PERCEPT
JI ACM T. Appl. Percept.
PD 2021 OCT
PY 2021
VL 18
IS 4
BP 1
EP 15
DI 10.1145/3486580
DA 2024-01-09
ER

PT [Placeholder]
AU Eyssel, F
    Kuchenbrandt, D 
    Bobinger, S
    de Ruiter, L
    Hegel, F
AF Eyssel, Friederike
    Kuchenbrandt, Dieta 
    Bobinger, Simon
    de Ruiter, Laura
    Hegel, Frank
TI 'If You Sound Like Me, You Must Be More Human’: On the Interplay of Robot and User Features on Human-Robot Acceptance and Anthropomorphism
SO HRI '12: Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction
LA [Placeholder]
DE Human-Robot Interaction; Anthropomorphism; Gender Stereotypes; Social Robotics.
AB In an experiment we manipulated a robot’s voice in two ways: First, we varied robot gender; second, we equipped the robot with a human-like or a robot-like synthesized voice. Moreover, we took into account user gender and tested effects of these factors on human–robot acceptance, psychological closeness and psychological anthropomorphism. When participants formed an impression of a same-gender robot, the robot was perceived more positively. Participants also felt more psychological closeness to the same-gender robot. Similarly, the same-gender robot was anthropomorphized more strongly, but only when it utilized a human-like voice. Results indicate that a projection mechanism could underlie these effects.
CR [Placeholder]
NR [Placeholder]
J9 ACM HRI '12
JI ACM HRI '12
PD 2012 MARCH
PY 2012
VL 7
BP 125
EP 126
DI 10.1145/2157689.2157717
DA 2024-01-09
ER

PT C
AU Ferstl, Y
    Thomas, S
    Guiard, C
    Ennis, C
    McDonnell, R
AF Ferstl, Ylva
    Thomas, Sean
    Guiard, Cédric
    Ennis, Cathy
    McDonnell, Rachel
TI Human or Robot?: Investigating voice, appearance and gesture motion realism of conversational social agents
SO IVA '21: Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents
LA English
DE gesture motion, text-to-speech, animation style, perception, conversational agents, agent design, human-computer interfaces, anthropomorphism
AB Research on creation of virtual humans enables increasing automatization of their behavior, including synthesis of verbal and nonverbal behavior. As the achievable realism of different aspects of agent design evolves asynchronously, it is important to understand if and how divergence in realism between behavioral channels can elicit negative user responses. Specifically, in this work, we investigate the question of whether autonomous virtual agents relying on synthetic text-to-speech voices should portray a corresponding level of realism in the non-verbal channels of motion and visual appearance, or if, alternatively, the best available realism of each channel should be used. In two perceptual studies, we assess how realism of voice, motion, and appearance influence the perceived match of speech and gesture motion, as well as the agent's likability and human-likeness. Our results suggest that maximizing realism of voice and motion is preferable even when this leads to realism mismatches, but for visual appearance, lower realism may be preferable. (A video abstract can be found at https://youtu.be/arfZZ-hxD1Y.)
CR [Placeholder]
NR [Placeholder]
J9 ACM INT Virt Agents
JI ACM Int. Virt. Agents
PD 2021 SEPT
PY 2021
VL 21
BP 76
EP 83
DI 10.1145/3472306.3478338
DA 2024-01-09
ER

PT C
AU Ilves, M
    Surakka, V
    Vanhala, T
AF Ilves, Mirja
    Surakka, Veikko
    Vanhala, Toni
TI The effects of emotionally worded synthesized speech on the ratings of emotions and voice quality
SO ACM Affective Computing and Intelligent Interaction 2011
LA English
DE emotions; speech synthesis; facial expression; voice quality
ID human-centered computing, human computer Interaction
AB The present research investigated how the verbal content of synthetic messages affects participants' emotional responses and the ratings of voice quality. 28 participants listened to emotionally worded sentences produced by a monotonous and a prosodic tone of voice while the activity of corrugator supercilii facial muscle was measured. Ratings of emotions and voice quality were also collected. The results showed that the ratings of emotions were significantly affected by the emotional contents of the sentences. The prosodic tone of voice evoked more emotion-relevant ratings of arousal than the monotonous voice. Corrugator responses did not seem to reflect emotional reactions. Interestingly, the quality of the same voice was rated higher when the content of the sentences was positive as compared to the neutral and negative sentences. Thus, the emotional content of the spoken messages can be used to regulate users' emotions and to evoke positive feelings about the voices.
CR [Placeholder]
NR [Placeholder]
J9 ACM ASCII
JI ACM ASCII 2011
PD 2011 OCT
PY 2011
VL 4
BP 588
EP 598
DA 2024-01-09
ER

PT J
AU Klopfenstein, M
AF Klopfenstein, Marie
TI  Speech naturalness ratings and perceptual correlates of highly natural and unnatural speech in hypokinetic dysarthria secondary to Parkinson’s disease
SO Journal of Interactional Research in Communication Disorders
LA English
DE dysarthria, speech naturalness, speech perception 
AB Despite the importance of speech naturalness to treatment outcomes, little research has been done on what constitutes speech naturalness and how to best maximize naturalness in relationship to other treatment goals like intelligibility. This study investigated the speech naturalness ratings of individuals with dysarthria and the associated perceptual correlates of highly natural and unnatural speech. Four speakers with hypokinetic dysarthria secondary to Parkinson’s disease were recorded and rated for naturalness by 69 students in Communication Disorders. Students were presented with 436 speech samples and asked to provide speech naturalness ratings on a 1-9 Likert scale. After rating speech samples, subjects listed perceptual cues associated with samples rated most and least natural and weighted each cue on a visual analog scale. The data on naturalness ratings showed that spontaneous speech was rated the least natural on average, while sentences from a short story were rated slightly more natural and individually read sentences were rated the most natural of all of the utterance types. Thirteen themes emerged from the perceptual cues collected. Of the thirteen themes, intelligibility was rated significantly more important than other cues in highly natural speech and intelligibility and articulation were rated significantly more important than other cues in highly unnatural speech.
CR [Placeholder]
NR [Placeholder]
J9 J INTERACT RES COM D
JI J. Interact. Res. Com. D.
PD 2016 JUNE 21
PY 2016
VL 7
IS 1
BP 123
EP 146
DI 10.1558/jircd.v7i1.27932 
DA 2024-01-09
ER

PT C
AU Malisz, Z
    Henter, G E
    Valentini-Botinhao, C
    Watts, O
    Beskow, J
    Gustafson, J
AF Malisz, Zofia
    Henter, Gustav Eje
    Valentini-Botinhao, Cassia
    Watts, Oliver
    Beskow, Jonas
    Gustafson, Joakim
TI [Placeholder]
SO Proceedings of the 19th International Congress of Phonetic Sciences ICPhS 2019.
LA Speech synthesis, scientific methodology, speech technology
DE English
AB Decades of gradual advances in speech synthesis have recently culminated in exponential improvements fuelled by deep learning. This quantum leap has the potential to finally deliver realistic, controllable, and robust synthetic stimuli for speech experiments. In this article, we discuss these and other implications for phonetic sciences. We substantiate our argument by evaluating classic rulebased formant synthesis against state-of-the-art synthesisers on a) subjective naturalness ratings and b) a behavioural measure (reaction times in a lexical decision task). We also differentiate between text-to-speech and speech-to-speech methods. Naturalness ratings indicate that all modern systems are substantially closer to natural speech than formant synthesis. Reaction times for several modern systems do not differ substantially from natural speech, meaning that the processing gap observed in older systems, and reproduced with our formant synthesiser, is no longer evident. Importantly, some speech-tospeech methods are nearly indistinguishable from natural speech on both measures.
CR [Placeholder]
NR [Placeholder]
J9 ICPhS 2019
JI ICPhS 2019
PD 2019 AUG 04
PY 2019
VL 19
BP 487
EP 491
DI 10.31234/osf.io/dxvhc
DA 2024-01-09
ER

PT C
AU McGinn, C
    Torre, I
AF McGinn, Conor
    Torre, Ilaria
TI Can you tell the robot by the voice? An exploratory study on the role of voice in the perception of robots
SO 14th ACM/IEEE international Conference on human-robot interaction (HRI)
LA English
DE Robot design; Voice; Speech; Mental model
AB It is well established that a robot’s visual appearance plays a significant role in how it is perceived. Considerable time and resources are usually dedicated to help ensure that the visual aesthetics of social robots are pleasing to users and helps facilitate clear communication. However, relatively little consideration is given to how the voice of the robot should sound, which may have adverse effects on acceptance and clarity of communication. In this study, we explore the mental images people form when they hear robots speaking. In our experiment, participants listened to several voices, and for each voice they were asked to choose a robot, from a selection of eight commonly used social robot platforms, that was best suited to have that voice. The voices were manipulated in terms of naturalness, gender, and accent. Results showed that a) participants seldom matched robots with the voices that were used in previous HRI studies, b) the gender and naturalness vocal manipulations strongly affected participants’ selection, and c) the linguistic content of the utterances spoken by the voices does not affect people’s selection. This finding suggests that people associate voices with robot pictures, even when the content of spoken utterances was unintelligible. Our findings indicate that both a robot’s voice and its appearance contribute to robot perception. Thus, giving a mismatched voice to a robot might introduce a confounding effect in HRI studies. We therefore suggest that voice design should be considered more thoroughly when planning spoken human-robot interactions.
CR [Placeholder]
NR [Placeholder]
J9 ACMIEEE INT CONF HUM
JI ACMIEEE Int. Conf. Hum.
PD 2019
PY 2019
VL 14
BP 211
EP 221
DI 10.1109/HRI.2019.8673279
DA 2024-01-09
ER

PT J
AU Nusbaum, H C
    Francis, A L
    Henly, A S
AF Nusbaum, Howard C
    Francis, Alexander L
    Henly, Anne S
TI Measuring the naturalness of synthetic speech
SO International Journal of Speech Technology,
LA English
DE synthetic speech, naturalness, intelligibility, perception
AB Even the highest quality synthetic speech generated by rule sounds unlike human speech. As the intel-ligibility of rule-based synthetic speech improves, and the number of applications for synthetic speech increases,the naturalness of synthetic speech will become an important factor in determining its use. In order to improve thisaspect of the quality of synthetic speech it is necessary to have diagnostic tests that can measure naturalness. Cur-rently, all of the available metrics for evaluating the acceptability of synthetic speech do not distinguish sufficientlybetween measuring overall acceptability (including naturalness) and simply measuring the ability of listeners toextract intelligible information from the signal. In this paper we propose a new methodology for measuring thenaturalness of particular aspects of synthesized speech, independent of the intelligibility of the speech. Althoughnaturalness is a multidimensional, subjective quality of speech, this methodology makes it possible to assess theseparate contributions of prosodic, segmental, and source characteristics of the utterance. In two experiments, lis-teners reliably differentiated the naturalness of speech produced by two male talkers and two text-to-speech systems.Furthermore, they reliably differentiated between the two text-to-speech systems. The results of these experimentsdemonstrate that perception of naturalness is affected by information contained within the smallest part of speech,the glottal pulse, and by information contained within the prosodic structure of a syllable. These results show thatthis new methodology does provide a solid basis for measuring and diagnosing the naturalness of synthetic speech.
CR [Placeholder]
NR [Placeholder]
J9 International Journal of Speech Technology
JI Int. J. of Speech Technology
PD 1995
PY 1995
VL 1
BP 7
EP 19
DI 10.1007/BF02277176
DA 2024-01-09
ER

PT J
AU Parmar, D
    Olafsson, S
    Utami, D
    Murali, P
    Bickmore, T
AF Parmar, Dhaval
    Olafsson, Stefan
    Utami, Dina
    Murali, Prasanth
    Bickmore, Timothy
TI Designing empathic virtual agents: manipulating animation, voice, rendering, and empathy to create persuasive agents
SO Autonomous agents and multi-agent systems
LA English
DE Virtual agents, Animation fidelity, Voice quality, Rendering style, Simulated empathy, Agent perception
AB Designers of virtual agents have a combinatorically large space of choices for the look andbehavior of their characters. We conducted two between-subjects studies to explore the systematicmanipulation of animation quality, speech quality, rendering style, and simulatedempathy, and its impact on perceptions of virtual agents in terms of naturalness, engagement,trust, credibility, and persuasion within a health counseling domain. In the firststudy, animation was varied between manually created, procedural, or no animations; voicequality was varied between recorded audio and synthetic speech; and rendering style wasvaried between realistic and toon-shaded. In the second study, simulated empathy of theagent was varied between no empathy, verbal-only empathic responses, and full empathyinvolving verbal, facial, and immediacy feedback. Results show that natural animations andrecorded voice are more appropriate for the agent’s general acceptance, trust, credibility,and appropriateness for the task. However, for a brief health counseling task, animationmight actually be distracting from the persuasive message, with the highest levels of persuasionfound when the amount of agent animation is minimized. Further, consistent andhigh levels of empathy improve agent perception but may interfere with forming a trustingbond with the agent.
CR [Placeholder]
NR [Placeholder]
J9 AUTON AGENT MULTI-AG
JI Auton. Agent. Multi.-Ag.
PD 2022 FEB 22
PY [Placeholder]
VL 36
IS 1
PG 24
DI 10.1007/s10458-021-09539-1
DA 2024-01-09
ER

PT J
AU Ratcliff, A
    Coughlin, S
    Lehman, M
AF Ratcliff, Ann
    Coughlin, Sue
    Lehman, Mark
TI Factors influencing ratings of speech naturalness in augmentative and alternative communication
SO Augmentative and Alternative Communication
LA [Placeholder]
DE augmentative and alternative communication (AAC), speech naturalness, synthesized speech 
AB The concept of speech naturalness has been used in the field of speech-language pathology as a clinical measure of perceptual quality of “normal” and “not normal” speech. Whereas measures of intelligibility have been commonly used to assess the quality of voice output augmentative and alternative communication (AAC) devices using DECTalk™ speech, measures of speech naturalness have not. Three studies were conducted to determine the effects of manipulation of rate, pitch, and pause on ratings of speech naturalness by naive listeners of DECTalk synthetic speech. The results indicate that DECTalk speech characterized by faster rate and no added pauses was perceived as being more natural than speech with slow rate and added pauses. Manipulation of pitch had no effect on naturalness ratings.
CR [Placeholder]
NR [Placeholder]
J9 AUGMENT ALTERN COMM
JI Augment. Altern. Comm.
PD 2002 MARCH 18
PY 2002
VL 18
IS 1
BP 11
EP 19
DI 10.1080/aac.18.1.11.19
DA 2024-01-09
ER

PT C
AU Romportl, J
AF Romportl, Jan
TI Speech synthesis and uncanny valley
SO International conference on text, speech, and dialogue
LA English
DE text-to-speech synthesis; spoken dialogue system; uncanny valley; experiment
AB The paper discusses a hypothesis relating high quality text-to-speech(TTS) synthesis in spoken dialogue systems with the concept of “uncannyvalley”. It introduces a “Wizard-of-Oz” experiment with 30 volunteers engagedin conversations with two synthetic voices of different naturalness. The results ofthe experiment are summarized and interpreted, leading to the conclusion that theTTS uncanny valley effect in dialogue systems can probably be superseded andinverted by a positive attitude of the systems’ users toward new technologies.
CR [Placeholder]
NR [Placeholder]
J9 Speech and Dialogue. 17th International Conference, TSD 2014. Proceedings: LNCS 8655
JI Speech and Dialogue. 17th International Conference, TSD 2014. Proceedings: LNCS 8655
PY 2014
BP 595
EP 602
DI 10.1007/978-3-319-10816-2_72
DA 2024-01-09
ER

PT C
AU Urakami, J
    Sutthithatip, S
    Moore, B A
AF Urakami, Jaqueline
    Sutthithatip, Sujitra
    Moore, Billie Akwa
TI The effect of naturalness of voice and empathic responses on enjoyment, attitudes and motivation for interacting with a voice user interface
SO Human-Computer Interaction. Multimodal and Natural Interaction: Thematic Area, HCI 2020
LA English
DE Voice user interface, Empathy, Human likeness
AB In human-computer interaction much attention is given to the devel-opment of natural and intuitive Voice User Interfaces (VUI). However, previousresearch has shown that humanlike systems will not necessarily be perceived pos-itive by users. The study reported here examined the effect of human likeness onusers’ rating of enjoyment, attitudes and motivation to use VUI in a Wizard-of-Ozexperiment. Two attributes of human likeness, voice of the system (humanlike vs.machinelike) and social behavior of the system (expressing empathy vs. neutral)were manipulated. Regression analyses confirmed that perceived empathy of theVUI improved interaction enjoyment, attitude towards the system, and intrinsicmotivation but no effect of voice was found. Session order also affected par-ticipants’ evaluation. In the second session, participants rated the VUI as morenegative than in the first session. The results indicate that a VUI that expressessocial behavior (e.g. showing empathy) is perceived as more favorable by the user.Furthermore, changing user expectations pose a challenge for the design of theVUI. The dynamics of user interactions must be taken into account when designingthe VUI.
CR [Placeholder]
NR [Placeholder]
J9 Human-Computer Interaction. Multimodal and Natural Interaction. Thematic Area, HCI 2020 Held as Part of the 22nd International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12182)
JI Human-Computer Interaction. Multimodal and Natural Interaction. Thematic Area, HCI 2020 Held as Part of the 22nd International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12182)
PD 2020 JULY 10
PY 2020
VL 12182
BP 244
EP 259
DI 10.1007/978-3-030-49062-1_17
DA 2024-01-09
ER


PT [Placeholder]
AU Velner, E
    Boersma, P P G
    de Graaf, M M A
AF Velner, Ella
    Boersma, Paul P G
    de Graaf, Maartje M A
TI Intonation in Robot Speech: Does it work the same as with people?
SO HRI '20: Proceedings of the 2020 ACM/IEEE International Conference on Human-Robot Interaction
LA English
DE Conversation Analysis; Human-Robot Interaction; Linguistics; Speech Intonation; Turn Taking
AB Human-robot interaction (HRI) research aims to design natural interactions between humans and robots. Intonation, a social signaling function in human speech investigated thoroughly in linguistics, has not yet been studied in HRI. This study investigates the effect of robot speech intonation in four conditions (no intonation, focus intonation, end-of-utterance intonation, or combined intonation) on conversational naturalness, social engagement, and people’s humanlike perception of the robot collecting objective and subjective data of participant conversations (n = 120). Our results showed that humanlike intonation partially improved subjective naturalness but not observed fluency, and that intonation partially improved social engagement but did not affect humanlike perceptions of the robot. Given that our results mainly differed from our hypotheses based on human speech intonation, we discuss the implications and provide suggestions for future research to further investigate conversational naturalness in robot speech intonation.
CR [Placeholder]
NR [Placeholder]
J9 ACMIEEE INT CONF HUM
JI ACMIEEE INT CONF HUM
PD 2020 MARCH 09
PY 2020
BP 569
EP 578
DI 10.1145/3319502.3374801
DA 2024-01-09
ER

PT B
AU Kreiman, J
    Sidtis, D
AF Kreiman, Jody
    Sidtis, Diana
TI Foundations of Voice Studies: An Interdisciplinary Approach to Voice Production and Perception
SO FDN VOICE STUDIES IN
LA English
AB Foundations of Voice Studies provides a comprehensive description and analysis of the multifaceted role that voice quality plays in human existence.
CR [Placeholder]
NR [Placeholder]
J9 FDN VOICE STUDIES IN
JI FDN VOICE STUDIES IN
PY 2011
IS 1
PG 516
DI 10.1002/9781444395068
DA 2024-01-09
ER

PT J
AU Lavan, N
    McGettigan, C
AF Lavan, Nadine
    McGettigan, Carolyn
TI A model for person perception from familiar and unfamiliar voices
SO Communications Psychology
LA English
AB When hearing a voice, listeners can form a detailed impression of the person behind the voice. Existing models of voice processing focus primarily on one aspect of person perception - identity recognition from familiar voices - but do not account for the perception of other person characteristics (e.g., sex, age, personality traits). Here, we present a broader perspective, proposing that listeners have a common perceptual goal of perceiving who they are hearing, whether the voice is familiar or unfamiliar. We outline and discuss a model - the Person Perception from Voices (PPV) model - that achieves this goal via a common mechanism of recognising a familiar person, persona, or set of speaker characteristics. Our PPV model aims to provide a more comprehensive account of how listeners perceive the person they are listening to, using an approach that incorporates and builds on aspects of the hierarchical frameworks and prototype-based mechanisms proposed within existing models of voice identity recognition.
CR [Placeholder]
NR [Placeholder]
J9 COMMUN PSYCHOL
JI Commun. Psychol.
PD 2023 JULY 2023
PY 2023
VL 1
BP 1
EP 11
DI 10.1038/s44271-023-00001-4
DA 2024-01-09
ER

EF