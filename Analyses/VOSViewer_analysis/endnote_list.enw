%0 Journal Article
%T Musician effect on perception of spectro-temporally degraded speech, vocal emotion, and music in young adolescents
%A Başkent, Deniz
%A Fuller, Christina D.
%A Galvin, John J.
%A Schepel, Like
%A Gaudrain, Etienne
%A Free, Rolien H.
%D 2018
%J The Journal of the Acoustical Society of America
%N 5
%V 143
%P 311–316
%G eng
%R 10.1121/1.5034489
%2 29857757
%X In adult normal-hearing musicians, perception of music, vocal emotion, and speech in noise has been previously shown to be better than non-musicians, sometimes even with spectro-temporally degraded stimuli. In this study, melodic contour identification, vocal emotion identification, and speech understanding in noise were measured in young adolescent normal-hearing musicians and non-musicians listening to unprocessed or degraded signals. Different from adults, there was no musician effect for vocal emotion identification or speech in noise. Melodic contour identification with degraded signals was significantly better in musicians, suggesting potential benefits from music training for young cochlear-implant users, who experience similar spectro-temporal signal degradations.
%K Acoustic Stimulation/methods
%K Adolescent
%K Auditory Perception/physiology
%K Child
%K Emotions/physiology
%K Female
%K Humans
%K Male
%K Music/psychology
%K Pitch Perception/physiology
%K Speech Perception/physiology
%K Time Factors
%K Voice/physiology
%Z Journal Article
Research Support, Non-U.S. Gov't
%~ PubMed


%0 Journal Article
%T The Effect of Training with Music on Happiness Recognition in Social Anxiety Disorder
%A Bodner, Ehud
%A Aharoni, Ronit
%A Iancu, Iulian
%D 2012
%J Journal of Psychopathology and Behavioral Assessment
%@ 0882-2689
%N 4
%V 34
%P 458–466
%R 10.1007/s10862-012-9304-7
%Z PII:  9304
%~ CrossRef


%0 Journal Article
%T Impact of Auditory-Motor Musical Training on Melodic Pattern Recognition in Cochlear Implant Users
%A Chari, Divya A.
%A Barrett, Karen C.
%A Patel, A. D.
%A Colgrove, Thomas R.
%A Jiradejvong, Patpong
%A Jacobs, Lauren Y.
%A Limb, Charles J.
%D 2020
%J Otology & Neurotology : official publication of the American Otological Society, American Neurotology Society [and] European Academy of Otology and Neurotology
%N 4
%V 41
%P e422-e431
%G eng
%R 10.1097/MAO.0000000000002525
%2 32176126
%X OBJECTIVE
Cochlear implant (CI) users struggle with tasks of pitch-based prosody perception. Pitch pattern recognition is vital for both music comprehension and understanding the prosody of speech, which signals emotion and intent. Research in normal-hearing individuals shows that auditory-motor training, in which participants produce the auditory pattern they are learning, is more effective than passive auditory training. We investigated whether auditory-motor training of CI users improves complex sound perception, such as vocal emotion recognition and pitch pattern recognition, compared with purely auditory training.
STUDY DESIGN
Prospective cohort study.
SETTING
Tertiary academic center.
PATIENTS
Fifteen postlingually deafened adults with CIs.
INTERVENTION(S)
Participants were divided into 3 one-month training groups: auditory-motor (intervention), auditory-only (active control), and no training (control). Auditory-motor training was conducted with the "Contours" software program and auditory-only training was completed with the "AngelSound" software program.
MAIN OUTCOME MEASURE
Pre and posttest examinations included tests of speech perception (consonant-nucleus-consonant, hearing-in-noise test sentence recognition), speech prosody perception, pitch discrimination, and melodic contour identification.
RESULTS
Participants in the auditory-motor training group performed better than those in the auditory-only and no-training (p < 0.05) for the melodic contour identification task. No significant training effect was noted on tasks of speech perception, speech prosody perception, or pitch discrimination.
CONCLUSIONS
These data suggest that short-term auditory-motor music training of CI users impacts pitch pattern recognition. This study offers approaches for enriching the world of complex sound in the CI user.
%Z Journal Article
%~ PubMed


%0 Journal Article
%T Emotion processing in congenital amusia: the deficits do not generalize to written emotion words
%A Cheung, Yi Lam
%A Zhang, Caicai
%A Zhang, Yubin
%D 2020
%J Clinical Linguistics & Phonetics
%P 1–16
%G eng
%R 10.1080/02699206.2020.1719209
%2 31986915
%X Congenital amusia is a lifelong impairment in musical ability. Individuals with amusia are found to show reduced sensitivity to emotion recognition in speech prosody and silent facial expressions, implying a possible cross-modal emotion-processing deficit. However, it is not clear whether the observed deficits are primarily confined to socio-emotional contexts, where visual cues (facial expression) often co-occur with auditory cues (emotion prosody) to express intended emotions, or extend to linguistic emotion processing. In order to better understand the underlying deficiency mechanism of emotion processing in individuals with amusia, we examined whether reduced sensitivity to emotional processing extends to the recognition of emotion category and valence of written words in individuals with amusia. Twenty Cantonese speakers with amusia and 17 controls were tested in three experiments: (1) emotion prosody rating, in which participants rated how much each spoken sentence was expressed in each of the four emotions on 7-point rating scales; (2) written word emotion recognition, in which participants recognized the emotion of written emotion words; and (3) written word valence judgment, in which participants judged the valence of written words. Results showed that participants with amusia preformed significantly less accurately than controls in emotion prosody recognition; in contrast, the two groups showed no significant difference in accuracy rates in both written word tasks (emotion recognition and valence judgment). The results indicate that the impairment of individuals with amusia in emotion processing may not generalize to linguistic emotion processing in written words, implying that the emotion deficit is likely to be restricted to socio-emotional contexts in individuals with amusia.
%Z Journal Article
%~ PubMed


%0 Journal Article
%T Do Individual Differences Influence Moment-by-Moment Reports of Emotion Perceived in Music and Speech Prosody?
%A Dibben, Nicola
%A Coutinho, Eduardo
%A Vilar, José A.
%A Estévez-Pérez, Graciela
%D 2018
%J Frontiers in Behavioral Neuroscience
%@ 1662-5153
%V 12
%P 184
%G eng
%R 10.3389/fnbeh.2018.00184
%2 30210316
%X Comparison of emotion perception in music and prosody has the potential to contribute to an understanding of their speculated shared evolutionary origin. Previous research suggests shared sensitivity to and processing of music and speech, but less is known about how emotion perception in the auditory domain might be influenced by individual differences. Personality, emotional intelligence, gender, musical training and age exert some influence on discrete, summative judgments of perceived emotion in music and speech stimuli. However, music and speech are temporal phenomena, and little is known about whether individual differences influence moment-by-moment perception of emotion in these domains. A behavioral study collected two main types of data: continuous ratings of perceived emotion while listening to extracts of music and speech, using a computer interface which modeled emotion on two dimensions (arousal and valence), and demographic information including measures of personality (TIPI) and emotional intelligence (TEIQue-SF). Functional analysis of variance on the time series data revealed a small number of statistically significant differences associated with Emotional Stability, Agreeableness, musical training and age. The results indicate that individual differences exert limited influence on continuous judgments of dynamic, naturalistic expressions. We suggest that this reflects a reliance on acoustic cues to emotion in moment-by-moment judgments of perceived emotions and is further evidence of the shared sensitivity to and processing of music and speech.
%Z Journal Article
%~ PubMed


%0 Journal Article
%T Ontogenetic features of the psychophysiological mechanisms of perception of the emotional component of speech in musically gifted children
%A Dmitrieva, E. S.
%A Gel'man, V. Ya
%A Zaitseva, K. A.
%A Am Orlov
%D 2006
%J Neuroscience and Behavioral Physiology
%@ 0097-0549
%N 1
%V 36
%P 53
%~ EndNote Tagged Import Format


%0 Journal Article
%T Comparison of Two Music Training Approaches on Music and Speech Perception in Cochlear Implant Users
%A Fuller, Christina D.
%A Galvin, John J.
%A Maat, Bert
%A Başkent, Deniz
%A Free, Rolien H.
%D 2018
%J Trends in hearing
%V 22
%P 2331216518765379
%G eng
%R 10.1177/2331216518765379
%2 29621947
%X In normal-hearing (NH) adults, long-term music training may benefit music and speech perception, even when listening to spectro-temporally degraded signals as experienced by cochlear implant (CI) users. In this study, we compared two different music training approaches in CI users and their effects on speech and music perception, as it remains unclear which approach to music training might be best. The approaches differed in terms of music exercises and social interaction. For the pitch/timbre group, melodic contour identification (MCI) training was performed using computer software. For the music therapy group, training involved face-to-face group exercises (rhythm perception, musical speech perception, music perception, singing, vocal emotion identification, and music improvisation). For the control group, training involved group nonmusic activities (e.g., writing, cooking, and woodworking). Training consisted of weekly 2-hr sessions over a 6-week period. Speech intelligibility in quiet and noise, vocal emotion identification, MCI, and quality of life (QoL) were measured before and after training. The different training approaches appeared to offer different benefits for music and speech perception. Training effects were observed within-domain (better MCI performance for the pitch/timbre group), with little cross-domain transfer of music training (emotion identification significantly improved for the music therapy group). While training had no significant effect on QoL, the music therapy group reported better perceptual skills across training sessions. These results suggest that more extensive and intensive training approaches that combine pitch training with the social aspects of music therapy may further benefit CI users.
%K Adult
%K Aged
%K Aged, 80 and over
%K Auditory Perception
%K Cochlear Implants
%K Humans
%K Middle Aged
%K music
%K pitch perception
%K Quality of Life
%K Speech Perception
%Z Journal Article
Research Support, Non-U.S. Gov't
%~ PubMed


%0 Journal Article
%T The musician effect: does it persist under degraded pitch conditions of cochlear implant simulations?
%A Fuller, Christina D.
%A Galvin, John J.
%A Maat, Bert
%A Free, Rolien H.
%A Başkent, Deniz
%D 2014
%J Frontiers in Neuroscience
%@ 1662-4548
%V 8
%P 179
%G eng
%R 10.3389/fnins.2014.00179
%2 25071428
%X Cochlear implants (CIs) are auditory prostheses that restore hearing via electrical stimulation of the auditory nerve. Compared to normal acoustic hearing, sounds transmitted through the CI are spectro-temporally degraded, causing difficulties in challenging listening tasks such as speech intelligibility in noise and perception of music. In normal hearing (NH), musicians have been shown to better perform than non-musicians in auditory processing and perception, especially for challenging listening tasks. This "musician effect" was attributed to better processing of pitch cues, as well as better overall auditory cognitive functioning in musicians. Does the musician effect persist when pitch cues are degraded, as it would be in signals transmitted through a CI? To answer this question, NH musicians and non-musicians were tested while listening to unprocessed signals or to signals processed by an acoustic CI simulation. The task increasingly depended on pitch perception: (1) speech intelligibility (words and sentences) in quiet or in noise, (2) vocal emotion identification, and (3) melodic contour identification (MCI). For speech perception, there was no musician effect with the unprocessed stimuli, and a small musician effect only for word identification in one noise condition, in the CI simulation. For emotion identification, there was a small musician effect for both. For MCI, there was a large musician effect for both. Overall, the effect was stronger as the importance of pitch in the listening task increased. This suggests that the musician effect may be more rooted in pitch perception, rather than in a global advantage in cognitive processing (in which musicians would have performed better in all tasks). The results further suggest that musical training before (and possibly after) implantation might offer some advantage in pitch processing that could partially benefit speech perception, and more strongly emotion and music perception.
%Z Journal Article
%~ PubMed


%0 Journal Article
%T Psychoacoustic abilities as predictors of vocal emotion recognition
%A Globerson, Eitan
%A Amir, Noam
%A Golan, Ofer
%A Kishon‐Rabin, Liat
%A Lavidor, Michal
%D 2013
%J Attention, Perception & Psychophysics
%N 8
%V 75
%P 1799–1810
%G eng
%R 10.3758/s13414-013-0518-x
%2 23893469
%X Prosodic attributes of speech, such as intonation, influence our ability to recognize, comprehend, and produce affect, as well as semantic and pragmatic meaning, in vocal utterances. The present study examines associations between auditory perceptual abilities and the perception of prosody, both pragmatic and affective. This association has not been previously examined. Ninety-seven participants (49 female and 48 male participants) with normal hearing thresholds took part in two experiments, involving both prosody recognition and psychoacoustic tasks. The prosody recognition tasks included a vocal emotion recognition task and a focus perception task requiring recognition of an accented word in a spoken sentence. The psychoacoustic tasks included a task requiring pitch discrimination and three tasks also requiring pitch direction (i.e., high/low, rising/falling, changing/steady pitch). Results demonstrate that psychoacoustic thresholds can predict 31% and 38% of affective and pragmatic prosody recognition scores, respectively. Psychoacoustic tasks requiring pitch direction recognition were the only significant predictors of prosody recognition scores. These findings contribute to a better understanding of the mechanisms underlying prosody recognition and may have an impact on the assessment and rehabilitation of individuals suffering from deficient prosodic perception.
%K Adult
%K Emotions/physiology
%K Female
%K Humans
%K Male
%K pitch discrimination
%K Psychoacoustics
%K Recognition, Psychology
%K Speech Perception/physiology
%K voice
%K Young Adult
%Z Journal Article
Research Support, Non-U.S. Gov't
%~ PubMed


%0 Journal Article
%T Benefits of Music Training for Perception of Emotional Speech Prosody in Deaf Children With Cochlear Implants
%A Good, Arla
%A Gordon, Karen A.
%A Papsin, Blake C.
%A Nespoli, Gabe
%A Hopyan, Talar
%A Peretz, I.
%A Russo, Frank A.
%D 2017
%J Ear & Hearing
%@ 0196-0202
%N 4
%V 38
%P 455–464
%G eng
%R 10.1097/AUD.0000000000000402
%2 28085739
%X OBJECTIVES
Children who use cochlear implants (CIs) have characteristic pitch processing deficits leading to impairments in music perception and in understanding emotional intention in spoken language. Music training for normal-hearing children has previously been shown to benefit perception of emotional prosody. The purpose of the present study was to assess whether deaf children who use CIs obtain similar benefits from music training. We hypothesized that music training would lead to gains in auditory processing and that these gains would transfer to emotional speech prosody perception.
DESIGN
Study participants were 18 child CI users (ages 6 to 15). Participants received either 6 months of music training (i.e., individualized piano lessons) or 6 months of visual art training (i.e., individualized painting lessons). Measures of music perception and emotional speech prosody perception were obtained pre-, mid-, and post-training. The Montreal Battery for Evaluation of Musical Abilities was used to measure five different aspects of music perception (scale, contour, interval, rhythm, and incidental memory). The emotional speech prosody task required participants to identify the emotional intention of a semantically neutral sentence under audio-only and audiovisual conditions.
RESULTS
Music training led to improved performance on tasks requiring the discrimination of melodic contour and rhythm, as well as incidental memory for melodies. These improvements were predominantly found from mid- to post-training. Critically, music training also improved emotional speech prosody perception. Music training was most advantageous in audio-only conditions. Art training did not lead to the same improvements.
CONCLUSIONS
Music training can lead to improvements in perception of music and emotional speech prosody, and thus may be an effective supplementary technique for supporting auditory rehabilitation following cochlear implantation.
%K Adolescent
%K Child
%K Cochlear Implantation
%K Cochlear Implants
%K Deafness/physiopathology/psychology/rehabilitation
%K Emotions
%K Female
%K Humans
%K Male
%K music
%K pitch perception
%K Social Perception
%K Speech Perception
%Z Journal Article
%~ PubMed


%0 Journal Article
%T Impaired socio-emotional processing in a developmental music disorder
%A Lima, César F.
%A Brancatisano, Olivia
%A Fancourt, Amy
%A Müllensiefen, Daniel
%A Scott, Sophie K.
%A Warren, Jason D.
%A Stewart, Lauren
%D 2016
%J Scientific reports
%V 6
%P 34911
%G eng
%R 10.1038/srep34911
%2 27725686
%X Some individuals show a congenital deficit for music processing despite normal peripheral auditory processing, cognitive functioning, and music exposure. This condition, termed congenital amusia, is typically approached regarding its profile of musical and pitch difficulties. Here, we examine whether amusia also affects socio-emotional processing, probing auditory and visual domains. Thirteen adults with amusia and 11 controls completed two experiments. In Experiment 1, participants judged emotions in emotional speech prosody, nonverbal vocalizations (e.g., crying), and (silent) facial expressions. Target emotions were: amusement, anger, disgust, fear, pleasure, relief, and sadness. Compared to controls, amusics were impaired for all stimulus types, and the magnitude of their impairment was similar for auditory and visual emotions. In Experiment 2, participants listened to spontaneous and posed laughs, and either inferred the authenticity of the speaker's state, or judged how much laughs were contagious. Amusics showed decreased sensitivity to laughter authenticity, but normal contagion responses. Across the experiments, mixed-effects models revealed that the acoustic features of vocal signals predicted socio-emotional evaluations in both groups, but the profile of predictive acoustic features was different in amusia. These findings suggest that a developmental music disorder can affect socio-emotional cognition in subtle ways, an impairment not restricted to auditory information.
%K Adult
%K Aged
%K Auditory Perceptual Disorders/psychology
%K Emotions
%K Female
%K Humans
%K Male
%K Middle Aged
%K Music/psychology
%K Visual Perception/physiology
%Z Journal Article
Research Support, Non-U.S. Gov't
%~ PubMed


%0 Journal Article
%T Speaking to the trained ear: musical expertise enhances the recognition of emotions in speech prosody
%A Lima, César F.
%A Castro, Sao Luis
%D 2011
%J Emotion
%@ 1528-3542 (Print) 1528-3542 (Linking)
%N 5
%V 11
%P 1021–1031
%7 2011/09/29
%R 10.1037/a0024521
%2 21942696
%X Language and music are closely related in our minds. Does musical expertise enhance the recognition of emotions in speech prosody? Forty highly trained musicians were compared with 40 musically untrained adults (controls) in the recognition of emotional prosody. For purposes of generalization, the participants were from two age groups, young (18-30 years) and middle adulthood (40-60 years). They were presented with short sentences expressing six emotions-anger, disgust, fear, happiness, sadness, surprise-and neutrality, by prosody alone. In each trial, they performed a forced-choice identification of the expressed emotion (reaction times, RTs, were collected) and an intensity judgment. General intelligence, cognitive control, and personality traits were also assessed. A robust effect of expertise was found: musicians were more accurate than controls, similarly across emotions and age groups. This effect cannot be attributed to socioeducational background, general cognitive or personality characteristics, because these did not differ between musicians and controls; perceived intensity and RTs were also similar in both groups. Furthermore, basic acoustic properties of the stimuli like fundamental frequency and duration were predictive of the participants' responses, and musicians and controls were similarly efficient in using them. Musical expertise was thus associated with cross-domain benefits to emotional prosody. These results indicate that emotional processing in music and in language engages shared resources.
%K *Emotional Intelligence
%K *Speech Perception
%K Adolescent
%K Adult
%K Age Factors
%K Anger
%K Emotions
%K Fear/psychology
%K Female
%K Happiness
%K Humans
%K Intelligence
%K Male
%K Middle Aged
%K Music/*psychology
%K Personality
%K Speech
%K Young Adult
%Z EndNoteID 178
Notes: Lima, Cesar FCastro, Sao LuisengResearch Support, Non-U.S. Gov'tEmotion. 2011 Oct;11(5):1021-31. doi: 10.1037/a0024521.
%~ EndNote


%0 Journal Article
%T Sound frequency affects speech emotion perception: results from congenital amusia
%A Lolli, Sydney L.
%A Lewenstein, Ari D.
%A Basurto, Julian
%A Winnik, Sean
%A Loui, Psyche
%D 2015
%J Frontiers in Psychology
%@ 1664-1078
%V 6
%P 1340
%G eng
%R 10.3389/fpsyg.2015.01340
%2 26441718
%X Congenital amusics, or "tone-deaf" individuals, show difficulty in perceiving and producing small pitch differences. While amusia has marked effects on music perception, its impact on speech perception is less clear. Here we test the hypothesis that individual differences in pitch perception affect judgment of emotion in speech, by applying low-pass filters to spoken statements of emotional speech. A norming study was first conducted on Mechanical Turk to ensure that the intended emotions from the Macquarie Battery for Evaluation of Prosody were reliably identifiable by US English speakers. The most reliably identified emotional speech samples were used in Experiment 1, in which subjects performed a psychophysical pitch discrimination task, and an emotion identification task under low-pass and unfiltered speech conditions. Results showed a significant correlation between pitch-discrimination threshold and emotion identification accuracy for low-pass filtered speech, with amusics (defined here as those with a pitch discrimination threshold >16 Hz) performing worse than controls. This relationship with pitch discrimination was not seen in unfiltered speech conditions. Given the dissociation between low-pass filtered and unfiltered speech conditions, we inferred that amusics may be compensating for poorer pitch perception by using speech cues that are filtered out in this manipulation. To assess this potential compensation, Experiment 2 was conducted using high-pass filtered speech samples intended to isolate non-pitch cues. No significant correlation was found between pitch discrimination and emotion identification accuracy for high-pass filtered speech. Results from these experiments suggest an influence of low frequency information in identifying emotional content of speech.
%Z Journal Article
%~ PubMed


%0 Journal Article
%T Music education intervention improves vocal emotion recognition
%A Mualem, Orit
%A Lavidor, Michal
%D 2015
%J International Journal of Music Education
%@ 0255-7614
%N 4
%V 33
%P 413–425
%R 10.1177/0255761415584292
%~ CrossRef


%0 Thesis
%A Nashkoff, Kenneth
%D 2007
%T The relationship between pitch discrimination skills and speech prosody decoding skills
%I Walden University
%~ EndNote Tagged Import Format


%0 Journal Article
%T Differences in Ability of Musicians and Nonmusicians to Judge Emotional State from the Fundamental Frequency of Voice Samples
%A Nilsonne, Åsa
%A Sundberg, Johan
%D 1985
%J Music Perception: An Interdisciplinary Journal
%@ 07307829
%N 4
%V 2
%P 507–516
%R 10.2307/40285316
%~ CrossRef


%0 Journal Article
%T Effects of musical expertise on oscillatory brain activity in response to emotional sounds
%A Nolden, S.
%A Rigoulot, Simon
%A Jolicoeur, P.
%A Armony, Jorge L.
%D 2017
%J Neuropsychologia
%@ 1873-3514 (Electronic) 0028-3932 (Linking)
%V 103
%P 96–105
%7 2017/07/20
%R 10.1016/j.neuropsychologia.2017.07.014
%2 28720526
%X Emotions can be conveyed through a variety of channels in the auditory domain, be it via music, non-linguistic vocalizations, or speech prosody. Moreover, recent studies suggest that expertise in one sound category can impact the processing of emotional sounds in other sound categories as they found that musicians process more efficiently emotional musical and vocal sounds than non-musicians. However, the neural correlates of these modulations, especially their time course, are not very well understood. Consequently, we focused here on how the neural processing of emotional information varies as a function of sound category and expertise of participants. Electroencephalogram (EEG) of 20 non-musicians and 17 musicians was recorded while they listened to vocal (speech and vocalizations) and musical sounds. The amplitude of EEG-oscillatory activity in the theta, alpha, beta, and gamma band was quantified and Independent Component Analysis (ICA) was used to identify underlying components of brain activity in each band. Category differences were found in theta and alpha bands, due to larger responses to music and speech than to vocalizations, and in posterior beta, mainly due to differential processing of speech. In addition, we observed greater activation in frontal theta and alpha for musicians than for non-musicians, as well as an interaction between expertise and emotional content of sounds in frontal alpha. The results reflect musicians' expertise in recognition of emotion-conveying music, which seems to also generalize to emotional expressions conveyed by the human voice, in line with previous accounts of effects of expertise on musical and vocal sounds processing.
%K *Music/psychology
%K *Speech
%K Acoustic Stimulation
%K Adult
%K Analysis of Variance
%K Auditory Perception/*physiology
%K Brain Waves/*physiology
%K Brain/*physiology
%K Eeg
%K emotion
%K Emotions/*physiology
%K Female
%K Humans
%K Male
%K music
%K Musical expertise
%K Neuropsychological Tests
%K Oscillations
%K Practice (Psychology)
%K Professional Competence
%K Signal Processing, Computer-Assisted
%K Sound processing
%K Speech
%K Vocal expression
%K Young Adult
%Z EndNoteID 181
Notes: Nolden, SophieRigoulot, SimonJolicoeur, PierreArmony, Jorge LengEnglandNeuropsychologia. 2017 Aug;103:96-105. doi: 10.1016/j.neuropsychologia.2017.07.014. Epub 2017 Jul 15.
%~ EndNote


%0 Journal Article
%T Sadness is unique: neural processing of emotions in speech prosody in musicians and non-musicians
%A Park, Mona
%A Gutyrchik, Evgeny
%A Welker, Lorenz
%A Carl, Petra
%A Pöppel, Ernst
%A Zaytseva, Yuliya
%A Meindl, Thomas
%A Blautzik, Janusch
%A Reiser, Maximilian
%A Bao, Yan
%D 2015
%J Frontiers in Human Neuroscience
%@ 1662-5161
%V 8
%P 1049
%R 10.3389/fnhum.2014.01049
%~ EndNote Tagged Import Format


%0 Journal Article
%T Music training and empathy positively impact adults' sensitivity to infant distress
%A Parsons, Christine E.
%A Young, Katherine S.
%A Jegindø, Else-Marie E.
%A Vuust, Peter
%A Stein, Alan
%A Kringelbach, Morten L.
%D 2014
%J Frontiers in Psychology
%@ 1664-1078
%V 5
%P 1440
%G eng
%R 10.3389/fpsyg.2014.01440
%2 25566122
%X Crying is the most powerful auditory signal of infant need. Adults' ability to perceive and respond to crying is important for infant survival and in the provision of care. This study investigated a number of listener variables that might impact on adults' perception of infant cry distress, namely parental status, musical training, and empathy. Sensitivity to infant distress was tested using a previously validated task, which experimentally manipulated distress by varying the pitch of infant cries. This task required that participants discriminate between pitch differences and interpret these as differences in infant distress. Parents with musical training showed a significant advantage on this task when compared with parents without. The extent of the advantage was correlated with the amount of self-reported musical training. For non-parents, individual differences in empathy were associated with task performance, with higher empathy scores corresponding to greater sensitivity to infant distress. We suggest that sensitivity to infant distress can be impacted by a number of listener variables, and may be amenable to training.
%Z Journal Article
%~ PubMed


%0 Journal Article
%T Singing in the key of life: A study on effects of musical ear training after cochlear implantation
%A Petersen, Bjørn
%A Mortensen, Malene Vejby
%A Hansen, Mads
%A Vuust, Peter
%D 2012
%J Psychomusicology: Music, Mind, and Brain
%@ 0275-3987
%N 2
%V 22
%P 134–151
%R 10.1037/a0031140
%~ CrossRef


%0 Journal Article
%T The music of language: an ERP investigation of the effects of musical training on emotional prosody processing
%A Pinheiro, Ana P.
%A Vasconcelos, M.
%A Dias, M.
%A Arrais, N.
%A Gonçalves, Óscar F.
%D 2015
%J Brain Lang
%@ 1090-2155 (Electronic) 0093-934X (Linking)
%V 140
%P 24–34
%7 2014/12/03
%R 10.1016/j.bandl.2014.10.009
%2 25461917
%X Recent studies have demonstrated the positive effects of musical training on the perception of vocally expressed emotion. This study investigated the effects of musical training on event-related potential (ERP) correlates of emotional prosody processing. Fourteen musicians and fourteen control subjects listened to 228 sentences with neutral semantic content, differing in prosody (one third with neutral, one third with happy and one third with angry intonation), with intelligible semantic content (semantic content condition--SCC) and unintelligible semantic content (pure prosody condition--PPC). Reduced P50 amplitude was found in musicians. A difference between SCC and PPC conditions was found in P50 and N100 amplitude in non-musicians only, and in P200 amplitude in musicians only. Furthermore, musicians were more accurate in recognizing angry prosody in PPC sentences. These findings suggest that auditory expertise characterizing extensive musical training may impact different stages of vocal emotional processing.
%K *Language
%K Acoustic Stimulation
%K Adolescent
%K Adult
%K Anger
%K Auditory Perception/physiology
%K Electroencephalography
%K Emotional prosody
%K Emotions/*physiology
%K Event-related potentials
%K Evoked Potentials, Auditory/*physiology
%K Evoked Potentials/*physiology
%K Female
%K Happiness
%K Humans
%K Language
%K Male
%K Music/*psychology
%K Musical training
%K Semantics
%K Speech
%K Young Adult
%Z EndNoteID 187
Notes: Pinheiro, Ana PVasconcelos, MargaridaDias, MarceloArrais, NunoGoncalves, Oscar FengResearch Support, Non-U.S. Gov'tNetherlandsBrain Lang. 2015 Jan;140:24-34. doi: 10.1016/j.bandl.2014.10.009. Epub 2014 Nov 21.
%~ EndNote


%0 Journal Article
%T Emotional prosody in congenital amusia: Impaired and spared processes
%A Pralus, A.
%A Fornoni, L.
%A Bouet, R.
%A Gomot, M.
%A Bhatara, A.
%A Tillmann, B.
%A Caclin, A.
%D 2019
%J Neuropsychologia
%@ 1873-3514 (Electronic) 0028-3932 (Linking)
%V 134
%P 107234
%G eng
%R 10.1016/j.neuropsychologia.2019.107234
%2 31647961
%X Congenital amusia is a lifelong deficit of music processing, in particular of pitch processing. Most research investigating this neurodevelopmental disorder has focused on music perception, but pitch also has a critical role for intentional and emotional prosody in speech. Two previous studies investigating amusics' emotional prosody recognition have shown either some deficit or no deficit (compared to controls). However, these previous studies have used only long sentence stimuli, which allow for limited control over acoustic content. Here, we tested amusic individuals for emotional prosody perception in sentences and vowels. For each type of material, participants performed an emotion categorization task, followed by intensity ratings of the recognized emotion. Compared to controls, amusic individuals had similar recognition of emotion in sentences, but poorer performance in vowels, especially when distinguishing sad and neutral stimuli. These lower performances in amusics were linked with difficulties in processing pitch and spectro-temporal parameters of the vowel stimuli. For emotion intensity, neither sentence nor vowel ratings differed between participant groups, suggesting preserved implicit processing of emotional prosody in amusia. These findings can be integrated into previous data showing preserved implicit processing of pitch and emotion in amusia alongside deficits in explicit recognition tasks. They are thus further supporting the hypothesis of impaired conscious analysis of pitch and timbre in this neurodevelopmental disorder.
%Z Journal Article
Research Support, Non-U.S. Gov't
%~ PubMed


%0 Journal Article
%T Time course of the influence of musical expertise on the processing of vocal and musical sounds
%A Rigoulot, Simon
%A Pell, Marc D.
%A Armony, Jorge L.
%D 2015
%J Neuroscience
%@ 1873-7544 (Electronic) 0306-4522 (Linking)
%V 290
%P 175–184
%7 2015/02/01
%R 10.1016/j.neuroscience.2015.01.033
%2 25637804
%X Previous functional magnetic resonance imaging (fMRI) studies have suggested that different cerebral regions preferentially process human voice and music. Yet, little is known on the temporal course of the brain processes that decode the category of sounds and how the expertise in one sound category can impact these processes. To address this question, we recorded the electroencephalogram (EEG) of 15 musicians and 18 non-musicians while they were listening to short musical excerpts (piano and violin) and vocal stimuli (speech and non-linguistic vocalizations). The task of the participants was to detect noise targets embedded within the stream of sounds. Event-related potentials revealed an early differentiation of sound category, within the first 100 ms after the onset of the sound, with mostly increased responses to musical sounds. Importantly, this effect was modulated by the musical background of participants, as musicians were more responsive to music sounds than non-musicians, consistent with the notion that musical training increases sensitivity to music. In late temporal windows, brain responses were enhanced in response to vocal stimuli, but musicians were still more responsive to music. These results shed new light on the temporal course of neural dynamics of auditory processing and reveal how it is impacted by the stimulus category and the expertise of participants.
%K *Music
%K Acoustic Stimulation
%K Adult
%K Auditory Perception/*physiology
%K Brain/*physiology
%K Electroencephalography
%K ERPs
%K Evoked Potentials
%K expertise
%K Female
%K Humans
%K Male
%K music
%K Principal Component Analysis
%K Professional Competence
%K Signal Processing, Computer-Assisted
%K Speech
%K speech prosody
%K Time Factors
%K vocalizations
%K voice
%K Young Adult
%Z EndNoteID 188
Notes: Rigoulot, SPell, M DArmony, J LengCanadian Institutes of Health Research/CanadaResearch Support, Non-U.S. Gov'tNeuroscience. 2015 Apr 2;290:175-84. doi: 10.1016/j.neuroscience.2015.01.033. Epub 2015 Jan 28.
%~ EndNote


%0 Journal Article
%T Musical experience and neural efficiency: effects of training on subcortical processing of vocal expressions of emotion
%A Strait, Dana L.
%A Kraus, Nina
%A Skoe, Erika
%A Ashley, Richard
%D 2009
%J The European journal of neuroscience
%N 3
%V 29
%P 661–668
%G eng
%R 10.1111/j.1460-9568.2009.06617.x
%2 19222564
%X Musicians exhibit enhanced perception of emotion in speech, although the biological foundations for this advantage remain unconfirmed. In order to gain a better understanding for the influences of musical experience on neural processing of emotionally salient sounds, we recorded brainstem potentials to affective human vocal sounds. Musicians showed enhanced time-domain response magnitude to the most spectrally complex portion of the stimulus and decreased magnitude to the more periodic, less complex portion. Enhanced phase-locking to stimulus periodicity was likewise seen in musicians' responses to the complex portion. These results suggest that auditory expertise engenders both enhancement and efficiency of subcortical neural responses that are intricately connected with acoustic features important for the communication of emotional states. Our findings provide the first biological evidence for behavioral observations indicating that musical training enhances the perception of vocally expressed emotion in addition to establishing a subcortical role in the auditory processing of emotional cues.
%K Acoustic Stimulation
%K Adult
%K Auditory Pathways/physiology
%K Auditory Perception/physiology
%K Brain Stem/physiology
%K Brain/physiology
%K Emotions/physiology
%K Evoked Potentials, Auditory, Brain Stem/physiology
%K Female
%K Humans
%K Learning/physiology
%K Male
%K Memory/physiology
%K Music/psychology
%K Neuropsychological Tests
%K Recognition, Psychology/physiology
%K Social Behavior
%K Speech Perception/physiology
%K Young Adult
%Z Journal Article
Research Support, Non-U.S. Gov't
Research Support, U.S. Gov't, Non-P.H.S.
%~ PubMed


%0 Journal Article
%T Reduced sensitivity to emotional prosody in congenital amusia rekindles the musical protolanguage hypothesis
%A Thompson, William Forde
%A Marin, M. M.
%A Stewart, Lauren
%D 2012
%J Proc Natl Acad Sci U S A
%@ 1091-6490 (Electronic) 0027-8424 (Linking)
%N 46
%V 109
%P 19027–19032
%7 2012/11/01
%R 10.1073/pnas.1210344109
%2 23112175
%X A number of evolutionary theories assume that music and language have a common origin as an emotional protolanguage that remains evident in overlapping functions and shared neural circuitry. The most basic prediction of this hypothesis is that sensitivity to emotion in speech prosody derives from the capacity to process music. We examined sensitivity to emotion in speech prosody in a sample of individuals with congenital amusia, a neurodevelopmental disorder characterized by deficits in processing acoustic and structural attributes of music. Twelve individuals with congenital amusia and 12 matched control participants judged the emotional expressions of 96 spoken phrases. Phrases were semantically neutral but prosodic cues (tone of voice) communicated each of six emotional states: happy, tender, afraid, irritated, sad, and no emotion. Congenitally amusic individuals were significantly worse than matched controls at decoding emotional prosody, with decoding rates for some emotions up to 20% lower than that of matched controls. They also reported difficulty understanding emotional prosody in their daily lives, suggesting some awareness of this deficit. The findings support speculations that music and language share mechanisms that trigger emotional responses to acoustic attributes, as predicted by theories that propose a common evolutionary link between these domains.
%K *Biological Evolution
%K *Expressed Emotion
%K *Language
%K *Music
%K Adult
%K Auditory Perceptual Disorders/*physiopathology
%K Female
%K Humans
%K Male
%K Middle Aged
%K Semantics
%Z EndNoteID 193
Notes: Thompson, William FordeMarin, Manuela MStewart, LaurenengResearch Support, Non-U.S. Gov'tProc Natl Acad Sci U S A. 2012 Nov 13;109(46):19027-32. doi: 10.1073/pnas.1210344109. Epub 2012 Oct 29.
%~ EndNote


%0 Journal Article
%T Decoding speech prosody: do music lessons help?
%A Thompson, William Forde
%A Schellenberg, E. Glenn
%A Husain, Gabriela
%D 2004
%J Emotion
%@ 1528-3542 (Print) 1528-3542 (Linking)
%N 1
%V 4
%P 46–64
%7 2004/04/01
%R 10.1037/1528-3542.4.1.46
%2 15053726
%X Three experiments revealed that music lessons promote sensitivity to emotions conveyed by speech prosody. After hearing semantically neutral utterances spoken with emotional (i.e., happy, sad, fearful, or angry) prosody, or tone sequences that mimicked the utterances' prosody, participants identified the emotion conveyed. In Experiment 1 (n=20), musically trained adults performed better than untrained adults. In Experiment 2 (n=56), musically trained adults outperformed untrained adults at identifying sadness, fear, or neutral emotion. In Experiment 3 (n=43), 6-year-olds were tested after being randomly assigned to 1 year of keyboard, vocal, drama, or no lessons. The keyboard group performed equivalently to the drama group and better than the no-lessons group at identifying anger or fear.
%K *Emotions
%K *Music
%K *Semantics
%K *Speech Perception
%K Adult
%K Female
%K Humans
%K Male
%Z EndNoteID 191
Notes: Thompson, William FordeSchellenberg, E GlennHusain, GabrielaengResearch Support, Non-U.S. Gov'tEmotion. 2004 Mar;4(1):46-64. doi: 10.1037/1528-3542.4.1.46.
%~ EndNote


%0 Journal Article
%T Emotional intelligence, not music training, predicts recognition of emotional speech prosody
%A Trimmer, Christopher G.
%A Cuddy, Lola L.
%D 2008
%J Emotion
%@ 1528-3542 (Print) 1528-3542 (Linking)
%N 6
%V 8
%P 838–849
%G eng
%R 10.1037/a0014080
%2 19102595
%X Is music training associated with greater sensitivity to emotional prosody in speech? University undergraduates (n = 100) were asked to identify the emotion conveyed in both semantically neutral utterances and melodic analogues that preserved the fundamental frequency contour and intensity pattern of the utterances. Utterances were expressed in four basic emotional tones (anger, fear, joy, sadness) and in a neutral condition. Participants also completed an extended questionnaire about music education and activities, and a battery of tests to assess emotional intelligence, musical perception and memory, and fluid intelligence. Emotional intelligence, not music training or music perception abilities, successfully predicted identification of intended emotion in speech and melodic analogues. The ability to recognize cues of emotion accurately and efficiently across domains may reflect the operation of a cross-modal processor that does not rely on gains of perceptual sensitivity such as those related to music training.
%K Adult
%K Affect
%K Expressed Emotion
%K Female
%K Humans
%K Intelligence
%K Male
%K music
%K Prospective Studies
%K Recognition, Psychology
%K Speech Discrimination Tests
%K Speech Perception
%K Teaching
%K Young Adult
%Z Journal Article
Research Support, Non-U.S. Gov't
%~ PubMed


%0 Journal Article
%T Examining Relationships Between Basic Emotion Perception and Musical Training in the Prosodic, Facial, and Lexical Channels of Communication and in Music
%A Twaite, Jamie
%D 2016
%~ EndNote Tagged Import Format


%0 Journal Article
%T Vocal Emotion Identification by Children Using Cochlear Implants, Relations to Voice Quality, and Musical Interests
%A Waaramaa, Teija
%A Kukkonen, Tarja
%A Mykkänen, Sari
%A Geneid, Ahmed
%D 2018
%J Journal of Speech, Language, and Hearing Research
%N 4
%V 61
%P 973–985
%G eng
%R 10.1044/2017_JSLHR-H-17-0054
%2 29587304
%X Purpose
Listening tests for emotion identification were conducted with 8-17-year-old children with hearing impairment (HI; N = 25) using cochlear implants, and their 12-year-old peers with normal hearing (N = 18). The study examined the impact of musical interests and acoustics of the stimuli on correct emotion identification.
Method
The children completed a questionnaire with their background information and noting musical interests. They then listened to vocal stimuli produced by actors (N = 5) and consisting of nonsense sentences and prolonged vowels ([a:], [i:], and [u:]; N = 32) expressing excitement, anger, contentment, and fear. The children's task was to identify the emotions they heard in the sample by choosing from the provided options. Acoustics of the samples were studied using Praat software, and statistics were examined using SPSS 24 software.
Results
The children with HI identified the emotions with 57% accuracy and the normal hearing children with 75% accuracy. Female listeners were more accurate than male listeners in both groups. Those who were implanted before age of 3 years identified emotions more accurately than others (p < .05). No connection between the child's audiogram and correct identification was observed. Musical interests and voice quality parameters were found to be related to correct identification.
Conclusions
Implantation age, musical interests, and voice quality tended to have an impact on correct emotion identification. Thus, in developing the cochlear implants, it may be worth paying attention to the acoustic structures of vocal emotional expressions, especially the formant frequency of F3. Supporting the musical interests of children with HI may help their emotional development and improve their social lives.
%K Adolescent
%K Child
%K Cochlear Implants
%K Emotions
%K Female
%K Hearing Loss/psychology/rehabilitation
%K Humans
%K Male
%K music
%K Sex Factors
%K Social Perception
%K Speech Acoustics
%K Speech Perception
%K Time-to-Treatment
%K voice quality
%Z Journal Article
Research Support, Non-U.S. Gov't
%~ PubMed


%0 Journal Article
%T Perception of emotionally loaded vocal expressions and its connection to responses to music. A cross-cultural investigation: Estonia, Finland, Sweden, Russia, and the USA
%A Waaramaa, Teija
%A Leisiö, Timo
%D 2013
%J Frontiers in Psychology
%@ 1664-1078
%V 4
%P 344
%G eng
%R 10.3389/fpsyg.2013.00344
%2 23801972
%X The present study focused on voice quality and the perception of the basic emotions from speech samples in cross-cultural conditions. It was examined whether voice quality, cultural, or language background, age, or gender were related to the identification of the emotions. Professional actors (n2) and actresses (n2) produced non-sense sentences (n32) and protracted vowels (n8) expressing the six basic emotions, interest, and a neutral emotional state. The impact of musical interests on the ability to distinguish between emotions or valence (on an axis positivity - neutrality - negativity) from voice samples was studied. Listening tests were conducted on location in five countries: Estonia, Finland, Russia, Sweden, and the USA with 50 randomly chosen participants (25 males and 25 females) in each country. The participants (total N = 250) completed a questionnaire eliciting their background information and musical interests. The responses in the listening test and the questionnaires were statistically analyzed. Voice quality parameters and the share of the emotions and valence identified correlated significantly with each other for both genders. The percentage of emotions and valence identified was clearly above the chance level in each of the five countries studied, however, the countries differed significantly from each other for the identified emotions and the gender of the speaker. The samples produced by females were identified significantly better than those produced by males. Listener's age was a significant variable. Only minor gender differences were found for the identification. Perceptual confusion in the listening test between emotions seemed to be dependent on their similar voice production types. Musical interests tended to have a positive effect on the identification of the emotions. The results also suggest that identifying emotions from speech samples may be easier for those listeners who share a similar language or cultural background with the speaker.
%Z Journal Article
%~ PubMed


%0 Journal Article
%T Attention to affective audio-visual information: Comparison between musicians and non-musicians
%A Weijkamp, Janne
%A Sadakata, Makiko
%D 2017
%J Psychology of Music
%@ 0305-7356
%N 2
%V 45
%P 204–215
%R 10.1177/0305735616654216
%~ CrossRef


%0 Journal Article
%T Interpreting infant vocal distress: the ameliorative effect of musical training in depression
%A Young, Katherine S.
%A Parsons, Christine E.
%A Stein, Alan
%A Kringelbach, Morten L.
%D 2012
%J Emotion
%@ 1528-3542 (Print) 1528-3542 (Linking)
%N 6
%V 12
%P 1200–1205
%G eng
%R 10.1037/a0028705
%2 22775126
%X An infant's cry is one of the most emotionally salient sounds in our environment. Depression is known to disrupt a mother's ability to respond to her infant, but it is not well-understood why such difficulties arise. One reason might be that depression disrupts the perceptual abilities necessary to interpret infant's affective cues. Given that musicians are known to have enhanced auditory perception, we assessed whether depression and previous musical training can impact on the ability to interpret distress in infant cries, as manipulated by changes in pitch. Depressed participants with musical training demonstrated better discriminative acuity of distress in infant cry bursts compared to those without. Non-depressed participants, with and without musical training, had levels comparable to musicians with depression. We suggest that previous musical training may act as a protective factor that maintains auditory perceptual abilities in the context of depression. These findings have potential implications for the development of novel training interventions to maintain sensitivity to infant vocal cues in individuals with postnatal depression.
%K Adolescent
%K Adult
%K Crying/psychology
%K Depression/physiopathology
%K Humans
%K Infant
%K Middle Aged
%K Music/psychology
%K Pitch Perception/physiology
%K Stress, Psychological/psychology
%K Young Adult
%Z Comparative Study
Journal Article
Randomized Controlled Trial
Research Support, Non-U.S. Gov't
%~ PubMed


%0 Journal Article
%T Emotional Prosody Perception in Mandarin-speaking Congenital Amusics
%A Zhang, Yixin
%A Geng, Tianzhu
%A Zhang, Jinsong
%D 2018
%J INTERSPEECH
%P 2196–2200
%R 10.21437/Interspeech.2018-91
%~ CrossRef


