FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Miller, EJ
   Foo, YZ
   Mewton, P
   Dawel, A
AF Miller, Elizabeth J.
   Foo, Yong Zhi
   Mewton, Paige
   Dawel, Amy
TI How do people respond to computer-generated versus human faces? A
   systematic review and meta-analyses
SO COMPUTERS IN HUMAN BEHAVIOR REPORTS
LA English
DT Review
DE Virtual; Avatar; Trustworthiness; Generative adversarial network
ID UNCANNY VALLEY; FACIAL EXPRESSIONS; EXTRASTRIATE CORTEX; 1ST
   IMPRESSIONS; PERCEPTION; RACE; RECOGNITION; EMOTION; AVATAR; INFORMATION
AB Computer-generated (CG) beings are rapidly infiltrating the human social world. Yet evidence about how humans respond to CG faces is mixed. The present systematic review and meta-analyses aimed to synthesise empirical evidence from studies comparing people's responses to CG and human faces, across key face processing domains of interest to psychology, neuroscience, and computer science. We tested whether effects were moderated by the perceived realism of CG relative to human faces, and whether CG and human faces showed the same identity or not. We hypothesised that people would be able to tell CG and human faces apart, and that other types of responses would favour human over CG faces. While results supported our hypotheses across several domains (perceptions of human-likeness, face memory, first impressions, emotion labelling), some responses did not differ for CG and human faces (quality of interactions, emotion ratings, facial mimicry, looking behaviour). We also found a reduced inversion effect for CG relative to human faces, though only minimal data were available for hallmark face effects (ORE, N170 and FFA responses). Overall, findings highlight potential strengths and challenges of using CG faces across a range of applications, including e-health, social companionship, videogaming, and scientific work.
C1 [Miller, Elizabeth J.; Foo, Yong Zhi; Mewton, Paige; Dawel, Amy] Australian Natl Univ, Sch Med & Psychol, Canberra, ACT 2600, Australia.
   [Foo, Yong Zhi] Univ Western Australia, Sch Biol Sci, Crawley, WA 6009, Australia.
C3 Australian National University; University of Western Australia
RP Dawel, A (corresponding author), Australian Natl Univ, Sch Med & Psychol, Canberra, ACT 2600, Australia.
EM elizabeth.miller@anu.edu.au; fooyongzhi@gmail.com;
   paige.mewton@anu.edu.au; amy.dawel@anu.edu.au
OI Mewton, Paige/0000-0003-1744-1155; Miller,
   Elizabeth/0000-0003-2572-6134; Foo, Yong Zhi/0000-0001-7627-2991; Dawel,
   Amy/0000-0001-6668-3121
FU Australian Government through the Australian Research Council's
   Discovery Projects funding scheme [DP220101026]; TRANSFORM Career
   Development Fellowship from The Australian National University (ANU)
   College of Health and Medicine
FX This research is supported by the Australian Government through the
   Australian Research Council's Discovery Projects funding scheme (project
   DP220101026) and by a TRANSFORM Career Development Fellowship to AD from
   The Australian National University (ANU) College of Health and Medicine.
   The funders had no role in developing or conducting this research. We
   have no conflicts of interest to disclose.
CR Abbott Miriam Bowers, 2016, Online J Issues Nurs, V21, P7, DOI 10.3912/OJIN.Vol21No03PPT39,05
   Andrade AD, 2010, J PALLIAT MED, V13, P1415, DOI 10.1089/jpm.2010.0108
   [Anonymous], 2009, P 3 INT C AFF COMP I, DOI DOI 10.1109/ACII.2009.5349549
   Arsalidou M, 2011, BRAIN TOPOGR, V24, P149, DOI 10.1007/s10548-011-0171-4
   Aviezer H, 2008, PSYCHOL SCI, V19, P724, DOI 10.1111/j.1467-9280.2008.02148.x
   Bailenson J. N., 2003, J FORENSIC IDENTIFIC, V53, P722
   Bailenson JN, 2004, PRESENCE-VIRTUAL AUG, V13, P416, DOI 10.1162/1054746041944858
   Balas B, 2017, COMPUT HUM BEHAV, V77, P240, DOI 10.1016/j.chb.2017.08.045
   Balas B, 2015, COMPUT HUM BEHAV, V52, P331, DOI 10.1016/j.chb.2015.06.018
   Balas B, 2014, PERCEPTION, V43, P355, DOI 10.1068/p7696
   Balas B, 2012, PERCEPTION, V41, P361, DOI 10.1068/p7166
   Balas B, 2011, DEVELOPMENTAL SCI, V14, P892, DOI 10.1111/j.1467-7687.2011.01039.x
   Ballew CC, 2007, P NATL ACAD SCI USA, V104, P17948, DOI 10.1073/pnas.0705435104
   Balsters MJH, 2013, EVOL PSYCHOL-US, V11, P148, DOI 10.1177/147470491301100114
   Baltrusaitis T., 2010, P 3 INT WORKSHOP AFF, P27, DOI [10.1145/1877826.1877835, DOI 10.1145/1877826.1877835]
   Bartneck C, 2001, USER MODEL USER-ADAP, V11, P279, DOI 10.1023/A:1011811315582
   Bartneck C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P367
   Bentin S, 1996, J COGNITIVE NEUROSCI, V8, P551, DOI 10.1162/jocn.1996.8.6.551
   Biele C, 2006, EXP BRAIN RES, V171, P1, DOI 10.1007/s00221-005-0254-0
   Calder AJ, 2005, NAT REV NEUROSCI, V6, P641, DOI 10.1038/nrn1724
   Calvo MG, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-35259-w
   Calvo MG, 2016, EMOTION, V16, P1186, DOI 10.1037/emo0000192
   Carlson CA, 2012, APPL COGNITIVE PSYCH, V26, P525, DOI 10.1002/acp.2824
   Carter E. J., 2013, Proceedings of the ACM Symposium on Applied Perception, P35
   Chattopadhyay D, 2016, J VISION, V16, DOI 10.1167/16.11.7
   Cheetham M, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00981
   Cheetham M, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01219
   Cheetham Marcus, 2013, Front Psychol, V4, P108, DOI 10.3389/fpsyg.2013.00108
   Cheetham M, 2011, FRONT HUM NEUROSCI, V5, DOI 10.3389/fnhum.2011.00126
   Chen F, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0210858
   Christensen JL, 2013, J INT AIDS SOC, V16, DOI 10.7448/IAS.16.3.18716
   Cohen J, 1988, STAT POWER ANAL BEHA
   Cooper M, 2019, BRIT J GUID COUNS, V47, P446, DOI 10.1080/03069885.2018.1506567
   Costantini E, 2004, LECT NOTES COMPUT SC, V3068, P276
   Costantini E., 2005, P 10 INT C INTELLIGE, P20, DOI [10.1145/1040830.1040846, DOI 10.1145/1040830.1040846]
   Craft AJ, 2012, ARCH SEX BEHAV, V41, P939, DOI 10.1007/s10508-012-9933-7
   Craig BM, 2012, EMOTION, V12, P1303, DOI 10.1037/a0028622
   Crookes K, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141353
   Cruwys T, 2014, J AFFECT DISORDERS, V159, P139, DOI 10.1016/j.jad.2014.02.019
   Dai ZY, 2018, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.168
   Dawel A, 2022, BEHAV RES METHODS, V54, P1889, DOI 10.3758/s13428-021-01705-3
   de Ruiter A, 2021, Philosophy & Technology, DOI [10.1007/s13347-021-00459-2, DOI 10.1007/S13347-021-00459-2]
   Deffler SA, 2015, PSYCHON B REV, V22, P1041, DOI 10.3758/s13423-014-0769-0
   Dellazizzo L, 2018, FRONT PSYCHIATRY, V9, DOI 10.3389/fpsyt.2018.00131
   Demos KE, 2008, CEREB CORTEX, V18, P2729, DOI 10.1093/cercor/bhn034
   Diel A, 2022, ACM T HUM-ROBOT INTE, V11, DOI 10.1145/3470742
   DIMBERG U, 1982, PSYCHOPHYSIOLOGY, V19, P643, DOI 10.1111/j.1469-8986.1982.tb02516.x
   Dyck M, 2010, PSYCHIAT RES, V179, P247, DOI 10.1016/j.psychres.2009.11.004
   Dyck M, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003628
   Egger M, 1997, BMJ-BRIT MED J, V315, P629, DOI 10.1136/bmj.315.7109.629
   EKMAN P, 1992, PSYCHOL SCI, V3, P34, DOI 10.1111/j.1467-9280.1992.tb00253.x
   Ert E, 2016, TOURISM MANAGE, V55, P62, DOI 10.1016/j.tourman.2016.01.013
   Fabri M., 2004, Virtual Reality, V7, P66, DOI 10.1007/s10055-003-0116-7
   Fan S., 2012, SIGGRAPH ASIA 2012 T, V1, P3, DOI [10.1145/2407746.2407763, DOI 10.1145/2407746.2407763]
   Flückiger C, 2018, PSYCHOTHERAPY, V55, P316, DOI 10.1037/pst0000172
   Foo YZ, 2022, PERS SOC PSYCHOL B, V48, P1580, DOI 10.1177/01461672211048110
   Foo YZ, 2017, BIOL REV, V92, P551, DOI 10.1111/brv.12243
   Freeman JB, 2008, J EXP PSYCHOL GEN, V137, P673, DOI 10.1037/a0013875
   Freeman JB, 2014, J NEUROSCI, V34, P10573, DOI 10.1523/JNEUROSCI.5063-13.2014
   Freeman JB, 2010, PERS SOC PSYCHOL B, V36, P1318, DOI 10.1177/0146167210378755
   Freeman JB, 2010, J EXP SOC PSYCHOL, V46, P179, DOI 10.1016/j.jesp.2009.10.002
   Gaither SE, 2019, J SOC PSYCHOL, V159, P592, DOI 10.1080/00224545.2018.1538929
   Geiger AR, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-97527-6
   Gendron M., 2013, EMOTION PERCEPTION P, DOI DOI 10.1093/OXFORDHB/9780195376746.013.0034
   Gibert G, 2013, SPEECH COMMUN, V55, P135, DOI 10.1016/j.specom.2012.07.001
   Gómez-Leal R, 2021, PEERJ, V9, DOI 10.7717/peerj.11274
   Gong L, 2008, COMPUT HUM BEHAV, V24, P1494, DOI 10.1016/j.chb.2007.05.007
   Gonzalez-Franco M, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00392
   Gracanin A, 2021, J NONVERBAL BEHAV, V45, P83, DOI 10.1007/s10919-020-00347-x
   Green RD, 2008, COMPUT HUM BEHAV, V24, P2456, DOI 10.1016/j.chb.2008.02.019
   Guise V, 2012, NURS EDUC TODAY, V32, P683, DOI 10.1016/j.nedt.2011.09.004
   Gwinn OS, 2018, NEUROPSYCHOLOGIA, V119, P405, DOI 10.1016/j.neuropsychologia.2018.09.001
   Hernandez N, 2009, NEUROPSYCHOLOGIA, V47, P1004, DOI 10.1016/j.neuropsychologia.2008.10.023
   Hourihan KL, 2013, MEM COGNITION, V41, P1021, DOI 10.3758/s13421-013-0316-7
   Hyde J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1787, DOI 10.1145/2556288.2557280
   Itz ML, 2014, NEUROIMAGE, V102, P736, DOI 10.1016/j.neuroimage.2014.08.042
   Jackson PL, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00112
   Javor A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0165998
   Jianying Wang, 2019, Proceedings of the International Conferences. Interfaces and Human Computer Interaction 2019, Game and Entertainment Technologies 2019, Computer Graphics, Visualization, Computer Vision and Image Processing 2019, P115
   John LK, 2011, J CONSUM RES, V37, P858, DOI 10.1086/656423
   Joyal CC, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00787
   Kala S, 2021, FRONT PSYCHIATRY, V12, DOI 10.3389/fpsyt.2021.709382
   Kanwisher N, 1997, J NEUROSCI, V17, P4302
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Katsyri J., 2003, P INT C AUD SPEECH P, P1
   Kätsyri J, 2020, NEUROIMAGE, V204, DOI 10.1016/j.neuroimage.2019.116216
   Kätsyri J, 2019, PERCEPTION, V48, P968, DOI 10.1177/0301006619869134
   Kätsyri J, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.01362
   Kegel L. C., 2020, SOCIAL COGNITIVE AFF
   Krumhuber EG, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137840
   Krumhuber EG, 2012, EMOTION, V12, P351, DOI 10.1037/a0026632
   Lewkowicz DJ, 2012, DEV PSYCHOBIOL, V54, P124, DOI 10.1002/dev.20583
   Lidestam B, 2001, SCAND AUDIOL, V30, P89, DOI 10.1080/010503901300112194
   Little AC, 2007, EVOL HUM BEHAV, V28, P18, DOI 10.1016/j.evolhumbehav.2006.09.002
   Lucas GM, 2017, FRONT ROBOT AI, V4, DOI 10.3389/frobt.2017.00051
   Lucas GM, 2014, COMPUT HUM BEHAV, V37, P94, DOI 10.1016/j.chb.2014.04.043
   MacDorman KF, 2019, COMPUT HUM BEHAV, V94, P140, DOI 10.1016/j.chb.2019.01.011
   MacDorman KF, 2017, COGNITION, V161, P132, DOI 10.1016/j.cognition.2017.01.009
   MacDorman KF, 2016, COGNITION, V146, P190, DOI 10.1016/j.cognition.2015.09.019
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   Malek N, 2019, EMOTION, V19, P234, DOI 10.1037/emo0000410
   Maras MH, 2019, INT J EVID PROOF, V23, P255, DOI 10.1177/1365712718807226
   Matheson HE, 2012, CAN J EXP PSYCHOL, V66, P51, DOI 10.1037/a0026062
   Matheson HE, 2011, BEHAV RES METHODS, V43, P224, DOI 10.3758/s13428-010-0029-9
   McKone E, 2007, TRENDS COGN SCI, V11, P8, DOI 10.1016/j.tics.2006.11.002
   McKone E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49202-0
   Meissner CA, 2001, PSYCHOL PUBLIC POL L, V7, P3, DOI 10.1037//1076-8971.7.1.3
   Milcent AS, 2019, PROCEEDINGS OF THE 19TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA' 19), P215, DOI 10.1145/3308532.3329446
   Miller EJ, 2022, EMOTION, V22, P907, DOI 10.1037/emo0000772
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Moser E, 2007, J NEUROSCI METH, V161, P126, DOI 10.1016/j.jneumeth.2006.10.016
   Mühlberger A, 2009, J NEURAL TRANSM, V116, P735, DOI 10.1007/s00702-008-0108-6
   Mundy ME, 2012, NEUROPSYCHOLOGIA, V50, P3053, DOI 10.1016/j.neuropsychologia.2012.07.006
   Mustafa M., 2016, P 13 EUR C VIS MED P, P1, DOI [10.1145/2998559.2998563, DOI 10.1145/2998559.2998563]
   Mustafa M, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5098, DOI 10.1145/3025453.3026043
   Nakagawa S, 2007, BIOL REV, V82, P591, DOI 10.1111/j.1469-185X.2007.00027.x
   Naples A, 2015, BEHAV RES METHODS, V47, P562, DOI 10.3758/s13428-014-0491-x
   Ni H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON CYBORG AND BIONIC SYSTEMS (CBS), P298, DOI 10.1109/CBS.2018.8612271
   Nightingale SJ, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2120481119
   Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105
   Papesh MH, 2009, CAN J EXP PSYCHOL, V63, P253, DOI 10.1037/a0015802
   Parmar D, 2022, AUTON AGENT MULTI-AG, V36, DOI 10.1007/s10458-021-09539-1
   Patel H, 2015, PRESENCE-VIRTUAL AUG, V24, P1, DOI 10.1162/PRES_a_00212
   Pointet VCP, 2021, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.576852
   Peromaa T, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0215610
   Philip L, 2018, I-PERCEPTION, V9, DOI 10.1177/2041669518786527
   Pickard MD, 2016, COMPUT HUM BEHAV, V65, P23, DOI 10.1016/j.chb.2016.08.004
   Recio G, 2011, BRAIN RES, V1376, P66, DOI 10.1016/j.brainres.2010.12.041
   Riedl R, 2014, J MANAGE INFORM SYST, V30, P83, DOI 10.2753/MIS0742-1222300404
   Rohatgi A., 2021, WebPlotDigitizer
   Rossion B., 2012, The Oxford Handbook of Event-Related Potential Components, P1, DOI [DOI 10.1093/OXFORDHB/9780195374148.013.0064, 10.1093/oxfordhb/9780195374148.013.0064]
   Rossion B, 2008, ACTA PSYCHOL, V128, P274, DOI 10.1016/j.actpsy.2008.02.003
   Roth D., 2019, PERCEIVED AUTHENTICI, P21, DOI [10.1145/3340764.3340797, DOI 10.1145/3340764.3340797]
   Royer J, 2018, COGNITION, V181, P12, DOI 10.1016/j.cognition.2018.08.004
   Rubin A, 2022, J GEN INTERN MED, V37, P70, DOI 10.1007/s11606-021-06945-9
   Schindler S, 2017, SCI REP-UK, V7, DOI 10.1038/srep45003
   Schyns PG, 2003, NEUROREPORT, V14, P1665, DOI 10.1097/00001756-200309150-00002
   Seo Y, 2017, COMPUT HUM BEHAV, V69, P120, DOI 10.1016/j.chb.2016.12.020
   Shen BY, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9667066
   Shepherd RM, 2005, PERS INDIV DIFFER, V39, P949, DOI 10.1016/j.paid.2005.04.001
   Singh B, 2022, PERS SOC PSYCHOL B, V48, P865, DOI 10.1177/01461672211024463
   Sollfrank T, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651044
   Sutherland CAM, 2013, COGNITION, V127, P105, DOI 10.1016/j.cognition.2012.12.001
   Syrjämäki AH, 2020, COMPUT HUM BEHAV, V112, DOI 10.1016/j.chb.2020.106454
   TANAKA JW, 1993, Q J EXP PSYCHOL-A, V46, P225, DOI 10.1080/14640749308401045
   Thorstenson CA, 2019, EMOTION, V19, P799, DOI 10.1037/emo0000485
   Tinwell A., 2015, International Journal of Mechanisms and Robotic Systems, V2, P97, DOI [DOI 10.1504/IJMRS.2015, 10.1504/IJMRS.2015.068991, DOI 10.1504/IJMRS.2015.068991]
   Tinwell A, 2014, COMPUT HUM BEHAV, V36, P286, DOI 10.1016/j.chb.2014.03.073
   Tinwell A, 2013, COMPUT HUM BEHAV, V29, P1617, DOI 10.1016/j.chb.2013.01.008
   Tinwell A, 2011, COMPUT HUM BEHAV, V27, P741, DOI 10.1016/j.chb.2010.10.018
   Todorov A, 2005, SCIENCE, V308, P1623, DOI 10.1126/science.1110589
   Todorov A, 2008, TRENDS COGN SCI, V12, P455, DOI 10.1016/j.tics.2008.10.001
   Vaitonyte J, 2021, COMPUT HUM BEHAV REP, V3, DOI 10.1016/j.chbr.2021.100065
   VALENTINE T, 1991, Q J EXP PSYCHOL-A, V43, P161, DOI 10.1080/14640749108400966
   Valentine T, 2016, Q J EXP PSYCHOL, V69, P1996, DOI 10.1080/17470218.2014.990392
   Vernon RJW, 2014, P NATL ACAD SCI USA, V111, pE3353, DOI 10.1073/pnas.1409860111
   Viechtbauer W, 2010, J STAT SOFTW, V36, P1, DOI 10.18637/jss.v036.i03
   Wang Liz C., 2010, International Journal of Electronic Marketing and Retailing, V3, P341, DOI 10.1504/IJEMR.2010.036881
   Wang NB, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44058-w
   Wieser MJ, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00471
   Willis J, 2006, PSYCHOL SCI, V17, P592, DOI 10.1111/j.1467-9280.2006.01750.x
   Wilson JP, 2015, PSYCHOL SCI, V26, P1325, DOI 10.1177/0956797615590992
   YIN RK, 1969, J EXP PSYCHOL, V81, P141, DOI 10.1037/h0027474
   YOUNG AW, 1987, PERCEPTION, V16, P747, DOI 10.1068/p160747
   Zhao TH, 2020, NEUROREPORT, V31, P437, DOI 10.1097/WNR.0000000000001420
   Zhou Y., 2016, P 10 INT S COMM SYST, P1, DOI DOI 10.1109/CSNDSP.2016.7573913
NR 166
TC 3
Z9 3
U1 6
U2 10
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 2451-9588
J9 COMPUT HUM BEHAV REP
JI Comput. Hum. Behav. Rep.
PD MAY
PY 2023
VL 10
AR 100283
DI 10.1016/j.chbr.2023.100283
EA MAY 2023
PG 25
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Emerging Sources Citation Index (ESCI)
SC Psychology
GA J0WZ3
UT WOS:001006909700001
OA gold
DA 2024-01-09
ER

PT J
AU Nussbaum, C
   Pöhlmann, M
   Kreysa, H
   Schweinberger, SR
AF Nussbaum, Christine
   Poehlmann, Manuel
   Kreysa, Helene
   Schweinberger, Stefan R.
TI Perceived naturalness of emotional voice morphs
SO COGNITION & EMOTION
LA English
DT Article
DE Naturalness; parameter-specific voice morphing; vocal emotions; F0;
   Timbre
ID FUNDAMENTAL-FREQUENCY; AUDITORY ADAPTATION; SPEECH NATURALNESS;
   PERCEPTION; INTELLIGIBILITY; GENDER; COMMUNICATION; APERIODICITY;
   PSYTOOLKIT; EXPRESSION
AB Research into voice perception benefits from manipulation software to gain experimental control over acoustic expression of social signals such as vocal emotions. Today, parameter-specific voice morphing allows a precise control of the emotional quality expressed by single vocal parameters, such as fundamental frequency (F0) and timbre. However, potential side effects, in particular reduced naturalness, could limit ecological validity of speech stimuli. To address this for the domain of emotion perception, we collected ratings of perceived naturalness and emotionality on voice morphs expressing different emotions either through F0 or Timbre only. In two experiments, we compared two different morphing approaches, using either neutral voices or emotional averages as emotionally non-informative reference stimuli. As expected, parameter-specific voice morphing reduced perceived naturalness. However, perceived naturalness of F0 and Timbre morphs were comparable with averaged emotions as reference, potentially making this approach more suitable for future research. Crucially, there was no relationship between ratings of emotionality and naturalness, suggesting that the perception of emotion was not substantially affected by a reduction of voice naturalness. We hold that while these findings advocate parameter-specific voice morphing as a suitable tool for research on vocal emotion perception, great care should be taken in producing ecologically valid stimuli.
C1 [Nussbaum, Christine; Poehlmann, Manuel; Kreysa, Helene; Schweinberger, Stefan R.] Friedrich Schiller Univ Jena, Dept Gen Psychol & Cognit Neurosci, Jena, Germany.
   [Nussbaum, Christine; Kreysa, Helene; Schweinberger, Stefan R.] Friedrich Schiller Univ, Voice Res Unit, Jena, Germany.
   [Schweinberger, Stefan R.] Univ Geneva, Swiss Ctr Affect Sci, Geneva, Switzerland.
C3 Friedrich Schiller University of Jena; Friedrich Schiller University of
   Jena; University of Geneva
RP Nussbaum, C; Schweinberger, SR (corresponding author), Friedrich Schiller Univ Jena, Dept Gen Psychol & Cognit Neurosci, Steiger 3 Haus 1, D-07743 Jena, Germany.
EM christine.nussbaum@uni-jena.de; stefan.schweinberger@uni-jena.de
OI Nussbaum, Christine/0000-0003-2718-2898; Kreysa,
   Helene/0000-0001-7163-7023; Schweinberger, Stefan/0000-0001-5762-0188
CR Alku P, 1999, CLIN NEUROPHYSIOL, V110, P1329, DOI 10.1016/S1388-2457(99)00088-7
   Anand S, 2015, J SPEECH LANG HEAR R, V58, P1134, DOI 10.1044/2015_JSLHR-S-14-0243
   ANSI, 1973, PSYCH S3 20
   Arias P, 2021, EMOT REV, V13, P12, DOI 10.1177/1754073920934544
   Assmann P. F., 2006, INTERSPEECH S COND M
   Assmann PF, 2000, J ACOUST SOC AM, V108, P1856, DOI 10.1121/1.1289363
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Baird A, 2018, J AUDIO ENG SOC, V66, P277, DOI 10.17743/jaes.2018.0023
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Belin P, 2011, BRIT J PSYCHOL, V102, P711, DOI 10.1111/j.2044-8295.2011.02041.x
   Bestelmeyer PEG, 2010, COGNITION, V117, P217, DOI 10.1016/j.cognition.2010.08.008
   Boersma P., 2021, Glot International
   Bruckert L, 2010, CURR BIOL, V20, P116, DOI 10.1016/j.cub.2009.11.034
   BURTON MW, 1995, J EXP PSYCHOL HUMAN, V21, P1230, DOI 10.1037/0096-1523.21.5.1230
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Calder AJ, 2000, COGNITION, V76, P105, DOI 10.1016/S0010-0277(00)00074-3
   Coughlin-Woods S, 2005, PERCEPT MOTOR SKILL, V100, P295
   Crookes K, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141353
   Crumpton J, 2016, INT J SOC ROBOT, V8, P271, DOI 10.1007/s12369-015-0329-4
   Cumming G, 2014, PSYCHOL SCI, V25, P7, DOI 10.1177/0956797613504966
   Eadie TL, 2002, J SPEECH LANG HEAR R, V45, P1088, DOI 10.1044/1092-4388(2002/087)
   EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550
   Fritz CO, 2012, J EXP PSYCHOL GEN, V141, P30, DOI 10.1037/a0026092
   Frühholz S, 2015, CEREB CORTEX, V25, P2752, DOI 10.1093/cercor/bhu074
   Giordano BL, 2021, NAT HUM BEHAV, V5, P1203, DOI 10.1038/s41562-021-01073-0
   Gong L, 2008, COMPUT HUM BEHAV, V24, P1494, DOI 10.1016/j.chb.2007.05.007
   Grichkovtsova I, 2012, SPEECH COMMUN, V54, P414, DOI 10.1016/j.specom.2011.10.005
   Haubo Rune., 2015, Stand, V19, P2016
   Heider F, 1944, AM J PSYCHOL, V57, P243, DOI 10.2307/1416950
   Hortensius R, 2018, IEEE T COGN DEV SYST, V10, P852, DOI 10.1109/TCDS.2018.2826921
   Hubbard DJ, 2013, J ACOUST SOC AM, V133, P2367, DOI 10.1121/1.4792145
   Ilves M, 2013, BEHAV INFORM TECHNOL, V32, P117, DOI 10.1080/0144929X.2012.702285
   Ilves M, 2011, LECT NOTES COMPUT SC, V6974, P588, DOI 10.1007/978-3-642-24600-5_62
   Juslin PN, 2003, PSYCHOL BULL, V129, P770, DOI 10.1037/0033-2909.129.5.770
   Kätsyri J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00390
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Kawahara H., 2013, IEEE INT C AC SPEECH, P1, DOI [10.1109/APSIPA.2013.6694355, DOI 10.1109/APSIPA.2013.6694355]
   Kawahara H., 2019, The Oxford Handbook of Voice Perception, P685
   Klopfenstein M, 2020, CLIN LINGUIST PHONET, V34, P327, DOI 10.1080/02699206.2019.1652692
   Kloth N, 2017, NEUROIMAGE, V155, P1, DOI 10.1016/j.neuroimage.2017.04.049
   Lakens D., 2019, Simulation-based power-analysis for factorial ANOVA designs, DOI DOI 10.31234/OSF.IO/BAXSF
   Mackey LS, 1997, J SPEECH LANG HEAR R, V40, P349, DOI 10.1044/jslhr.4002.349
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   MATLAB, 2020, VERS 9 8 0 R2020A
   Mayo C, 2011, SPEECH COMMUN, V53, P311, DOI 10.1016/j.specom.2010.10.003
   McAleer P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0090779
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   Meltzner GS, 2005, J SPEECH LANG HEAR R, V48, P766, DOI 10.1044/1092-4388(2005/053)
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Nadler JT, 2015, J GEN PSYCHOL, V142, P71, DOI 10.1080/00221309.2014.994590
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nusbaum H. C., 1995, International Journal of Speech Technology, V1, P7, DOI 10.1007/BF02277176
   Nussbaum C, 2022, SOC COGN AFFECT NEUR, V17, P1145, DOI 10.1093/scan/nsac033
   Nussbaum C, 2022, COGNITION, V219, DOI 10.1016/j.cognition.2021.104967
   Paulmann S., 2018, The Oxford Handbook of Voice Perception, P458
   Pell MD, 2015, BIOL PSYCHOL, V111, P14, DOI 10.1016/j.biopsycho.2015.08.008
   Péron J, 2015, CORTEX, V63, P172, DOI 10.1016/j.cortex.2014.08.023
   SCHERER KR, 1986, PSYCHOL BULL, V99, P143, DOI 10.1037/0033-2909.99.2.143
   Schindler S, 2017, SCI REP-UK, V7, DOI 10.1038/srep45003
   Schirmer A, 2017, SOC COGN AFFECT NEUR, V12, P902, DOI 10.1093/scan/nsx020
   Schweinberger SR, 2008, CURR BIOL, V18, P684, DOI 10.1016/j.cub.2008.04.015
   Schweinberger SR, 2020, COMPUT HUM BEHAV, V106, DOI 10.1016/j.chb.2020.106256
   Skuk VG, 2020, J SPEECH LANG HEAR R, V63, P3155, DOI 10.1044/2020_JSLHR-20-00026
   Skuk VG, 2015, J ACOUST SOC AM, V138, P1180, DOI 10.1121/1.4927696
   Skuk VG, 2014, J SPEECH LANG HEAR R, V57, P285, DOI 10.1044/1092-4388(2013/12-0314)
   Spatola N, 2021, COMPUT HUM BEHAV, V124, DOI 10.1016/j.chb.2021.106934
   Stoet G, 2017, TEACH PSYCHOL, V44, P24, DOI 10.1177/0098628316677643
   Stoet G, 2010, BEHAV RES METHODS, V42, P1096, DOI 10.3758/BRM.42.4.1096
   Vojtech JM, 2019, AM J SPEECH-LANG PAT, V28, P875, DOI 10.1044/2019_AJSLP-MSC18-18-0052
   von Eiff CI, 2022, EAR HEARING, V43, P1178, DOI 10.1097/AUD.0000000000001181
   Webster MA, 1999, PSYCHON B REV, V6, P647, DOI 10.3758/BF03212974
   Whiting CM, 2020, COGNITION, V200, DOI 10.1016/j.cognition.2020.104249
   Yamagishi J, 2012, ACOUST SCI TECHNOL, V33, P1, DOI 10.1250/ast.33.1
   Yamasaki R, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.09.020
   Yorkston K.M., 1999, Management of motor speech disorders in children and adults
   YORKSTON KM, 1990, J SPEECH HEAR DISORD, V55, P550, DOI 10.1044/jshd.5503.550
   Young AW, 2011, BRIT J PSYCHOL, V102, P959, DOI 10.1111/j.2044-8295.2011.02045.x
NR 78
TC 2
Z9 2
U1 10
U2 13
PU ROUTLEDGE JOURNALS, TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND
SN 0269-9931
EI 1464-0600
J9 COGNITION EMOTION
JI Cogn. Emot.
PD MAY 19
PY 2023
VL 37
IS 4
BP 731
EP 747
DI 10.1080/02699931.2023.2200920
EA APR 2023
PG 17
WC Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA J5MB6
UT WOS:000975713700001
PM 37104118
DA 2024-01-09
ER

PT J
AU Im, H
   Sung, BLY
   Lee, GR
   Kok, KQX
AF Im, Hyunjoo
   Sung, Billy
   Lee, Garim
   Kok, Keegan Qi Xian
TI Let voice assistants sound like a machine: Voice and task type effects
   on perceived fluency, competence, and consumer attitude
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Voice assistant; Synthetic voice; Voice perception
ID WARMTH; PERCEPTION; COMPUTER; MODEL
AB Voice Assistants (VA) are increasingly penetrating consumers' daily lives. This study aimed to investigate the effects of synthetic vs. human voice on users' perception of the voice, social judgments of the VA, and attitudes towards VAs. Drawing from CASA(Computers-as-socialactors) framework and social perception literature, we developed a theoretical model that explains the psychological underlying mechanism of the voice effects. Through two online experiments, we rejected our initial hypotheses that human voice would increase users' perception and evaluation of the VAs. Instead, our findings support that the VAs were favored when they spoke with a synthetic voice only when the users engaged in functional tasks. There was no difference between the voices for social tasks. A further investigation revealed that participants perceived the synthetic voice to be more fluent when VA responds to functional tasks. This enhanced perception of fluency increased competence perception and attitudes. The findings imply that VAs should not be designed to closely resemble humans. Rather, consideration of usage contexts and consumer expectations should be prioritized in developing most likable VAs.
C1 [Im, Hyunjoo; Lee, Garim] Univ Minnesota Twin Cities, Retailing & Consumer Studies, 240 McNeal Hall, 1985 Buford Ave, St Paul, MN 55108 USA.
   [Sung, Billy; Kok, Keegan Qi Xian] Curtin Univ, Sch Management & Mkt, Consumer Res Lab, Bentley, WA 6102, Australia.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   Curtin University
RP Im, H (corresponding author), Univ Minnesota Twin Cities, Retailing & Consumer Studies, 240 McNeal Hall, 1985 Buford Ave, St Paul, MN 55108 USA.
EM hjim@umn.edu; billy.sung@curtin.edu.au; lee02169@umn.edu;
   keegan.kok@curtin.edu.au
OI Lee, Garim/0000-0002-7054-1967; Kok, Keegan/0000-0002-6623-8957; Sung,
   Billy/0000-0003-0028-6574; Im, Hyunjoo/0000-0002-7932-0087
CR Amick LJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01148
   [Anonymous], NUMB VOIC ASS US WOR
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Belanche D, 2021, PSYCHOL MARKET, V38, P2357, DOI 10.1002/mar.21532
   Cambre Julia, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359325
   Carolus Astrid, 2021, Frontiers in Computer Science, V3, P46, DOI DOI 10.3389/FCOMP.2021.682982
   Chattaraman V, 2019, COMPUT HUM BEHAV, V90, P315, DOI 10.1016/j.chb.2018.08.048
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Cohen J, 1988, STAT POWER ANAL BEHA
   Core dna, 2021, CHATBOTFAIL 4 CHATBO
   Craig Scotty D., 2019, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V63, P2272, DOI 10.1177/1071181319631517
   Demeure V, 2011, PRESENCE-VIRTUAL AUG, V20, P431, DOI 10.1162/PRES_a_00065
   Dickerson R, 2006, STUD HEALTH TECHNOL, V119, P114
   Dou X, 2021, INT J SOC ROBOT, V13, P615, DOI 10.1007/s12369-020-00654-9
   Fan A, 2016, J SERV MARK, V30, P713, DOI 10.1108/JSM-07-2015-0225
   Fernandes T, 2021, J BUS RES, V122, P180, DOI 10.1016/j.jbusres.2020.08.058
   Fiske ST, 2002, J PERS SOC PSYCHOL, V82, P878, DOI 10.1037//0022-3514.82.6.878
   Fiske ST, 2007, TRENDS COGN SCI, V11, P77, DOI 10.1016/j.tics.2006.11.005
   Fraj S., 2009, 2009 INTERSPEECH, P2907, DOI [10.21437/Interspeech.2009-736, DOI 10.21437/INTERSPEECH.2009-736]
   Gambino A., 2020, HumanMachine Communication, V1, P71, DOI [DOI 10.30658/HMC.1.5, 10.30658/hmc.1.5]
   Go E, 2019, COMPUT HUM BEHAV, V97, P304, DOI 10.1016/j.chb.2019.01.020
   Guha A, 2023, J ACAD MARKET SCI, V51, P843, DOI 10.1007/s11747-022-00874-7
   Hayes A. F., 2013, Introduction to Mediation, Moderation, and Conditional Process Analysis: Methodology in the Social Sciences, DOI DOI 10.1111/JEDM.12050
   Higgins D, 2022, COMPUT GRAPH-UK, V104, P116, DOI 10.1016/j.cag.2022.03.009
   Imhof M., 2010, INT J LIST, V24, P19, DOI [10.1080/10904010903466295, DOI 10.1080/10904010903466295]
   Kim SY, 2019, MARKET LETT, V30, P1, DOI 10.1007/s11002-019-09485-9
   Krenn B, 2017, AI SOC, V32, P65, DOI 10.1007/s00146-014-0569-0
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   Latinus M, 2011, CURR BIOL, V21, pR143, DOI 10.1016/j.cub.2010.12.033
   Lee JER, 2010, TRUST TECHNOLOGY UBI, P1, DOI DOI 10.4018/978-1-61520-901-9.CH001
   Leigh T. W., 2002, J PERS SELL SALES M, V22, P41, DOI [10.1080/08853134.2002.10754292, DOI 10.1080/08853134.2002.10754292]
   Li M., 2021, Proceedings of the 54th Hawaii International Conference on System Sciences, P4053, DOI [https://doi.org/10.24251/HICSS.2021.493, DOI 10.24251/HICSS.2021.493]
   Longoni C, 2022, J MARKETING, V86, P91, DOI 10.1177/0022242920957347
   McDonough M., 2020, CURR CONTENTS
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   Meppelink CS, 2019, J HEALTH COMMUN, V24, P129, DOI 10.1080/10810730.2019.1583701
   Mourey JA, 2017, J CONSUM RES, V44, P414, DOI 10.1093/jcr/ucx038
   Nass C, 2000, J SOC ISSUES, V56, P81, DOI 10.1111/0022-4537.00153
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Nijholt A, 2003, 2003 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P128, DOI 10.1109/CYBER.2003.1253445
   Nusbaum H. C., 1995, International Journal of Speech Technology, V1, P7, DOI 10.1007/BF02277176
   PETTY RE, 1983, J CONSUM RES, V10, P135, DOI 10.1086/208954
   Pitardi V, 2021, PSYCHOL MARKET, V38, P626, DOI 10.1002/mar.21457
   PricewaterhouseCoopers, 2018, CONS INT SER PREP VO
   Reeves B., 1996, The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Pla, DOI DOI 10.1007/S42452-020-2192-7
   Rella S., 2021, SPEECH TECHNOLO 1026
   Salem M, 2013, LECT NOTES ARTIF INT, V8239, P531, DOI 10.1007/978-3-319-02675-6_53
   Schreibelmayr S, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.787499
   Seymour William, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3479515
   Sherman JW, 2009, J PERS SOC PSYCHOL, V96, P305, DOI 10.1037/a0013778
   Sohn H, 2019, NEURON, V103, P934, DOI 10.1016/j.neuron.2019.06.012
   Stern SE, 2006, INT J HUM-COMPUT ST, V64, P43, DOI 10.1016/j.ijhcs.2005.07.002
   Stroessner SJ, 2019, INT J SOC ROBOT, V11, P305, DOI 10.1007/s12369-018-0502-7
   Torre I., 2021, Voice Attractiveness, P299, DOI [10.1007/978-981-15-6627-1_16, DOI 10.1007/978-981-15-6627-1_16]
   Torre I, 2020, IEEE ROMAN, P215, DOI 10.1109/RO-MAN47096.2020.9223449
   Urakami Jacqueline, 2020, Human-Computer Interaction. Multimodal and Natural Interaction. Thematic Area, HCI 2020 Held as Part of the 22nd International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12182), P244, DOI 10.1007/978-3-030-49062-1_17
   Vernuccio M, 2023, J CONSUM BEHAV, V22, P1074, DOI 10.1002/cb.1984
   Voorveld HAM, 2020, CYBERPSYCH BEH SOC N, V23, P689, DOI 10.1089/cyber.2019.0205
   Wang WH, 2017, COMPUT HUM BEHAV, V68, P334, DOI 10.1016/j.chb.2016.11.022
   WASON PC, 1966, BRIT J PSYCHOL, V57, P413, DOI 10.1111/j.2044-8295.1966.tb01044.x
   Whang C, 2021, PSYCHOL MARKET, V38, P581, DOI 10.1002/mar.21437
   Williams R., 2019, MARKETING DIVE 0919
   Xie ZH, 2022, PSYCHOL MARKET, V39, P1902, DOI 10.1002/mar.21706
NR 64
TC 4
Z9 4
U1 30
U2 39
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD AUG
PY 2023
VL 145
AR 107791
DI 10.1016/j.chb.2023.107791
EA APR 2023
PG 11
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA F8LK0
UT WOS:000984806300001
DA 2024-01-09
ER

PT J
AU Ko, S
   Barnes, J
   Dong, J
   Park, CH
   Howard, A
   Jeon, M
AF Ko, Sangjin
   Barnes, Jaclyn
   Dong, Jiayuan
   Park, Chung Hyuk
   Howard, Ayanna
   Jeon, Myounghoon
TI The Effects of Robot Voices and Appearances on Users' Emotion
   Recognition and Subjective Perception
SO INTERNATIONAL JOURNAL OF HUMANOID ROBOTICS
LA English
DT Article
DE Social robots; conversational agent; emotive voices; user perception;
   user preference
ID TRUST; FACE
AB As the influence of social robots in people's daily lives grows, research on understanding people's perception of robots including sociability, trust, acceptance, and preference becomes more pervasive. Research has considered visual, vocal, or tactile cues to express robots' emotions, whereas little research has provided a holistic view in examining the interactions among different factors influencing emotion perception. We investigated multiple facets of user perception on robots during a conversational task by varying the robots' voice types, appearances, and emotions. In our experiment, 20 participants interacted with two robots having four different voice types. While participants were reading fairy tales to the robot, the robot gave vocal feedback with seven emotions and the participants evaluated the robot's profiles through post surveys. The results indicate that (1) the accuracy of emotion perception differed depending on presented emotions, (2) a regular human voice showed higher user preferences and naturalness, (3) but a characterized voice was more appropriate for expressing emotions with significantly higher accuracy in emotion perception, and (4) participants showed significantly higher emotion recognition accuracy with the animal robot than the humanoid robot. A follow-up study (N=10) with voice-only conditions confirmed that the importance of embodiment. The results from this study could provide the guidelines needed to design social robots that consider emotional aspects in conversations between robots and users.
C1 [Ko, Sangjin; Dong, Jiayuan; Jeon, Myounghoon] Virginia Polytech Inst & State Univ, Grad Dept Ind & Syst Engn, Blacksburg, VA 24061 USA.
   [Barnes, Jaclyn; Jeon, Myounghoon] Michigan Technol Univ, Dept Comp Sci, Houghton, MI 49931 USA.
   [Park, Chung Hyuk] George Washington Univ, Dept Biomed Engn, Dept Comp Sci, Washington, DC USA.
   [Howard, Ayanna] Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH USA.
C3 Virginia Polytechnic Institute & State University; Michigan
   Technological University; George Washington University; University
   System of Ohio; Ohio State University
RP Jeon, M (corresponding author), Virginia Polytech Inst & State Univ, Grad Dept Ind & Syst Engn, Blacksburg, VA 24061 USA.; Jeon, M (corresponding author), Michigan Technol Univ, Dept Comp Sci, Houghton, MI 49931 USA.
EM myounghoonjeon@vt.edu
OI Jeon, Myounghoon/0000-0003-2908-671X
FU National Institutes of Health (US) [1 R01 HD082914-01]
FX This work was partly supported by National Institutes of Health (US)
   (No.1 R01 HD082914-01).
CR Bachorowski JA, 2003, ANN NY ACAD SCI, V1000, P244, DOI 10.1196/annals.1280.012
   Bänziger T, 2009, EMOTION, V9, P691, DOI 10.1037/a0017088
   Barnes J., 2018, P 24 INT C AUD DISPL, P271
   Barnes J, 2017, INT CONF UBIQ ROBOT, P51
   Barnes JA, 2021, INT J HUM-COMPUT INT, V37, P249, DOI 10.1080/10447318.2020.1819667
   Beer JS, 2006, BRAIN RES, V1079, P98, DOI 10.1016/j.brainres.2006.01.002
   Bevill R, 2016, ACMIEEE INT CONF HUM, P421, DOI 10.1109/HRI.2016.7451786
   Birkholz P, 2015, J ACOUST SOC AM, V137, P1503, DOI 10.1121/1.4906836
   Bryant D, 2020, ACMIEEE INT CONF HUM, P13, DOI 10.1145/3319502.3374778
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Coeckelbergh M, 2010, ETHICS INF TECHNOL, V12, P235, DOI 10.1007/s10676-010-9221-y
   Conti D, 2020, INTERACT STUD, V21, P220, DOI 10.1075/is.18024.con
   Cordaro DT, 2016, EMOTION, V16, P117, DOI 10.1037/emo0000100
   Darwin C., 1872, P374
   Decety J, 2010, DEV NEUROSCI-BASEL, V32, P257, DOI 10.1159/000317771
   Diaz M., 2011, FACE GESTURE, V27, P927, DOI [10.1109/FG.2011.5771375, DOI 10.1109/FG.2011.5771375]
   DiSalvo C.F., 2002, P DIS02 DES INT SYST, P321
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Ekman P, 2011, EMOT REV, V3, P364, DOI 10.1177/1754073911410740
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   Eyssel F., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P851, DOI 10.1109/ROMAN.2012.6343858
   Eyssel F, 2012, ACMIEEE INT CONF HUM, P125
   FakhrHosseini, 2017, 2017 26 IEEE INT S R
   Feldmaier, 2017, PROC COMPANION 2017
   Fischer K, 2019, ACMIEEE INT CONF HUM, P29, DOI 10.1109/HRI.2019.8673078
   Fong T, 2003, ROBOT AUTON SYST, V42, P143, DOI 10.1016/S0921-8890(02)00372-X
   Fraune MR, 2015, ACMIEEE INT CONF HUM, P109, DOI [10.1145/2696454.2696483, 10.1145/2696454.2696472]
   Frith CD, 2006, NEURON, V50, P531, DOI 10.1016/j.neuron.2006.05.001
   Ham J, 2015, INT J SOC ROBOT, V7, P479, DOI 10.1007/s12369-015-0280-4
   Haring KS, 2013, ACMIEEE INT CONF HUM, P131, DOI 10.1109/HRI.2013.6483536
   Jeon M, 2011, LECT NOTES COMPUT SC, V6762, P523
   Jung MF, 2017, ACMIEEE INT CONF HUM, P263, DOI 10.1145/2909824.3020224
   Kishi T, 2013, IEEE INT CONF ROBOT, P1663, DOI 10.1109/ICRA.2013.6630793
   Kraus M, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P112
   Kwak S. S., 2013, 2013 IEEE RO MAN, P180, DOI DOI 10.1109/ROMAN.2013.6628441
   LEWIS JD, 1985, SOC FORCES, V63, P967, DOI 10.2307/2578601
   Li J, 2015, INT J HUM-COMPUT ST, V77, P23, DOI 10.1016/j.ijhcs.2015.01.001
   Liu ZT, 2016, CHIN CONTR CONF, P6363, DOI 10.1109/ChiCC.2016.7554357
   Lowe, 2016, GROUNDING EMOTIONS R
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   Mitchell RLC, 2015, NEUROPSYCHOLOGIA, V70, P1, DOI 10.1016/j.neuropsychologia.2015.02.018
   Nabe S., 2006, ROB HUM INT COMM 200, P384, DOI DOI 10.1109/ROMAN.2006.314464
   Nass Clifford, 2001, P AAAI S EM INT 2 TA
   Nonaka S, 2004, IEEE INT CONF ROBOT, P2770, DOI 10.1109/ROBOT.2004.1307480
   Ortony A, 2022, PERSPECT PSYCHOL SCI, V17, P41, DOI 10.1177/1745691620985415
   Pereira A, 2008, P 7 INT JOINT C AUT, V3, P1253
   Phillips ML, 2003, BRIT J PSYCHIAT, V182, P190, DOI 10.1192/bjp.182.3.190
   Plutchik Robert, 1980, Theories of Emotion, P3, DOI DOI 10.1016/B978-0-12-558701-3.50007-7
   Reisenzein R, 2013, IEEE T AFFECT COMPUT, V4, P246, DOI 10.1109/T-AFFC.2013.14
   Rodriguez-Lizundia E, 2015, INT J HUM-COMPUT ST, V82, P83, DOI 10.1016/j.ijhcs.2015.06.001
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Russell JA, 2017, EMOTIONS AND AFFECT IN HUMAN FACTORS AND HUMAN-COMPUTER INTERACTION, P123, DOI 10.1016/B978-0-12-801851-4.00004-5
   Saint-Aime Sebastien, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P919
   Sakai K, 2022, IEEE ROBOT AUTOM LET, V7, P366, DOI 10.1109/LRA.2021.3128233
   Sangjin Ko, 2020, HCI International 2020 - Late Breaking Papers. Multimodality and Intelligence. 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12424), P174, DOI 10.1007/978-3-030-60117-1_13
   Schilhab, 2002, ANIM BEHAV
   Schirmer A, 2017, TRENDS COGN SCI, V21, P216, DOI 10.1016/j.tics.2017.01.001
   Seo SH, 2015, ACMIEEE INT CONF HUM, P125, DOI 10.1145/2696454.2696471
   Sharma, 2013, PROC 8 ACMIEEE INT C
   Sims, 2009, PROC HUMAN FACTORS E
   Song SC, 2017, ACMIEEE INT CONF HUM, P2, DOI 10.1145/2909824.3020239
   Susskind JM, 2008, NAT NEUROSCI, V11, P843, DOI 10.1038/nn.2138
   Thomaz A., 2016, Foundations and Trends in Robotics, V4, P105, DOI 10.1561/2300000049
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Waytz A, 2014, J EXP SOC PSYCHOL, V52, P113, DOI 10.1016/j.jesp.2014.01.005
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   ZIGMOND AS, 1983, ACTA PSYCHIAT SCAND, V67, P361, DOI 10.1111/j.1600-0447.1983.tb09716.x
NR 67
TC 1
Z9 2
U1 20
U2 24
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0219-8436
EI 1793-6942
J9 INT J HUM ROBOT
JI Int. J. Humanoid Robot.
PD FEB
PY 2023
VL 20
IS 01
DI 10.1142/S0219843623500019
EA FEB 2023
PG 33
WC Robotics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Robotics
GA C2FG7
UT WOS:000937023400001
OA Green Published, hybrid
DA 2024-01-09
ER

PT J
AU van Prooije, T
   Knuijt, S
   Oostveen, J
   Kapteijns, K
   Vogel, AP
   van de Warrenburg, B
AF van Prooije, Teije
   Knuijt, Simone
   Oostveen, Judith
   Kapteijns, Kirsten
   Vogel, Adam P. P.
   van de Warrenburg, Bart
TI Perceptual and Acoustic Analysis of Speech in Spinocerebellar ataxia
   Type 1
SO CEREBELLUM
LA English
DT Article; Early Access
DE Spinocerebellar ataxia type 1; Speech; Dysarthria
ID FRIEDREICH ATAXIA; DYSARTHRIA; OUTCOMES; MARKERS; DISEASE
AB This study characterizes the speech phenotype of spinocerebellar ataxia type 1 (SCA1) using both perceptual and objective acoustic analysis of speech in a cohort of SCA1 patients. Twenty-seven symptomatic SCA1 patients in various disease stages (SARA score range: 3-32 points) and 18 sex and age matched healthy controls underwent a clinical assessment addressing ataxia severity, non-ataxia signs, cognitive functioning, and speech. Speech samples were perceptually rated by trained speech therapists, and acoustic metrics representing speech timing, vocal control, and voice quality were extracted. Perceptual analysis revealed reduced intelligibility and naturalness in speech samples of SCA1 patients. Acoustically, SCA1 patients presented with slower speech rate and diadochokinetic rate as well as longer syllable duration compared to healthy controls. No distinct abnormalities in voice quality in the acoustic analysis were detected at group level. Both the affected perceptual and acoustic variables correlated with ataxia severity. Longitudinal assessment of speech is needed to place changes in speech in the context of disease progression and potential response to treatment.
C1 [van Prooije, Teije; Kapteijns, Kirsten; van de Warrenburg, Bart] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Dept Neurol, Med Ctr, Nijmegen, Netherlands.
   [Knuijt, Simone; Oostveen, Judith] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Dept Rehabil, Med Ctr, Nijmegen, Netherlands.
   [Vogel, Adam P. P.] Univ Melbourne, Ctr Neurosci Speech, Melbourne, Australia.
   [Vogel, Adam P. P.] Univ Tubingen, Hertie Inst Clin Brain Res, Translat Genom Neurodegenerat Dis, Tubingen, Germany.
   [Vogel, Adam P. P.] Univ Tubingen, Ctr Neurol, Tubingen, Germany.
   [Vogel, Adam P. P.] Redenlab Inc, Melbourne, Australia.
C3 Radboud University Nijmegen; Radboud University Nijmegen; University of
   Melbourne; Eberhard Karls University of Tubingen; Eberhard Karls
   University Hospital; Eberhard Karls University of Tubingen
RP van de Warrenburg, B (corresponding author), Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Dept Neurol, Med Ctr, Nijmegen, Netherlands.
EM Bart.vandeWarrenburg@radboudumc.nl
RI Vogel, Adam/ABU-8530-2022
OI Kapteijns, Kirsten/0000-0001-5735-1934; van Prooije,
   Teije/0000-0002-6372-0801; Vogel, Adam/0000-0002-3505-2631
CR Boersma P., 2021, Glot International
   Brendel B, 2013, CEREBELLUM, V12, P475, DOI 10.1007/s12311-012-0440-0
   Brooker SM, 2021, ANN CLIN TRANSL NEUR, V8, P1543, DOI 10.1002/acn3.51370
   Bürk K, 2003, J NEUROL, V250, P207, DOI 10.1007/s00415-003-0976-5
   Chan JCS, 2022, J HUNTINGTONS DIS, V11, P71, DOI 10.3233/JHD-210501
   Chan JCS, 2019, NEUROSCI BIOBEHAV R, V107, P450, DOI 10.1016/j.neubiorev.2019.08.009
   Hoche F, 2018, BRAIN, V141, P248, DOI 10.1093/brain/awx317
   Jacobi H, 2013, CEREBELLUM, V12, P418, DOI 10.1007/s12311-012-0421-3
   Jacobi H, 2015, LANCET NEUROL, V14, P1101, DOI 10.1016/S1474-4422(15)00202-1
   Klockgether T, 2019, NAT REV DIS PRIMERS, V5, DOI 10.1038/s41572-019-0074-3
   Knuijt S, 2017, FOLIA PHONIATR LOGO, V69, P143, DOI 10.1159/000484556
   Luo L, 2017, CEREBELLUM, V16, P615, DOI 10.1007/s12311-016-0836-3
   Nasreddine ZS, 2005, J AM GERIATR SOC, V53, P695, DOI 10.1111/j.1532-5415.2005.53221.x
   Nigri A, 2022, CEREBELLUM, V21, P133, DOI 10.1007/s12311-021-01285-0
   ORR HT, 1993, NAT GENET, V4, P221, DOI 10.1038/ng0793-221
   Rosen KM, 2012, J NEUROL, V259, P2471, DOI 10.1007/s00415-012-6547-x
   Schalling E, 2013, BRAIN LANG, V127, P317, DOI 10.1016/j.bandl.2013.10.002
   Schmahmann JD, 1998, BRAIN, V121, P561, DOI 10.1093/brain/121.4.561
   Schmitz-Hübsch T, 2006, NEUROLOGY, V66, P1717, DOI 10.1212/01.wnl.0000219042.60538.92
   Sidtis JJ, 2011, J COMMUN DISORD, V44, P478, DOI 10.1016/j.jcomdis.2011.03.002
   Vogel AP, 2020, NEUROLOGY, V95, pE194, DOI 10.1212/WNL.0000000000009776
   Vogel AP, 2018, J NEUROL, V265, P2060, DOI 10.1007/s00415-018-8950-4
   Vogel AP, 2017, MITOCHONDRION, V37, P1, DOI 10.1016/j.mito.2017.06.002
   Vogel AP, 2017, NEUROLOGY, V89, P837, DOI 10.1212/WNL.0000000000004248
   Vogel AP, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.04.015
   Vogel Adam P., 2015, Frontiers in Bioengineering and Biotechnology, V3, P98, DOI 10.3389/fbioe.2015.00098
   Vogel AP, 2014, SPEECH COMMUN, V65, P1, DOI 10.1016/j.specom.2014.05.002
   Vogel AP, 2012, NEUROPSYCHOLOGIA, V50, P3273, DOI 10.1016/j.neuropsychologia.2012.09.011
   Vogel AP, 2011, J VOICE, V25, P137, DOI 10.1016/j.jvoice.2009.09.003
   Walton MK, 2015, VALUE HEALTH, V18, P741, DOI 10.1016/j.jval.2015.08.006
   Weismer G, 2002, J SPEECH LANG HEAR R, V45, P421, DOI 10.1044/1092-4388(2002/033)
NR 31
TC 0
Z9 0
U1 1
U2 1
PU SPRINGER
PI NEW YORK
PA ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
SN 1473-4222
EI 1473-4230
J9 CEREBELLUM
JI Cerebellum
PD 2023 JAN 12
PY 2023
DI 10.1007/s12311-023-01513-9
EA JAN 2023
PG 9
WC Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Neurosciences & Neurology
GA 8C9KD
UT WOS:000917918400001
PM 36633828
OA hybrid
DA 2024-01-09
ER

PT J
AU Li, MM
   Guo, F
   Wang, XS
   Chen, JH
   Ham, J
AF Li, Mingming
   Guo, Fu
   Wang, Xueshuang
   Chen, Jiahao
   Ham, Jaap
TI Effects of robot gaze and voice human-likeness on users' subjective
   perception, visual attention, and cerebral activity in voice
   conversations
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Robot gaze; Voice human-likeness; Subjective perception; Eye-tracking;
   fNIRS
ID PREFRONTAL CORTEX; PERCEIVED DIFFICULTY; OLDER-PEOPLE; EYE CONTACT;
   EMOTION; FNIRS; STEREOTYPES; INTEGRATION; ATTITUDES; COGNITION
AB Robot gaze and voice are essential anthropomorphic features to promote users' engagement in voice conver-sations. Earlier research chiefly examined how robot gaze and voice human-likeness separately influenced users' subjective perception. When implementing gaze on robots with different human-like voices, there has little evidence of their possible interaction effects, particularly on users' visual attention and cerebral activity, which could help to understand the perceptual and cognitive processing of anthropomorphic features. Therefore, a within-subject experiment of voice conversations with diverse robot gaze (gaze versus no gaze) and human-like voices (high human-like versus low human-like) using subjective reporting, eye-tracker, and fNIRS was con-ducted. The results showed that the robot with gaze or a high human-like voice evoked more pleasure, higher arousal, more perceived likability, and less negative attitudes. Robot gaze significantly increased users' average fixation durations and total fixation time, while voice human-likeness prolonged first fixation durations. Moreover, the robot with a high human-like voice (or gaze) induced increased activity in the left DLPFC and decreased activity in the right Broca's area than that had no gaze (or a low human-like voice). The results suggest that robot gaze might chiefly capture users' sustained attention, voice human-likeness might attract users' initial attention, and they might jointly influence users' perceptual processing of prosodic features and emotional processing.
C1 [Li, Mingming; Guo, Fu; Chen, Jiahao] Northeastern Univ, Sch Business Adm, Dept Ind Engn, Shenyang, Peoples R China.
   [Wang, Xueshuang] Shenyang Univ Technol, Sch Mech Engn, Shenyang, Peoples R China.
   [Ham, Jaap] Eindhoven Univ Technol, Dept Ind Engn & Innovat Sci, Res Grp Human Technol Interact, Eindhoven, Netherlands.
   [Guo, Fu] 195 Chuangxin Rd, Shenyang 110167, Peoples R China.
C3 Northeastern University - China; Shenyang University of Technology;
   Eindhoven University of Technology
RP Guo, F (corresponding author), 195 Chuangxin Rd, Shenyang 110167, Peoples R China.
EM fguo@mail.neu.edu.cn
RI Ham, Jaap/H-4754-2011
OI Chen, Jiahao/0000-0002-6110-7851; Ham, Jaap/0000-0003-1703-5165; Li,
   Mingming/0000-0002-4702-6350
FU National Natural Science Foundation of China [72071035, 72171042]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 72071035 and Grant No. 72171042) . No conflict of
   interest exists in submitting this paper, and all authors approve it for
   publication. We are grateful to all the experimental participants for
   this study. Furthermore, we are genuinely pleased to extend our
   gratitude to editors and reviewers for their valuable work.
CR Admoni H, 2017, J HUM-ROBOT INTERACT, V6, P25, DOI 10.5898/JHRI.6.1.Admoni
   Amso D, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0085701
   Andrist S, 2014, ACMIEEE INT CONF HUM, P25, DOI 10.1145/2559636.2559666
   Argyle M., 1972, Non-verbal communication, V2
   Babel F, 2021, INT J SOC ROBOT, V13, P1485, DOI 10.1007/s12369-020-00730-0
   Baillon A, 2013, EVOL HUM BEHAV, V34, P146, DOI 10.1016/j.evolhumbehav.2012.12.001
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Balconi M, 2021, J MOTOR BEHAV, V53, P296, DOI 10.1080/00222895.2020.1774490
   Balconi M, 2015, BRAIN COGNITION, V95, P67, DOI 10.1016/j.bandc.2015.02.001
   Bartneck C., 2010, Paladyn, Journal of Behavioral Robotics, V1, P109, DOI DOI 10.2478/S13230-010-0011-3
   Behe BK, 2015, J RETAIL CONSUM SERV, V24, P10, DOI 10.1016/j.jretconser.2015.01.002
   Belin P, 2006, PHILOS T R SOC B, V361, P2091, DOI 10.1098/rstb.2006.1933
   Belkaid M, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abc5044
   Bourguet Marie-Luce, 2020, HAI '20: Proceedings of the 8th International Conference on Human-Agent Interaction, P60, DOI 10.1145/3406499.3415073
   Burleigh TJ, 2013, COMPUT HUM BEHAV, V29, P759, DOI 10.1016/j.chb.2012.11.021
   Burton MW, 2000, J COGNITIVE NEUROSCI, V12, P679, DOI 10.1162/089892900562309
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Callaway C, 2006, COMPUT LINGUIST, V32, P451, DOI 10.1162/coli.2006.32.3.451
   Chang RCS, 2018, COMPUT HUM BEHAV, V84, P194, DOI 10.1016/j.chb.2018.02.025
   Charest I, 2009, BMC NEUROSCI, V10, DOI 10.1186/1471-2202-10-127
   COPE M, 1988, MED BIOL ENG COMPUT, V26, P289, DOI 10.1007/BF02447083
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Degutyte Z, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.616471
   Dou X, 2022, INT J SOC ROBOT, V14, P229, DOI 10.1007/s12369-021-00782-w
   Dou X, 2021, INT J SOC ROBOT, V13, P615, DOI 10.1007/s12369-020-00654-9
   Duffy BR, 2003, ROBOT AUTON SYST, V42, P177, DOI 10.1016/S0921-8890(02)00374-3
   Emery NJ, 2000, NEUROSCI BIOBEHAV R, V24, P581, DOI 10.1016/S0149-7634(00)00025-7
   Eyssel, 2012, P 7 ANN ACMIEEE INT, DOI DOI 10.1145/2157689.2157717
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Ferrari M, 2012, NEUROIMAGE, V63, P921, DOI 10.1016/j.neuroimage.2012.03.049
   Forbes CE, 2010, ANNU REV NEUROSCI, V33, P299, DOI 10.1146/annurev-neuro-060909-153230
   Friederici AD, 2004, BRAIN LANG, V89, P267, DOI 10.1016/S0093-934X(03)00351-1
   Gameiro RR, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-02526-1
   Garza R, 2016, EVOL PSYCHOL-US, V14, DOI 10.1177/1474704916631614
   Ghiglino Davide, 2020, Paladyn, Journal of Behavioral Robotics, V11, P31, DOI 10.1515/pjbr-2020-0004
   Ghiglino D, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.642796
   Glotzbach Evelyn, 2011, Open Neuroimag J, V5, P33, DOI 10.2174/1874440001105010033
   Gray JR, 2002, P NATL ACAD SCI USA, V99, P4115, DOI 10.1073/pnas.062381899
   Guo F, 2019, INT J HUM-COMPUT INT, V35, P1947, DOI 10.1080/10447318.2019.1587938
   Ham J, 2015, INT J SOC ROBOT, V7, P479, DOI 10.1007/s12369-015-0280-4
   Heim S, 2003, COGNITIVE BRAIN RES, V16, P285, DOI 10.1016/S0926-6410(02)00284-7
   Herrington JD, 2005, EMOTION, V5, P200, DOI 10.1037/1528-3542.5.2.200
   Hietanen JK, 2008, NEUROPSYCHOLOGIA, V46, P2423, DOI 10.1016/j.neuropsychologia.2008.02.029
   Hoshi Y, 2003, PSYCHOPHYSIOLOGY, V40, P511, DOI 10.1111/1469-8986.00053
   Hou X, 2021, NEUROPHOTONICS, V8, DOI 10.1117/1.NPh.8.1.010802
   Huang JL, 2022, HUM FACTORS, V64, P1051, DOI 10.1177/0018720820987443
   Husic-Mehmedovic M, 2017, J BUS RES, V80, P145, DOI 10.1016/j.jbusres.2017.04.019
   Jiang J, 2017, SOC COGN AFFECT NEUR, V12, P319, DOI 10.1093/scan/nsw127
   Jiang J, 2012, J NEUROSCI, V32, P16064, DOI 10.1523/JNEUROSCI.2926-12.2012
   JUST MA, 1976, COGNITIVE PSYCHOL, V8, P441, DOI 10.1016/0010-0285(76)90015-3
   Kawasaki M, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00318
   Kelley MS, 2021, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.599581
   Keshmiri S, 2019, IEEE ROBOT AUTOM LET, V4, P4108, DOI 10.1109/LRA.2019.2930495
   Keshmiri S, 2019, IEEE ROBOT AUTOM LET, V4, P3263, DOI 10.1109/LRA.2019.2925732
   KLEINKE CL, 1986, PSYCHOL BULL, V100, P78, DOI 10.1037/0033-2909.100.1.78
   Klüber K, 2022, COMPUT HUM BEHAV, V128, DOI 10.1016/j.chb.2021.107128
   Kompatsiari K, 2019, IEEE INT C INT ROBOT, P6979, DOI [10.1109/IROS40897.2019.8967747, 10.1109/iros40897.2019.8967747]
   Kompatsiari K, 2021, SOC COGN AFFECT NEUR, V16, P383, DOI 10.1093/scan/nsab001
   Kompatsiari K, 2021, INT J SOC ROBOT, V13, P525, DOI 10.1007/s12369-019-00565-4
   Kompatsiari K, 2017, LECT NOTES ARTIF INT, V10652, P443, DOI 10.1007/978-3-319-70022-9_44
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   Kuo JY, 2021, APPL ERGON, V94, DOI 10.1016/j.apergo.2021.103393
   Kwak SS, 2017, INT J SOC ROBOT, V9, P359, DOI 10.1007/s12369-016-0388-1
   Leong V, 2017, P NATL ACAD SCI USA, V114, P13290, DOI 10.1073/pnas.1702493114
   Levy DA, 2003, PSYCHOPHYSIOLOGY, V40, P291, DOI 10.1111/1469-8986.00031
   Li MM, 2022, INT J IND ERGONOM, V88, DOI 10.1016/j.ergon.2021.103159
   Liew TW, 2021, TELEMAT INFORM, V65, DOI 10.1016/j.tele.2021.101721
   Loh HW, 2022, COMPUT METH PROG BIO, V226, DOI 10.1016/j.cmpb.2022.107161
   Lu XP, 2019, HUM BRAIN MAPP, V40, P1942, DOI 10.1002/hbm.24503
   Manzi F, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-69140-6
   Mavridis N, 2015, ROBOT AUTON SYST, V63, P22, DOI 10.1016/j.robot.2014.09.031
   Mehrabian A., 1974, APPROACH ENV PSYCHOL
   Morillo-Mendez L, 2021, LECT NOTES ARTIF INT, V13086, P350, DOI 10.1007/978-3-030-90525-5_30
   Murray LJ, 2007, J NEUROSCI, V27, P5515, DOI 10.1523/JNEUROSCI.0406-07.2007
   Mutlu Bilge, 2012, ACM Trans Interact Intell Syst (TiiS), V2, P1, DOI DOI 10.1145/2070719.2070725
   NASS C, 1995, INT J HUM-COMPUT ST, V43, P223, DOI 10.1006/ijhc.1995.1042
   Nomura T, 2006, INTERACT STUD, V7, P437, DOI 10.1075/is.7.3.14nom
   Okafuji Y, 2020, ADV ROBOTICS, V34, P931, DOI 10.1080/01691864.2020.1769724
   Perugia G, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.645956
   Piva M, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00571
   Quaresima V, 2019, ORGAN RES METHODS, V22, P46, DOI 10.1177/1094428116658959
   Rapp A, 2021, INT J HUM-COMPUT ST, V151, DOI 10.1016/j.ijhcs.2021.102630
   Roesler E, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abj5425
   Sarigul B., 2020, COMPANION 2020 ACMIE
   Schreibelmayr S, 2022, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.787499
   Seaborn K, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3386867
   Singh AK, 2005, NEUROIMAGE, V27, P842, DOI 10.1016/j.neuroimage.2005.05.019
   Tatarian K, 2022, INT J SOC ROBOT, V14, P893, DOI 10.1007/s12369-021-00839-w
   Tay B, 2014, COMPUT HUM BEHAV, V38, P75, DOI 10.1016/j.chb.2014.05.014
   Thepsoonthorn C, 2021, INT J SOC ROBOT, V13, P1443, DOI 10.1007/s12369-020-00726-w
   Tiberio L, 2013, ROBOTICS, V2, P92, DOI 10.3390/robotics2020092
   Tsiourti C, 2019, INT J SOC ROBOT, V11, P555, DOI 10.1007/s12369-019-00524-z
   Unema PJA, 2005, VIS COGN, V12, P473, DOI 10.1080/13506280444000409
   Vollmer AL, 2018, SCI ROBOT, V3, DOI 10.1126/scirobotics.aat7111
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Wang JH, 2020, COMPUT EDUC, V146, DOI 10.1016/j.compedu.2019.103779
   WASSERTHEIL S, 1970, BIOMETRICS, V26, P588, DOI 10.2307/2529115
   Wei Y, 2016, IND ROBOT, V43, P380, DOI 10.1108/IR-08-2015-0164
   WHEELER RE, 1993, PSYCHOPHYSIOLOGY, V30, P82, DOI 10.1111/j.1469-8986.1993.tb03207.x
   Wiese E, 2019, PHILOS T R SOC B, V374, DOI 10.1098/rstb.2018.0430
   Winkle K, 2020, INT J SOC ROBOT, V12, P847, DOI [10.1007/s12369-019-00536-9, 10.1080/10494820.2019.1696844]
   Wood LJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059448
   Xu K, 2019, NEW MEDIA SOC, V21, P2522, DOI 10.1177/1461444819851479
   Yang GZ, 2018, SCI ROBOT, V3, DOI 10.1126/scirobotics.aar7650
   Zhang DD, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-18683-2
   Zhang H, 2020, BIOL PSYCHOL, V150, DOI 10.1016/j.biopsycho.2019.107827
   Zhang RH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4951, DOI 10.24963/ijcai.2020/689
   Zhang YX, 2017, LECT NOTES ARTIF INT, V10652, P556, DOI 10.1007/978-3-319-70022-9_55
NR 108
TC 2
Z9 2
U1 39
U2 85
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD APR
PY 2023
VL 141
AR 107645
DI 10.1016/j.chb.2022.107645
EA JAN 2023
PG 15
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA 8C0RC
UT WOS:000917324100001
DA 2024-01-09
ER

PT J
AU Shiramizu, VKM
   Lee, AJ
   Altenburg, D
   Feinberg, DR
   Jones, BC
AF Shiramizu, Victor Kenji M.
   Lee, Anthony J.
   Altenburg, Daria
   Feinberg, David R.
   Jones, Benedict C.
TI The role of valence, dominance, and pitch in perceptions of artificial
   intelligence (AI) conversational agents' voices
SO SCIENTIFIC REPORTS
LA English
DT Article
ID BIAS
AB There is growing concern that artificial intelligence conversational agents (e.g., Siri, Alexa) reinforce voice-based social stereotypes. Because little is known about social perceptions of conversational agents' voices, we investigated (1) the dimensions that underpin perceptions of these synthetic voices and (2) the role that acoustic parameters play in these perceptions. Study 1 (N = 504) found that perceptions of synthetic voices are underpinned by Valence and Dominance components similar to those previously reported for natural human stimuli and that the Dominance component was strongly and negatively related to voice pitch. Study 2 (N = 160) found that experimentally manipulating pitch in synthetic voices directly influenced dominance-related, but not valence-related, perceptions. Collectively, these results suggest that greater consideration of the role that voice pitch plays in dominance-related perceptions when designing conversational agents may be an effective method for controlling stereotypic perceptions of their voices and the downstream consequences of those perceptions.
C1 [Shiramizu, Victor Kenji M.; Jones, Benedict C.] Univ Strathclyde, Sch Psychol Sci & Hlth, Glasgow, Lanark, Scotland.
   [Lee, Anthony J.] Univ Stirling, Div Psychol, Stirling, Scotland.
   [Altenburg, Daria] Univ Ghent, Dept Mkt Innovat & Org, Ghent, Belgium.
   [Feinberg, David R.] McMaster Univ, Dept Psychol Neurosci & Behav, Hamilton, ON, Canada.
C3 University of Strathclyde; University of Stirling; Ghent University;
   McMaster University
RP Jones, BC (corresponding author), Univ Strathclyde, Sch Psychol Sci & Hlth, Glasgow, Lanark, Scotland.
EM benedict.jones@strath.ac.uk
RI Shiramizu, Victor/GPP-0919-2022; Altenburg, Daria/ISU-0474-2023;
   Feinberg, David R/C-1249-2009; Lee, Anthony J./I-8220-2012; Jones,
   Benedict C/A-7850-2008
OI Feinberg, David R/0000-0003-4179-1446; Lee, Anthony
   J./0000-0001-8288-3393; Jones, Benedict C/0000-0001-7777-0220
FU EPSRC grant 'Designing Conversational Assistants to Reduce Gender Bias'
   [EP/T023783/1]; Ghent University [BOF.24Y.2019.0006.01]
FX This research was supported by the EPSRC grant 'Designing Conversational
   Assistants to Reduce Gender Bias' (EP/T023783/1), awarded to Benedict
   Jones. Daria Altenburg was supported by Grant BOF.24Y.2019.0006.01 of
   Ghent University, awarded to Adriaan Spruyt. For the purpose of Open
   Access, the authors have applied a Creative Commons Attribution (CC BY)
   to any Author Accepted Manuscript (AAM) version arising from this
   submission.
CR Apicella CL, 2009, P ROY SOC B-BIOL SCI, V276, P1077, DOI 10.1098/rspb.2008.1542
   Armstrong MM, 2019, ANIM BEHAV, V147, P43, DOI 10.1016/j.anbehav.2018.11.005
   Aung T, 2020, CURR OPIN PSYCHOL, V33, P154, DOI 10.1016/j.copsyc.2019.07.028
   Balas B, 2018, COMPUT HUM BEHAV, V88, P236, DOI 10.1016/j.chb.2018.07.013
   Balas B, 2017, COMPUT HUM BEHAV, V77, P240, DOI 10.1016/j.chb.2017.08.045
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Baus C, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36518-6
   Boersma P., 2018, PRAAT DOING PHONETIC
   Bolker B., 2022, R package version 0.2.9.4
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Dinno A., 2018, paran: Horn's Test of Principal Components/Factors
   Feinberg DR, 2005, ANIM BEHAV, V69, P561, DOI 10.1016/j.anbehav.2004.06.012
   Hester N, 2021, J EXP PSYCHOL GEN, V150, P1147, DOI 10.1037/xge0000989
   Hodges-Simeon CR, 2010, HUM NATURE-INT BIOS, V21, P406, DOI 10.1007/s12110-010-9101-5
   Jones BC, 2021, NAT HUM BEHAV, V5, P159, DOI 10.1038/s41562-020-01007-2
   Jones BC, 2010, ANIM BEHAV, V79, P57, DOI 10.1016/j.anbehav.2009.10.003
   Kuznetsova A, 2017, J STAT SOFTW, V82, P1, DOI 10.18637/jss.v082.i13
   Long J., 2020, JTOOLS ANAL PRESENTA
   McAleer P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0090779
   Oh D, 2019, PSYCHOL SCI, V30, P65, DOI 10.1177/0956797618813092
   Olivola CY, 2014, TRENDS COGN SCI, V18, P566, DOI 10.1016/j.tics.2014.09.007
   Oosterhof NN, 2008, P NATL ACAD SCI USA, V105, P11087, DOI 10.1073/pnas.0805664105
   Puts DA, 2006, EVOL HUM BEHAV, V27, P283, DOI 10.1016/j.evolhumbehav.2005.11.003
   R Core Team, 2018, R: A Language and Environment for Statistical Computing
   Revelle W, 2016, Psych: procedures for personality and psychological research, DOI DOI 10.1109/TEM.2010.2048913
   Rhodes G, 2006, ANNU REV PSYCHOL, V57, P199, DOI 10.1146/annurev.psych.57.102904.190208
   Schild C, 2022, ADAPT HUM BEHAV PHYS, V8, P538, DOI 10.1007/s40750-022-00194-8
   Schild C, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-77940-z
   Sutherland CAM, 2013, COGNITION, V127, P105, DOI 10.1016/j.cognition.2012.12.001
   West Mark, 2019, I'd blush if I could: closing gender divides in digital skills through education
   Wester Mirjam, 2017, P 19 ACM INT C MULT, P506
   Wickham H., 2017, R PACKAGE VERSION, V1, P1, DOI DOI 10.1590/S0100-204X2017000300003
   Wickham H., 2019, readxl: read Excel files
   Wilson JP, 2015, PSYCHOL SCI, V26, P1325, DOI 10.1177/0956797615590992
   Xie Y., 2014, Implementing Reproducible Computational Research, DOI DOI 10.1201/9781315373461-1
   Zhu H., 2019, kableExtra: Construct Complex Table with 'kable' and Pipe Syntax
NR 36
TC 0
Z9 0
U1 1
U2 6
PU NATURE PORTFOLIO
PI BERLIN
PA HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD DEC 28
PY 2022
VL 12
IS 1
AR 22479
DI 10.1038/s41598-022-27124-8
PG 9
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA 7L1VX
UT WOS:000905762900011
PM 36577918
OA Green Accepted, gold, Green Published
DA 2024-01-09
ER

PT J
AU Duville, MM
   Alonso-Valerdi, LM
   Ibarra-Zarate, DI
AF Duville, Mathilde Marie
   Alonso-Valerdi, Luz Maria
   Ibarra-Zarate, David I. I.
TI Neuronal and behavioral affective perceptions of human and
   naturalness-reduced emotional prosodies
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
LA English
DT Article
DE electroencephalography (EEG); single-trial event-related potential
   (ERP); affective prosody; emotions; naturalness; valence; arousal;
   synthesized speech
ID DISCRIMINATION; SEMANTICS; DYNAMICS; STRESS
AB Artificial voices are nowadays embedded into our daily lives with latest neural voices approaching human voice consistency (naturalness). Nevertheless, behavioral, and neuronal correlates of the perception of less naturalistic emotional prosodies are still misunderstood. In this study, we explored the acoustic tendencies that define naturalness from human to synthesized voices. Then, we created naturalness-reduced emotional utterances by acoustic editions of human voices. Finally, we used Event-Related Potentials (ERP) to assess the time dynamics of emotional integration when listening to both human and synthesized voices in a healthy adult sample. Additionally, listeners rated their perceptions for valence, arousal, discrete emotions, naturalness, and intelligibility. Synthesized voices were characterized by less lexical stress (i.e., reduced difference between stressed and unstressed syllables within words) as regards duration and median pitch modulations. Besides, spectral content was attenuated toward lower F2 and F3 frequencies and lower intensities for harmonics 1 and 4. Both psychometric and neuronal correlates were sensitive to naturalness reduction. (1) Naturalness and intelligibility ratings dropped with emotional utterances synthetization, (2) Discrete emotion recognition was impaired as naturalness declined, consistent with P200 and Late Positive Potentials (LPP) being less sensitive to emotional differentiation at lower naturalness, and (3) Relative P200 and LPP amplitudes between prosodies were modulated by synthetization. Nevertheless, (4) Valence and arousal perceptions were preserved at lower naturalness, (5) Valence (arousal) ratings correlated negatively (positively) with Higuchi's fractal dimension extracted on neuronal data under all naturalness perturbations, (6) Inter-Trial Phase Coherence (ITPC) and standard deviation measurements revealed high inter-individual heterogeneity for emotion perception that is still preserved as naturalness reduces. Notably, partial between-participant synchrony (low ITPC), along with high amplitude dispersion on ERPs at both early and late stages emphasized miscellaneous emotional responses among subjects. In this study, we highlighted for the first time both behavioral and neuronal basis of emotional perception under acoustic naturalness alterations. Partial dependencies between ecological relevance and emotion understanding outlined the modulation but not the annihilation of emotional integration by synthetization.
C1 [Duville, Mathilde Marie; Alonso-Valerdi, Luz Maria; Ibarra-Zarate, David I. I.] Tecnol Monterrey, Escuela Ingn & Ciencias, Monterrey, NL, Mexico.
C3 Tecnologico de Monterrey
RP Duville, MM (corresponding author), Tecnol Monterrey, Escuela Ingn & Ciencias, Monterrey, NL, Mexico.
EM A00829725@tec.mx
OI Ibarra Zarate, David Isaac/0000-0002-9870-9645
CR Akçay MB, 2020, SPEECH COMMUN, V116, P56, DOI 10.1016/j.specom.2019.12.001
   Aldeneh Z, 2023, IEEE T AFFECT COMPUT, V14, P1351, DOI 10.1109/TAFFC.2021.3086050
   Amin N, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0061417
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Boersma P., 2020, Praat: Doing phonetics by computer: Version 6.1.28
   Boersma P, 1993, P I PHONETIC SCI, V17, P97, DOI DOI 10.1371/JOURNAL.PONE.0069107
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Brück C, 2011, NEUROIMAGE, V58, P259, DOI 10.1016/j.neuroimage.2011.06.005
   Chang CY, 2020, IEEE T BIO-MED ENG, V67, P1114, DOI 10.1109/TBME.2019.2930186
   Chou LC, 2020, COGN AFFECT BEHAV NE, V20, P1294, DOI 10.3758/s13415-020-00835-z
   Delorme A, 2007, NEUROIMAGE, V34, P1443, DOI 10.1016/j.neuroimage.2006.11.004
   DiIeva A, 2016, SPR SER COMPUT NEURO, P1, DOI 10.1007/978-1-4939-3995-4
   Dong L, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00601
   Duville MM, 2021, IEEE ENG MED BIO, P1644, DOI 10.1109/EMBC46164.2021.9629934
   Duville MM, 2021, DATA, V6, DOI 10.3390/data6120130
   Elmer S, 2021, NEUROIMAGE, V235, DOI 10.1016/j.neuroimage.2021.118051
   Gao XQ, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00026
   Gatti E, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.120
   Gervain J, 2019, TRENDS NEUROSCI, V42, P56, DOI 10.1016/j.tins.2018.09.004
   Gervain J, 2016, NEUROIMAGE, V133, P144, DOI 10.1016/j.neuroimage.2016.03.001
   Goldman JP, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P3240
   Gutiérrez-Palma N, 2016, LEARN INDIVID DIFFER, V45, P144, DOI 10.1016/j.lindif.2015.11.026
   Hardy TLD, 2020, J VOICE, V34, DOI 10.1016/j.jvoice.2018.10.002
   Herbert C, 2011, SOC NEUROSCI-UK, V6, P277, DOI 10.1080/17470919.2010.523543
   Huang KL, 2021, FRONT PSYCHOL, V12, DOI 10.3389/fpsyg.2021.664925
   Iseli M, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P669
   James J, 2018, IEEE ROMAN, P632, DOI 10.1109/ROMAN.2018.8525652
   Kappenman E.S., 2011, The Oxford Handbook of Event-Related Potential Components, DOI DOI 10.1093/OXFORDHB/9780195374148.001.0001
   Kotz SA, 2007, BRAIN RES, V1151, P107, DOI 10.1016/j.brainres.2007.03.015
   Kranzbuhler AM, 2020, J ACAD MARKET SCI, V48, P478, DOI 10.1007/s11747-019-00707-0
   Ku LC, 2020, COGN AFFECT BEHAV NE, V20, P371, DOI 10.3758/s13415-020-00774-9
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   Liu R, 2021, INTERSPEECH, P4648, DOI 10.21437/Interspeech.2021-1236
   Liu ZT, 2018, NEUROCOMPUTING, V273, P271, DOI 10.1016/j.neucom.2017.07.050
   Mariooryad S, 2014, SPEECH COMMUN, V57, P1, DOI 10.1016/j.specom.2013.07.011
   Mauchand M, 2021, COGN AFFECT BEHAV NE, V21, P74, DOI 10.3758/s13415-020-00849-7
   McDonald J. H., 2014, Handbook of biological statistics, V3rd
   Moore B., 2007, SPRINGER HDB ACOUSTI, DOI 10.1007/978-0-387-30425-0_13
   MOULINES E, 1990, SPEECH COMMUN, V9, P453, DOI 10.1016/0167-6393(90)90021-Z
   Nash-Kille A, 2014, CLIN NEUROPHYSIOL, V125, P1459, DOI 10.1016/j.clinph.2013.11.017
   Ning YS, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9194050
   Oostenveld R, 2011, COMPUT INTEL NEUROSC, V2011, DOI 10.1155/2011/156869
   Paulmann S, 2008, BRAIN LANG, V105, P59, DOI 10.1016/j.bandl.2007.11.005
   Paulmann S, 2017, PSYCHOPHYSIOLOGY, V54, P555, DOI 10.1111/psyp.12812
   Paulmann S, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00345
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   Pell MD, 2015, BIOL PSYCHOL, V111, P14, DOI 10.1016/j.biopsycho.2015.08.008
   Pell MD, 2021, EMOT REV, V13, P51, DOI 10.1177/1754073920954288
   Pereira DR, 2021, COGN AFFECT BEHAV NE, V21, P172, DOI 10.3758/s13415-020-00858-6
   PERRIN F, 1989, ELECTROEN CLIN NEURO, V72, P184, DOI 10.1016/0013-4694(89)90180-6
   Pinheiro AP, 2015, BRAIN LANG, V140, P24, DOI 10.1016/j.bandl.2014.10.009
   Reddy VR, 2016, NEUROCOMPUTING, V171, P1323, DOI 10.1016/j.neucom.2015.07.053
   Renard Y, 2010, PRESENCE-VIRTUAL AUG, V19, P35, DOI 10.1162/pres.19.1.35
   Rodero E, 2021, NEW MEDIA SOC, DOI 10.1177/14614448211024142
   Ruiz-Padial E, 2018, BIOL PSYCHOL, V137, P42, DOI 10.1016/j.biopsycho.2018.06.008
   Schirmer A, 2006, TRENDS COGN SCI, V10, P24, DOI 10.1016/j.tics.2005.11.009
   Schirmer A, 2013, COGN AFFECT BEHAV NE, V13, P80, DOI 10.3758/s13415-012-0132-8
   Schuller DM, 2021, EMOT REV, V13, P44, DOI 10.1177/1754073919898526
   Schwa S, 2017, J ACOUST SOC AM, V142, P2419, DOI 10.1121/1.5008849
   Selvam V.S., 2022, COMPLETE HIGUCHI FRA
   Singh P, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107316
   Sorati M, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02562
   Steber S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-62761-x
   Striepe H, 2021, INT J SOC ROBOT, V13, P441, DOI 10.1007/s12369-019-00570-7
   Tamura Y, 2015, SCI REP-UK, V5, DOI 10.1038/srep08799
   Treder MS, 2016, NEUROIMAGE, V129, P279, DOI 10.1016/j.neuroimage.2016.01.019
   Viswanathan M, 2005, COMPUT SPEECH LANG, V19, P55, DOI 10.1016/j.csl.2003.12.001
   Vos RR, 2018, J VOICE, V32, pE126, DOI 10.1016/j.jvoice.2017.03.017
   Wang C, 2021, PSYCHOPHYSIOLOGY, V58, DOI 10.1111/psyp.13775
   Xue YW, 2018, SPEECH COMMUN, V102, P54, DOI 10.1016/j.specom.2018.06.006
   Yasoda K, 2020, SOFT COMPUT, V24, P16011, DOI 10.1007/s00500-020-04920-w
   Zhao GZ, 2018, FRONT BEHAV NEUROSCI, V12, DOI 10.3389/fnbeh.2018.00225
   Zhao TC, 2019, BRAIN LANG, V194, P77, DOI 10.1016/j.bandl.2019.05.002
   Zheng XW, 2021, INT J INTELL SYST, V36, P152, DOI 10.1002/int.22295
   Zhou SL, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00046
NR 75
TC 1
Z9 1
U1 3
U2 6
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
EI 1662-5188
J9 FRONT COMPUT NEUROSC
JI Front. Comput. Neurosci.
PD NOV 18
PY 2022
VL 16
AR 1022787
DI 10.3389/fncom.2022.1022787
PG 22
WC Mathematical & Computational Biology; Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Mathematical & Computational Biology; Neurosciences & Neurology
GA 6S6IX
UT WOS:000893089900001
PM 36465969
OA gold, Green Published
DA 2024-01-09
ER

PT J
AU Abdulrahman, A
   Richards, D
AF Abdulrahman, Amal
   Richards, Deborah
TI Is Natural Necessary? Human Voice versus Synthetic Voice for Intelligent
   Virtual Agents
SO MULTIMODAL TECHNOLOGIES AND INTERACTION
LA English
DT Article
DE embodied conversational agent; stress management; voice; text-to-speech;
   co-presence; trust; working alliance
ID SOCIAL PRESENCE; ANTHROPOMORPHISM; USERS; CONSISTENCY; VALIDATION;
   MODEL; TRUST; SENSE; FACE
AB The use of intelligent virtual agents (IVA) to support humans in social contexts will depend on their social acceptability. Acceptance will be related to the human's perception of the IVAs as well as the IVAs' ability to respond and adapt their conversation appropriately to the human. Adaptation implies computer-generated speech (synthetic speech), such as text-to-speech (TTS). In this paper, we present the results of a study to investigate the effect of voice type (human voice vs. synthetic voice) on two aspects: (1) the IVA's likeability and voice impression in the light of co-presence, and (2) the interaction outcome, including human-agent trust and behavior change intention. The experiment included 118 participants who interacted with either the virtual advisor with TTS or the virtual advisor with human voice to gain tips for reducing their study stress. Participants in this study found the voice of the virtual advisor with TTS to be more eerie, but they rated both agents, with recorded voice and with TTS, similarly in terms of likeability. They further showed a similar attitude towards both agents in terms of co-presence and building trust. These results challenge previous studies that favor human voice over TTS, and suggest that even if human voice is preferred, TTS can deliver equivalent benefits.
C1 [Abdulrahman, Amal; Richards, Deborah] Macquarie Univ, Sch Comp, 4 Res Pk Dr, Macquarie Pk, NSW 2113, Australia.
C3 Macquarie University
RP Richards, D (corresponding author), Macquarie Univ, Sch Comp, 4 Res Pk Dr, Macquarie Pk, NSW 2113, Australia.
EM amal.abdulrahman@students.mq.edu.au; deborah.richards@mq.edu.au
OI Abdulrahman, Amal/0000-0001-5360-0833; Richards,
   Deborah/0000-0002-7363-1511
FU International Macquarie University Research Training Program (iMQRTP)
   [2015113]
FX This research was funded by an International Macquarie University
   Research Training Program (iMQRTP) scholarship-No. 2015113.
CR Abdulrahman A., 2021, P 20 INT C AUTONOMOU, P10
   Abdulrahman A., 2019, P 40 INT C INF SYST, P1
   Abdulrahman A, 2022, AUTON AGENT MULTI-AG, V36, DOI 10.1007/s10458-022-09553-x
   Abdulrahman A, 2021, J MULTIMODAL USER IN, V15, P189, DOI 10.1007/s12193-020-00359-3
   Aljameel SS, 2017, IEEE INT CONF COMP, P24, DOI 10.1109/CIVEMSA.2017.7995296
   Barcelos RH, 2018, J INTERACT MARK, V41, P60, DOI 10.1016/j.intmar.2017.10.001
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   Black A.W., 2000, LTD DOMAIN SYNTHESIS
   Blascovich J, 2002, PSYCHOL INQ, V13, P103, DOI 10.1207/S15327965PLI1302_01
   Brenton H., 2005, P C HUM COMP INT WOR
   Cambre J, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376789
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Ciechanowski L, 2019, FUTURE GENER COMP SY, V92, P539, DOI 10.1016/j.future.2018.01.055
   Clore GL, 2013, EMOT REV, V5, P335, DOI 10.1177/1754073913489751
   Cowan BR, 2015, INT J HUM-COMPUT ST, V83, P27, DOI 10.1016/j.ijhcs.2015.05.008
   de Visser EJ, 2016, J EXP PSYCHOL-APPL, V22, P331, DOI 10.1037/xap0000092
   Dickerson R, 2006, STUD HEALTH TECHNOL, V119, P114
   Diederich S, 2019, Online
   Georgila K, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P3519
   Goffman E, 1978, PRESENTATION SELF EV
   Gong L, 2003, P HUMAN COMPUTER INT, P160
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   GREEN EJ, 1981, PERCEPT PSYCHOPHYS, V30, P459, DOI 10.3758/BF03204842
   Hatcher RL, 2006, PSYCHOTHER RES, V16, P12, DOI 10.1080/10503300500352500
   Higgins D, 2022, COMPUT GRAPH-UK, V104, P116, DOI 10.1016/j.cag.2022.03.009
   Ho CC, 2010, COMPUT HUM BEHAV, V26, P1508, DOI 10.1016/j.chb.2010.05.015
   HORVATH AO, 1989, J COUNS PSYCHOL, V36, P223, DOI 10.1037/0022-0167.36.2.223
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   Jia H., 2013, P CHI 13 HUM FACT CO, DOI DOI 10.1145/2468356.2468649
   Kang H, 2020, INT J HUM-COMPUT ST, V133, P45, DOI 10.1016/j.ijhcs.2019.09.002
   Kim S, 2019, IEEE WCNC, P1, DOI [DOI 10.1080/09593985.2019.1566940, DOI 10.1109/wcnc.2019.8885834]
   Lee EJ, 2010, COMPUT HUM BEHAV, V26, P665, DOI 10.1016/j.chb.2010.01.003
   Li M., 2021, Proceedings of the 54th Hawaii International Conference on System Sciences, P4053, DOI [https://doi.org/10.24251/HICSS.2021.493, DOI 10.24251/HICSS.2021.493]
   MacDorman KF, 2006, INTERACT STUD, V7, P297, DOI 10.1075/is.7.3.03mac
   Mascarenhas S, 2022, ACM T INTERACT INTEL, V12, DOI 10.1145/3510822
   Mayer RC, 1999, J APPL PSYCHOL, V84, P123, DOI 10.1037/0021-9010.84.1.123
   McNaughton H, 2021, CLIN REHABIL, V35, P1021, DOI 10.1177/0269215521993648
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Moore RK, 2012, SCI REP-UK, V2, DOI 10.1038/srep00864
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   NASS C, 1993, HUM COMMUN RES, V19, P504, DOI 10.1111/j.1468-2958.1993.tb00311.x
   Nelekar S, 2022, BRIT J EDUC TECHNOL, V53, P491, DOI 10.1111/bjet.13174
   Ning YS, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9194050
   Noah Ben, 2021, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, P1448, DOI 10.1177/1071181321651128
   Nowak K, 2001, PRES 2001 C PHIL PA, DOI 10.1.1.19.5482
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Oh CS, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00114
   Picard Rosalind W, 2000, Affective Computing
   Pitardi V, 2021, PSYCHOL MARKET, V38, P626, DOI 10.1002/mar.21457
   Provoost S, 2017, J MED INTERNET RES, V19, DOI 10.2196/jmir.6553
   Ranjbartabar H, 2020, MULTIMODAL TECHNOLOG, V4, DOI 10.3390/mti4030055
   Reeves B., 1998, MEDIA EQUATION PEOPL
   Richards D, 2018, IEEE J BIOMED HEALTH, V22, P1699, DOI 10.1109/JBHI.2017.2782210
   Rothstein N., 2020, P AHFE 2020 VIRTUAL, V28, P190, DOI [10.1007/978-3-030-51041-1_26, DOI 10.1007/978-3-030-51041-1_26]
   Schmitt A., 2021, ECIS 2021 RES PAPERS
   Schultze U, 2019, INFORM SYST J, V29, P707, DOI 10.1111/isj.12230
   Seaborn K, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3386867
   Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779, DOI 10.1109/ICASSP.2018.8461368
   Sisman B, 2018, IEEE W SP LANG TECH, P282, DOI 10.1109/SLT.2018.8639507
   Smith Barry, 1988, FDN GESTALT THEORY
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   ter Stal S, 2020, JMIR HUM FACTORS, V7, DOI 10.2196/19987
   Torre I, 2020, IEEE ROMAN, P215, DOI 10.1109/RO-MAN47096.2020.9223449
   Vaidyam AN, 2019, CAN J PSYCHIAT, V64, P456, DOI 10.1177/0706743719828977
   Van Pinxteren MME, 2020, J SERV MANAGE, V31, P203, DOI 10.1108/JOSM-06-2019-0175
   Wagner P., 2019, P 10 SPEECH SYNTHESI
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Xie Y, 2020, J RETAIL CONSUM SERV, V55, DOI 10.1016/j.jretconser.2020.102119
   Yuan X, 2005, COMPUT ANIMAT VIRT W, V16, P109, DOI 10.1002/cav.65
   Zanbaka C., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1153
NR 71
TC 4
Z9 4
U1 1
U2 15
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2414-4088
J9 MULTIMODAL TECHNOLOG
JI Multimodal Technol. Interaction
PD JUL
PY 2022
VL 6
IS 7
AR 51
DI 10.3390/mti6070051
PG 17
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Information Systems
WE Emerging Sources Citation Index (ESCI)
SC Computer Science
GA 3L0EF
UT WOS:000834443500001
OA gold
DA 2024-01-09
ER

PT J
AU Schreibelmayr, S
   Mara, M
AF Schreibelmayr, Simon
   Mara, Martina
TI Robot Voices in Daily Life: Vocal Human-Likeness and Application Context
   as Determinants of User Acceptance
SO FRONTIERS IN PSYCHOLOGY
LA English
DT Article
DE speech interface; voice assistant; human-robot interaction; synthetic
   voice; anthropomorphism; uncanny valley; application context; user
   acceptance
ID UNCANNY VALLEY; INDIVIDUAL-DIFFERENCES; SYNTHESIZED SPEECH; SOCIAL
   ROBOTS; PERSONALITY; ANTHROPOMORPHISM; RESPONSES; EERINESS;
   METAANALYSIS; HABITUATION
AB The growing popularity of speech interfaces goes hand in hand with the creation of synthetic voices that sound ever more human. Previous research has been inconclusive about whether anthropomorphic design features of machines are more likely to be associated with positive user responses or, conversely, with uncanny experiences. To avoid detrimental effects of synthetic voice design, it is therefore crucial to explore what level of human realism human interactors prefer and whether their evaluations may vary across different domains of application. In a randomized laboratory experiment, 165 participants listened to one of five female-sounding robot voices, each with a different degree of human realism. We assessed how much participants anthropomorphized the voice (by subjective human-likeness ratings, a name-giving task and an imagination task), how pleasant and how eerie they found it, and to what extent they would accept its use in various domains. Additionally, participants completed Big Five personality measures and a tolerance of ambiguity scale. Our results indicate a positive relationship between human-likeness and user acceptance, with the most realistic sounding voice scoring highest in pleasantness and lowest in eeriness. Participants were also more likely to assign real human names to the voice (e.g., "Julia" instead of "T380") if it sounded more realistic. In terms of application context, participants overall indicated lower acceptance of the use of speech interfaces in social domains (care, companionship) than in others (e.g., information & navigation), though the most human-like voice was rated significantly more acceptable in social applications than the remaining four. While most personality factors did not prove influential, openness to experience was found to moderate the relationship between voice type and user acceptance such that individuals with higher openness scores rated the most human-like voice even more positively. Study results are discussed in the light of the presented theory and in relation to open research questions in the field of synthetic voice design.
C1 [Schreibelmayr, Simon; Mara, Martina] Johannes Kepler Univ Linz, LIT Robopsychol Lab, Linz, Austria.
C3 Johannes Kepler University Linz
RP Schreibelmayr, S (corresponding author), Johannes Kepler Univ Linz, LIT Robopsychol Lab, Linz, Austria.
EM simon.schreibelmayr@jku.at
CR Aaltonen I, 2017, ACMIEEE INT CONF HUM, P53, DOI 10.1145/3029798.3038362
   Amazon, 2017, AM POLL INTR NEW GER
   [Anonymous], 2012, EUR SPEC
   [Anonymous], 2019, ONL RES PLATF
   [Anonymous], 2019, ADOBE AUDITION
   Anthony LM, 2000, COMPUT HUM BEHAV, V16, P31, DOI 10.1016/S0747-5632(99)00050-3
   Appel M, 2016, ACMIEEE INT CONF HUM, P411, DOI 10.1109/HRI.2016.7451781
   Atkinson RK, 2005, CONTEMP EDUC PSYCHOL, V30, P117, DOI 10.1016/j.cedpsych.2004.07.001
   Audacity, 2019, FREE OP SOURC CROSS
   Audiveris, 2019, TOOL AUD LOUDN
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Baltes-Gotz B., 2017, MEDIATOR MODERATORAN
   Bartneck C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P367
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   Bendel O., 2021, SOZIALE ROBOTER, DOI [10.1007/978-3-658-31114-8, DOI 10.1007/978-3-658-31114-8_1]
   Blut M, 2021, J ACAD MARKET SCI, V49, P632, DOI 10.1007/s11747-020-00762-y
   BOCHNER S, 1965, PSYCHOL REC, V15, P393, DOI 10.1007/BF03393605
   Brédart S, 2021, ADV COGN PSYCHOL, V17, P33, DOI 10.5709/acp-0314-1
   Broadbent E., 2011, 25 C ARTIFICIAL INTE
   Burleigh TJ, 2013, COMPUT HUM BEHAV, V29, P759, DOI 10.1016/j.chb.2012.11.021
   Carpenter J., 2019, Interactions, V26, P56
   Carpinella CM, 2017, ACMIEEE INT CONF HUM, P254, DOI 10.1145/2909824.3020208
   Chang M, 2020, IEEE T NEUR SYS REH, V28, P2805, DOI 10.1109/TNSRE.2020.3038175
   Charness N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02589
   COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155
   Cohn M, 2020, INTERSPEECH, P1733, DOI 10.21437/Interspeech.2020-1336
   Costa P. T., 1985, The NEO Personality Inventory manual
   Couper M. P., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P412, DOI 10.1145/365024.365306
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Davison A.C., 1997, BOOTSTRAP METHODS TH, DOI [DOI 10.1017/CBO9780511802843, 10.1017/CBO9780511802843]
   de Graaf MMA, 2015, LECT NOTES ARTIF INT, V9388, P184, DOI 10.1007/978-3-319-25554-5_19
   Devaraj S, 2008, INFORM SYST RES, V19, P93, DOI 10.1287/isre.1070.0153
   Diel A, 2021, J VISION, V21, DOI 10.1167/jov.21.4.1
   DIGMAN JM, 1990, ANNU REV PSYCHOL, V41, P417, DOI 10.1146/annurev.ps.41.020190.002221
   Douven I, 2018, PSYCHON B REV, V25, P1203, DOI 10.3758/s13423-017-1344-2
   Duffy BR, 2003, ROBOT AUTON SYST, V42, P177, DOI 10.1016/S0921-8890(02)00374-3
   EAGLY AH, 1982, J PERS SOC PSYCHOL, V43, P915, DOI 10.1037/0022-3514.43.5.915
   Elkins AC, 2013, GROUP DECIS NEGOT, V22, P897, DOI 10.1007/s10726-012-9339-x
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   European Commission, 2019, Ethics Guidelines for Trustworthy AL
   Eyssel F, 2012, ACMIEEE INT CONF HUM, P125
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   FESTINGER L, 1962, SCI AM, V207, P93, DOI 10.1038/scientificamerican1062-93
   Fink J, 2012, 2012 IEEE WORKSHOP ON ADVANCED ROBOTICS AND ITS SOCIAL IMPACTS (ARSO), P54, DOI 10.1109/ARSO.2012.6213399
   FREESTON MH, 1994, PERS INDIV DIFFER, V17, P791, DOI 10.1016/0191-8869(94)90048-5
   Furnham A, 1995, CURR PSYCHOL, V14, P179, DOI 10.1007/BF02686907
   Gambino A, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312870
   Gaudiello I, 2016, COMPUT HUM BEHAV, V61, P633, DOI 10.1016/j.chb.2016.03.057
   Giles H., 1979, Social Markers in Speech. Ed. by, P343
   Goetz J, 2003, RO-MAN 2003: 12TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P55
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   Google Duplex, 2018, AI ASS CALLS LOC BUS
   Hayes AF, 2007, BEHAV RES METHODS, V39, P709, DOI 10.3758/BF03192961
   Hedda, 2019, MICR SPEECH PLATF
   Hentschel T, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00011
   Ho CC, 2017, INT J SOC ROBOT, V9, P129, DOI 10.1007/s12369-016-0380-9
   Ho CC, 2010, COMPUT HUM BEHAV, V26, P1508, DOI 10.1016/j.chb.2010.05.015
   HOPE ACA, 1968, J ROY STAT SOC B, V30, P582
   Ilves M, 2013, BEHAV INFORM TECHNOL, V32, P117, DOI 10.1080/0144929X.2012.702285
   Imhof M., 2010, BERUFLICHE BILDUNG F, P15
   Jia JW, 2021, IND MANAGE DATA SYST, V121, P1457, DOI 10.1108/IMDS-11-2020-0664
   John OP., 1991, BIG 5 INVENTORY VERS
   Jung Y., 2018, 22 BIENN C INT TEL S
   KAPLAN PS, 1995, DEV PSYCHOBIOL, V28, P45, DOI 10.1002/dev.420280105
   Kaur Ravneet, 2020, Smart Systems and IoT: Innovations in Computing. Proceeding of SSIC 2019. Smart Innovation, Systems and Technologies (SIST 141), P401, DOI 10.1007/978-981-13-8406-6_38
   Kiesler S., 2002, CHI 02 EXTENDED ABST, P576, DOI DOI 10.1145/506443.506491
   Kohlberg L., 1987, Child psychology and childhood education: A cognitive-developmental view
   Krauss RM, 2002, J EXP SOC PSYCHOL, V38, P618, DOI 10.1016/S0022-1031(02)00510-3
   Kühne K, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.593732
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lischetzke T, 2017, J RES PERS, V68, P96, DOI 10.1016/j.jrp.2017.02.001
   Lopatovska I, 2019, J LIBR INF SCI, V51, P984, DOI 10.1177/0961000618759414
   MacDorman KF, 2015, INTERACT STUD, V16, P141, DOI 10.1075/is.16.2.01mac
   Mara M, 2022, Z PSYCHOL, V230, P33, DOI 10.1027/2151-2604/a000486
   Mara M, 2020, ACMIEEE INT CONF HUM, P355, DOI 10.1145/3371382.3378285
   Mara M, 2015, COMPUT HUM BEHAV, V48, P156, DOI 10.1016/j.chb.2015.01.007
   Mara M, 2015, COMPUT HUM BEHAV, V44, P326, DOI 10.1016/j.chb.2014.09.025
   Maricutoiu LP, 2014, PROCD SOC BEHV, V127, P311, DOI 10.1016/j.sbspro.2014.03.262
   Marikyan D, 2023, INFORM SYST FRONT, V25, P1101, DOI 10.1007/s10796-020-10042-3
   Mathur MB, 2016, COGNITION, V146, P22, DOI 10.1016/j.cognition.2015.09.008
   Mayer RE, 2003, J EDUC PSYCHOL, V95, P419, DOI 10.1037/0022-0663.95.2.419
   McGee TJ, 2001, PSYCHOPHYSIOLOGY, V38, P653, DOI 10.1017/S0048577201990973
   Meah LFS, 2014, LECT NOTES ARTIF INT, V8755, P256, DOI 10.1007/978-3-319-11973-1_26
   Meinecke C., 2019, DELOITTE READER 2022
   Mejia C, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7121316
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Morsunbul U., 2019, J. Hum. Sci, V16, P499
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   NORTON RW, 1975, J PERS ASSESS, V39, P607, DOI 10.1207/s15327752jpa3906_11
   Nov O., 2008, P 41 ANN HAWAII INT, P448, DOI DOI 10.1109/HICSS.2008.348
   Oshio A, 2009, SOC BEHAV PERSONAL, V37, P729, DOI 10.2224/sbp.2009.37.6.729
   Oyedele A, 2007, CYBERPSYCHOL BEHAV, V10, P624, DOI 10.1089/cpb.2007.9977
   Pérula-Martínez R, 2017, ACMIEEE INT CONF HUM, P259, DOI 10.1145/3029798.3038434
   Pinker S, 2003, LANGUAGE INSTINCT MI
   Polly, 2019, PHYLOGENETICS MATH V
   Process, 2019, SPSS MACR
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Questback, 2018, FEEDB PLATTF UNT FUN
   Radant M., 2003, 7 DPPD TAG DTSCH GES
   Reeves B., 1998, MEDIA EQUATION PEOPL
   Robinson MD, 2003, PSYCHOLOGY OF EVALUATION, P275
   Robinson MD, 2004, CURR DIR PSYCHOL SCI, V13, P127, DOI 10.1111/j.0963-7214.2004.00290.x
   Roesler E, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abj5425
   Roesler E, 2022, INT J SOC ROBOT, V14, P1155, DOI 10.1007/s12369-021-00860-z
   Romportl Jan, 2014, Text, Speech and Dialogue. 17th International Conference, TSD 2014. Proceedings: LNCS 8655, P595, DOI 10.1007/978-3-319-10816-2_72
   Schlink S, 2007, Z SOZIALPSYCHOL, V38, P153, DOI 10.1024/0044-3514.38.3.153
   Schupp J., 2014, Zusammenstellung sozialwissenschaftlicher Items und Skalen, DOI [10.6102/zis54, DOI 10.6102/ZIS54]
   Seaborn K., 2021, 2021 CHI C HUMAN FAC, P1, DOI DOI 10.1145/3411763.3451712
   Shao J., 1995, The Jackknife and Bootstrap. Springer Series in Statistics, P23, DOI DOI 10.1007/978-1-4612-0795-5
   Smith HMJ, 2016, EVOL PSYCHOL-US, V14, DOI 10.1177/1474704916630317
   Sporer SL, 2006, APPL COGNITIVE PSYCH, V20, P421, DOI 10.1002/acp.1190
   Sprent P, 2007, J ROY STAT SOC A STA, V170, P1178, DOI 10.1111/j.1467-985X.2007.00506_2.x
   Statista, 2021, ONL RES PLATF
   Statistical CJ., 1992, Curr. Dir. Psychol. Sci, V1, P98, DOI [10.1111/1467-8721.ep10768783, DOI 10.1111/1467-8721.EP10768783]
   Sutton SJ, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300833
   Tiwari Manjul, 2012, J Nat Sci Biol Med, V3, P3, DOI 10.4103/0976-9668.95933
   Torre I, 2018, PROCEEDINGS OF THE TECHNOLOGY, MIND, AND SOCIETY CONFERENCE (TECHMINDSOCIETY'18), DOI 10.1145/3183654.3183691
   Torre Ilaria, 2015, ICPHS
   Tourangeau R, 2003, COMPUT HUM BEHAV, V19, P1, DOI 10.1016/S0747-5632(02)00032-8
   Ullman D, 2021, 2021 16TH ACM/IEEE INTERNATIONAL CONFERENCE ON HUMAN-ROBOT INTERACTION, HRI, P110, DOI 10.1145/3434073.3444652
   van den Oord A., 2016, WAVENET GEN MODEL RA, P125
   Vlachos E, 2016, INTERACT STUD, V17, P390, DOI 10.1075/is.17.3.04vla
   Voxal, 2019, VOIC CHANG
   Wada K, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2847
   Wada K, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P4940, DOI 10.1109/IROS.2006.282455
   Waytz A, 2010, PERSPECT PSYCHOL SCI, V5, P219, DOI 10.1177/1745691610369336
   West Mark, 2019, I'd blush if I could: closing gender divides in digital skills through education
   Whang C, 2021, PSYCHOL MARKET, V38, P581, DOI 10.1002/mar.21437
   Yang H., 2021, Proceedings of the 2021 CHI conference on human factors in computing systems, P1
   Zhang TR, 2020, TRANSPORT RES C-EMER, V112, P220, DOI 10.1016/j.trc.2020.01.027
NR 136
TC 8
Z9 8
U1 24
U2 70
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 1664-1078
J9 FRONT PSYCHOL
JI Front. Psychol.
PD MAY 13
PY 2022
VL 13
AR 787499
DI 10.3389/fpsyg.2022.787499
PG 17
WC Psychology, Multidisciplinary
WE Social Science Citation Index (SSCI)
SC Psychology
GA 1N4BQ
UT WOS:000800602900001
PM 35645911
OA gold, Green Published
DA 2024-01-09
ER

PT J
AU Higgins, D
   Zibrek, K
   Cabral, J
   Egan, D
   McDonnell, R
AF Higgins, Darragh
   Zibrek, Katja
   Cabral, Joao
   Egan, Donal
   McDonnell, Rachel
TI Sympathy for the digital: Influence of synthetic voice on affinity,
   social presence and empathy for photorealistic virtual humans
SO COMPUTERS & GRAPHICS-UK
LA English
DT Article
DE Virtual humans; Perception; Synthetic voice; Virtual reality
ID UNCANNY VALLEY; HUMAN REALISM; FACE; CONSISTENCY; APPEARANCE; AGENT
AB In this paper, we investigate the effect of a realism mismatch in the voice and appearance of a photorealistic virtual character in both immersive and screen-mediated virtual contexts. While many studies have investigated voice attributes for robots, not much is known about the effect voice naturalness has on the perception of realistic virtual characters. We conducted the first experiment in Virtual Reality (VR) with over two hundred participants investigating the mismatch between realistic appearance and unrealistic voice on the feeling of presence, and the emotional response of the user to the character expressing a strong negative emotion. We predicted that the mismatched voice would lower social presence and cause users to have a negative emotional reaction and feelings of discomfort towards the character. We found that the concern for the virtual character was indeed altered by the unnatural voice, though interestingly it did not affect social presence. The second experiment was conducted with a view towards heightening the appearance realism of the same character for the same scenarios, with an additional lower level of voice realism employed to strengthen the mismatch of perceptual cues. While voice type did not appear to impact reports of empathic responses towards the character, there was an observed effect of voice realism on reported social presence, which was not detected in the first study. There were also significant results on affinity and voice trait measurements that provide evidence in support of perceptual mismatch theories of the Uncanny Valley. (c) 2022 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
C1 [Higgins, Darragh; Cabral, Joao; Egan, Donal; McDonnell, Rachel] Trinity Coll Dublin, Dublin, Ireland.
   [Zibrek, Katja] INRIA Rennes, Rennes, France.
C3 Trinity College Dublin; Universite de Rennes
RP McDonnell, R (corresponding author), Trinity Coll Dublin, Dublin, Ireland.
EM HIGGIND3@tcd.ie; katja.zibrek@inria.fr; CABRALJ@tcd.ie; doegan@tcd.ie;
   ramcdonn@tcd.ie
RI McDonnell, Rachel/HGC-4337-2022; Zibrek, Katja/JQW-2981-2023
OI McDonnell, Rachel/0000-0002-1957-2506; Zibrek,
   Katja/0000-0002-0204-3472; Cabral, Joao Paulo/0000-0002-9298-1787; Egan,
   Donal/0000-0001-6491-9575
FU Sci-ence Foundation Ireland Centre for Research Training in
   Digitally-Enhanced Reality (d-real) [18/CRT/6224]; ADAPT Centre for
   Digital Content Technology [13/RC/2106_P2, 19/FFP/6409]
FX Acknowledgment This work was conducted with the financial support of the
   Sci-ence Foundation Ireland Centre for Research Training in
   Digitally-Enhanced Reality (d-real) under Grant No. 18/CRT/6224 and
   un-der the ADAPT Centre for Digital Content Technology (Grant No.
   13/RC/2106_P2) and RADICal (Grant No. 19/FFP/6409) .
CR [Anonymous], 2001, CRITERIA SCOPE CONDI
   Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2003, PERS SOC PSYCHOL B, V29, P819, DOI 10.1177/0146167203029007002
   Baxter M., 2021, INVESTIGATING IMPACT, DOI [10.1145/3469595.3469609, DOI 10.1145/3469595.3469609]
   Biocca F., 2001, 4 ANN INT WORKSH PRE, P1
   Bouchard S, 2013, CYBERPSYCH BEH SOC N, V16, P61, DOI 10.1089/cyber.2012.1571
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Ciechanowski L, 2019, FUTURE GENER COMP SY, V92, P539, DOI 10.1016/j.future.2018.01.055
   DAVIS MH, 1983, J PERS SOC PSYCHOL, V44, P113, DOI 10.1037/0022-3514.44.1.113
   Devesse A, 2018, INT J AUDIOL, V57, P908, DOI 10.1080/14992027.2018.1511922
   Ferstl Y, 2021, PROCEEDINGS OF THE 21ST ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA), P76, DOI 10.1145/3472306.3478338
   Go E, 2019, COMPUT HUM BEHAV, V97, P304, DOI 10.1016/j.chb.2019.01.020
   Golan O, 2006, J AUTISM DEV DISORD, V36, P169, DOI 10.1007/s10803-005-0057-y
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   Hall E. T., 1966, HIDDEN DIMENSION, V609
   Hartmann T, 2010, MEDIA PSYCHOL, V13, P339, DOI 10.1080/15213269.2010.524912
   Higgins D, REMOTELY PERCEIVED I, DOI [10.3389/frvir.2021.668499, DOI 10.3389/FRVIR.2021.668499]
   Ho JCF, 2022, BEHAV INFORM TECHNOL, V41, P1185, DOI 10.1080/0144929X.2020.1864018
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Kawahara H, 2013, ASIAPAC SIGN INFO PR
   Kawahara H, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P38
   Lee KM., 2003, DESIGNING SOCIAL PRE
   MacDorman KF, 2016, COGNITION, V146, P190, DOI 10.1016/j.cognition.2015.09.019
   Macdorman KF, 2008, TOO REAL COMFORT UNC
   Maloney D., 2020, PROC ACM HUM COMPUT, V4, P175, DOI DOI 10.1145/3415246
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Meah LFS, 2014, LECT NOTES ARTIF INT, V8755, P256, DOI 10.1007/978-3-319-11973-1_26
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Moore RK., 2016, IS SPOKEN LANGUAGE A
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Oh CS, 2018, FRONT ROBOT AI, V5, DOI 10.3389/frobt.2018.00114
   Parmar D, 2020, NAVIGATING COMBINATO
   Potard B, 2016, LECT NOTES ARTIF INT, V10011, P190, DOI 10.1007/978-3-319-47665-0_17
   Sallnäs EL, 2010, LECT NOTES COMPUT SC, V6192, P178, DOI 10.1007/978-3-642-14075-4_26
   Sarigul B, 2020, ACMIEEE INT CONF HUM, P430, DOI 10.1145/3371382.3378302
   Saygin AP, 2012, SOC COGN AFFECT NEUR, V7, P413, DOI 10.1093/scan/nsr025
   Seyama J, 2007, PRESENCE-TELEOP VIRT, V16, P337, DOI 10.1162/pres.16.4.337
   Shen LJ, 2010, HUM COMMUN RES, V36, DOI 10.1111/j.1468-2958.2010.01381.x
   Skalski P, 2010, PSYCHNOLOGY J, V8, P67
   Skarbez R, 2017, IEEE T VIS COMPUT GR, V23, P1322, DOI 10.1109/TVCG.2017.2657158
   Slater M, 2009, PHILOS T R SOC B, V364, P3549, DOI 10.1098/rstb.2009.0138
   Thézé R, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-72375-y
   Torre I, HRI 2021 ROBO IDENTI
   Urgen BA, 2018, NEUROPSYCHOLOGIA, V114, P181, DOI 10.1016/j.neuropsychologia.2018.04.027
   van Loon A, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202442
   Volonte M, 2016, IEEE T VIS COMPUT GR, V22, P1326, DOI 10.1109/TVCG.2016.2518158
   Wang SS, 2015, REV GEN PSYCHOL, V19, P393, DOI 10.1037/gpr0000056
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zibrek K., 2019, Motion, interaction and games, P1, DOI DOI 10.1145/3359566.3360064
   Zibrek K, P MOTION INTERACTION
   Zibrek K, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3349609
   Zibrek K, 2017, ACM SYMPOSIUM ON APPLIED PERCEPTION (SAP 2017), DOI 10.1145/3119881.3119887
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 54
TC 12
Z9 12
U1 18
U2 60
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0097-8493
EI 1873-7684
J9 COMPUT GRAPH-UK
JI Comput. Graph.-UK
PD MAY
PY 2022
VL 104
BP 116
EP 128
DI 10.1016/j.cag.2022.03.009
EA APR 2022
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 1J6KP
UT WOS:000798026700004
OA Green Published, hybrid
DA 2024-01-09
ER

PT J
AU Lavado-Nalvaiz, N
   Lucia-Palacios, L
   Pérez-López, R
AF Lavado-Nalvaiz, Natalia
   Lucia-Palacios, Laura
   Perez-Lopez, Raul
TI The role of the humanisation of smart home speakers in the
   personalisation-privacy paradox
SO ELECTRONIC COMMERCE RESEARCH AND APPLICATIONS
LA English
DT Article
DE Smart home speakers; Privacy paradox; Personalisation; Humanisation;
   Uncanny valley
ID UNCANNY VALLEY; INFORMATION DISCLOSURE; CONSUMER ADOPTION; INTERNET
   USERS; E-COMMERCE; CALCULUS; SERVICE; WILLINGNESS; ACCEPTANCE; INTENTION
AB This article examines the personalisation-privacy paradox through the privacy calculus lens in the context of smart home speakers. It also considers the direct and moderating role of humanisation in the personalisation-privacy paradox. This characteristic refers to how human the device is perceived to be, given its voice's tone and pacing, original responses, sense of humour, and recommendations. The model was tested on a sample of 360 users of different brands of smart home speakers. These users were heterogeneous in terms of age, gender, income, and frequency of use of the device. The results confirm the personalisation-privacy paradox and verify uncanny valley theory, finding the U-shaped effect that humanisation has on risks of information disclosure. They also show that humanisation increases benefits, which supports the realism maximisation theory. Specifically, they reveal that users will perceive the messages received as more useful and credible if the devices seem human. However, the human-likeness of these devices should not exceed certain levels as it increases perceived risk. These results should be used to highlight the importance of the human-like communication of smart home speakers.
C1 [Lavado-Nalvaiz, Natalia; Lucia-Palacios, Laura; Perez-Lopez, Raul] Univ Zaragoza, Dept Mkt, Fac Econ & Business Adm, Gran Via 2, Zaragoza 50005, Spain.
C3 University of Zaragoza
RP Lavado-Nalvaiz, N (corresponding author), Univ Zaragoza, Dept Mkt, Fac Econ & Business Adm, Gran Via 2, Zaragoza 50005, Spain.
EM 651789@unizar.es; llucia@unizar.es; raperez@unizar.es
RI Palacios, Laura Lucia/K-8921-2017
OI Palacios, Laura Lucia/0000-0002-8798-3294; Perez Lopez,
   Raul/0000-0001-6441-2504
FU MCIN/AEI [PID2020-114874GB-I00]; Government of Aragon; European Social
   Fund (GENERES Group) [S-54_20R]
FX The authors wish to express their gratitude for financial support
   received from the Grant PID2020-114874GB-I00 funded by
   MCIN/AEI/10.13039/501100011033; the funding received from the Government
   of Aragon and the European Social Fund (GENERES Group S-54_20R) .
CR Acquisti A., 2004, P 5 ACM C EL COMM, P21, DOI DOI 10.1145/988772.988777
   Adapa S, 2020, J RETAIL CONSUM SERV, V52, DOI 10.1016/j.jretconser.2019.101901
   Aguirre E, 2016, J CONSUM MARK, V33, P98, DOI 10.1108/JCM-06-2015-1458
   Anic ID, 2019, ELECTRON COMMER R A, V36, DOI 10.1016/j.elerap.2019.100868
   [Anonymous], 1979, RELIABILITY VALIDITY, DOI DOI 10.4135/9781412985642
   Bagozzi R.P., 1988, Principles of Marketing Research, V16, P74, DOI [DOI 10.1007/BF02723327, 10.1007/bf02723327]
   Bandara R, 2020, J RETAIL CONSUM SERV, V52, DOI 10.1016/j.jretconser.2019.101947
   Bavaresco R, 2020, COMPUT SCI REV, V36, DOI 10.1016/j.cosrev.2020.100239
   Benlian A, 2020, INFORM SYST J, V30, P1010, DOI 10.1111/isj.12243
   Bhatia J, 2018, ACM T COMPUT-HUM INT, V25, DOI 10.1145/3267808
   Bhattacherjee A, 2001, DECIS SUPPORT SYST, V32, P201, DOI 10.1016/S0167-9236(01)00111-7
   Burleigh TJ, 2013, COMPUT HUM BEHAV, V29, P759, DOI 10.1016/j.chb.2012.11.021
   Cazier J.A., 2009, TECHNIQUES APPL ADV
   Cazier JA, 2008, COMMUN ASSOC INF SYS, V23, P235
   Cheetham M, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01738
   Chen SC, 2015, TECHNOL FORECAST SOC, V96, P40, DOI 10.1016/j.techfore.2014.11.011
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Culnan MJ, 1999, ORGAN SCI, V10, P104, DOI 10.1287/orsc.10.1.104
   Davenport T, 2020, J ACAD MARKET SCI, V48, P24, DOI 10.1007/s11747-019-00696-0
   Diederich S., 2020, P 28 EUR C INF SYST, P1
   Dinev T, 2006, INFORM SYST RES, V17, P61, DOI 10.1287/isre.1060.0080
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   Feldman R., 2008, P SERV SYST SERV MAN, P1
   Foehr J, 2020, J ASSOC CONSUM RES, V5, P181, DOI 10.1086/707731
   Forde M, 2018, Relig Cult Afr Afr D, P1, DOI 10.1007/s00779-018-1174-x
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Frick NRJ, 2021, ELECTRON COMMER R A, V47, DOI 10.1016/j.elerap.2021.101046
   Gartner, 2018, WHATS AHEAD SMART SP
   GEISSER S, 1974, BIOMETRIKA, V61, P101, DOI 10.1093/biomet/61.1.101
   Gironda JT, 2018, ELECTRON COMMER R A, V29, P64, DOI 10.1016/j.elerap.2018.03.007
   Go E, 2019, COMPUT HUM BEHAV, V97, P304, DOI 10.1016/j.chb.2019.01.020
   Groom V, 2009, INT J HUM-COMPUT ST, V67, P842, DOI 10.1016/j.ijhcs.2009.07.001
   Guzman A. L., 2018, NETWORKED SELF HUMAN, P83
   Hagen P, 1999, Smart Personalization-The Forrester Report
   Han S, 2018, IND MANAGE DATA SYST, V118, P618, DOI 10.1108/IMDS-05-2017-0214
   Hayes JL, 2021, J INTERACT MARK, V55, P16, DOI 10.1016/j.intmar.2021.01.001
   Henseler J, 2015, J ACAD MARKET SCI, V43, P115, DOI 10.1007/s11747-014-0403-8
   Holtrop N, 2017, INT J RES MARK, V34, P154, DOI 10.1016/j.ijresmar.2016.06.001
   Hong JC, 2017, COMPUT HUM BEHAV, V67, P264, DOI 10.1016/j.chb.2016.11.001
   Hsu CL, 2016, COMPUT HUM BEHAV, V62, P516, DOI 10.1016/j.chb.2016.04.023
   Hsu MH, 2014, INTERNET RES, V24, P332, DOI 10.1108/IntR-01-2013-0007
   Jai TM, 2016, J RETAIL CONSUM SERV, V28, P296, DOI 10.1016/j.jretconser.2015.01.005
   Kaplan A, 2020, BUS HORIZONS, V63, P37, DOI 10.1016/j.bushor.2019.09.003
   Keh HT, 2010, J MARKETING, V74, P55, DOI 10.1509/jmkg.74.2.55
   Kim D, 2019, COMPUT HUM BEHAV, V92, P273, DOI 10.1016/j.chb.2018.11.022
   Kim MS, 2018, COMPUT HUM BEHAV, V88, P143, DOI 10.1016/j.chb.2018.06.031
   Kim SY, 2017, SUSTAINABILITY-BASEL, V9, DOI 10.3390/su9122262
   Kim YJ, 2014, COMPUT HUM BEHAV, V33, P256, DOI 10.1016/j.chb.2014.01.015
   Klaus P, 2022, J RETAIL CONSUM SERV, V65, DOI 10.1016/j.jretconser.2021.102490
   Kowalczuk P, 2018, J RES INTERACT MARK, V12, P418, DOI 10.1108/JRIM-01-2018-0022
   Krafft M, 2017, J INTERACT MARK, V39, P39, DOI 10.1016/j.intmar.2017.03.001
   Lee AR, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su131910679
   Lee CH, 2011, TOURISM MANAGE, V32, P987, DOI 10.1016/j.tourman.2010.08.011
   Lee JM, 2016, COMPUT HUM BEHAV, V63, P453, DOI 10.1016/j.chb.2016.05.056
   Lee S, 2021, J BUS RES, V129, P455, DOI 10.1016/j.jbusres.2019.09.053
   Liyanaarachchi G, 2021, J RETAIL CONSUM SERV, V60, DOI 10.1016/j.jretconser.2021.102500
   López G, 2018, ADV INTELL SYST, V592, P241, DOI 10.1007/978-3-319-60366-7_23
   Lu L, 2019, INT J HOSP MANAG, V80, P36, DOI 10.1016/j.ijhm.2019.01.005
   Luo XM, 2019, MARKET SCI, V38, P937, DOI 10.1287/mksc.2019.1192
   MacDorman KF, 2019, COMPUT HUM BEHAV, V94, P140, DOI 10.1016/j.chb.2019.01.011
   Malhotra NK, 2004, INFORM SYST RES, V15, P336, DOI 10.1287/isre.1040.0032
   Manheim K., 2019, YALE J LAW TECHNOLOG, V21, P106
   Mani Z, 2019, J MARKET MANAG-UK, V35, P1460, DOI 10.1080/0267257X.2019.1667856
   Martin BAS, 2020, J HOSP TOUR MANAG, V44, P108, DOI 10.1016/j.jhtm.2020.06.004
   Maya BM, 2020, COMPUT HUM BEHAV, V103, P21, DOI 10.1016/j.chb.2019.08.029
   McLean G, 2019, COMPUT HUM BEHAV, V99, P28, DOI 10.1016/j.chb.2019.05.009
   Min S, 2019, J TRAVEL TOUR MARK, V36, P770, DOI 10.1080/10548408.2018.1507866
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Morosan C, 2015, INT J HOSP MANAG, V47, P120, DOI 10.1016/j.ijhm.2015.03.008
   Norberg PA, 2007, J CONSUM AFF, V41, P100, DOI 10.1111/j.1745-6606.2006.00070.x
   Nunnally J. C., 1978, Psychometric Methods, V2nd
   Pantano E, 2020, J RETAIL CONSUM SERV, V55, DOI 10.1016/j.jretconser.2020.102096
   Poushneh A, 2021, J RETAIL CONSUM SERV, V58, DOI 10.1016/j.jretconser.2020.102283
   Puzakova M, 2013, J MARKETING, V77, P81, DOI 10.1509/jm.11.0510
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Rosenthal-von der Pütten AM, 2014, COMPUT HUM BEHAV, V36, P422, DOI 10.1016/j.chb.2014.03.066
   Sharma S, 2014, ELECTRON COMMER R A, V13, P305, DOI 10.1016/j.elerap.2014.06.007
   Sheehan B, 2020, J BUS RES, V115, P14, DOI 10.1016/j.jbusres.2020.04.030
   Sheng H, 2008, J ASSOC INF SYST, V9, P344, DOI 10.17705/1jais.00161
   Stein JP, 2017, COGNITION, V160, P43, DOI 10.1016/j.cognition.2016.12.010
   STONE M, 1974, J R STAT SOC B, V36, P111, DOI 10.1111/j.2517-6161.1974.tb00994.x
   Strait MK, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01366
   Sun YQ, 2015, COMPUT HUM BEHAV, V52, P278, DOI 10.1016/j.chb.2015.06.006
   Sweeney JC, 1999, J RETAILING, V75, P77, DOI 10.1016/S0022-4359(99)80005-0
   Teng WC, 2010, INT J MOB COMMUN, V8, P1, DOI 10.1504/IJMC.2010.030517
   Turel O, 2010, INFORM MANAGE-AMSTER, V47, P53, DOI 10.1016/j.im.2009.10.002
   Wang T, 2016, INT J INFORM MANAGE, V36, P531, DOI 10.1016/j.ijinfomgt.2016.03.003
   Waytz A, 2010, J PERS SOC PSYCHOL, V99, P410, DOI 10.1037/a0020240
   Xie Y, 2020, J RETAIL CONSUM SERV, V55, DOI 10.1016/j.jretconser.2020.102119
   Xu H, 2011, J ASSOC INF SYST, V12, P798
   Yang H, 2016, TELEMAT INFORM, V33, P256, DOI 10.1016/j.tele.2015.08.007
   Yang SQ, 2012, COMPUT HUM BEHAV, V28, P129, DOI 10.1016/j.chb.2011.08.019
   Yee N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1
   Yu J, 2017, TELEMAT INFORM, V34, P206, DOI 10.1016/j.tele.2015.11.004
   Zeng FE, 2021, J BUS RES, V124, P667, DOI 10.1016/j.jbusres.2020.02.006
   Zhu H, 2017, INFORM MANAGE-AMSTER, V54, P427, DOI 10.1016/j.im.2016.10.001
   Zlotowski JA, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00883
NR 97
TC 8
Z9 8
U1 30
U2 75
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1567-4223
EI 1873-7846
J9 ELECTRON COMMER R A
JI Electron. Commer. Res. Appl.
PD MAY-JUN
PY 2022
VL 53
AR 101146
DI 10.1016/j.elerap.2022.101146
EA APR 2022
PG 11
WC Business; Computer Science, Information Systems; Computer Science,
   Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Business & Economics; Computer Science
GA 2R9TQ
UT WOS:000821446800007
OA Green Published, hybrid
DA 2024-01-09
ER

PT J
AU Parmar, D
   Olafsson, S
   Utami, D
   Murali, P
   Bickmore, T
AF Parmar, Dhaval
   Olafsson, Stefan
   Utami, Dina
   Murali, Prasanth
   Bickmore, Timothy
TI Designing empathic virtual agents: manipulating animation, voice,
   rendering, and empathy to create persuasive agents
SO AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS
LA English
DT Article
DE Virtual agents; Animation fidelity; Voice quality; Rendering style;
   Simulated empathy; Agent perception
ID HUMAN REALISM; FEEDBACK; PERSONALITY; CONSISTENCY; CHARACTERS
AB Designers of virtual agents have a combinatorically large space of choices for the look and behavior of their characters. We conducted two between-subjects studies to explore the systematic manipulation of animation quality, speech quality, rendering style, and simulated empathy, and its impact on perceptions of virtual agents in terms of naturalness, engagement, trust, credibility, and persuasion within a health counseling domain. In the first study, animation was varied between manually created, procedural, or no animations; voice quality was varied between recorded audio and synthetic speech; and rendering style was varied between realistic and toon-shaded. In the second study, simulated empathy of the agent was varied between no empathy, verbal-only empathic responses, and full empathy involving verbal, facial, and immediacy feedback. Results show that natural animations and recorded voice are more appropriate for the agent's general acceptance, trust, credibility, and appropriateness for the task. However, for a brief health counseling task, animation might actually be distracting from the persuasive message, with the highest levels of persuasion found when the amount of agent animation is minimized. Further, consistent and high levels of empathy improve agent perception but may interfere with forming a trusting bond with the agent.
C1 [Parmar, Dhaval; Olafsson, Stefan; Utami, Dina; Murali, Prasanth; Bickmore, Timothy] Northeastern Univ, Boston, MA 02115 USA.
   [Olafsson, Stefan] Reykjavik Univ, Reykjavik, Iceland.
C3 Northeastern University; Reykjavik University
RP Parmar, D (corresponding author), Northeastern Univ, Boston, MA 02115 USA.
EM d.parmar@northeastern.edu; olafsson.s@northeastern.edu;
   utami.d@northeastern.edu; murali.pr@northeastern.edu;
   t.bickmore@northeastern.edu
OI Olafsson, Stefan/0000-0002-3549-8705
FU US National Institutes of Health [R01NR016131]
FX This work was supported by the US National Institutes of Health Grant
   R01NR016131.
CR Adobe, 2020, AD CREAT MARK DOC MA
   Amazon, 2020, AM MECH TURK
   Bickmore T, 2007, P 2007 HUM FACT COMP, P2291, DOI [10.1145/1240866.1240996, DOI 10.1145/1240866.1240996]
   Bickmore T. W., 2005, ACM Transactions on Computer-Human Interaction, V12, P293, DOI 10.1145/1067860.1067867
   Bickmore TW, 2013, J AM GERIATR SOC, V61, P1676, DOI 10.1111/jgs.12449
   Bickmore TW, 2010, IEEE T AFFECT COMPUT, V1, P60, DOI 10.1109/T-AFFC.2010.4
   Bigi B, 2012, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON SPEECH PROSODY, VOLS I AND II, P19
   Cassell J, 2004, COG TECH, P163
   Cassell J, 1999, APPL ARTIF INTELL, V13, P519, DOI 10.1080/088395199117360
   Cassell Justine, 2000, Embodied conversational agents
   Cereproc, 2020, CER TEXT TO SPEECH
   Dai ZY, 2018, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.168
   Fogg B.J., 2001, CHI 20001 EXTENDED A, P295, DOI 10.1145/634067.634242
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   Kätsyri J, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00390
   Kim J, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3383075
   Kimani E, 2016, LECT NOTES ARTIF INT, V10011, P120, DOI 10.1007/978-3-319-47665-0_11
   Klein J, 2002, INTERACT COMPUT, V14, P119, DOI 10.1016/S0953-5438(01)00053-4
   Lane HC, 2013, J EDUC PSYCHOL, V105, P1026, DOI 10.1037/a0031506
   Lee A., 2009, Proceedings of the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), P131
   Li Gong, 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P158, DOI 10.1145/365024.365090
   MacDorman KF, 2016, COGNITION, V146, P190, DOI 10.1016/j.cognition.2015.09.019
   McDonnell R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185587
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Nass C., 1999, AUDITORY VISUAL SPEE, P1
   Nguyen H., 2009, P INT C ULTR TEL WOR, P7
   Parmar D., 2020, P 19 INT C AUT AG MU, P1010, DOI [10.5555/3398761.3398879, DOI 10.5555/3398761.3398879]
   Petty R.E., 2010, ADV SOCIAL PSYCHOL S, P217, DOI DOI 10.1016/B978-0-12-375000-6.00040-9
   Richmond VP., 1995, IMMEDIACY
   Ring L, 2014, LECT NOTES ARTIF INT, V8637, P374, DOI 10.1007/978-3-319-09767-1_49
   Shams L, 2010, PHYS LIFE REV, V7, P269, DOI 10.1016/j.plrev.2010.04.006
   Slote M., 2003, TIME ETHICS ESSAYS I, P179, DOI [10.1007/978-94-017-3530-8_12, DOI 10.1007/978-94-017-3530-8_12]
   Stern SE, 1999, HUM FACTORS, V41, P588, DOI 10.1518/001872099779656680
   Taipale J, 2015, CONT PHILOS REV, V48, P161, DOI 10.1007/s11007-015-9327-3
   Tinwell A, 2010, J GAMING VIRTUAL WOR, V2, P3, DOI 10.1386/jgvw.2.1.3_1
   Vinayagamoorthy Vinoba, 2005, P SOC MECH ANDR SCI, P119
   Volonte M, 2018, 25TH 2018 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P25, DOI 10.1109/VR.2018.8446364
   Volonte M, 2016, IEEE T VIS COMPUT GR, V22, P1326, DOI 10.1109/TVCG.2016.2518158
   Welch RB, 1996, PRESENCE-TELEOP VIRT, V5, P263, DOI 10.1162/pres.1996.5.3.263
   Wheeless Lawrence R, 1977, Human Communication Research, V3, P250, DOI [DOI 10.1111/J.1468-2958.1977.TB00523.X, 10.1111/j.1468-2958.1977.tb00523.x]
   Wobbrock JO, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P143, DOI 10.1145/1978942.1978963
   Wu YX, 2014, IEEE T VIS COMPUT GR, V20, P626, DOI 10.1109/TVCG.2014.19
   Yee N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1
   Zanbaka C., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1153
   Zell E, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818126
   Zibrek K., 2014, S APPL PERC 2014, P111, DOI DOI 10.1145/2628257.2628270
   Zibrek K, 2018, IEEE T VIS COMPUT GR, V24, P1681, DOI 10.1109/TVCG.2018.2794638
NR 48
TC 3
Z9 4
U1 7
U2 30
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1387-2532
EI 1573-7454
J9 AUTON AGENT MULTI-AG
JI Auton. Agents Multi-Agent Syst.
PD APR
PY 2022
VL 36
IS 1
AR 17
DI 10.1007/s10458-021-09539-1
PG 24
WC Automation & Control Systems; Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Automation & Control Systems; Computer Science
GA ZF4OX
UT WOS:000759550300001
PM 35387204
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Mawalim, CO
   Galajit, K
   Karnjana, J
   Kidani, S
   Unoki, M
AF Mawalim, Candy Olivia
   Galajit, Kasorn
   Karnjana, Jessada
   Kidani, Shunsuke
   Unoki, Masashi
TI Speaker anonymization by modifying fundamental frequency and x-vector
   singular value
SO COMPUTER SPEECH AND LANGUAGE
LA English
DT Article
DE Speaker anonymization; X-vector singular value; Fundamental frequency;
   Clustering; Subjective evaluation
ID SPEECH; TRANSFORMATION; VERIFICATION
AB Speaker anonymization is a method of protecting voice privacy by concealing individual speaker characteristics while preserving linguistic information. The VoicePrivacy Challenge 2020 was initiated to generalize the task of speaker anonymization. In the challenge, two frameworks for speaker anonymization were introduced; in this study, we propose a method of improving the primary framework by modifying the state-of-the-art speaker individuality feature (namely, x-vector) in a neural waveform speech synthesis model. Our proposed method is constructed based on x-vector singular value modification with a clustering model. We also propose a technique of modifying the fundamental frequency and speech duration to enhance the anonymization performance. To evaluate our method, we carried out objective and subjective tests. The overall objective test results show that our proposed method improves the anonymization performance in terms of the speaker verifiability, whereas the subjective evaluation results show improvement in terms of the speaker dissimilarity. The intelligibility and naturalness of the anonymized speech with speech prosody modification were slightly reduced (less than 5% of word error rate) compared to the results obtained by the baseline system.
C1 [Mawalim, Candy Olivia; Galajit, Kasorn; Kidani, Shunsuke; Unoki, Masashi] Japan Adv Inst Sci & Technol, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.
   [Galajit, Kasorn; Karnjana, Jessada] Natl Sci & Technol Dev Agcy, NECTEC, Pathum Thani, Thailand.
C3 Japan Advanced Institute of Science & Technology (JAIST); National
   Science & Technology Development Agency - Thailand; National Electronics
   & Computer Technology Center (NECTEC)
RP Mawalim, CO (corresponding author), Japan Adv Inst Sci & Technol, 1-1 Asahidai, Nomi, Ishikawa 9231292, Japan.
EM candyolivia@jaist.ac.jp; kasorn@jaist.ac.jp;
   jessada.karnjana@nectec.or.th; kidani@jaist.ac.jp; unoki@jaist.ac.jp
RI Kidani, Shunsuke/AAS-2593-2021
OI Kidani, Shunsuke/0000-0001-6491-9540; GALAJIT,
   Kasorn/0000-0003-3723-5658; Mawalim, Candy Olivia/0000-0001-9853-8893
FU JSPS, Japan KAKENHI Grant [20J20580]; Fund for the Promotion of Joint
   International Research (Fostering Joint International Research (B)),
   Japan [20KK0233]; JSPS-NSFC Bilateral Joint Research Projects/Seminars,
   Japan [JSJSBP120197416]; KDDI foundation, Japan;  [17H01761];
   Grants-in-Aid for Scientific Research [20KK0233] Funding Source: KAKEN
FX This work was supported by a Grant-in-Aid for Scientific Research (B),
   Japan (No. 17H01761) and JSPS, Japan KAKENHI Grant (No. 20J20580). This
   work was also supported by Fund for the Promotion of Joint International
   Research (Fostering Joint International Research (B)), Japan (20KK0233),
   JSPS-NSFC Bilateral Joint Research Projects/Seminars, Japan
   (JSJSBP120197416), and KDDI foundation, Japan (Research Grant Program).
CR Abou-Zleikha M, 2015, EUR SIGNAL PR CONF, P2102, DOI 10.1109/EUSIPCO.2015.7362755
   Akagi H., 1997, Journal of the Acoustical Society of Japan (E), V18, P73, DOI 10.1250/ast.18.73
   Bottou L., 1994, ADV NEURAL INFORM PR, P585
   Brümmer N, 2006, COMPUT SPEECH LANG, V20, P230, DOI 10.1016/j.csl.2005.08.001
   Buttle F., 1996, European Journal of Marketing, V30, DOI DOI 10.1108/03090569610105762
   Champion P., PPAI 2021 2 AAAI WOR
   Chung JS, 2018, INTERSPEECH, P1086
   COX EP, 1980, J MARKETING RES, V17, P407, DOI 10.1177/002224378001700401
   Das RK, 2020, INTERSPEECH, P4213, DOI 10.21437/Interspeech.2020-1052
   Das RK, 2018, IETE TECH REV, V35, P599, DOI 10.1080/02564602.2017.1357507
   Das RK, 2018, INT J SPEECH TECHNOL, V21, P401, DOI 10.1007/s10772-017-9474-5
   Dellwo Volker, 2007, Speaker Classification I. Fundamentals, Features, and Methods. (Lecture Notes in Artificial Intelligence vol. 4343), P1, DOI 10.1007/978-3-540-74200-5_1
   Dubagunta S.P., 2020, ADJUSTABLE DETERMINI
   Eriksson A., 1995, The frequency range of the voice fundamental in the speech of male and female adults
   Espinoza-Cuadros F.M., 2020, ARXIV201104696 CORR
   Fabian Pedregosa, 2011, Journal of Machine Learning Research, V12, P2825
   Fang F., 2019, P 10 ISCA WORKSHOP S, P155, DOI 10.21437/SSW.2019-28
   Fang FM, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5279, DOI 10.1109/ICASSP.2018.8462342
   GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gussenhoven C, 1997, J ACOUST SOC AM, V102, P3009, DOI 10.1121/1.420355
   Han YW, 2020, CCS '20: PROCEEDINGS OF THE 2020 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P2125, DOI 10.1145/3372297.3420025
   Hautamäki RG, 2020, INTERSPEECH, P4313, DOI 10.21437/Interspeech.2020-2715
   Hirose K, 2002, SPEECH COMMUN, V36, P97, DOI 10.1016/S0167-6393(01)00028-0
   Irum Amna, 2019, International Journal of Machine Learning and Computing, V9, P20, DOI 10.18178/ijmlc.2019.9.1.760
   Jin Q, 2008, INT CONF ACOUST SPEE, P4845
   Jin Q, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P529, DOI 10.1109/ASRU.2009.5373356
   Magariños C, 2017, COMPUT SPEECH LANG, V46, P36, DOI 10.1016/j.csl.2017.05.001
   Mawalim CO, 2020, INTERSPEECH, P1703, DOI 10.21437/Interspeech.2020-1887
   McAdams S., 1984, THESIS STANFORD
   Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Patino J., 2020, ARXIV PREPRINT ARXIV
   Peddinti V, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3214
   Pobar M, 2014, 2014 37TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P1264, DOI 10.1109/MIPRO.2014.6859761
   Povey Daniel, 2011, IEEE WORKSH AUT SPEE
   Sahidullah M, 2016, DIGIT SIGNAL PROCESS, V50, P1, DOI 10.1016/j.dsp.2015.10.011
   Sathiyamurthi P, 2017, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-017-0118-0
   Snyder D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5329
   Tomashenko N, 2020, INTERSPEECH, P1693, DOI 10.21437/Interspeech.2020-1333
   Tomashenko N., 2021, VOICEPRIVACY 2020 CH
   Turner H., 2020, ARXIV PREPRINT ARXIV
   Veaux C., 2017, University of Edinburgh. The Centre for Speech Technology Research (CSTR)
   Vestman V, 2020, COMPUT SPEECH LANG, V59, P36, DOI 10.1016/j.csl.2019.05.005
   Wang X, 2019, INT CONF ACOUST SPEE, P5916, DOI 10.1109/ICASSP.2019.8682298
   Zen HG, 2019, INTERSPEECH, P1526, DOI 10.21437/Interspeech.2019-2441
NR 47
TC 4
Z9 4
U1 0
U2 0
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0885-2308
EI 1095-8363
J9 COMPUT SPEECH LANG
JI Comput. Speech Lang.
PD MAY
PY 2022
VL 73
AR 101326
DI 10.1016/j.csl.2021.101326
EA DEC 2021
PG 17
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YI8MP
UT WOS:000744097300003
DA 2024-01-09
ER

PT J
AU Abur, D
   Subaciute, A
   Daliri, A
   Lester-Smith, RA
   Lupiani, AA
   Cilento, D
   Enos, NM
   Weerathunge, HR
   Tardif, MC
   Stepp, CE
AF Abur, Defne
   Subaciute, Austeja
   Daliri, Ayoub
   Lester-Smith, Rosemary A.
   Lupiani, Ashling A.
   Cilento, Dante
   Enos, Nicole M.
   Weerathunge, Hasini R.
   Tardif, Monique C.
   Stepp, Cara E.
TI Feedback and Feedforward Auditory-Motor Processes for Voice and
   Articulation in Parkinson's Disease
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
ID VOCAL PITCH; SENSORIMOTOR ADAPTATION; INTELLIGIBILITY SCORES;
   FUNDAMENTAL-FREQUENCY; SENSORY MEMORY; SPEECH RATE; RESPONSES; LEVODOPA;
   SPEAKERS; PERTURBATIONS
AB Purpose: Unexpected and sustained manipulations of auditory feedback during speech production result in "reflexive" and "adaptive" responses, which can shed light on feedback and feedforward auditory-motor control processes, respectively. Persons with Parkinson's disease (PwPD) have shown aberrant reflexive and adaptive responses, but responses appear to differ for control of vocal and articulatory features. However, these responses have not been examined for both voice and articulation in the same speakers and with respect to auditory acuity and functional speech outcomes (speech intelligibility and naturalness).
   Method: Here, 28 PwPD on their typical dopaminergic medication schedule and 28 age-, sex-, and hearing-matched controls completed tasks yielding reflexive and adaptive responses as well as auditory acuity for both vocal and articulatory features.
   Results: No group differences were found for any measures of auditory-motor control, conflicting with prior findings in PwPD while off medication. Auditory-motor measures were also compared with listener ratings of speech function: first formant frequency acuity was related to speech intelligibility, whereas adaptive responses to vocal fundamental frequency manipulations were related to speech naturalness.
   Conclusions: These results support that auditory-motor processes for both voice and articulatory features are intact for PwPD receiving medication. This work is also the first to suggest associations between measures of auditory-motor control and speech intelligibility and naturalness.
C1 [Abur, Defne; Daliri, Ayoub; Lester-Smith, Rosemary A.; Lupiani, Ashling A.; Cilento, Dante; Tardif, Monique C.; Stepp, Cara E.] Boston Univ, Dept Speech Language & Hearing Sci, Boston, MA 02215 USA.
   [Subaciute, Austeja; Enos, Nicole M.; Weerathunge, Hasini R.; Stepp, Cara E.] Boston Univ, Dept Biomed Engn, Boston, MA 02215 USA.
   [Daliri, Ayoub] Arizona State Univ, Coll Hlth Solut, Tempe, AZ USA.
   [Lester-Smith, Rosemary A.] Univ Texas Austin, Moody Coll Commun, Dept Speech Language & Hearing Sci, Austin, TX USA.
   [Lupiani, Ashling A.] Univ N Carolina, Joint Dept Biomed Engn, Chapel Hill, NC 27515 USA.
   [Lupiani, Ashling A.] North Carolina State Univ, Raleigh, NC USA.
   [Enos, Nicole M.] Boston Univ, Dept Elect & Comp Engn, Boston, MA USA.
   [Tardif, Monique C.] Univ Pittsburgh, Dept Commun Sci & Disorders, Pittsburgh, PA USA.
   [Stepp, Cara E.] Boston Univ, Dept Otolaryngol Head & Neck Surg, Sch Med, Boston, MA USA.
C3 Boston University; Boston University; Arizona State University; Arizona
   State University-Tempe; University of Texas System; University of Texas
   Austin; University of North Carolina; University of North Carolina
   Chapel Hill; North Carolina State University; Boston University;
   Pennsylvania Commonwealth System of Higher Education (PCSHE); University
   of Pittsburgh; Boston University
RP Abur, D (corresponding author), Boston Univ, Dept Speech Language & Hearing Sci, Boston, MA 02215 USA.
EM dabur@bu.edu
RI Weerathunge, Hasini/GQZ-2356-2022
OI Weerathunge, Hasini/0000-0002-0240-9104; Lester-Smith,
   Rosemary/0000-0002-9111-7399; Tardif, Monique/0000-0002-6029-6507;
   Stepp, Cara/0000-0002-8045-252X
FU National Institute of Deafness and Other Communication Disorders [R01
   DC016270, T32 DC013017, F31 DC019032]; Rafik B. Hariri Institute for
   Computing and Computational Science and Engineering; ASH Foundation New
   Century Doctoral Scholarship; Dudley Allen Sargent Research Fund Award
FX The authors would like to thank Talia Mittelman, Paige Clabby, Katherine
   Brown, and Halle Duggan for help with participant recruitment and data
   collection. This work was supported by grants R01 DC016270 (Cara E.
   Stepp and Frank H. Guenther), T32 DC013017 (Cara E. Stepp and
   Christopher A. Moore), and F31 DC019032 (Defne Abur) from the National
   Institute of Deafness and Other Communication Disorders. It was also
   supported by a Graduate Fellow Award from the Rafik B. Hariri Institute
   for Computing and Computational Science and Engineering (Defne Abur), an
   ASH Foundation New Century Doctoral Scholarship (Defne Abur), and a
   Dudley Allen Sargent Research Fund Award (Defne Abur).
CR Abur D, 2020, J SPEECH LANG HEAR R, V63, P3208, DOI 10.1044/2020_JSLHR-20-00003
   Abur D, 2019, AM J SPEECH-LANG PAT, V28, P1222, DOI 10.1044/2019_AJSLP-18-0275
   Abur D, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191839
   American Speech-Language-Hearing Association, 2005, Guidelines for Manual Pure-Tone Threshold Audiometry
   Anand S, 2015, J SPEECH LANG HEAR R, V58, P1134, DOI 10.1044/2015_JSLHR-S-14-0243
   [Anonymous], 2007, SPEECH INTELLIGIBILI
   Behroozmand R, 2009, CLIN NEUROPHYSIOL, V120, P1303, DOI 10.1016/j.clinph.2009.04.022
   Beverly D, 2010, J MED SPEECH-LANG PA, V18, P9
   Broadfoot C K, 2019, Perspect ASHA Spec Interest Groups, V4, P825, DOI 10.1044/2019_pers-sig3-2019-0001
   Bunton K, 2006, FOLIA PHONIATR LOGO, V58, P323, DOI 10.1159/000094567
   Burnett TA, 1997, J VOICE, V11, P202, DOI 10.1016/S0892-1997(97)80079-3
   Burnett TA, 1998, J ACOUST SOC AM, V103, P3153, DOI 10.1121/1.423073
   Cai S., 2008, 8 INT SEM SPEECH PRO
   Cai SQ, 2011, J NEUROSCI, V31, P16483, DOI 10.1523/JNEUROSCI.3653-11.2011
   Cannito MP, 2012, J VOICE, V26, P214, DOI 10.1016/j.jvoice.2011.08.014
   Chang EF, 2013, P NATL ACAD SCI USA, V110, P2653, DOI 10.1073/pnas.1216827110
   Chen X, 2013, BRAIN RES, V1527, P99, DOI 10.1016/j.brainres.2013.06.030
   De Letter M, 2006, ACTA NEUROL BELG, V106, P19
   Eadie TL, 2016, HEAD NECK-J SCI SPEC, V38, pE1955, DOI 10.1002/hed.24353
   García-Pérez MA, 1998, VISION RES, V38, P1861, DOI 10.1016/S0042-6989(97)00340-4
   Goberman A, 2002, J COMMUN DISORD, V35, P217, DOI 10.1016/S0021-9924(01)00072-7
   Goberman AM, 2003, J FLUENCY DISORD, V28, P55, DOI 10.1016/S0094-730X(03)00005-6
   Hain TC, 2000, EXP BRAIN RES, V130, P133, DOI 10.1007/s002219900237
   Ho AK, 2008, MOVEMENT DISORD, V23, P574, DOI 10.1002/mds.21899
   Hustad KC, 2007, FOLIA PHONIATR LOGO, V59, P306, DOI 10.1159/000108337
   Jones JA, 2008, EXP BRAIN RES, V190, P279, DOI 10.1007/s00221-008-1473-y
   Karlsen KH, 2000, J NEUROL NEUROSUR PS, V69, P584, DOI 10.1136/jnnp.69.5.584
   Kearney E, 2020, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.02995
   Kempster GB, 2009, AM J SPEECH-LANG PAT, V18, P124, DOI 10.1044/1058-0360(2008/08-0017)
   KENT RD, 1989, J SPEECH HEAR DISORD, V54, P482, DOI 10.1044/jshd.5404.482
   Kiran S, 2001, J SPEECH LANG HEAR R, V44, P975, DOI 10.1044/1092-4388(2001/076)
   Lametti DR, 2012, J NEUROSCI, V32, P9351, DOI 10.1523/JNEUROSCI.0404-12.2012
   Lester-Smith RA, 2020, J SPEECH LANG HEAR R, V63, P3628, DOI 10.1044/2020_JSLHR-20-00192
   Liu HJ, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0033629
   Liu HJ, 2010, J ACOUST SOC AM, V127, P1042, DOI 10.1121/1.3273880
   Micheyl C, 2006, HEARING RES, V219, P36, DOI 10.1016/j.heares.2006.05.004
   Mollaei F, 2019, J SPEECH LANG HEAR R, V62, P4256, DOI 10.1044/2019_JSLHR-S-18-0425
   Mollaei F, 2016, BRAIN RES, V1646, P269, DOI 10.1016/j.brainres.2016.06.013
   Mollaei F, 2013, MOVEMENT DISORD, V28, P1668, DOI 10.1002/mds.25588
   Moore BCJ, 2006, HEARING RES, V222, P16, DOI 10.1016/j.heares.2006.08.007
   Murray ESH, 2019, J SPEECH LANG HEAR R, V62, P2270, DOI 10.1044/2019_JSLHR-S-18-0408
   Nasreddine ZS, 2005, J AM GERIATR SOC, V53, P695, DOI 10.1111/j.1532-5415.2005.53221.x
   Olsen Wayne O, 1998, Am J Audiol, V7, P21, DOI 10.1044/1059-0889(1998/012)
   PEKKONEN E, 1994, NEUROREPORT, V5, P2537, DOI 10.1097/00001756-199412000-00033
   Pekkonen E, 2001, NEUROIMAGE, V14, P376, DOI 10.1006/nimg.2001.0805
   Pichora-Fuller M. K., 2003, INT J AUDIOL, V42, p2S11
   Plowman-Prine EK, 2009, NEUROREHABILITATION, V24, P131, DOI 10.3233/NRE-2009-0462
   SCHOW RL, 1991, EAR HEARING, V12, P337, DOI 10.1097/00003446-199110000-00006
   SHEARD C, 1991, J SPEECH HEAR RES, V34, P285, DOI 10.1044/jshr.3402.285
   Skodda S, 2013, PARKINSONS DIS-US, V2013, DOI 10.1155/2013/389195
   Skodda S, 2011, J VOICE, V25, pE199, DOI 10.1016/j.jvoice.2010.04.007
   Skodda S, 2010, J NEURAL TRANSM, V117, P197, DOI 10.1007/s00702-009-0351-5
   Spencer KA, 2009, J MED SPEECH-LANG PA, V17, P125
   Stebbins GT, 2013, MOVEMENT DISORD, V28, P668, DOI 10.1002/mds.25383
   Stepp CE, 2017, J SPEECH LANG HEAR R, V60, P1545, DOI 10.1044/2017_JSLHR-S-16-0282
   Stipancic KL, 2018, J SPEECH LANG HEAR R, V61, P2757, DOI 10.1044/2018_JSLHR-S-17-0366
   Tjaden K, 2014, J SPEECH LANG HEAR R, V57, P779, DOI 10.1044/2014_JSLHR-S-12-0372
   Tourville JA, 2008, NEUROIMAGE, V39, P1429, DOI 10.1016/j.neuroimage.2007.09.054
   Tourville JA, 2011, LANG COGNITIVE PROC, V26, P952, DOI 10.1080/01690960903498424
   Villacorta VM, 2007, J ACOUST SOC AM, V122, P2306, DOI 10.1121/1.2773966
   Wang WD, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00815
   Weerathunge HR, 2020, J SPEECH LANG HEAR R, V63, P2846, DOI 10.1044/2020_JSLHR-19-00407
   Yorkston K., 1996, Sentence Intelligibility Test for Windows
   YORKSTON KM, 1990, J SPEECH HEAR DISORD, V55, P550, DOI 10.1044/jshd.5503.550
   YORKSTON KM, 2010, MANAGEMENT MOTOR SPE
   Zarate JM, 2005, ANN NY ACAD SCI, V1060, P404, DOI 10.1196/annals.1360.058
NR 66
TC 9
Z9 10
U1 1
U2 2
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 2200 RESEARCH BLVD, #271, ROCKVILLE, MD 20850-3289 USA
SN 1092-4388
EI 1558-9102
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD DEC
PY 2021
VL 64
IS 12
BP 4682
EP 4694
DI 10.1044/2021_JSLHR-21-00153
PG 13
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA ZA4NO
UT WOS:000756142500009
PM 34731577
OA Green Published
DA 2024-01-09
ER

PT J
AU Cistola, G
   Peiro-Lilja, A
   Cambara, G
   van der Meulen, I
   Farrus, M
AF Cistola, Giorgia
   Peiro-Lilja, Alex
   Cambara, Guillermo
   van der Meulen, Ineke
   Farrus, Mireia
TI Influence of TTS Systems Performance on Reaction Times in People with
   Aphasia
SO APPLIED SCIENCES-BASEL
LA English
DT Article
DE aphasia; intelligibility; jitter; naturalness; reading impairments;
   shimmer; text-to-speech systems
ID SPEECH; COMPREHENSION
AB Text-to-speech (TTS) systems provide fundamental reading support for people with aphasia and reading difficulties. However, artificial voices are more difficult to process than natural voices. The current study is an extended analysis of the results of a clinical experiment investigating which, among three artificial voices and a digitised human voice, is more suitable for people with aphasia and reading impairments. Such results show that the voice synthesised with Ogmios TTS, a concatenative speech synthesis system, caused significantly slower reaction times than the other three voices used in the experiment. The present study explores whether and what voice quality metrics are linked to delayed reaction times. For this purpose, the voices were analysed using an automatic assessment of intelligibility, naturalness, and jitter and shimmer voice quality parameters. This analysis revealed that Ogmios TTS, in general, performed worse than the other voices in all parameters. These observations could explain the significantly delayed reaction times in people with aphasia and reading impairments when listening to Ogmios TTS and could open up consideration about which TTS to choose for compensative devices for these patients based on the voice analysis of these parameters.
C1 [Cistola, Giorgia; Peiro-Lilja, Alex; Cambara, Guillermo] Univ Pompeu Fabra, TALN Res Grp, Barcelona 08017, Spain.
   [van der Meulen, Ineke] Erasmus MC, Univ Med Ctr Rotterdam, Dept Rehabil Med, NL-3015 GD Rotterdam, Netherlands.
   [van der Meulen, Ineke] Rijndam Rehabil, NL-3015 LJ Rotterdam, Netherlands.
   [Farrus, Mireia] Univ Barcelona, Language & Computat Ctr, Barcelona 08007, Spain.
C3 Pompeu Fabra University; Erasmus University Rotterdam; Erasmus MC;
   University of Barcelona
RP Cistola, G (corresponding author), Univ Pompeu Fabra, TALN Res Grp, Barcelona 08017, Spain.
EM giorgia.cistola@upf.edu; alex.peiro@upf.edu; guillermo.cambara@upf.edu;
   ivdmeulen@rijndam.nl; mfarrus@ub.edu
RI Farrús, Mireia/O-1402-2019; Cistola, Giorgia/GQQ-1362-2022
OI Farrús, Mireia/0000-0002-7160-9513; Cistola, Giorgia/0000-0002-6990-5786
FU Agencia Estatal de Investigacion (AEI); Ministerio de Ciencia,
   Innovacion y Universidades; Fondo Social Europeo (FSE) [RYC-2015-17239]
FX The last author has been funded by the Agencia Estatal de Investigacion
   (AEI), Ministerio de Ciencia, Innovacion y Universidades and the Fondo
   Social Europeo (FSE) under grant RYC-2015-17239 (AEI/FSE, UE).
CR Baby A., 2020, ARXIV200601463
   Boersma P., 2021, Glot International
   Bonafonte A, P TC STAR WORKSH SPE, P199
   Brown JA, 2019, AM J SPEECH-LANG PAT, V28, P278, DOI 10.1044/2018_AJSLP-17-0132
   Carlsen K., 1994, J MED SPEECH-LANG PA, V2, P105
   Cistola G, 2021, INT J LANG COMM DIS, V56, P161, DOI 10.1111/1460-6984.12569
   Cohn M, 2020, INTERSPEECH, P1733, DOI 10.21437/Interspeech.2020-1336
   Farrus M, P INTERSPEECH 2007 8, P778
   Forster KI, 2003, BEHAV RES METH INS C, V35, P116, DOI 10.3758/BF03195503
   Hux K, 2021, J COMMUN DISORD, V91, DOI 10.1016/j.jcomdis.2021.106098
   Hux K, 2017, J COMMUN DISORD, V69, P15, DOI 10.1016/j.jcomdis.2017.06.006
   King S, 2011, SADHANA-ACAD P ENG S, V36, P837, DOI 10.1007/s12046-011-0048-y
   Knollman-Porter K, 2019, AM J SPEECH-LANG PAT, V28, P1206, DOI 10.1044/2019_AJSLP-19-0013
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   McNeil M. R., 1991, CLIN APHASIOL, V20, P21
   Mittag G, 2020, INTERSPEECH, P1748, DOI 10.21437/Interspeech.2020-2382
   Murray LL, 1999, APHASIOLOGY, V13, P91, DOI 10.1080/026870399402226
   Pratap V, 2020, INTERSPEECH, P2757, DOI 10.21437/Interspeech.2020-2826
   Pratap V, 2019, INT CONF ACOUST SPEE, P6460, DOI 10.1109/ICASSP.2019.8683535
   Sanders W.R, 1968, U LEVEL COMPUTER ASS
   Slyh R.E, P 1999 IEEE INT C AC, VVolume 4, P2091, DOI [10.1109/ICASSP.1999.758345, DOI 10.1109/ICASSP.1999.758345]
   Soni M.H, P 9 ISCA SPEECH SYNT
   Taylor P, P 3 ESCA WORKSH SPEE, P147
   Teixeira JP, 2014, PROC TECH, V16, P1190, DOI 10.1016/j.protcy.2014.10.134
   Teixeira JP., 2013, Proc. Technol, V9, P1112, DOI DOI 10.1016/J.PROTCY.2013.12.124
   Wallace SE, 2019, APHASIOLOGY, V33, P731, DOI 10.1080/02687038.2018.1506088
   Ziegler A., 2011, GEN ESTIMATING EQUAT, DOI DOI 10.1007/978-1-4614-0499-6
   Zwetsch IC, 2006, SCI MED, V16, P109
NR 28
TC 1
Z9 1
U1 0
U2 1
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 2076-3417
J9 APPL SCI-BASEL
JI Appl. Sci.-Basel
PD DEC
PY 2021
VL 11
IS 23
AR 11320
DI 10.3390/app112311320
PG 11
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Chemistry; Engineering; Materials Science; Physics
GA XW5KH
UT WOS:000735657500001
OA Green Published, gold
DA 2024-01-09
ER

PT J
AU Kapolowicz, MR
   Guest, DR
   Montazeri, V
   Baese-Berk, MM
   Assmann, PF
AF Kapolowicz, Michelle R.
   Guest, Daniel R.
   Montazeri, Vahid
   Baese-Berk, Melissa M.
   Assmann, Peter F.
TI Effects of Spectral Envelope and Fundamental Frequency Shifts on the
   Perception of Foreign-Accented Speech
SO LANGUAGE AND SPEECH
LA English
DT Article
DE Foreign-accented speech; fundamental frequency; spectral envelope;
   talker discrimination
ID ACOUSTIC DIFFERENCES; VOWEL NORMALIZATION; TALKER VARIABILITY; RAPID
   ADAPTATION; WORD RECOGNITION; SPEAKING RATE; VOICE; INDIVIDUALITY;
   PARAMETERS; RESOLUTION
AB To investigate the role of spectral pattern information in the perception of foreign-accented speech, we measured the effects of spectral shifts on judgments of talker discrimination, perceived naturalness, and intelligibility when listening to Mandarin-accented English and native-accented English sentences. In separate conditions, the spectral envelope and fundamental frequency (F0) contours were shifted up or down in three steps using coordinated scale factors (multiples of 8% and 30%, respectively). Experiment 1 showed that listeners perceive spectrally shifted sentences as coming from a different talker for both native-accented and foreign-accented speech. Experiment 2 demonstrated that downward shifts applied to male talkers and the largest upward shifts applied to all talkers reduced the perceived naturalness, regardless of accent. Overall, listeners rated foreign-accented speech as sounding less natural even for unshifted speech. In Experiment 3, introducing spectral shifts further lowered the intelligibility of foreign-accented speech. When speech from the same foreign-accented talker was shifted to simulate five different talkers, increased exposure failed to produce an improvement in intelligibility scores, similar to the pattern observed when listeners actually heard five foreign-accented talkers. Intelligibility of spectrally shifted native-accented speech was near ceiling performance initially, and no further improvement or decrement was observed. These experiments suggest a mechanism that utilizes spectral envelope and F0 cues in a talker-dependent manner to support the perception of foreign-accented speech.
C1 [Kapolowicz, Michelle R.] Univ Calif Irvine, 114 Med Sci E, Irvine, CA 92697 USA.
   [Guest, Daniel R.] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Montazeri, Vahid; Assmann, Peter F.] Univ Texas Dallas, Richardson, TX 75083 USA.
   [Baese-Berk, Melissa M.] Univ Oregon, Eugene, OR 97403 USA.
C3 University of California System; University of California Irvine;
   University of Minnesota System; University of Minnesota Twin Cities;
   University of Texas System; University of Texas Dallas; University of
   Oregon
RP Kapolowicz, MR (corresponding author), Univ Calif Irvine, 114 Med Sci E, Irvine, CA 92697 USA.
EM m.kapolowicz@uci.edu
RI Kapolowicz, Michelle/IUP-4526-2023
OI Kapolowicz, Michelle/0000-0003-4404-5270; Baese-Berk,
   Melissa/0000-0002-4855-9319
CR Abdi H, 2009, EXPT DESIGN ANAL PSY
   ANDERSONHSIEH J, 1988, LANG LEARN, V38, P561, DOI 10.1111/j.1467-1770.1988.tb00167.x
   [Anonymous], 1969, IEEE T ACOUST SPEECH, VAU17, P225
   Assmann Peter F., 2008, Canadian Acoustics, V36, P148
   Assmann P.F., 2007, Journal of the Acoustical Society of America, V122, P3064
   Assmann PF, 2008, J ACOUST SOC AM, V124, P3203, DOI 10.1121/1.2980456
   Assmann PF, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P889
   Bachorowski JA, 1999, J ACOUST SOC AM, V106, P1054, DOI 10.1121/1.427115
   Baese-Berk MM, 2021, JASA EXPRESS LETT, V1, DOI 10.1121/10.0003326
   Baese-Berk MM, 2015, J ACOUST SOC AM, V138, pEL223, DOI 10.1121/1.4929622
   Baese-Berk MM, 2013, J ACOUST SOC AM, V133, pEL174, DOI 10.1121/1.4789864
   Barreda S, 2020, LANGUAGE, V96, P224, DOI 10.1353/lan.2020.0018
   Barreda S, 2018, J ACOUST SOC AM, V143, pEL361, DOI 10.1121/1.5037614
   Barreda S, 2012, J ACOUST SOC AM, V132, P3453, DOI 10.1121/1.4747011
   Baskent D., 2016, SCI FDN AUDIOLOGY PE, P285
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Baumann O, 2010, PSYCHOL RES-PSYCH FO, V74, P110, DOI 10.1007/s00426-008-0185-z
   Bent T, 2013, J ACOUST SOC AM, V133, P1677, DOI 10.1121/1.4776212
   Best CT., 1995, SPEECH PERCEPTION LI, P171
   Bradlow AR, 2008, COGNITION, V106, P707, DOI 10.1016/j.cognition.2007.04.005
   Chang YP, 2006, J SPEECH LANG HEAR R, V49, P1331, DOI 10.1044/1092-4388(2006/095)
   Clarke CM, 2004, J ACOUST SOC AM, V116, P3647, DOI 10.1121/1.1815131
   Cooper A, 2016, J ACOUST SOC AM, V140, pEL378, DOI 10.1121/1.4966585
   Flege JE., 1995, SPEECH PERCEPTION LI, P233, DOI DOI 10.1111/J.1600-0404.1995.TB01710.X
   Friendly Michael, 2023, CRAN
   Fu QJ, 1999, EAR HEARING, V20, P332, DOI 10.1097/00003446-199908000-00006
   Fuller CD, 2014, JARO-J ASSOC RES OTO, V15, P1037, DOI 10.1007/s10162-014-0483-7
   Gaudrain E, 2018, EAR HEARING, V39, P226, DOI 10.1097/AUD.0000000000000480
   Gaudrain E, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P152
   Graddol David., 2006, English next
   Hillenbrand JM, 2009, ATTEN PERCEPT PSYCHO, V71, P1150, DOI 10.3758/APP.71.5.1150
   Holmes E, 2018, PSYCHOL SCI, V29, P1575, DOI 10.1177/0956797618779083
   Hothorn T, 2008, BIOMETRICAL J, V50, P346, DOI 10.1002/bimj.200810425
   Jenkins J., 2000, The phonology of English as an international language: New models, new norms, new goals
   Ji CL, 2014, J SPEECH LANG HEAR R, V57, P532, DOI 10.1044/2014_JSLHR-H-12-0404
   Johnsrude IS, 2013, PSYCHOL SCI, V24, P1995, DOI 10.1177/0956797613482467
   Joos M.A., 1948, Language, V24, P1, DOI [10.2307/522229, DOI 10.2307/522229]
   Kaiser AR, 2003, J SPEECH LANG HEAR R, V46, P390, DOI 10.1044/1092-4388(2003/032)
   Kapolowicz MR, 2020, J ACOUST SOC AM, V148, pEL267, DOI 10.1121/10.0001941
   Kapolowicz MR, 2018, J ACOUST SOC AM, V143, pEL99, DOI 10.1121/1.5023594
   Kapolowicz MR, 2016, INTERSPEECH, P3289, DOI 10.21437/Interspeech.2016-1585
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   Kawahara H., 2005, Proceedings from Interspeech 2005: The 9th Biennial Conference of the International Speech Communication Association ISCA, P537
   Kreiman J., 2011, FDN VOICE STUDIES IN, DOI DOI 10.1002/9781444395068
   KUWABARA H, 1991, SPEECH COMMUN, V10, P491, DOI 10.1016/0167-6393(91)90052-U
   KUWABARA H, 1995, SPEECH COMMUN, V16, P165, DOI 10.1016/0167-6393(94)00053-D
   LANE H, 1963, J ACOUST SOC AM, V35, P451, DOI 10.1121/1.1918501
   Lawrence Michael A, 2016, CRAN
   Lenth R., 2020, emmeans: Estimated marginal means, aka least-squares means
   Lenth Russell V, 2023, CRAN
   Mackey LS, 1997, J SPEECH LANG HEAR R, V40, P349, DOI 10.1044/jslhr.4002.349
   Magnuson JS, 2007, J EXP PSYCHOL HUMAN, V33, P391, DOI 10.1037/0096-1523.33.2.391
   Nearey T.M., 2007, Experimental Approaches to Phonology, P246
   NEAREY TM, 1989, J ACOUST SOC AM, V85, P2088, DOI 10.1121/1.397861
   Nygaard LC, 1998, PERCEPT PSYCHOPHYS, V60, P355, DOI 10.3758/BF03206860
   Sidaras SK, 2009, J ACOUST SOC AM, V125, P3306, DOI 10.1121/1.3101452
   Sjerps MJ, 2011, NEUROPSYCHOLOGIA, V49, P3831, DOI 10.1016/j.neuropsychologia.2011.09.044
   Smith DRR, 2005, J ACOUST SOC AM, V117, P305, DOI 10.1121/1.1828637
   STUDEBAKER GA, 1985, J SPEECH HEAR RES, V28, P455, DOI 10.1044/jshr.2803.455
   Tamati TN, 2021, J SPEECH LANG HEAR R, V64, P683, DOI 10.1044/2020_JSLHR-20-00496
   Tamati TN, 2020, J ACOUST SOC AM, V147, pEL370, DOI 10.1121/10.0001097
   Thomson RI, 2009, J ACOUST SOC AM, V126, P1447, DOI 10.1121/1.3177260
   TITZE IR, 1989, J ACOUST SOC AM, V85, P1699, DOI 10.1121/1.397959
   Trude AM, 2013, J MEM LANG, V69, P349, DOI 10.1016/j.jml.2013.05.002
   VANDOMMELEN WA, 1990, LANG SPEECH, V33, P259, DOI 10.1177/002383099003300302
   Wong PCM, 2003, J SPEECH LANG HEAR R, V46, P413, DOI 10.1044/1092-4388(2003/034)
   Xie X, 2018, LANG COGN NEUROSCI, V33, P196, DOI 10.1080/23273798.2017.1369551
NR 67
TC 1
Z9 1
U1 0
U2 5
PU SAGE PUBLICATIONS LTD
PI LONDON
PA 1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND
SN 0023-8309
EI 1756-6053
J9 LANG SPEECH
JI Lang. Speech
PD JUN
PY 2022
VL 65
IS 2
BP 418
EP 443
AR 00238309211029679
DI 10.1177/00238309211029679
EA JUL 2021
PG 26
WC Audiology & Speech-Language Pathology; Linguistics; Psychology,
   Experimental
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Psychology
GA 0N4KD
UT WOS:000672553400001
PM 34240630
DA 2024-01-09
ER

PT J
AU Seaborn, K
   Miyake, NP
   Pennefather, P
   Otake-Matsuura, M
AF Seaborn, Katie
   Miyake, Norihisa P.
   Pennefather, Peter
   Otake-Matsuura, Mihoko
TI Voice in Human-Agent Interaction: A Survey
SO ACM COMPUTING SURVEYS
LA English
DT Article
DE Computer agent; computer voice; synthetic speech; voice perception;
   vocalics; conversational agents; voice assistants; robots; embodied AI;
   embodied agents; voice-user interface (VUI); human-agent interaction
   (HAI); human-computer interaction (HCI); human-robot interaction (HRI);
   human-machine communication (HMC)
ID SOCIAL ROBOTS; COMPUTER; SPEECH; GENDER; PERCEPTION; FRAMEWORK; USERS;
   CUES; STEREOTYPES; RECOGNITION
AB Social robots, conversational agents, voice assistants, and other embodied AI are increasingly a feature of everyday life. What connects these various types of intelligent agents is their ability to interact with people through voice. Voice is becoming an essential modality of embodiment, communication, and interaction between computer-based agents and end-users. This survey presents a meta-synthesis on agent voice in the design and experience of agents from a human-centered perspective: voice-based human-agent interaction (vHAI). Findings emphasize the social role of voice in HAI as well as circumscribe a relationship between agent voice and body, corresponding to human models of social psychology and cognition. Additionally, changes in perceptions of and reactions to agent voice over time reveals a generational shift coinciding with the commercial proliferation of mobile voice assistants. The main contributions of this work are a vHAI classification framework for voice across various agent forms, contexts, and user groups, a critical analysis grounded in key theories, and an identification of future directions for the oncoming wave of vocal machines.
C1 [Seaborn, Katie] Tokyo Inst Technol, Meguru Ku, W9-51,2-12-1 Ookayama, Tokyo 1528552, Japan.
   [Seaborn, Katie] RIKEN, Ctr Adv Intelligence Project AIP, Meguru Ku, W9-51,2-12-1 Ookayama, Tokyo 1528552, Japan.
   [Miyake, Norihisa P.; Otake-Matsuura, Mihoko] RIKEN, Ctr Adv Intelligence Project AIP, Chuo Ku, Nihonbashi 1 Chome Mitsui Bldg,15th Floor, Tokyo 1030027, Japan.
   [Pennefather, Peter] gDial Inc, 87 Earl Grey Rd, Toronto, ON M4J 3L6, Canada.
C3 Tokyo Institute of Technology; RIKEN; RIKEN
RP Seaborn, K (corresponding author), Tokyo Inst Technol, Meguru Ku, W9-51,2-12-1 Ookayama, Tokyo 1528552, Japan.; Seaborn, K (corresponding author), RIKEN, Ctr Adv Intelligence Project AIP, Meguru Ku, W9-51,2-12-1 Ookayama, Tokyo 1528552, Japan.
EM seaborn.k.aa@m.titech.ac.jp; norihisa.miyake@riken.jp;
   p.pennefather@gmail.com; mihoko.otake@riken.jp
RI Otake-Matsuura, Mihoko/N-4686-2017; Seaborn, Katie/JFK-2295-2023
OI Otake-Matsuura, Mihoko/0000-0003-3644-276X; Seaborn,
   Katie/0000-0002-7812-9096
FU Japanese Society for the Promotion of Science (JSPS)
FX We thank the editors and reviewers. We also thank the Japanese Society
   for the Promotion of Science (JSPS) for supporting this work.
CR Abdul-Kader SA, 2015, INT J ADV COMPUT SC, V6, P72
   Abdulrahman A, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364754
   ACM Council, ACM ETH
   AI for the Social Good, 2019, DAGSTH DECL
   AJZEN I, 1991, ORGAN BEHAV HUM DEC, V50, P179, DOI 10.1016/0749-5978(91)90020-T
   Amershi S, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300233
   Andrist S, 2015, ACMIEEE INT CONF HUM, P157, DOI 10.1145/2696454.2696464
   [Anonymous], 2019, PARTNERSHIP, DOI Buffalo Bayou
   [Anonymous], 1993, PARALANGUAGE LINGUIS
   [Anonymous], 2003, CHI Letters, DOI [10.1145/642611.642662, DOI 10.1145/642611.642662]
   [Anonymous], 2010, SOCIAL IDENTITY INTE
   [Anonymous], 1980, The Phonetic Description of Voice Quality
   Arik SO, 2017, PR MACH LEARN RES, V70
   Atkinson RK, 2005, CONTEMP EDUC PSYCHOL, V30, P117, DOI 10.1016/j.cedpsych.2004.07.001
   Baird A, 2018, J AUDIO ENG SOC, V66, P277, DOI 10.17743/jaes.2018.0023
   Baird Alice, 2017, P 12 INT AUDIO MOSTL, DOI [10.1145/3123514.3123528, DOI 10.1145/3123514.3123528]
   Barsalou LW, 2003, PSYCHOL LEARN MOTIV, V43, P43, DOI 10.1016/S0079-7421(03)01011-9
   Beauvoir S, 2011, The Second Sex
   Behrens SI, 2018, ACMIEEE INT CONF HUM, P63, DOI 10.1145/3173386.3177009
   Bhagdikar S, 2019, INT CONF SIM SEMI PR, P13, DOI 10.1109/sispad.2019.8870524
   Bracken CC, 2004, CYBERPSYCHOL BEHAV, V7, P349, DOI 10.1089/1094931041291358
   Braun M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300270
   Breazeal C, 2016, SPRINGER HANDBOOK OF ROBOTICS, P1935
   BRENNAN SE, 1995, KNOWL-BASED SYST, V8, P143, DOI 10.1016/0950-7051(95)98376-H
   Burgin Mark, 2009, SYSTEMATIC APPROACH
   Cambre Julia, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359325
   Cambre J, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376789
   Cangelosi Angelo, 2016, HUMANOID ROBOTICS RE, P1
   Cao HL, 2019, INTEL SERV ROBOT, V12, P45, DOI 10.1007/s11370-018-0266-9
   Chang RCS, 2018, COMPUT HUM BEHAV, V84, P194, DOI 10.1016/j.chb.2018.02.025
   Chérif E, 2019, RECH APPL MARKET-ENG, V34, P28, DOI 10.1177/2051570719829432
   Chidambaram V, 2012, ACMIEEE INT CONF HUM, P293
   Chiou EK, 2020, COMPUT EDUC, V146, DOI 10.1016/j.compedu.2019.103756
   Chita-Tegmark M, 2019, ACMIEEE INT CONF HUM, P230, DOI [10.1109/HRI.2019.8673222, 10.1109/hri.2019.8673222]
   Coeckelbergh M, 2011, INT J SOC ROBOT, V3, P197, DOI 10.1007/s12369-010-0075-6
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Cosentino Sarah, 2016, IEEE Rev Biomed Eng, V9, P148, DOI 10.1109/RBME.2016.2527638
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   Creed C, 2008, INTERACT COMPUT, V20, P225, DOI 10.1016/j.intcom.2007.11.004
   Creswell J. W., 2016, QUAL INQ
   Crowell CR, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3735, DOI 10.1109/IROS.2009.5354204
   Crumpton J, 2016, INT J SOC ROBOT, V8, P271, DOI 10.1007/s12369-015-0329-4
   Dahlbäck N, 2001, HUMAN-COMPUTER INTERACTION - INTERACT'01, P294
   DAHLBACK N, 1993, KNOWL-BASED SYST, V6, P258, DOI 10.1016/0950-7051(93)90017-N
   Dahlbäck N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1553
   Data for Democracy, 2019, GLOB DAT ETH PLEDG G
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Davis RO, 2019, COMPUT EDUC, V140, DOI 10.1016/j.compedu.2019.103605
   Donahue TJ, 2015, INT CONF COGN INFO, P397, DOI 10.1109/CogInfoCom.2015.7390626
   Dou X, 2021, INT J SOC ROBOT, V13, P615, DOI 10.1007/s12369-020-00654-9
   Drager KDR, 2010, AM J SPEECH-LANG PAT, V19, P259, DOI 10.1044/1058-0360(2010/09-0024)
   Elkins AC, 2013, GROUP DECIS NEGOT, V22, P897, DOI 10.1007/s10726-012-9339-x
   Elmlinger SL, 2019, J CHILD LANG, V46, P998, DOI 10.1017/S0305000919000291
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   Evans RE, 2010, INTERACT COMPUT, V22, P606, DOI 10.1016/j.intcom.2010.07.001
   Eyssel F., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P851, DOI 10.1109/ROMAN.2012.6343858
   Eyssel F, 2012, ACMIEEE INT CONF HUM, P125
   Feine J, 2019, INT J HUM-COMPUT ST, V132, P138, DOI 10.1016/j.ijhcs.2019.07.009
   Fessler Leah, 2017, Quartz
   Fields C, 2018, COGN SYST RES, V47, P186, DOI 10.1016/j.cogsys.2017.10.003
   Firestone W.A., 1993, Educational Researcher, V22, P16, DOI DOI 10.3102/0013189X022004016
   Fitch WT, 2017, PSYCHON B REV, V24, P3, DOI 10.3758/s13423-017-1236-5
   Fox Sarah, 2016, P 2016 CHI C HUM FAC, P3293, DOI DOI 10.1145/2851581
   Gangamohan P, 2016, INTEL SYST REF LIBR, V105, P205, DOI 10.1007/978-3-319-31056-5_11
   Ghazali AS, 2019, ACMIEEE INT CONF HUM, P586, DOI [10.1109/hri.2019.8673266, 10.1109/HRI.2019.8673266]
   Goble H, 2018, COMMUN RES REP, V35, P256, DOI 10.1080/08824096.2018.1447454
   Gong L, 2007, HUM COMMUN RES, V33, P163, DOI 10.1111/j.1468-2958.2007.00295.x
   Govind D, 2013, INT J SPEECH TECHNOL, V16, P237, DOI 10.1007/s10772-012-9180-2
   Gravano A, 2011, COMPUT SPEECH LANG, V25, P601, DOI 10.1016/j.csl.2010.10.003
   Grifoni P., 2009, MULTIMODAL HUMAN COM
   Grudin J, 2009, AI MAG, V30, P48, DOI 10.1609/aimag.v30i4.2271
   Gunkel D.J., 2012, Communication +1, V1, P1, DOI [10.7275/R5QJ7F7R, DOI 10.7275/R5QJ7F7R]
   Guzman AL, 2020, NEW MEDIA SOC, V22, P70, DOI 10.1177/1461444819858691
   Guzman AL, 2019, COMPUT HUM BEHAV, V90, P343, DOI 10.1016/j.chb.2018.08.009
   Guzman AL, 2017, SOCIALBOTS AND THEIR FRIENDS: DIGITAL MEDIA AND THE AUTOMATION OF SOCIALITY, P69
   Hall Edward Twitchell, 1974, Handbook for proxemic research
   Hare B, 2017, ANNU REV PSYCHOL, V68, P155, DOI 10.1146/annurev-psych-010416-044201
   Hennig S., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P589, DOI 10.1109/ROMAN.2012.6343815
   Heyvaert M, 2013, QUAL QUANT, V47, P659, DOI 10.1007/s11135-011-9538-6
   HOCKETT CF, 1960, SCI AM, V203, P88, DOI 10.1038/scientificamerican0960-88
   Hoegen R, 2019, PROCEEDINGS OF THE 19TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA' 19), P111, DOI 10.1145/3308532.3329473
   Howard A, 2018, SCI ENG ETHICS, V24, P1521, DOI 10.1007/s11948-017-9975-2
   Hyde JS, 2019, AM PSYCHOL, V74, P171, DOI 10.1037/amp0000307
   IEEE Robotics and Automation Society, 2019, ROBOT ETHICS
   JACOBY S, 1995, RES LANG SOC INTERAC, V28, P171, DOI 10.1207/s15327973rlsi2803_1
   James J, 2018, IEEE ROMAN, P632, DOI 10.1109/ROMAN.2018.8525652
   Jensen LA, 1996, QUAL HEALTH RES, V6, P553, DOI 10.1177/104973239600600407
   Jeong Y, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312913
   Johar S, 2015, EMOTION AFFECT PERSO
   Kim S, 2018, ACMIEEE INT CONF HUM, P151, DOI 10.1145/3173386.3176970
   Kitchenham B., 2004, Keele University, P1, DOI DOI 10.5144/0256-4947.2017.79
   Knote R, 2019, PROCEEDINGS OF THE 52ND ANNUAL HAWAII INTERNATIONAL CONFERENCE ON SYSTEM SCIENCES, P2024
   Komatsu T, 2011, INT J HUM-COMPUT INT, V27, P260, DOI 10.1080/10447318.2011.537209
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   Kreiman Jody, 2003, VOQUAL'03 Geneva, August 27-29, 2003, P115
   Krenn B, 2017, AI SOC, V32, P65, DOI 10.1007/s00146-014-0569-0
   Krosnick JA, 2010, HANDBOOK OF SURVEY RESEARCH, 2ND EDITION, P263
   Kumar Neha, 2019, Interactions, V26, P50, DOI DOI 10.1145/3305360
   Latinus M, 2011, CURR BIOL, V21, pR143, DOI 10.1016/j.cub.2010.12.033
   Lazzeri N, 2018, INT J ADV ROBOT SYST, V15, DOI 10.1177/1729881418783158
   Lee Eun Ju, 2000, P CHI 00 HUM FACT CO, P289, DOI [10.1145/633292.633461, DOI 10.1145/633292.633461]
   Lee KM, 2004, HUM COMMUN RES, V30, P182, DOI 10.1093/hcr/30.2.182
   Lee S. C., 2019, P 11 INT C AUT US IN, P209
   Lee S, 2019, MULTIMODAL TECHNOLOG, V3, DOI 10.3390/mti3010020
   Lewis M, 1998, AI MAG, V19, P67
   Li Gong, 2003, International Journal of Speech Technology, V6, P123, DOI 10.1023/A:1022382413579
   Lorber Judith, 1994, Paradoxes of gender, P32
   Lubold N, 2016, ACMIEEE INT CONF HUM, P255, DOI 10.1109/HRI.2016.7451760
   Mara M, 2020, ACMIEEE INT CONF HUM, P355, DOI 10.1145/3371382.3378285
   Marakas GM, 2000, INT J HUM-COMPUT ST, V52, P719, DOI 10.1006/ijhc.1999.0348
   MAYER RC, 1995, ACAD MANAGE REV, V20, P709, DOI 10.2307/258792
   Mayer RE, 2003, J EDUC PSYCHOL, V95, P419, DOI 10.1037/0022-0663.95.2.419
   McColl D, 2016, J INTELL ROBOT SYST, V82, P101, DOI 10.1007/s10846-015-0259-2
   McCrae R. R., 2008, Handbook of Personality: Theory and Research, V3rd, P159, DOI DOI 10.3905/JPE.2000.319978
   McDonald JD., 2008, Enquire, V1, P75
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   McTear Michael, 2016, The Conversational Interface: Talking to Smart Devices, DOI 10.1007/978-3-319-32967-3
   Mendelson J, 2017, INTERSPEECH, P249, DOI 10.21437/Interspeech.2017-1438
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Miller Blanca, 2019, Humanoid Robotics: A Reference, P2313, DOI 10.1007/978-94-007-7194-9_130-1
   Mitchell T, 2018, COMMUN ACM, V61, P103, DOI 10.1145/3191513
   Moher D, 2009, ANN INTERN MED, V151, P264, DOI [10.1016/j.ijsu.2010.02.007, 10.1136/bmj.i4086, 10.1016/j.ijsu.2010.07.299, 10.1186/2046-4053-4-1, 10.1371/journal.pmed.1000097, 10.1136/bmj.b2535, 10.1136/bmj.b2700]
   Molnár C, 2008, ANIM COGN, V11, P389, DOI 10.1007/s10071-007-0129-9
   Moreno R, 2001, COGNITION INSTRUCT, V19, P177, DOI 10.1207/S1532690XCI1902_02
   Mori M, 2012, IEEE ROBOT AUTOM MAG, V19, P98, DOI 10.1109/MRA.2012.2192811
   Moses DA, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-10994-4
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Murad C, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376522
   Murad Christine, 2020, P 2 C CONVERSATIONAL, P1, DOI [10.1145/3405755.3406137, DOI 10.1145/3405755.3406137]
   Nadal Kevin L., 2017, NEUROSEXISM, P1243
   Nass C, 1997, J APPL SOC PSYCHOL, V27, P864, DOI 10.1111/j.1559-1816.1997.tb00275.x
   Nass C., 2003, International Journal of Speech Technology, V6, P113, DOI 10.1023/A:1022378312670
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass Clifford, 2001, P AAAI S EM INT
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Nass Clifford, 2005, P HUM FACT COMP SYST, P1973, DOI [DOI 10.1145/1056808.1057070, 10.1145/1056808.1057070]
   Niculescu A., 2011, Proceedings of the 2011 International Conference on User Science and Engineering (i-USEr 2011), P18, DOI 10.1109/iUSEr.2011.6150529
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Nilsson N.J., 2009, The Quest for Artificial Intelligence
   Ohshima N, 2015, 2015 24TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P325, DOI 10.1109/ROMAN.2015.7333677
   Ohta Kengo, 2014, INT J COMPUTERS, V8, P136
   Parviainen E, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376187
   Pfeifer R., 2001, Understanding Intelligence.
   Pieraccini R, 2012, VOICE IN THE MACHINE: BUILDING COMPUTERS THAT UNDERSTAND SPEECH, P1
   Pittam Jeff, 1994, Voice in social interaction
   Polit DF, 2010, INT J NURS STUD, V47, P1451, DOI 10.1016/j.ijnurstu.2010.06.004
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Read R, 2013, ACMIEEE INT CONF HUM, P209, DOI 10.1109/HRI.2013.6483575
   Read R, 2012, ACMIEEE INT CONF HUM, P219
   Rode JA, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P123
   Rogers Yvonne, 2017, Research in the wild, DOI 10.1007/978-3-031-02220-3
   Rosenberg-Kima RB, 2007, LECT NOTES COMPUT SC, V4744, P214
   Rosenthal-von der Pütten AM, 2016, LECT NOTES ARTIF INT, V10011, P256, DOI 10.1007/978-3-319-47665-0_23
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Ryoko S., 2012, Proceedings of the 2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel & Distributed Computing (SNPD 2012), P19, DOI 10.1109/SNPD.2012.72
   Sandygulova A, 2018, INT J SOC ROBOT, V10, P687, DOI 10.1007/s12369-018-0472-9
   Sandygulova A, 2015, LECT NOTES ARTIF INT, V9388, P594, DOI 10.1007/978-3-319-25554-5_59
   Sarigul B, 2020, ACMIEEE INT CONF HUM, P430, DOI 10.1145/3371382.3378302
   Schlesinger A, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5412, DOI 10.1145/3025453.3025766
   Sezgin Emre, 2019, SSRN J, V10, P3, DOI [10.2139/ssrn.3381183, DOI 10.2139/SSRN.3381183]
   Shamekhi A, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3173965
   Shi Y, 2018, CHI 2018: EXTENDED ABSTRACTS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3170427.3188560
   Shiwa T., 2008, 2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI 2008), P153
   Siegel M, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2563, DOI 10.1109/IROS.2009.5354116
   Sims Valerie K., 2009, P HUMAN FACTORS ERGO, V53, P1418, DOI 10.1177/154193120905301853
   Soraa RA, 2017, GEND TECHNOL DEV, V21, P99, DOI 10.1080/09718524.2017.1385320
   STEELE CM, 1995, J PERS SOC PSYCHOL, V69, P797, DOI 10.1037/0022-3514.69.5.797
   Stern SE, 2006, INT J HUM-COMPUT ST, V64, P43, DOI 10.1016/j.ijhcs.2005.07.002
   Stern SE, 1999, HUM FACTORS, V41, P588, DOI 10.1518/001872099779656680
   Sutton SJ, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300833
   SWELLER J, 1988, COGNITIVE SCI, V12, P257, DOI 10.1207/s15516709cog1202_4
   Tamagawa R, 2011, INT J SOC ROBOT, V3, P253, DOI 10.1007/s12369-011-0100-4
   Tannen Deborah, 2005, Conversational style: Analyzing talk among friends
   Tay B, 2014, COMPUT HUM BEHAV, V38, P75, DOI 10.1016/j.chb.2014.05.014
   Tomasev N, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-15871-z
   Torre I, 2018, PROCEEDINGS OF THE TECHNOLOGY, MIND, AND SOCIETY CONFERENCE (TECHMINDSOCIETY'18), DOI 10.1145/3183654.3183691
   Torrey C, 2013, ACMIEEE INT CONF HUM, P275, DOI 10.1109/HRI.2013.6483599
   Trivers R., 2002, Natural selection and social theory
   Trovato Gabriele, 2017, Paladyn, Journal of Behavioral Robotics, V8, P1, DOI 10.1515/pjbr-2017-0001
   Tsiourti C, 2019, INT J SOC ROBOT, V11, P555, DOI 10.1007/s12369-019-00524-z
   Vannucci F., 2018, P IEEE RAS 18 INT C, P1, DOI DOI 10.1109/HUMANOIDS.2018.8625004
   Wada K, 2004, P IEEE, V92, P1780, DOI 10.1109/JPROC.2004.835378
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Whittlestone J., 2019, Ethical and Societal Implications of Algorithms, Data, and Artificial Intelligence: A Roadmap for Research
   Wieringa S, 2018, J EVAL CLIN PRACT, V24, P930, DOI 10.1111/jep.13010
   Wigdor N, 2016, IEEE ROMAN, P219, DOI 10.1109/ROMAN.2016.7745134
   Wintre MG, 2001, CAN PSYCHOL, V42, P216
   Xu K, 2019, NEW MEDIA SOC, V21, P2522, DOI 10.1177/1461444819851479
   Yarosh S, 2018, PROCEEDINGS OF THE 2018 ACM CONFERENCE ON INTERACTION DESIGN AND CHILDREN (IDC 2018), P300, DOI 10.1145/3202185.3202207
   Yilmazyildiz S, 2016, INT J HUM-COMPUT INT, V32, P63, DOI 10.1080/10447318.2015.1093856
   Yilmazyildiz S, 2015, MULTIMED TOOLS APPL, V74, P9959, DOI 10.1007/s11042-014-2165-1
   Yohanan Steve, 2005, Proceedings of the 7th international conference on Multimodal interfaces, P222, DOI DOI 10.1145/1088463.1088502
   Yu Q, 2019, CHI EA '19 EXTENDED ABSTRACTS: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290607.3312918
   Zaga C, 2016, ACMIEEE INT CONF HUM, P541, DOI 10.1109/HRI.2016.7451846
   Zanbaka C., 2006, Conference on Human Factors in Computing Systems. CHI2006, P1153
   Zimman L, 2018, LANG LINGUIST COMPAS, V12, DOI 10.1111/lnc3.12284
NR 198
TC 40
Z9 43
U1 46
U2 149
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 0360-0300
EI 1557-7341
J9 ACM COMPUT SURV
JI ACM Comput. Surv.
PD JUL
PY 2021
VL 54
IS 4
AR 81
DI 10.1145/3386867
PG 43
WC Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA TF3DS
UT WOS:000670593100001
OA Bronze
DA 2024-01-09
ER

PT J
AU Rodero, E
   Lucas, I
AF Rodero, Emma
   Lucas, Ignacio
TI Synthetic versus human voices in audiobooks: The human emotional
   intimacy effect
SO NEW MEDIA & SOCIETY
LA English
DT Article; Early Access
DE Audiobooks; cognitive processing; human voice; perception; physiological
   response; synthetic voice; talking books
ID EVOKED IMAGERY; RADIO; TRANSPORTATION; ATTENTION; SPEECH;
   PERSUASIVENESS; RECALL; MEMORY; CUES; AGE
AB Human voices narrate most audiobooks, but the fast development of speech synthesis technology has enabled the possibility of using artificial voices. This raises the question of whether the listeners' cognitive processing is the same when listening to a synthetic or a human voice telling a story. This research aims to compare the listeners' perception, creation of mental images, narrative engagement, physiological response, and recognition of information when listening to stories conveyed by human and synthetic voices. The results showed that listeners enjoyed stories narrated by a human voice more than a synthetic one. Also, they created more mental images, were more engaged, paid more attention, had a more positive emotional response, and remembered more information. Speech synthesis has experienced considerable progress. However, there are still significant differences versus human voices, so that using them to narrate long stories, such as audiobooks do, is difficult.
C1 [Rodero, Emma] Pompeu Fabra Univ UPF, Dept Commun, Media Psychol Lab, Barcelona, Spain.
   [Lucas, Ignacio] Inst Invest Biomed Bellvitge IDIBELL, Neurosci Program, Psychiat & Mental Hlth Grp, Barcelona, Spain.
C3 Pompeu Fabra University; Institut d'Investigacio Biomedica de Bellvitge
   (IDIBELL)
RP Rodero, E (corresponding author), Pompeu Fabra Univ UPF, Dept Commun, Roc Boronat 138, Barcelona 08108, Spain.
EM emma.rodero@upf.edu
RI Lucas Adell, Ignacio/P-9102-2016
OI Lucas Adell, Ignacio/0000-0001-9426-5082
FU Leonardo Grants for Researchers and Cultural Creators, BBVA, Spain
FX The author(s) disclosed receipt of the following financial support for
   the research, authorship, and/or publication of this article: This
   research was supported by Leonardo Grants for Researchers and Cultural
   Creators, BBVA, Spain.
CR [Anonymous], 2012, PSYCHOPHYSIOLOGICAL, DOI DOI 10.4324/9780203181027
   [Anonymous], 2005, MORE ONE VOICE PHILO
   Assmann P., P 9 INT C SPOK LANG
   Audio Publishers Association (APA), 2021, VOIC IND
   Babin LA, 1998, PSYCHOL MARKET, V15, P261, DOI 10.1002/(SICI)1520-6793(199805)15:3<261::AID-MAR4>3.3.CO;2-X
   Barker P., 2015, VOICE STUDIES CRITIC, P16
   Barthes Roland, 1985, Responsibility of forms: Critical essays on music, art, and representation
   Bilandzic H, 2008, J COMMUN, V58, P508, DOI 10.1111/j.1460-2466.2008.00397.x
   Bolls PD, 2007, J ADVERTISING, V36, P35, DOI 10.2753/JOA0091-3367360403
   Bolls PD, 2003, MEDIA PSYCHOL, V5, P33, DOI 10.1207/S1532785XMEP0501_2
   BONE PF, 1992, J CONSUM RES, V19, P93, DOI 10.1086/209289
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Chen F., 2006, Designing human interface in speech technology
   Chion M., 2019, Audio-Vision: Sound on Screen
   Connor Stephen, 2000, Dumbstruck. A Cultural History of Ventriloquism
   Craig SD, 2019, J EDUC COMPUT RES, V57, P1534, DOI 10.1177/0735633118802877
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   Delogu C, 1998, SPEECH COMMUN, V24, P153, DOI 10.1016/S0167-6393(98)00009-0
   Di Matteo P., 2015, VOICE STUDIES CRITIC, P104
   Edison Research, 2020, SMART AUD REP 2020
   Edison Research, 2019, INF DIAL 2019
   ELLEN PS, 1991, ADV CONSUM RES, V18, P806
   Goossens C., 1994, Journal of Mental Imagery, V18, P119
   Green MC, 2000, J PERS SOC PSYCHOL, V79, P701, DOI 10.1037/0022-3514.79.5.701
   Green MC, 2004, COMMUN THEOR, V14, P311, DOI 10.1111/j.1468-2885.2004.tb00317.x
   Have I, 2020, NEW MEDIA SOC, V22, P409, DOI 10.1177/1461444819863407
   Humphry J, 2021, NEW MEDIA SOC, V23, P1971, DOI 10.1177/1461444820923679
   JENKINS JJ, 1982, B PSYCHONOMIC SOC, V20, P203
   Kosslyn S. M., 1994, IMAGE BRAIN RESOLUTI
   Kosslyn SM, 2001, NAT REV NEUROSCI, V2, P635, DOI 10.1038/35090055
   Lai J., P SIGCHI C HUM FACT, P321
   Lang A, 2015, COMMUN RES, V42, P759, DOI 10.1177/0093650213490722
   LUCE PA, 1983, HUM FACTORS, V25, P17, DOI 10.1177/001872088302500102
   Luce PA., 1981, 7 IND U SPEECH RES L, P229
   Mar RA, 2008, PERSPECT PSYCHOL SCI, V3, P173, DOI 10.1111/j.1745-6924.2008.00073.x
   Mayer R. E., 2014, The Cambridge handbook of multimedia learning, V2, P345
   Miller DW, 1997, PSYCHOL MARKET, V14, P337, DOI 10.1002/(SICI)1520-6793(199707)14:4<337::AID-MAR3>3.0.CO;2-A
   Mulac A, 1996, HEALTH COMMUN, V8, P199, DOI 10.1207/s15327027hc0803_2
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   PAIVIO A, 1991, CAN J PSYCHOL, V45, P255, DOI 10.1037/h0084295
   Paris CR, 2000, HUM FACTORS, V42, P421, DOI 10.1518/001872000779698132
   Parker B., 2013, SHOULD YOU HIRE COMP
   Pisoni DB., IEEE INT C AC SPEECH, P572
   Rodero E, 2020, UNESCO COURIER, V1, P18
   Rodero E, 2017, COMPUT HUM BEHAV, V77, P336, DOI 10.1016/j.chb.2017.08.044
   Rodero E, 2017, HUM COMMUN RES, V43, P397, DOI 10.1111/hcre.12109
   Rodero E, 2016, MEDIA PSYCHOL, V19, P224, DOI 10.1080/15213269.2014.1002942
   Rodero E, 2012, COMMUN RES, V39, P458, DOI 10.1177/0093650210386947
   Roring RW, 2007, HUM FACTORS, V49, P25, DOI 10.1518/001872007779598055
   Sanderman AA, 1997, LANG SPEECH, V40, P391, DOI 10.1177/002383099704000405
   Stern SE, 1999, HUM FACTORS, V41, P588, DOI 10.1518/001872099779656680
   Syrdal A. K., 1994, APPL SPEECH TECHNOLO
   Taake KP., 2009, THESIS WASHINGTON U
   Thoet A., 2017, SHORT HIST AUDIOBOOK
   vanDommelen WA, 1995, LANG SPEECH, V38, P267, DOI 10.1177/002383099503800304
   Wallin ET, 2020, NEW MEDIA SOC, V22, P470, DOI 10.1177/1461444819864691
   Winters SJ., 2004, RES SPOKEN LANGUAGE
   Wolters MK, 2015, J AM MED INFORM ASSN, V22, P35, DOI 10.1136/amiajnl-2014-002820
   Xu K, 2019, NEW MEDIA SOC, V21, P2522, DOI 10.1177/1461444819851479
NR 59
TC 4
Z9 4
U1 20
U2 47
PU SAGE PUBLICATIONS LTD
PI LONDON
PA 1 OLIVERS YARD, 55 CITY ROAD, LONDON EC1Y 1SP, ENGLAND
SN 1461-4448
EI 1461-7315
J9 NEW MEDIA SOC
JI New Media Soc.
PD 2021 JUN 28
PY 2021
AR 14614448211024142
DI 10.1177/14614448211024142
EA JUN 2021
PG 19
WC Communication
WE Social Science Citation Index (SSCI)
SC Communication
GA TD4VP
UT WOS:000669326300001
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Birkholz, P
   Drechsel, S
AF Birkholz, Peter
   Drechsel, Susanne
TI Effects of the piriform fossae, transvelar acoustic coupling, and
   laryngeal wall vibration on the naturalness of articulatory speech
   synthesis
SO SPEECH COMMUNICATION
LA English
DT Article
DE Transvelar coupling; Piriform fossae; Vocal tract model
ID VOCAL-TRACT; HYPOPHARYNGEAL CAVITIES; MODEL; FREQUENCY; SIMULATION;
   SEQUENCES; PHONOLOGY; SINUSES; TONGUE; LENGTH
AB Acoustic models of the vocal tract for articulatory speech synthesis often neglect a range of acoustic effects that are known to exist in the human vocal tract. Here we extended a basic acoustic vocal tract model by three features: the piriform fossae, transvelar acoustic coupling of the oral and nasal cavities, and sound radiation from the skin of the neck. The main goal was to find out how these features affect the naturalness of the synthesized speech. To this end, ten German words were synthesized with different combinations of the additional features, and listeners compared the naturalness of these stimuli. Surprisingly, all three features reduced the perceived naturalness, although they should make the synthesis more realistic. A closer analysis revealed that all new features emphasized the low frequencies compared to the high frequencies of the synthetic speech, leading to slightly more muffled speech with the used glottal excitation. An additional perception experiment with synthetic stimuli with a slightly more tense voice revealed no perceptual preference for the synthesis with or without the piriform fossae. These results indicate that the examined features play a minor role for the naturalness of articulatory synthesis compared to the voice source characteristics.
C1 [Birkholz, Peter; Drechsel, Susanne] Tech Univ Dresden, Inst Acoust & Speech Commun, Dresden, Germany.
C3 Technische Universitat Dresden
RP Birkholz, P (corresponding author), Tech Univ Dresden, Inst Acoust & Speech Commun, Dresden, Germany.
EM peter.birkholz@tu-dresden.de
CR Alexander R, 2019, J ACOUST SOC AM, V146, P4458, DOI 10.1121/1.5139413
   [Anonymous], 2005, 3D ARTIKULATORISCHE
   [Anonymous], 2011, P INTERSPEECH
   Badin P, 2002, J PHONETICS, V30, P533, DOI 10.1006/jpho.2002.0166
   Beautemps D, 2001, J ACOUST SOC AM, V109, P2165, DOI 10.1121/1.1361090
   Birkholz P., 2014, P 10 INT SEM SPEECH, P37
   Birkholz P., 2004, P INTERSPEECH 2004, P1125, DOI [10.21437/Interspeech.2004-409, DOI 10.21437/INTERSPEECH.2004-409]
   Birkholz P., 2011, STUDIENTEXTE SPRACHK, P47
   Birkholz P., 2007, 8 ANN C INT SPEECH C, P2865
   Birkholz P, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00597-w
   Birkholz P, 2019, INTERSPEECH, P3765, DOI 10.21437/Interspeech.2019-2410
   Birkholz P, 2019, SPEECH COMMUN, V110, P108, DOI 10.1016/j.specom.2019.04.009
   Birkholz P, 2017, COMPUT SPEECH LANG, V41, P116, DOI 10.1016/j.csl.2016.06.004
   Birkholz P, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060603
   Birkholz P, 2011, IEEE T AUDIO SPEECH, V19, P1422, DOI 10.1109/TASL.2010.2091632
   Birkhoz P, 2007, IEEE T AUDIO SPEECH, V15, P1218, DOI 10.1109/TASL.2006.889731
   Blandin R, 2015, J ACOUST SOC AM, V137, P832, DOI 10.1121/1.4906166
   Bouabana S, 1998, SPEECH COMMUN, V24, P227, DOI 10.1016/S0167-6393(98)00012-0
   BROWMAN CP, 1992, PHONETICA, V49, P155, DOI 10.1159/000261913
   Cranen B, 1996, SPEECH COMMUN, V19, P1, DOI 10.1016/0167-6393(96)00016-7
   Dang J, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P965, DOI 10.1109/ICSLP.1996.607763
   Dang JW, 2016, J ACOUST SOC AM, V139, P441, DOI 10.1121/1.4939964
   Dang JW, 1997, J ACOUST SOC AM, V101, P456, DOI 10.1121/1.417990
   Dang JW, 2004, J ACOUST SOC AM, V115, P853, DOI 10.1121/1.1639325
   Dang JW, 1996, J ACOUST SOC AM, V100, P3374, DOI 10.1121/1.416978
   Delvaux B, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102680
   Deng L, 1998, SPEECH COMMUN, V24, P299, DOI 10.1016/S0167-6393(98)00023-5
   Elie B, 2016, SPEECH COMMUN, V82, P85, DOI 10.1016/j.specom.2016.06.002
   Erath BD, 2013, SPEECH COMMUN, V55, P667, DOI 10.1016/j.specom.2013.02.002
   Fant G., 1976, STL QPSR, V4, P13
   FLANAGAN JL, 1975, AT&T TECH J, V54, P485, DOI 10.1002/j.1538-7305.1975.tb02852.x
   Fleischer M, 2015, BIOMECH MODEL MECHAN, V14, P719, DOI 10.1007/s10237-014-0632-2
   Freixes M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9214535
   Fujita S, 2005, ACOUST SCI TECHNOL, V26, P353, DOI 10.1250/ast.26.353
   Godoy E, 2016, INTERSPEECH, P948, DOI 10.21437/Interspeech.2016-1362
   ISHIZAKA K, 1972, AT&T TECH J, V51, P1233, DOI 10.1002/j.1538-7305.1972.tb02651.x
   Iskarous K, 2003, INT C PHON SCI BARC, P185
   Kitamura T, 2005, ACOUST SCI TECHNOL, V26, P16, DOI 10.1250/ast.26.16
   Kröger BJ, 2004, HNO, V52, P837, DOI 10.1007/s00106-004-1097-x
   KROGER BJ, 1993, PHONETICA, V50, P213
   KROGER BJ, 1998, PHONETISCHES MODELL
   Liu Li-Juan, 2017, BLIZZ CHALL WORKSH
   MAEDA S, 1990, NATO ADV SCI I D-BEH, V55, P131
   Maeda S., 1982, Speech Communication, V1, P199, DOI 10.1016/0167-6393(82)90017-6
   Meltzner GS, 2003, J ACOUST SOC AM, V114, P1035, DOI 10.1121/1.1582440
   MERMELSTEIN P, 1973, J ACOUST SOC AM, V53, P1070, DOI 10.1121/1.1913427
   Monson BB, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00587
   Murphy D.T., 2015, 18 INT C DIG AUD EFF, P1
   Okadome T, 2001, J ACOUST SOC AM, V110, P453, DOI 10.1121/1.1377633
   Payan Y, 1997, SPEECH COMMUN, V22, P185, DOI 10.1016/S0167-6393(97)00019-8
   Piepho HP, 2018, AGRON J, V110, P431, DOI 10.2134/agronj2017.10.0580
   Pont A, 2020, INT J NUMER METH BIO, V36, DOI 10.1002/cnm.3302
   Saltzman EL., 1989, Ecological Psychology, V1, P333, DOI DOI 10.1207/S15326969EC00104_
   Shadle C. H., 2001, 4 ISCA TUTORIAL RES, P121
   Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779, DOI 10.1109/ICASSP.2018.8461368
   SONDHI MM, 1987, IEEE T ACOUST SPEECH, V35, P955, DOI 10.1109/TASSP.1987.1165240
   Stavness I, 2011, INT J NUMER METH BIO, V27, P367, DOI 10.1002/cnm.1423
   Stevens K., 1998, Acoustic phonetics
   Stone S, 2018, IEEE-ACM T AUDIO SPE, V26, P1381, DOI 10.1109/TASLP.2018.2825601
   Story BH, 2019, J ACOUST SOC AM, V146, P2522, DOI 10.1121/1.5127756
   Story BH, 2018, J ACOUST SOC AM, V143, P3079, DOI 10.1121/1.5038264
   Suzuki H., 1990, 1 INT C SPOK LANG PR, P437
   Svec JG, 2005, J ACOUST SOC AM, V117, P1386, DOI 10.1121/1.1850074
   Takemoto H, 2013, J ACOUST SOC AM, V134, P2955, DOI 10.1121/1.4818744
   Takemoto H, 2010, J ACOUST SOC AM, V128, P3724, DOI 10.1121/1.3502470
   Teixeira AJS, 2005, EURASIP J APPL SIG P, V2005, P1435, DOI 10.1155/ASP.2005.1435
   TITZE IR, 1989, SPEECH COMMUN, V8, P191, DOI 10.1016/0167-6393(89)90001-0
   Toutios A, 2011, J ACOUST SOC AM, V129, P3245, DOI 10.1121/1.3569714
   Vampola T, 2020, J ACOUST SOC AM, V148, P3218, DOI 10.1121/10.0002487
   Vampola T, 2015, ACTA ACUST UNITED AC, V101, P594, DOI 10.3813/AAA.918855
   van den Doel K, 2008, IEEE T AUDIO SPEECH, V16, P1163, DOI 10.1109/TASL.2008.2001107
   Wu L, 2014, J ACOUST SOC AM, V136, P350, DOI 10.1121/1.4883355
   Xu Y, 2006, ITAL J LINGUIST, V18, P125
   Zhang C., 2016, P 15 INT C COMPUTER, P1, DOI [10.1109/MESA.2016.7587178, DOI 10.1109/ICIS.2016.7550911]
   Zhang J, 2019, J ACOUST SOC AM, V145, P734, DOI 10.1121/1.5089220
NR 75
TC 5
Z9 6
U1 0
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-6393
EI 1872-7182
J9 SPEECH COMMUN
JI Speech Commun.
PD SEP
PY 2021
VL 132
BP 96
EP 105
DI 10.1016/j.specom.2021.06.002
EA JUN 2021
PG 10
WC Acoustics; Computer Science, Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Acoustics; Computer Science
GA TT5VK
UT WOS:000680415400009
DA 2024-01-09
ER

PT J
AU Lu, L
   Zhang, P
   Zhang, TT
AF Lu, Lu
   Zhang, Pei
   Zhang, Tingting (Christina)
TI Leveraging "<i>human</i>-<i>likeness</i>" of robotic service at
   restaurants
SO INTERNATIONAL JOURNAL OF HOSPITALITY MANAGEMENT
LA English
DT Article
DE Service robot; Human-likeness; Physical appearance; Voice; Language;
   Restaurant
ID UNCANNY VALLEY; BEHAVIORAL INTENTIONS; SOCIAL ROBOTS; ANTHROPOMORPHISM;
   ACCEPTANCE; EXPERIENCES; APPEARANCE; QUALITY; STYLES
AB Despite the rise of human-robot interaction research, the mixed findings of human-likeness in consumer evaluation exist. Focusing on the restaurant sector, this research investigates how service robots' varying levels of human-likeness of attributes (i.e., visual, vocal and verbal) influence consumption outcomes (e.g., service encounter evaluation, revisit intentions and positive word of mouth intentions) and the underlying mechanisms through cognition (i.e., perceived credibility) and positive emotion per Appraisal Theory. Drawing on a consumer experiment involving a total of 587 participants, results suggest that humanlike voice emerges as a dominant attribute affecting all three consumption outcomes. Humanlike language style positively affects service encounter evaluation but barely affects the other two outcomes. The significant effect of humanlike voice on three consumption outcomes is only explained by positive emotion whereas the effect of humanlike language style on service encounter evaluation is explained by both cognition (i.e., perceived credibility) and emotion.
C1 [Lu, Lu] Temple Univ, Sch Sport Tourism & Hospitality Management, 1810 N 13th St,Speakman Hall 329, Philadelphia, PA 19122 USA.
   [Zhang, Pei] Univ Kentucky, Dept Retailing & Tourism Management, Coll Agr Food & Environm, Erikson Hall, Lexington, KY 40506 USA.
   [Zhang, Tingting (Christina)] Univ Cent Florida, Rosen Coll Hospitality Management, 9907 Universal Blvd, Orlando, FL 32821 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple
   University; University of Kentucky; State University System of Florida;
   University of Central Florida
RP Lu, L (corresponding author), Temple Univ, Sch Sport Tourism & Hospitality Management, 1810 N 13th St,Speakman Hall 329, Philadelphia, PA 19122 USA.
EM lu.lu0001@temple.edu; pei.zhang@uky.edu; tingting.zhang@ucf.edu
RI Lu, Lu/AAR-8010-2021
OI Lu, Lu/0000-0001-9852-3560
CR [Anonymous], 1995, J SERV MARK
   Bartneck C, 2009, INT J SOC ROBOT, V1, P71, DOI 10.1007/s12369-008-0001-3
   Bartsch S., 2008, CURRENT RES QUESTION, P45
   Belk R, 2016, RECH APPL MARKET-ENG, V31, P83, DOI 10.1177/2051570716658467
   Berezina K, 2019, ROBOTS, ARTIFICIAL INTELLIGENCE, AND SERVICE AUTOMATION IN TRAVEL, TOURISM AND HOSPITALITY, P185, DOI 10.1108/978-1-78756-687-320191010
   Bischoff R., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P999, DOI 10.1109/ICSMC.1999.825399
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Burgers A, 2000, INT J SERV IND MANAG, V11, P142, DOI 10.1108/09564230010323642
   Choi S, 2019, INT J HOSP MANAG, V82, P32, DOI 10.1016/j.ijhm.2019.03.026
   Choi Y, 2021, CURR ISSUES TOUR, V24, P717, DOI 10.1080/13683500.2020.1735318
   Conti D, 2017, INT J SOC ROBOT, V9, P51, DOI 10.1007/s12369-016-0359-6
   Dautenhahn K., 2014, The Encyclopedia of Human -Computer Interaction, V2
   Duffy BR, 2003, ROBOT AUTON SYST, V42, P177, DOI 10.1016/S0921-8890(02)00374-3
   Edwards C, 2019, COMPUT HUM BEHAV, V90, P357, DOI 10.1016/j.chb.2018.08.027
   Epley N, 2007, PSYCHOL REV, V114, P864, DOI 10.1037/0033-295X.114.4.864
   Eyssel F., 2011, 2011 RO-MAN: The 20th IEEE International Symposium on Robot and Human Interactive Communication, P467, DOI 10.1109/ROMAN.2011.6005233
   Eyssel F, 2012, ACMIEEE INT CONF HUM, P125
   Ferrari F, 2016, INT J SOC ROBOT, V8, P287, DOI 10.1007/s12369-016-0338-y
   Fink Julia, 2012, Social Robotics. 4th International Conference (ICSR 2012). Proceedings, P199, DOI 10.1007/978-3-642-34103-8_20
   Fong T, 2003, ROBOT AUTON SYST, V42, P143, DOI 10.1016/S0921-8890(02)00372-X
   Gilbody-Dickerson C., 2019, RESTAURANT WAITERS H
   Gockley R., 2006, 1st Annual Conference on Human-Robot Interaction, P186
   Goetz J, 2003, RO-MAN 2003: 12TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P55
   Goudey A, 2016, RECH APPL MARKET-ENG, V31, P2, DOI 10.1177/2051570716643961
   Graefe A., 2016, Guide to Automated Journalism, DOI DOI 10.7916/D80G3XDJ
   Hebesberger D, 2017, INT J SOC ROBOT, V9, P417, DOI 10.1007/s12369-016-0391-6
   Ho TH, 2020, INT J HOSP MANAG, V87, DOI 10.1016/j.ijhm.2020.102501
   Hosany S, 2012, J TRAVEL RES, V51, P303, DOI 10.1177/0047287511410320
   Hutchinson J, 2009, TOURISM MANAGE, V30, P298, DOI 10.1016/j.tourman.2008.07.010
   Ivanov S., 2017, Revista Turismo & Desenvolvimento, P1501
   Jensen ML, 2013, J MANAGE INFORM SYST, V30, P293, DOI 10.2753/MIS0742-1222300109
   Kim SY, 2019, MARKET LETT, V30, P1, DOI 10.1007/s11002-019-09485-9
   Larivière B, 2017, J BUS RES, V79, P238, DOI 10.1016/j.jbusres.2017.03.008
   LAZARUS RS, 1991, AM PSYCHOL, V46, P352, DOI 10.1037/0003-066X.46.4.352
   Lee KM, 2005, HUM COMMUN RES, V31, P538, DOI 10.1111/j.1468-2958.2005.tb00882.x
   Lee M, 2019, INT J CONTEMP HOSP M, V31, P4313, DOI 10.1108/IJCHM-03-2018-0263
   Lin HX, 2020, J HOSP MARKET MANAG, V29, P530, DOI 10.1080/19368623.2020.1685053
   Lu L, 2019, INT J HOSP MANAG, V80, P36, DOI 10.1016/j.ijhm.2019.01.005
   Luna N., 2020, IS PENNY ROBOT FOOD
   Luo XM, 2019, MARKET SCI, V38, P937, DOI 10.1287/mksc.2019.1192
   Mathur M. B., 2009, 2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI), P313
   Matthews K, 2020, PANDEMIC PROVES UTIL
   Mende M, 2019, J MARKETING RES, V56, P535, DOI 10.1177/0022243718822827
   Montoya RM, 2017, PSYCHOL BULL, V143, P459, DOI 10.1037/bul0000085
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Murphy J, 2019, J TRAVEL TOUR MARK, V36, P784, DOI 10.1080/10548408.2019.1571983
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Nickson Dennis, 2005, Managing Service Quality, V15, P195
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Perfect TJ, 2002, APPL COGNITIVE PSYCH, V16, P973, DOI 10.1002/acp.920
   Phillips E, 2018, ACMIEEE INT CONF HUM, P105, DOI 10.1145/3171221.3171268
   Poliakoff E, 2013, PERCEPTION, V42, P998, DOI 10.1068/p7569
   Pouliot L., 2016, ROBOTS RESTAURANTS F
   Prakash A, 2015, INT J SOC ROBOT, V7, P309, DOI 10.1007/s12369-014-0269-4
   Prentice C, 2020, J HOSP MARKET MANAG, V29, P739, DOI 10.1080/19368623.2020.1722304
   Qiu HL, 2020, J HOSP MARKET MANAG, V29, P247, DOI 10.1080/19368623.2019.1645073
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Roseman Ira J., 2001, Appraisal Processes in Emotion: Theory, Methods, Research, P68
   Ryu K. S., 2007, Journal of Hospitality & Tourism Research, V31, P56, DOI 10.1177/1096348006295506
   Seiter JS, 2020, INT J HOSP MANAG, V84, DOI 10.1016/j.ijhm.2019.102320
   Shnabel N, 2013, PERS SOC PSYCHOL B, V39, P663, DOI 10.1177/0146167213480816
   Singer, 2016, ARTIFICIAL INTELLIGE
   SMITH CA, 1993, COGNITION EMOTION, V7, P233, DOI 10.1080/02699939308409189
   Sugiura K, 2015, ADV ROBOTICS, V29, P449, DOI 10.1080/01691864.2015.1009164
   SURPRENANT CF, 1987, J MARKETING, V51, P86, DOI 10.2307/1251131
   Tamagawa R, 2011, INT J SOC ROBOT, V3, P253, DOI 10.1007/s12369-011-0100-4
   Trovato G, 2015, 2015 24TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION (RO-MAN), P235, DOI 10.1109/ROMAN.2015.7333573
   Tsaur SH, 2015, INT J HOSP MANAG, V51, P115, DOI 10.1016/j.ijhm.2015.08.015
   Tung VWS, 2017, INT J CONTEMP HOSP M, V29, P2498, DOI 10.1108/IJCHM-09-2016-0520
   Tussyadiah I, 2020, ANN TOURISM RES, V81, DOI 10.1016/j.annals.2020.102883
   Tussyadiah SP, 2018, TOURISM MANAGE, V67, P261, DOI 10.1016/j.tourman.2018.02.002
   van Pinxteren MME, 2019, J SERV MARK, V33, P507, DOI 10.1108/JSM-01-2018-0045
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   Walters ML, 2008, AUTON ROBOT, V24, P159, DOI 10.1007/s10514-007-9058-3
   Wan J., 2015, STRONG BRANDS STRONG, P119, DOI DOI 10.4324/9781315767079
   Webster C., 2017, ADOPTION ROBOTS ARTI
   Wirtz J, 2018, J SERV MANAGE, V29, P907, DOI 10.1108/JOSM-04-2018-0119
   Yamada Y, 2013, JPN PSYCHOL RES, V55, P20, DOI 10.1111/j.1468-5884.2012.00538.x
   Youngtaek Kim, 2011, 2011 International Symposium on Low Power Electronics and Design (ISLPED 2011), P253, DOI 10.1109/ISLPED.2011.5993645
   Yu CE, 2020, J HOSP MARKET MANAG, V29, P22, DOI 10.1080/19368623.2019.1592733
   Zeng ZJ, 2020, TOURISM GEOGR, V22, P724, DOI 10.1080/14616688.2020.1762118
   Zhang T, 2008, IEEE INT CON AUTO SC, P674, DOI 10.1109/COASE.2008.4626532
   Zhu DH, 2020, INT J CONTEMP HOSP M, V32, P1367, DOI 10.1108/IJCHM-10-2019-0904
   Zlotowski J, 2015, INT J SOC ROBOT, V7, P347, DOI 10.1007/s12369-014-0267-6
NR 84
TC 72
Z9 74
U1 53
U2 224
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0278-4319
EI 1873-4693
J9 INT J HOSP MANAG
JI Int. J. Hosp. Manag.
PD APR
PY 2021
VL 94
AR 102823
DI 10.1016/j.ijhm.2020.102823
PG 9
WC Hospitality, Leisure, Sport & Tourism
WE Social Science Citation Index (SSCI)
SC Social Sciences - Other Topics
GA RA2OQ
UT WOS:000631257900026
DA 2024-01-09
ER

PT J
AU Hu, P
   Lu, YB
   Gong, YM
AF Hu, Peng
   Lu, Yaobin
   Gong, Yeming (Yale)
TI Dual humanness and trust in conversational AI: A person-centered
   approach
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Artificial intelligence; Humanness perception; Trust; Person-centered
   approach; Finite mixture modeling
ID EXPLORATORY FACTOR-ANALYSIS; LATENT PROFILE ANALYSIS; SOCIAL RESPONSES;
   ANTHROPOMORPHISM; MODELS; ROBOTS; INFORMATION; VARIABLES; IDENTITY;
   ALEXA
AB Conversational Artificial Intelligence (AI) is digital agents that interact with users by natural language. To advance the understanding of trust in conversational AI, this study focused on two humanness factors manifested by conversational AI: speaking and listening. First, we explored users? heterogeneous perception patterns based on the two humanness factors. Next, we examined how this heterogeneity relates to trust in conversational AI. A two-stage survey was conducted to collect data. Latent profile analysis revealed three distinct patterns: parahuman perception, para-machine perception, and asymmetric perception. Finite mixture modeling demonstrated that the benefit of humanizing AI?s voice for competence-related trust can evaporate once AI?s language understanding is perceived as poor. Interestingly, the asymmetry between humanness perceptions in speaking and listening can impede morality-related trust. By adopting a person-centered approach to address the relationship between dual humanness and user trust, this study contributes to the literature on trust in conversational AI and the practice of trust-inducing AI design.
C1 [Hu, Peng; Lu, Yaobin] Huazhong Univ Sci & Technol, Sch Management, Luoyu Rd 1037, Wuhan, Peoples R China.
   [Gong, Yeming (Yale)] EMLYON Business Sch, Business Intelligence Ctr, Yon, France.
C3 Huazhong University of Science & Technology; emlyon business school
RP Lu, YB (corresponding author), Huazhong Univ Sci & Technol, Sch Management, Luoyu Rd 1037, Wuhan, Peoples R China.
EM hpeng@hust.edu.cn; luyb@hust.edu.cn; gong@em-lyon.com
RI GONG, Yeming/I-7148-2012; Hu, Peng/HQZ-8723-2023
OI GONG, Yeming/0000-0001-9270-5507; Hu, Peng/0000-0003-1624-466X
FU National Natural Science Foundation of China [71810107003]; National
   Social Science Fund of China [18ZDA109]; Business Intelligence Center of
   EMLYON; Modern Information Management Research Center at HUST
FX This work was supported by the grants from National Natural Science
   Foundation of China (Project No. 71810107003) and National Social
   Science Fund of China (Project No. 18ZDA109) . This work was also
   supported by Business Intelligence Center of EMLYON and Modern
   Information Management Research Center at HUST.
CR Alashoor T. M., 2017, P 38 INT C INF SYST
   [Anonymous], COMMUNICATION Q
   Asparouhov T, 2014, STRUCT EQU MODELING, V21, P329, DOI 10.1080/10705511.2014.915181
   Bakk Z, 2016, STRUCT EQU MODELING, V23, P20, DOI 10.1080/10705511.2014.955104
   Braun M, 2019, J MULTIMODAL USER IN, V13, P71, DOI 10.1007/s12193-019-00301-2
   Califf CB, 2020, TECHNOL FORECAST SOC, V154, DOI 10.1016/j.techfore.2020.119968
   Chang RCS, 2018, COMPUT HUM BEHAV, V84, P194, DOI 10.1016/j.chb.2018.02.025
   Cheng HF, 2014, ELECTRON COMMER R A, V13, P1, DOI 10.1016/j.elerap.2013.07.002
   Cho E, 2019, CYBERPSYCH BEH SOC N, V22, P515, DOI 10.1089/cyber.2018.0571
   Culley KE, 2013, COMPUT HUM BEHAV, V29, P577, DOI 10.1016/j.chb.2012.11.023
   de Kleijn R, 2019, KNOWL-BASED SYST, V163, P794, DOI 10.1016/j.knosys.2018.10.006
   de Visser EJ, 2016, J EXP PSYCHOL-APPL, V22, P331, DOI 10.1037/xap0000092
   Demetis DS, 2018, J ASSOC INF SYST, V19, P929, DOI 10.17705/1jais.00514
   Dietvorst BJ, 2018, MANAGE SCI, V64, P1155, DOI 10.1287/mnsc.2016.2643
   Edwards C, 2019, COMPUT HUM BEHAV, V90, P357, DOI 10.1016/j.chb.2018.08.027
   Evermann J, 2011, J ASSOC INF SYST, V12, P632
   Fabrigar LR, 1999, PSYCHOL METHODS, V4, P272, DOI 10.1037/1082-989X.4.3.272
   Faraon M., 2019, P 2019 2 ART INT CLO
   Foehr J, 2020, J ASSOC CONSUM RES, V5, P181, DOI 10.1086/707731
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Gabriel AS, 2015, J APPL PSYCHOL, V100, P863, DOI 10.1037/a0037408
   Gefen D, 2003, MIS QUART, V27, P51, DOI 10.2307/30036519
   Gillath O, 2021, COMPUT HUM BEHAV, V115, DOI 10.1016/j.chb.2020.106607
   Go E, 2019, COMPUT HUM BEHAV, V97, P304, DOI 10.1016/j.chb.2019.01.020
   Gursoy D, 2019, INT J INFORM MANAGE, V49, P157, DOI 10.1016/j.ijinfomgt.2019.03.008
   Haas J.W., 1995, J BUS COMMUN, V32, P123, DOI DOI 10.1177/002194369503200202
   Henson RK, 2006, EDUC PSYCHOL MEAS, V66, P393, DOI 10.1177/0013164405282485
   HINKIN TR, 1995, J MANAGE, V21, P967, DOI 10.1177/014920639502100509
   Howard MC, 2018, ORGAN RES METHODS, V21, P846, DOI 10.1177/1094428117744021
   Khatri C, 2018, AI MAG, V39, P40, DOI 10.1609/aimag.v39i3.2810
   Klaus P, 2020, J SERV MARK, V34, P389, DOI 10.1108/JSM-01-2019-0043
   Lankton NK, 2015, J ASSOC INF SYST, V16, P880, DOI 10.17705/1jais.00411
   Lanza ST, 2013, STRUCT EQU MODELING, V20, P1, DOI 10.1080/10705511.2013.742377
   Lee KM, 2006, J COMMUN, V56, P754, DOI 10.1111/j.1460-2466.2006.00318.x
   Lee O. K. D, 2017, P 23 AM C INF SYST
   Lortie CL, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025085
   MacKenzie SB, 2011, MIS QUART, V35, P293
   Maedche A, 2018, P EUR C INF SYST ECI
   Marsh HW, 2009, STRUCT EQU MODELING, V16, P191, DOI 10.1080/10705510902751010
   Martin T, 2019, 73 DONT TRUST CONVER
   MAYER RC, 1995, ACAD MANAGE REV, V20, P709, DOI 10.2307/258792
   McKnight DH, 2002, INFORM SYST RES, V13, P334, DOI 10.1287/isre.13.3.334.81
   McKone E, 2001, J EXP PSYCHOL HUMAN, V27, P573, DOI 10.1037/0096-1523.27.3.573
   Meade AW, 2012, PSYCHOL METHODS, V17, P437, DOI 10.1037/a0028085
   Meyer JP, 2013, HUM RESOUR MANAGE R, V23, P190, DOI 10.1016/j.hrmr.2012.07.007
   Morin AJS, 2011, ORGAN RES METHODS, V14, P58, DOI 10.1177/1094428109356476
   Nass C, 2000, J SOC ISSUES, V56, P81, DOI 10.1111/0022-4537.00153
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Nylund KL, 2007, STRUCT EQU MODELING, V14, P535, DOI 10.1080/10705510701575396
   Nylund-Gibson K, 2014, STRUCT EQU MODELING, V21, P439, DOI 10.1080/10705511.2014.915375
   Olson P., 2019, NEW REPORT TACKLES T
   Peugh J, 2013, STRUCT EQU MODELING, V20, P616, DOI 10.1080/10705511.2013.824780
   Qiu LY, 2009, J MANAGE INFORM SYST, V25, P145, DOI 10.2753/MIS0742-1222250405
   Rhee CE, 2020, COMPUT HUM BEHAV, V109, DOI [10.1016/j.chb.2020.102659, 10.1016/j.chb.2020.106359]
   ROTTER JB, 1971, AM PSYCHOL, V26, P443, DOI 10.1037/h0031464
   Santos R, 2020, MULTIMED TOOLS APPL, V79, P35689, DOI 10.1007/s11042-020-08710-2
   Schuetz S, 2020, J ASSOC INF SYST, V21, P460, DOI 10.17705/1jais.00608
   Schuetzler RM, 2020, J MANAGE INFORM SYST, V37, P875, DOI 10.1080/07421222.2020.1790204
   Schwartz E. H., 2020, KIDS DONT TRUST VOIC
   Sheehan B, 2020, J BUS RES, V115, P14, DOI 10.1016/j.jbusres.2020.04.030
   Shin M, 2019, COMPUT HUM BEHAV, V94, P100, DOI 10.1016/j.chb.2019.01.016
   Silva AD, 2020, EXPERT SYST APPL, V147, DOI 10.1016/j.eswa.2020.113193
   Sonpar K, 2009, J BUS ETHICS, V90, P345, DOI 10.1007/s10551-009-0045-9
   Specht J, 2014, J PERS SOC PSYCHOL, V107, P540, DOI 10.1037/a0036863
   Srivastava SC, 2018, MIS QUART, V42, P779, DOI 10.25300/MISQ/2018/11914
   Strohmann T., 2019, AIS Transactions on Human-Computer Interaction, V11, P54, DOI 10.17705/1thci.00113
   Tamagawa R, 2011, INT J SOC ROBOT, V3, P253, DOI 10.1007/s12369-011-0100-4
   Torre I, 2020, COMPUT HUM BEHAV, V105, DOI 10.1016/j.chb.2019.106215
   Wang WQ, 2016, J MANAGE INFORM SYST, V33, P744, DOI 10.1080/07421222.2016.1243949
   Waytz A, 2014, J EXP SOC PSYCHOL, V52, P113, DOI 10.1016/j.jesp.2014.01.005
   Westerman D, 2020, COMMUN STUD, V71, P393, DOI 10.1080/10510974.2020.1749683
   Westerman D, 2019, COMMUN STUD, V70, P295, DOI 10.1080/10510974.2018.1557233
   Wiese E, 2020, INT J HUM-COMPUT ST, V133, P1, DOI 10.1016/j.ijhcs.2019.08.002
   Wise AF, 2019, COMPUT HUM BEHAV, V96, P273, DOI 10.1016/j.chb.2018.01.034
   Woo SE, 2018, ORGAN RES METHODS, V21, P814, DOI 10.1177/1094428117752467
   Xie H, 2020, INT J HUM-COMPUT INT, V36, P1095, DOI 10.1080/10447318.2020.1712061
   Xu K, 2019, NEW MEDIA SOC, V21, P2522, DOI 10.1177/1461444819851479
   Yang SQ, 2018, COMPUT HUM BEHAV, V89, P16, DOI 10.1016/j.chb.2018.07.032
   Zyphur MJ, 2009, ACAD MANAGE REV, V34, P677, DOI 10.5465/AMR.2009.44885862
NR 79
TC 25
Z9 25
U1 34
U2 187
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD JUN
PY 2021
VL 119
AR 106727
DI 10.1016/j.chb.2021.106727
EA FEB 2021
PG 18
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA RC2XB
UT WOS:000632667000002
OA Green Published
DA 2024-01-09
ER

PT J
AU Euler, HA
   Merkel, A
   Hente, K
   Neef, N
   von Gudenberg, AW
   Neumann, K
AF Euler, Harald A.
   Merkel, Anna
   Hente, Katja
   Neef, Nicole
   von Gudenberg, Alexander Wolff
   Neumann, Katrin
TI Speech restructuring group treatment for 6-to-9-year-old children who
   stutter: A therapeutic trial
SO JOURNAL OF COMMUNICATION DISORDERS
LA English
DT Article
DE Stuttering; Treatment; Effectiveness; Speech restructuring; Fluency
   shaping; Children
ID SCHOOL-AGE-CHILDREN; PHASE-II TRIAL; LIDCOMBE-PROGRAM; LONG-TERM;
   GRAMMATICAL COMPLEXITY; ADOLESCENTS; LENGTH; OUTCOMES; QUALITY; IMPACT
AB For children who stutter (CWS), there is good evidence of the benefits of treatment for pre-school age, but an evidence gap for elementary school age. Here we report on the effectiveness of a fluency shaping treatment for 6to 9-year-old children. The main treatment component is the reinforcement of soft voice onsets. An intensive in-patient group treatment phase lasts 6 days, followed by a 6-month maintenance phase with 3 in-patient weekend group refresher courses. Child and a parent participate together in various treatment activities. In this controlled intervention study (waitlist control, intention-to-treat design) assessments were performed before treatment (T1), 4 weeks after the intensive phase (T2), at the end of the maintenance phase (T3), and 1 year later (T4). Participants were 119 children (108 boys, 11 girls, age 5.5-10.4 years). Control conditions included a subgroup with delayed treatment (N=25) as well as the assessment of complexity of utterances, inter-rater reliability, and speech naturalness. From before treatment to 1-year follow-up, percent stuttered syllables and OASES-S (Overall Assessment of the Speaker's Experience with Stuttering School-age) scores decreased with large effect size. Speech naturalness improved during this period but did not reach the level of non-stuttering children. Complexity of utterances increased during the intensive phase, but only temporarily. Twenty children (16.8 %, including dropouts) showed no demonstrable treatment benefit. Fluency shaping treatment can be effectively applied to young school children. It is assumed that parental support, group therapy, intensive treatment, and regular exercises at home are essential.
C1 [Euler, Harald A.; Neumann, Katrin] Westphalian Wilhelms Univ Munster, Univ Hosp Munster, Dept Phoniatr & Pediat Audiol, Kardinal von Galen Ring 10, D-48149 Munster, Germany.
   [Merkel, Anna; Hente, Katja; von Gudenberg, Alexander Wolff] Inst Kassel Stuttering Therapy, Feriendorfstr 1, D-34208 Bad Emstal, Germany.
   [Neef, Nicole] Georg August Univ, Dept Diagnost & Intervent Neuroradiol, Robert Koch Str 40, D-37075 Gottingen, Germany.
C3 University of Munster; University of Gottingen
RP Euler, HA (corresponding author), Westphalian Wilhelms Univ Munster, Univ Hosp Munster, Dept Phoniatr & Pediat Audiol, Kardinal von Galen Ring 10, D-48149 Munster, Germany.
EM euler@uni-kassel.de; anna.merkel@kasseler-stottertherapie.de;
   katja.hente@kasseler-stottertherapie.de; nneef@gwdg.de;
   awvgudenberg@kasseler-stottertherapie.de; Katrin.Neumann@uni-muenster.de
RI Neef, Nicole E./F-4103-2018
OI Neef, Nicole E./0000-0003-2414-7595
CR Andrews C, 2016, J FLUENCY DISORD, V48, P44, DOI 10.1016/j.jfludis.2016.06.001
   Andrews C, 2012, LANG SPEECH HEAR SER, V43, P359, DOI 10.1044/0161-1461(2012/11-0038)
   [Anonymous], Public Health, DOI DOI 10.1016/S0033-3506(02)00027-6
   [Anonymous], 2011, OXFORD 2011 LEVELS E
   Bakker K., 2009, COMPUTERIZED SCORING
   Block S, 2005, INT J LANG COMM DIS, V40, P455, DOI 10.1080/03093640500088161
   Bloodstein O., 2008, A Handbook on Stuttering, V6th
   BUDD KS, 1986, BEHAV THER, V17, P538, DOI 10.1016/S0005-7894(86)80093-4
   Carey B, 2010, INT J LANG COMM DIS, V45, P108, DOI 10.3109/13682820902763944
   Cook S., 2013, LOGOS, V21, P97
   Craig A, 1996, J SPEECH HEAR RES, V39, P808, DOI 10.1044/jshr.3904.808
   de Sonneville-Koedoot C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0133758
   Downs SH, 1998, J EPIDEMIOL COMMUN H, V52, P377, DOI 10.1136/jech.52.6.377
   Dunlap WP, 1996, PSYCHOL METHODS, V1, P170, DOI 10.1037/1082-989X.1.2.170
   Euler HA, 2009, SPRACHE-STIMME-GEHOR, V33, P193, DOI 10.1055/s-0029-1242747
   Euler H. A., 2016, LOGOS, V24, P84, DOI 10.7345/prolog-1602084.
   Euler HA, 2014, J FLUENCY DISORD, V39, P1, DOI 10.1016/j.jfludis.2014.01.002
   FESTINGER L, 1961, AM PSYCHOL, V16, P1, DOI 10.1037/h0045112
   Franken MCJ, 2005, J FLUENCY DISORD, V30, P189, DOI 10.1016/j.jfludis.2005.05.002
   Harris V, 2002, J FLUENCY DISORD, V27, P203, DOI 10.1016/S0094-730X(02)00127-4
   Harrison E, 2004, INT J LANG COMM DIS, V39, P257, DOI 10.1080/13682820310001644551
   Haynes B, 1999, BRIT MED J, V319, P652, DOI 10.1136/bmj.319.7211.652
   HUNT KW, 1970, MONOGR SOC RES CHILD, V35, P1
   Jones M, 2008, INT J LANG COMM DIS, V43, P649, DOI 10.1080/13682820801895599
   Kalinowski J, 2005, INT J LANG COMM DIS, V40, P349, DOI 10.1080/13693780400027779
   Karimi H., 2013, J SPEECH LANG HEAR R, V56
   Keilmann A, 2018, LOGOP PHONIATR VOCO, V43, P155, DOI 10.1080/14015439.2018.1498917
   Koushik S, 2009, J FLUENCY DISORD, V34, P279, DOI 10.1016/j.jfludis.2009.11.001
   Laiho A, 2007, INT J LANG COMM DIS, V42, P367, DOI 10.1080/13682820600939028
   Langevin M, 2006, J FLUENCY DISORD, V31, P229, DOI 10.1016/j.jfludis.2006.06.001
   Langevin M, 2010, J FLUENCY DISORD, V35, P123, DOI 10.1016/j.jfludis.2010.04.002
   Lattermann C, 2008, J FLUENCY DISORD, V33, P52, DOI 10.1016/j.jfludis.2007.12.002
   Lewis C, 2008, AM J SPEECH-LANG PAT, V17, P139, DOI 10.1044/1058-0360(2008/014)
   LOGAN KJ, 1995, J FLUENCY DISORD, V20, P35, DOI 10.1016/0094-730X(94)00008-H
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   Melnick KS, 2000, J FLUENCY DISORD, V25, P21, DOI 10.1016/S0094-730X(99)00028-5
   Metten C, 2007, SPRACHE-STIMME-GEHOR, V31, P72, DOI 10.1055/s-2007-958631
   Millard SK, 2018, AM J SPEECH-LANG PAT, V27, P1211, DOI 10.1044/2018_AJSLP-ODC11-17-0199
   Neumann K., 2016, Pathogenesis, diagnostic and treatment of speech fluency disorders. Evidence- and consensus-based S3-guidelines
   Neumann K, 2017, DTSCH ARZTEBL INT, V114, P383, DOI 10.3238/arztebl.2017.0383
   Nippold MA, 2008, AM J SPEECH-LANG PAT, V17, P356, DOI 10.1044/1058-0360(2008/07-0049)
   Nippold MA, 2012, LANG SPEECH HEAR SER, V43, P549, DOI 10.1044/0161-1461(2012/12-0054)
   Nippold MA, 2012, LANG SPEECH HEAR SER, V43, P338, DOI 10.1044/0161-1461(2012/12-0035)
   Nippold MA, 2011, LANG SPEECH HEAR SER, V42, P99, DOI 10.1044/0161-1461(2011/ed-02)
   Nye C, 2013, J SPEECH LANG HEAR R, V56, P921, DOI 10.1044/1092-4388(2012/12-0036)
   Onslow M., 2003, The Lidcombe Program of Early Stuttering Intervention: A Clinician's Guide
   Riley G. D., 2009, SSI-4 stuttering severity instrument fourth edition, V4th
   Riley GD, 2000, J SPEECH LANG HEAR R, V43, P965, DOI 10.1044/jslhr.4304.965
   RYAN BP, 1995, J SPEECH HEAR RES, V38, P61, DOI 10.1044/jshr.3801.61
   Sawilowsky SS, 2009, J MOD APPL STAT METH, V8, P597, DOI 10.22237/jmasm/1257035100
   Senkal OA, 2018, HEARING BALANC COMMU, V16, P134, DOI 10.1080/21695717.2018.1484627
   Sprangers MAG, 1999, SOC SCI MED, V48, P1507, DOI 10.1016/S0277-9536(99)00045-3
   Webster R. L, 1980, PRECISION FLUENCY PR
   Wolff von, 2006, FORUM LOGOPADIE, V20, P24
   Wolff von Gudenberg A, 2000, SPRACHE STIMME GEHOR, V24, P71
   World Health Organization, 2018, INT CLASSIFICATION F
   Yaruss J. S, 2010, Overall Assessment of the Speaker's Experience of Stuttering: Manual
   Yaruss JS, 2006, J FLUENCY DISORD, V31, P90, DOI 10.1016/j.jfludis.2006.02.002
   Yaruss JS, 2012, LANG SPEECH HEAR SER, V43, P536, DOI 10.1044/0161-1461(2012/11-0044)
   Yaruss J. Scott, 2007, Seminars in Speech and Language, V28, P312, DOI 10.1055/s-2007-986528
   Yaruss JS, 1999, J SPEECH LANG HEAR R, V42, P329, DOI 10.1044/jslhr.4202.329
   Yaruss JS., 2014, OASES OVERALL ASSESS
NR 62
TC 3
Z9 4
U1 2
U2 13
PU ELSEVIER SCIENCE INC
PI NEW YORK
PA STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
SN 0021-9924
EI 1873-7994
J9 J COMMUN DISORD
JI J. Commun. Disord.
PD JAN-FEB
PY 2021
VL 89
AR 106073
DI 10.1016/j.jcomdis.2020.106073
EA JAN 2021
PG 12
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA QP4XM
UT WOS:000623840000001
PM 33444874
OA hybrid
DA 2024-01-09
ER

PT J
AU Kühne, K
   Fischer, MH
   Zhou, YF
AF Kuehne, Katharina
   Fischer, Martin H.
   Zhou, Yuefang
TI The Human Takes It All: Humanlike Synthesized Voices Are Perceived as
   Less Eerie and More Likable. Evidence From a Subjective Ratings Study
SO FRONTIERS IN NEUROROBOTICS
LA English
DT Article
DE human-robot interaction; paralinguistic features; synthesized voice;
   text-to-speech; uncanny valley
ID SOCIAL ROBOTS; INDIVIDUAL-DIFFERENCES; SPEECH SYNTHESIS; PERSONALITY;
   PERCEPTION; RECOGNITION; ATTRACTION; LIKABILITY; JUDGMENTS; ATTITUDES
AB Background: The increasing involvement of social robots in human lives raises the question as to how humans perceive social robots. Little is known about human perception of synthesized voices.
   Aim: To investigate which synthesized voice parameters predict the speaker's eeriness and voice likability; to determine if individual listener characteristics (e.g., personality, attitude toward robots, age) influence synthesized voice evaluations; and to explore which paralinguistic features subjectively distinguish humans from robots/artificial agents.
   Methods: 95 adults (62 females) listened to randomly presented audio-clips of three categories: synthesized (Watson, IBM), humanoid (robot Sophia, Hanson Robotics), and human voices (five clips/category). Voices were rated on intelligibility, prosody, trustworthiness, confidence, enthusiasm, pleasantness, human-likeness, likability, and naturalness. Speakers were rated on appeal, credibility, human-likeness, and eeriness. Participants' personality traits, attitudes to robots, and demographics were obtained.
   Results: The human voice and human speaker characteristics received reliably higher scores on all dimensions except for eeriness. Synthesized voice ratings were positively related to participants' agreeableness and neuroticism. Females rated synthesized voices more positively on most dimensions. Surprisingly, interest in social robots and attitudes toward robots played almost no role in voice evaluation. Contrary to the expectations of an uncanny valley, when the ratings of human-likeness for both the voice and the speaker characteristics were higher, they seemed less eerie to the participants. Moreover, when the speaker's voice was more humanlike, it was more liked by the participants. This latter point was only applicable to one of the synthesized voices. Finally, pleasantness and trustworthiness of the synthesized voice predicted the likability of the speaker's voice. Qualitative content analysis identified intonation, sound, emotion, and imageability/embodiment as diagnostic features.
   Discussion: Humans clearly prefer human voices, but manipulating diagnostic speech features might increase acceptance of synthesized voices and thereby support human-robot interaction. There is limited evidence that human-likeness of a voice is negatively linked to the perceived eeriness of the speaker.
C1 [Kuehne, Katharina; Fischer, Martin H.; Zhou, Yuefang] Univ Potsdam, Div Cognit Sci, Potsdam, Germany.
C3 University of Potsdam
RP Kühne, K (corresponding author), Univ Potsdam, Div Cognit Sci, Potsdam, Germany.
EM kkuehne@uni-potsdam.de
RI Fischer, Martin H./D-9965-2017
OI Kuehne, Katharina/0000-0002-1334-1563
CR [Anonymous], 1998, Handbook of Social Psychology
   Antona M, 2019, 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2019), P416, DOI 10.1145/3316782.3322777
   Aylett MP, 2020, ACMIEEE INT CONF HUM, P110, DOI 10.1145/3371382.3378330
   Aylett MP, 2020, IEEE T AFFECT COMPUT, V11, P361, DOI 10.1109/TAFFC.2017.2763134
   Aylett MP, 2019, PROCEEDINGS OF THE 1ST INTERNATIONAL CONFERENCE ON CONVERSATIONAL USER INTERFACES (CUI 2019), DOI 10.1145/3342775.3342806
   Baird A, 2018, INTERSPEECH, P2863, DOI 10.21437/Interspeech.2018-1093
   Bartneck C, 2007, 2007 RO-MAN: 16TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1-3, P367
   Belpaeme T, 2018, INT J SOC ROBOT, V10, P325, DOI 10.1007/s12369-018-0467-6
   Bendel O., 2020, MENSCH ROBOTER KOLLA, P1, DOI [10.1007/978-3-658-28307-0_1, DOI 10.1007/978-3-658-28307-0_1]
   Bernier Emily P., 2010, 2010 IEEE 9th International Conference on Development and Learning (ICDL 2010), P286, DOI 10.1109/DEVLRN.2010.5578828
   Beskow J., 2019, P 10 SPEECH SYNTH WO, P105, DOI DOI 10.21437/SSW.2019-19
   Birkholz P, 2017, COMPUT SPEECH LANG, V41, P116, DOI 10.1016/j.csl.2016.06.004
   Bombelli Griselda, 2013, DELTA, V29, P267
   Braun M, 2019, CHI 2019: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3290605.3300270
   Breazeal C. L., 2002, DESIGNING SOCIABLE R, DOI [10.1007/0-306-47373-9_18, DOI 10.1007/0-306-47373-9_18]
   Breazeal C, 2017, ACMIEEE INT CONF HUM, P1, DOI 10.1145/2909824.3020258
   Broadbent E, 2017, ANNU REV PSYCHOL, V68, P627, DOI 10.1146/annurev-psych-010416-043958
   Bryman A., 1994, ANAL QUALITATIVE DAT, DOI DOI 10.4324/9780203413081
   Bucholtz M, 2016, SOCIOLINGUISTICS: THEORETICAL DEBATES, P173
   Burgoon J. K., 2015, The International Encyclopedia of Interpersonal Communication, P1, DOI DOI 10.1002/9781118540190.WBEIC102
   Burgoon J. K., 1978, Hum. Commun. Res, V4, P129, DOI [10.1111/j.1468-2958.1978.tb00603.x, DOI 10.1111/J.1468-2958.1978.TB00603.X, 10.1111/j.1468-2958.1978]
   Burgoon JK, 2016, INT J HUM-COMPUT ST, V91, P24, DOI 10.1016/j.ijhcs.2016.02.002
   Burleigh TJ, 2015, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01488
   Cabral JP, 2017, INTERSPEECH, P229, DOI 10.21437/Interspeech.2017-325
   Castro-González A, 2016, INT J HUM-COMPUT ST, V90, P27, DOI 10.1016/j.ijhcs.2016.02.004
   Chang RCS, 2018, COMPUT HUM BEHAV, V84, P194, DOI 10.1016/j.chb.2018.02.025
   Coursey K., 2019, AI LOVE YOU, P77, DOI [10.1007/978-3-030-19734-6_4, DOI 10.1007/978-3-030-19734-6_4]
   Craig SD, 2017, COMPUT EDUC, V114, P193, DOI 10.1016/j.compedu.2017.07.003
   Danaher J, 2017, ROBOT SEX: SOCIAL AND ETHICAL IMPLICATIONS, P3
   Dennett D. C., 1971, Journal of Philosophy, V68, DOI 10.2307/2025382
   Dey I., 1993, QUALITATIVE DATA ANA
   Elkins AC, 2013, GROUP DECIS NEGOT, V22, P897, DOI 10.1007/s10726-012-9339-x
   Goy H, 2016, J ACOUST SOC AM, V139, P1648, DOI 10.1121/1.4945094
   Graziano WG, 2007, J PERS SOC PSYCHOL, V93, P565, DOI 10.1037/0022-3514.93.4.565
   Greenwald AG, 1998, J PERS SOC PSYCHOL, V74, P1464, DOI 10.1037/0022-3514.74.6.1464
   Hannuschke M, 2020, J PERS, V88, P217, DOI 10.1111/jopy.12480
   Harris R. A., 2004, VOICE INTERACTION DE
   Hayamizu Y, 2018, IEEE INT WORK TECH, P13
   Hinterleitner F., 2017, QUALITY SYNTHETIC SP, DOI [10.1007/978-981-10-3734-4_5, DOI 10.1007/978-981-10-3734-4_5]
   Hirai S., 2015, 14 INFORM SCI TECHNO, V2, P289
   Hodari Zack, 2019, 10 ISCA WORKSH SPEEC, P239
   Jaiswal J., 2019, AS C PATT REC, P580, DOI [10.1007/978-3-030-41299-9_45, DOI 10.1007/978-3-030-41299-9_45]
   Kätsyri J, 2019, PERCEPTION, V48, P968, DOI 10.1177/0301006619869134
   Kayte SN, 2016, ADV INTELL SYST, V439, P253, DOI 10.1007/978-981-10-0755-2_27
   Kenny D. A., 1994, Interpersonal perception: A social relations analysis
   Kim SY, 2019, MARKET LETT, V30, P1, DOI 10.1007/s11002-019-09485-9
   Kraus M, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P112
   Kuratate T., 2009, AVSP, P65
   Leiner D. J., 2018, SoSci Survey (Version 2.5.00-i1142
   Li Gong, 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P158, DOI 10.1145/365024.365090
   Liscombe J., 2003, P INT 2013 EUR
   MacDorman K. F., 2006, ICCS COGSCI 2006 LON, P26, DOI DOI 10.1093/SCAN/NSR025
   MacDorman KF, 2015, INTERACT STUD, V16, P141, DOI 10.1075/is.16.2.01mac
   Marchesi S, 2019, FRONT PSYCHOL, V10, DOI 10.3389/fpsyg.2019.00450
   Massaro D. W., 2014, SPEECH PERCEPTION EA, DOI [10.4324/9781315808253, DOI 10.4324/9781315808253]
   Massaro D. W., 1987, SPEECH PERCEPTION EA
   Mayring P., 2014, Handbuch Methoden der empirischen Sozialforschung, P543
   McCrae RR, 1997, AM PSYCHOL, V52, P509, DOI 10.1037/0003-066X.52.5.509
   McCrae RR, 2005, J PERS SOC PSYCHOL, V88, P547, DOI 10.1037/0022-3514.88.3.547
   McGinn C, 2019, ACMIEEE INT CONF HUM, P211, DOI [10.1109/hri.2019.8673305, 10.1109/HRI.2019.8673305]
   Mendelson J, 2017, INTERSPEECH, P249, DOI 10.21437/Interspeech.2017-1438
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Nomura T, 2008, IEEE T ROBOT, V24, P442, DOI 10.1109/TRO.2007.914004
   Polkosky M. D., 2003, International Journal of Speech Technology, V6, P161, DOI 10.1023/A:1022390615396
   Rammstedt B, 2007, J RES PERS, V41, P203, DOI 10.1016/j.jrp.2006.02.001
   Rodero E, 2017, COMPUT HUM BEHAV, V77, P336, DOI 10.1016/j.chb.2017.08.044
   Romportl Jan, 2014, Text, Speech and Dialogue. 17th International Conference, TSD 2014. Proceedings: LNCS 8655, P595, DOI 10.1007/978-3-319-10816-2_72
   Rosenthal-von der Pütten AM, 2019, J NEUROSCI, V39, P6555, DOI 10.1523/JNEUROSCI.2956-18.2019
   Salza PL, 1996, ACUSTICA, V82, P650
   Schmidt-Nielsen A., 1995, P 1995 IEEE WORKSH S, P5, DOI [10.1109/SCFT.1995.658104, DOI 10.1109/SCFT.1995.658104]
   Schuller B, 2013, COMPUT SPEECH LANG, V27, P4, DOI 10.1016/j.csl.2012.02.005
   Scott K, 2020, REFERRING EXPRESSIONS, PRAGMATICS AND STYLE: REFERENCE AND BEYOND, P1
   Seymour M., 2017, HAW INT C SYST SCI 2
   Sims Valerie K., 2009, P HUMAN FACTORS ERGO, V53, P1418, DOI 10.1177/154193120905301853
   Stern SE, 1999, HUM FACTORS, V41, P588, DOI 10.1518/001872099779656680
   Strait M, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3593, DOI 10.1145/2702123.2702415
   Thepsoonthorn C, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-25314-x
   Thomas David R, 2003, A general inductive approach for qualitative data analysis
   Torre I, 2018, PROCEEDINGS OF THE TECHNOLOGY, MIND, AND SOCIETY CONFERENCE (TECHMINDSOCIETY'18), DOI 10.1145/3183654.3183691
   Tschöpe N, 2017, ACMIEEE INT CONF HUM, P307, DOI 10.1145/3029798.3038319
   Tsiourti C, 2019, INT J SOC ROBOT, V11, P555, DOI 10.1007/s12369-019-00524-z
   Tyagi S., 2019, ARXIV191200955
   Uhrig S, 2017, QUAL USER EXP, V2, P10, DOI [DOI 10.1007/S41233-017-0011-8, 10.1007/s41233-017-0011-8]
   Vallee M, 2017, BODY SOC, V23, P83, DOI 10.1177/1357034X17697366
   Velner E, 2020, ACMIEEE INT CONF HUM, P569, DOI 10.1145/3319502.3374801
   Wasala K, 2020, PROCEEDINGS OF THE 31ST AUSTRALIAN CONFERENCE ON HUMAN-COMPUTER-INTERACTION (OZCHI'19), P503, DOI 10.1145/3369457.3369542
   Westlund JMK, 2017, FRONT HUM NEUROSCI, V11, DOI 10.3389/fnhum.2017.00295
   Westlund JMK, 2016, IEEE ROMAN, P688, DOI 10.1109/ROMAN.2016.7745193
   Winquist LA, 1998, J RES PERS, V32, P370, DOI 10.1006/jrpe.1998.2221
   Wood D, 2010, J PERS SOC PSYCHOL, V99, P174, DOI 10.1037/a0019390
   Zhou Y, 2019, I COMP CONF WAVELET, P170, DOI 10.1109/ICCWAMTIP47768.2019.9067532
   Zotowski J. A., 2018, GEMINOID STUDIES, P163, DOI [10.1007/978-981-10-8702-8_10, DOI 10.1007/978-981-10-8702-8_10]
NR 96
TC 25
Z9 28
U1 6
U2 54
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 1662-5218
J9 FRONT NEUROROBOTICS
JI Front. Neurorobotics
PD DEC 16
PY 2020
VL 14
AR 593732
DI 10.3389/fnbot.2020.593732
PG 15
WC Computer Science, Artificial Intelligence; Robotics; Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Robotics; Neurosciences & Neurology
GA PL8OO
UT WOS:000603374200001
PM 33390923
OA gold, Green Published
DA 2024-01-09
ER

PT J
AU Merritt, B
   Bent, T
AF Merritt, Brandon
   Bent, Tessa
TI Perceptual Evaluation of Speech Naturalness in Speakers of Varying
   Gender Identities
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
ID SPEAKING FUNDAMENTAL-FREQUENCY; VOWEL FORMANT FREQUENCIES; MAGNITUDE
   ESTIMATION; VOICE PROBLEMS; FEMALE; SEX; IDENTIFICATION; TESTOSTERONE;
   INTONATION; THERAPY
AB Purpose: The purpose of this study was to investigate how speech naturalness relates to masculinity-femininity and gender identification (accuracy and reaction time) for cisgender male and female speakers as well as transmasculine and transfeminine speakers.
   Method: Stimuli included spontaneous speech samples from 20 speakers who are transgender (10 transmasculine and 10 transfeminine) and 20 speakers who are cisgender (10 male and 10 female). Fifty-two listeners completed three tasks: a two-alternative forced-choice gender identification task, a speech naturalness rating task, and a masculinity/femininity rating task.
   Results: Transfeminine and transmasculine speakers were rated as significantly less natural sounding than cisgender speakers. Speakers rated as less natural took longer to identify and were identified less accurately in the gender identification task; furthermore, they were rated as less prototypically masculine/feminine.
   Conclusions: Perceptual speech naturalness for both transfeminine and transmasculine speakers is strongly associated with gender cues in spontaneous speech. Training to align a speaker's voice with their gender identity may concurrently improve perceptual speech naturalness.
C1 [Merritt, Brandon; Bent, Tessa] Indiana Univ, Dept Speech Language & Hearing Sci, Bloomington, IN 47405 USA.
C3 Indiana University System; Indiana University Bloomington
RP Merritt, B (corresponding author), Indiana Univ, Dept Speech Language & Hearing Sci, Bloomington, IN 47405 USA.
EM bmmerrit@iu.edu
OI Merritt, Brandon/0000-0002-6166-0198
FU Department of Speech, Language, and Hearing Sciences at Indiana
   University
FX We would like to thank Karly Jones for assistance with data collection.
   We are also grateful to the Department of Speech, Language, and Hearing
   Sciences at Indiana University for providing funding in support of this
   research.
CR American National Standards Institute, 2010, S362010 ANSI
   Anand S, 2015, J SPEECH LANG HEAR R, V58, P1134, DOI 10.1044/2015_JSLHR-S-14-0243
   [Anonymous], 1993, Early Development and Parenting, DOI [DOI 10.1111/1467-9280.02435, DOI 10.1002/EDP.2430020405]
   [Anonymous], 2012, Voice and communication therapy for the transgender/transsexual client: A comprehensive clinical guide
   [Anonymous], WORLD ENGLISHES
   Azul D, 2019, J SPEECH LANG HEAR R, V62, P3320, DOI 10.1044/2019_JSLHR-S-19-0063
   Azul D, 2018, J SPEECH LANG HEAR R, V61, P25, DOI 10.1044/2017_JSLHR-S-16-0410
   Azul D, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.05.005
   Azur D., 2015, Perspectives on Voice and Voice Disorders, V25, P75, DOI [DOI 10.1044/VVD25.2.75, 10.1044/vvd25.2.75]
   Babel M, 2015, COGNITIVE SCI, V39, P766, DOI 10.1111/cogs.12179
   Boersma P., 2018, PRAAT DOING PHONETIC
   Braun V, 2012, RES DESIGNS QUANTITA, V2, P57, DOI 10.1037/13620-004
   Byrne L. A., 2007, THESIS
   Carew L, 2007, J VOICE, V21, P591, DOI 10.1016/j.jvoice.2006.05.005
   Cosyns M, 2014, LARYNGOSCOPE, V124, P1409, DOI 10.1002/lary.24480
   Coughlin-Woods S, 2005, PERCEPT MOTOR SKILL, V100, P295
   Couper-Kuhlen Elizabeth., 2015, HDB DISCOURSE ANAL, P82
   Davidson L, 2019, PHONETICA, V76, P235, DOI 10.1159/000490948
   Davies S, 2015, INT J TRANSGENDERISM, V16, P117, DOI 10.1080/15532739.2015.1075931
   Davies S, 2006, INT J TRANSGENDERISM, V9, P167, DOI 10.1300/J485v09n03_08
   Davis SA, 2014, INT J SEX HEALTH, V26, P113, DOI 10.1080/19317611.2013.833152
   de Jong NH, 2009, BEHAV RES METHODS, V41, P385, DOI 10.3758/BRM.41.2.385
   Descloux P, 2012, Rev Laryngol Otol Rhinol (Bord), V133, P41
   Eadie TL, 2002, J SPEECH LANG HEAR R, V45, P1088, DOI 10.1044/1092-4388(2002/087)
   Fant G., 1960, ACOUSTIC THEORY SPEE
   Gallena SJK, 2018, J VOICE, V32, P592, DOI 10.1016/j.jvoice.2017.06.023
   Gelfer MP, 2013, J VOICE, V27, P556, DOI 10.1016/j.jvoice.2012.11.008
   Gelfer MP, 2013, J VOICE, V27, P321, DOI 10.1016/j.jvoice.2012.07.008
   Gelfer MP, 2013, J VOICE, V27, P335, DOI 10.1016/j.jvoice.2012.07.009
   Gelfer MP, 2005, J VOICE, V19, P544, DOI 10.1016/j.jvoice.2004.10.006
   Gelfer MP, 2000, J VOICE, V14, P22, DOI 10.1016/S0892-1997(00)80092-2
   Gelfer MP, 1999, AM J SPEECH-LANG PAT, V8, P201, DOI 10.1044/1058-0360.0803.201
   Goldinger SD, 1998, PSYCHOL REV, V105, P251, DOI 10.1037/0033-295X.105.2.251
   Gooren LJ, 2015, CULT HEALTH SEX, V17, P92, DOI 10.1080/13691058.2014.950982
   Gorton RN, 2017, PSYCHIAT CLIN N AM, V40, P79, DOI 10.1016/j.psc.2016.10.005
   Hancock A, 2014, J VOICE, V28, P203, DOI 10.1016/j.jvoice.2013.08.009
   Hancock AB, 2017, J SPEECH LANG HEAR R, V60, P2472, DOI 10.1044/2017_JSLHR-S-16-0320
   Hancock AB, 2017, J LANG SOC PSYCHOL, V36, P599, DOI 10.1177/0261927X17704460
   Hancock AB, 2011, J VOICE, V25, P553, DOI 10.1016/j.jvoice.2010.07.013
   Hardy TLD, 2020, J VOICE, V34, DOI 10.1016/j.jvoice.2018.10.002
   Hardy TLD, 2016, AM J SPEECH-LANG PAT, V25, DOI 10.1044/2015_AJSLP-14-0098
   HENTON CG, 1989, LANG COMMUN, V9, P299, DOI 10.1016/0271-5309(89)90026-8
   HOLLIEN H, 1972, J SPEECH HEAR RES, V15, P155, DOI 10.1044/jshr.1501.155
   Holmberg EB, 2010, J VOICE, V24, P511, DOI 10.1016/j.jvoice.2009.02.002
   Houle N., 2019, JOURNAL OF VOICE ADV, DOI 10.1016/j.jvoice.2019.10.011
   INGHAM RJ, 1978, J SPEECH HEAR RES, V21, P63, DOI 10.1044/jshr.2101.63
   Irwig MS, 2017, ANDROLOGY-US, V5, P107, DOI 10.1111/andr.12278
   Junger J, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0111672
   Junger J, 2013, NEUROIMAGE, V79, P275, DOI 10.1016/j.neuroimage.2013.04.105
   Kawitzky D, 2020, J VOICE, V34, P53, DOI 10.1016/j.jvoice.2018.07.017
   Kent RD, 2018, J COMMUN DISORD, V74, P74, DOI 10.1016/j.jcomdis.2018.05.004
   King RS, 2012, INT J TRANSGENDERISM, V13, P117, DOI 10.1080/15532739.2011.664464
   Kleinschmidt DF, 2015, PSYCHOL REV, V122, P148, DOI 10.1037/a0038695
   Klopfenstein M, 2015, CLIN LINGUIST PHONET, V29, P938, DOI 10.3109/02699206.2015.1081293
   LANE HL, 1961, J ACOUST SOC AM, V33, P160, DOI 10.1121/1.1908608
   LASS NJ, 1976, J ACOUST SOC AM, V59, P675, DOI 10.1121/1.380917
   LASS NJ, 1980, J PHONETICS, V8, P101, DOI 10.1016/S0095-4470(19)31445-7
   Leung Y, 2018, J SPEECH LANG HEAR R, V61, P266, DOI 10.1044/2017_JSLHR-S-17-0067
   LIEBERMAN P, 1985, J ACOUST SOC AM, V77, P649, DOI 10.1121/1.391883
   Lotfian R., 2017, T AFFECTIVE COMPUTIN, V10, P471, DOI 10.1109/TAFFC.2017.
   Mackey LS, 1997, J SPEECH LANG HEAR R, V40, P349, DOI 10.1044/jslhr.4002.349
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   MILLER CL, 1982, INFANT BEHAV DEV, V5, P143, DOI 10.1016/S0163-6383(82)80024-6
   MOUNT KH, 1988, J COMMUN DISORD, V21, P229, DOI 10.1016/0021-9924(88)90031-7
   Munson B, 2007, LANG SPEECH, V50, P125, DOI 10.1177/00238309070500010601
   Nakamura M, 2008, COMPUT SPEECH LANG, V22, P171, DOI 10.1016/j.csl.2007.07.003
   Nygren U, 2016, J VOICE, V30, DOI 10.1016/j.jvoice.2015.10.016
   Oates J., 2015, Perspectives on Voice and Voice Disorders, V25, P48, DOI [DOI 10.1044/VVD25.2.48, https://doi.org/10.1044/vvd25.2.48, 10.1044/vvd25.2, DOI 10.1044/VVD25.2]
   Peirce J, 2019, BEHAV RES METHODS, V51, P195, DOI 10.3758/s13428-018-01193-y
   PETERSON GE, 1952, J ACOUST SOC AM, V24, P175, DOI 10.1121/1.1906875
   Pettit JM, 2004, MIT ENCY COMMUNICATI, P223
   Pierrehumbert JB, 2016, ANNU REV LINGUIST, V2, P33, DOI 10.1146/annurev-linguistics-030514-125050
   Remez R. E., 1985, J ACOUST SOC AM, V77, pS38, DOI DOI 10.1121/1.2022306
   Roenfeldt K., 2018, BETTER AVERAGE CALCU, P1
   Scheidt D., 2004, 26 WORLD C IALP BRIS
   Schilt K, 2009, GENDER SOC, V23, P440, DOI 10.1177/0891243209340034
   SCHWARTZ MF, 1968, J ACOUST SOC AM, V44, P1736, DOI 10.1121/1.1911324
   SCHWARTZ MF, 1968, J ACOUST SOC AM, V43, P1178, DOI 10.1121/1.1910954
   Schwarz K, 2018, J VOICE, V32, P602, DOI 10.1016/j.jvoice.2017.07.003
   Skuk VG, 2014, J SPEECH LANG HEAR R, V57, P285, DOI 10.1044/1092-4388(2013/12-0314)
   Smith E, 2018, HORM BEHAV, V105, P11, DOI 10.1016/j.yhbeh.2018.07.001
   Snow MP, 1998, HUM FACTORS, V40, P386, DOI 10.1518/001872098779591395
   Southwood M., 1996, J MED SPEECH-LANG PA, V4, P13
   Stevens S. S., 1975, Psychophysics
   STOICHEFF ML, 1981, J SPEECH HEAR RES, V24, P437, DOI 10.1044/jshr.2403.437
   Strand Elizabeth A., 2000, THESIS
   TALAAT M, 1987, ANN OTO RHINOL LARYN, V96, P468, DOI 10.1177/000348948709600423
   Van Borsel J, 2000, INT J LANG COMM DIS, V35, P427
   Van Borsel J, 2001, J VOICE, V15, P570, DOI 10.1016/S0892-1997(01)00059-5
   Van Borsel J, 2013, J VOICE, V27, DOI 10.1016/j.jvoice.2013.04.008
   van Son RJJH, 2005, ACTA ACUST UNITED AC, V91, P771
   Whelan R, 2008, PSYCHOL REC, V58, P475, DOI 10.1007/BF03395630
   WOLFE VI, 1990, J SPEECH HEAR DISORD, V55, P43, DOI 10.1044/jshd.5501.43
   Wong P, 2017, J SOCIOLING, V21, P603, DOI 10.1111/josl.12264
   Yoho SE, 2019, ATTEN PERCEPT PSYCHO, V81, P558, DOI 10.3758/s13414-018-1635-3
NR 95
TC 5
Z9 8
U1 1
U2 7
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 2200 RESEARCH BLVD, #271, ROCKVILLE, MD 20850-3289 USA
SN 1092-4388
EI 1558-9102
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD JUL
PY 2020
VL 63
IS 7
BP 2054
EP 2069
DI 10.1044/2020_JSLHR-19-00337
PG 16
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA MR0JC
UT WOS:000553279000002
PM 32598195
DA 2024-01-09
ER

PT J
AU Aylett, MP
   Vinciarelli, A
   Wester, M
AF Aylett, Matthew P.
   Vinciarelli, Alessandro
   Wester, Mirjam
TI Speech Synthesis for the Generation of Artificial Personality
SO IEEE TRANSACTIONS ON AFFECTIVE COMPUTING
LA English
DT Article
DE Speech; Speech synthesis; Robots; Psychology; Hidden Markov models;
   Digital signal processing; Computational modeling; Personality;
   automatic personality perception; automatic personality recognition;
   automatic personality synthesis
ID VOICE QUALITY; PERCEPTION; BEHAVIOR
AB A synthetic voice personifies the system using it. In this work we examine the impact text content, voice quality and synthesis system have on the perceived personality of two synthetic voices. Subjects rated synthetic utterances based on the Big-Five personality traits and naturalness. The naturalness rating of synthesis output did not correlate significantly with any Big-Five characteristic except for a marginal correlation with openness. Although text content is dominant in personality judgments, results showed that voice quality change implemented using a unit selection synthesis system significantly affected the perception of the Big-Five, for example tense voice being associated with being disagreeable and lax voice with lower conscientiousness. In addition a comparison between a parametric implementation and unit selection implementation of the same voices showed that parametric voices were rated as significantly less neurotic than both the text alone and the unit selection system, while the unit selection was rated as more open than both the text alone and the parametric system. The results have implications for synthesis voice and system type selection for applications such as personal assistants and embodied conversational agents where developing an emotional relationship with the user, or developing a branding experience is important.
C1 [Aylett, Matthew P.; Wester, Mirjam] Univ Edinburgh, Sch Informat, Edinburgh EH8 9AB, Midlothian, Scotland.
   [Vinciarelli, Alessandro] Univ Glasgow, Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
C3 University of Edinburgh; University of Glasgow
RP Aylett, MP (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh EH8 9AB, Midlothian, Scotland.
EM matthewaylett@gmail.com; vincia@dcs.gla.ac.uk; mwester@inf.ed.ac.uk
RI Vinciarelli, Alessandro/HZI-8274-2023; Vinciarelli,
   Alessandro/C-1651-2012
OI Vinciarelli, Alessandro/0000-0002-9048-0524
FU Royal Society through a Royal Society Industrial Fellowship; European
   Union [645378]; EPSRC project [EP/N035305/1]
FX This research was funded by the Royal Society through a Royal Society
   Industrial Fellowship, and by the European Union's Horizon 2020 research
   and innovation programme under grant agreement No 645378 (Aria VALUSPA).
   The work of A.Vinciarelli was supported by the EPSRC project
   EP/N035305/1.
CR Aylett M., 2008, U.K. Patent, Patent No. [GB2447263A, 2447263]
   Aylett M., 2013, SSW8, P133
   Aylett M. P., 2007, AISB, P174
   Bennett C. L., 2006, P BLIZZ CHALL
   BURKE PJ, 1967, SOCIOMETRY, V30, P379, DOI 10.2307/2786183
   Cabral JP, 2014, IEEE J-STSP, V8, P195, DOI 10.1109/JSTSP.2014.2307274
   Chen LZ, 2015, IEEE-ACM T AUDIO SPE, V23, P605, DOI 10.1109/TASLP.2014.2385478
   Chin J., 1996, P C COMP HUM FACT CO, P248
   Clark R. A., 2004, P 5 ISCA WORKSH SPEE, P147
   Corr PJ, 2009, CAMBRIDGE HANDBOOK OF PERSONALITY PSYCHOLOGY, P1, DOI 10.1017/CBO9780511596544
   de Sevin E, 2010, LECT NOTES ARTIF INT, V6356, P187, DOI 10.1007/978-3-642-15892-6_20
   Deary IJ, 2009, CAMBRIDGE HANDBOOK OF PERSONALITY PSYCHOLOGY, P89
   EKMAN P, 1980, J PERS SOC PSYCHOL, V38, P270, DOI 10.1037/0022-3514.38.2.270
   Funder DC, 2001, ANNU REV PSYCHOL, V52, P197, DOI 10.1146/annurev.psych.52.1.197
   Gobl C, 2003, SPEECH COMMUN, V40, P189, DOI 10.1016/S0167-6393(02)00082-1
   Gordon M, 2001, J PHONETICS, V29, P383, DOI 10.1006/jpho.2001.0147
   HAYTER AJ, 1986, J AM STAT ASSOC, V81, P1000
   Hofer G.O., 2005, PROC 9 EUR C SPEECH, P501
   Howell D.C., 2012, Statistical methods for psychology
   Hu Q., 2013, 8 ISCA WORKSH SPEECH, P155
   Hunt A., 1996, P AC SPEECH SIGN PRO, P192
   Imai S., 1983, Proceedings of ICASSP 83. IEEE International Conference on Acoustics, Speech and Signal Processing, P93
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   KENNY DA, 1994, PSYCHOL BULL, V116, P245, DOI 10.1037/0033-2909.116.2.245
   Kenny DA, 2004, PERS SOC PSYCHOL REV, V8, P265, DOI 10.1207/s15327957pspr0803_3
   King S, 2014, LOQUENS, V1, DOI 10.3989/loquens.2014.006
   KLATT DH, 1990, J ACOUST SOC AM, V87, P820, DOI 10.1121/1.398894
   Kominek J., 2005, P ANN C INT SPEECH C, P85
   Krahmer E. J., 2003, PRIVATIZATION SECURI, P7
   Laver J., 1980, Cambridge Studies in Linguistics London, V31, P1
   LEVIN JR, 1994, PSYCHOL BULL, V115, P153, DOI 10.1037/0033-2909.115.1.153
   Ling ZH, 2013, IEEE T AUDIO SPEECH, V21, P2129, DOI 10.1109/TASL.2013.2269291
   Matthews G., 2009, PERSONALITY TRAITS
   McCrae RR, 2009, CAMBRIDGE HANDBOOK OF PERSONALITY PSYCHOLOGY, P148
   McRorie M, 2012, IEEE T AFFECT COMPUT, V3, P311, DOI 10.1109/T-AFFC.2011.38
   Mustafa K, 2006, IEEE T AUDIO SPEECH, V14, P435, DOI 10.1109/TSA.2005.855840
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Nettle D., 2009, PERSONALITY WHAT MAK
   Ohtani Y, 2016, INTERSPEECH, P2258, DOI 10.21437/Interspeech.2016-290
   Ozer DJ, 2006, ANNU REV PSYCHOL, V57, P401, DOI 10.1146/annurev.psych.57.102904.190127
   Partan S, 1999, SCIENCE, V283, P1272, DOI 10.1126/science.283.5406.1272
   Partan SR, 2005, AM NAT, V166, P231, DOI 10.1086/431246
   Potard B, 2016, INTERSPEECH, P2293, DOI 10.21437/Interspeech.2016-1188
   Raitio T, 2011, IEEE T AUDIO SPEECH, V19, P153, DOI 10.1109/TASL.2010.2045239
   Reeves B., 1996, The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Pla, DOI DOI 10.1007/S42452-020-2192-7
   SCHERER KR, 1978, EUR J SOC PSYCHOL, V8, P467, DOI 10.1002/ejsp.2420080405
   Scherer KR., 1979, SOCIAL MARKERS SPEEC, P147
   Schmitz M, 2007, 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P313
   Scrhoder M., 2003, P 15 INT C PHON SCI, P2589
   Slater PE, 1955, AM SOCIOL REV, V20, P300, DOI 10.2307/2087389
   Stevens C, 2005, COMPUT SPEECH LANG, V19, P129, DOI 10.1016/j.csl.2004.03.003
   Tapus A., 2008, AAAI SPRING S EMOTIO, P133
   Tapus A, 2008, INTEL SERV ROBOT, V1, P169, DOI 10.1007/s11370-008-0017-4
   Tausczik YR, 2010, J LANG SOC PSYCHOL, V29, P24, DOI 10.1177/0261927X09351676
   Trouvain J., 2006, P SPEECH PROS DRESD
   Uleman JS, 2008, ANNU REV PSYCHOL, V59, P329, DOI 10.1146/annurev.psych.59.103006.093707
   van den Oord A., 2016, 9 ISCA SPEECH SYNTH
   Vinciarelli A, 2014, IEEE T AFFECT COMPUT, V5, P273, DOI 10.1109/TAFFC.2014.2330816
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   Watts O, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2217
   Wester M, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3365
   Wiggins J. S., 1996, The five-factor model of personality, P21
   Woods S, 2005, IEEE-RAS INT C HUMAN, P375
   Wu Z., 2016, SSW, P202, DOI 10.21437/SSW.2016-33
   Yamagishi Junichi, 2007, SSW, P294
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
   Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004
NR 68
TC 14
Z9 14
U1 10
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1949-3045
J9 IEEE T AFFECT COMPUT
JI IEEE Trans. Affect. Comput.
PD APR-JUN
PY 2020
VL 11
IS 2
BP 361
EP 372
DI 10.1109/TAFFC.2017.2763134
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA MB9ZP
UT WOS:000542957100015
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Hardy, TLD
   Rieger, JM
   Wells, K
   Boliek, CA
AF Hardy, Teresa L. D.
   Rieger, Jana M.
   Wells, Kristopher
   Boliek, Carol A.
TI Acoustic Predictors of Gender Attribution, Masculinity -Femininity, and
   Vocal Naturalness Ratings Amongst Transgender and Cisgender Speakers
SO JOURNAL OF VOICE
LA English
DT Article
DE Transgender; Acoustics; Gender; Femininity; Naturalness; Voice
ID SPEAKING FUNDAMENTAL-FREQUENCY; TO-FEMALE TRANSSEXUALS; VOICE QUALITY;
   FORMANT FREQUENCIES; INTERSPEAKER VARIATION; PERCEPTUAL EVALUATION;
   LISTENER PERCEPTIONS; SPEECH RATE; PARAMETERS; IDENTIFICATION
AB Purpose. This study aimed to identify the most salient set of acoustic predictors of (1) gender attribution; (2) perceived masculinity-femininity; and (3) perceived vocal naturalness amongst a group of transgender and cisgender speakers to inform voice and communication feminization training programs. This study used a unique set of acoustic variables and included a third, androgynous, choice for gender attribution ratings.
   Method. Data were collected across two phases and involved two separate groups of participants: communicators and raters. In the first phase, audio recordings were captured of communicators (n = 40) during cartoon retell, sustained vowel, and carrier phrase tasks. Acoustic measures were obtained from these recordings. In the second phase, raters (n = 20) provided ratings of gender attribution, perceived masculinity-femininity, and vocal naturalness based on a sample of the cartoon description recording.
   Results. Results of a multinomial logistic regression analysis identified mean fundamental frequency (f(o)) as the sole acoustic measure that changed the odds of being attributed as a woman or ambiguous in gender rather than as a man. Multiple linear regression analyses identified mean f(o), average formant frequency of /i/, and mean sound pressure level as predictors of masculinity-femininity ratings and mean f(o), average formant frequency, and rate of speech as predictors of vocal naturalness ratings.
   Conclusion. The results of this study support the continued targeting of f o and vocal tract resonance in voice and communication feminization/masculinization training programs and provide preliminary evidence for more emphasis being placed on vocal intensity and rate of speech. Modification of these voice parameters may help clients to achieve a natural-sounding voice that satisfactorily represents their affirmed gender.
C1 [Hardy, Teresa L. D.; Rieger, Jana M.; Wells, Kristopher; Boliek, Carol A.] Univ Alberta, Fac Rehabil Med, 3-48 Corbett Hall, Edmonton, AB T6G 2G4, Canada.
   [Hardy, Teresa L. D.] Glenrose Rehabil Hosp, Alberta Hlth Serv, Edmonton, AB, Canada.
   [Wells, Kristopher] MacEwan Univ, Fac Hlth & Community Studies, Dept Child & Youth Care, Edmonton, AB, Canada.
C3 University of Alberta; Alberta Health Services (AHS)
RP Hardy, TLD (corresponding author), Univ Alberta, Fac Rehabil Med, 3-48 Corbett Hall, Edmonton, AB T6G 2G4, Canada.
EM teresa.hardy@ualberta.ca; jana.rieger@ualberta.ca;
   Kristopher.Wells@Macewan.ca; carol.boliek@ualberta.ca
OI Hardy, Teresa/0000-0003-0768-6909; Rieger, Jana/0000-0002-3906-316X;
   Boliek, Carol/0000-0002-7141-2809
FU Social Sciences and Humanities Research Council of Canada (SSHRC);
   Alberta Innovates
FX The authors would like to thank the research participants for their
   contribution to the research. We also thank the team at Webzao
   Innovations for their tireless work in developing Gender Finder; Ming Ye
   for guidance with statistical analyses; Simone Emery, Jessica Frankel,
   Brianne Haeusler, and Riley Steel for assistance with acoustic analyses;
   and Matthew Kelley for creating PRAAT scripts for the acoustic analyses.
   This research was supported by the Social Sciences and Humanities
   Research Council of Canada (SSHRC) and Alberta Innovates.
CR Andrews ML, 1997, J VOICE, V11, P307, DOI 10.1016/S0892-1997(97)80009-4
   [Anonymous], TF32 COMPUTER PROGRA
   [Anonymous], 2006, VOICE COMMUNICATION
   [Anonymous], 2012, Voice and Communication Therapy for the Transgender/Transsexual Client: A Comprehensive Clinical Guide
   Baken R. J., 2000, Clinical measurement of speech and voice
   Bauer G., 2015, Transgender people in Ontario, Canada: statistics from the Trans PULSE Project to Inform Human Rights Policy
   Bhuta T, 2004, J VOICE, V18, P299, DOI 10.1016/j.jvoice.2003.12.004
   Boersma  P., 2017, PRAAT DOING PHONETIC
   Boonin J, 2012, VOICE COMMUNICATION, P237
   Byrne L. A., 2007, THESIS
   Carew L, 2007, J VOICE, V21, P591, DOI 10.1016/j.jvoice.2006.05.005
   Coleman E, 2012, INT J TRANSGENDERISM, V13, P165, DOI 10.1080/15532739.2011.700873
   COLEMAN RO, 1976, J SPEECH HEAR RES, V19, P168, DOI 10.1044/jshr.1901.168
   Dacakis G, 2012, TRANSSEXUAL VOICE QU
   Dacakis G, 2017, INT J LANG COMM DIS, V52, P831, DOI 10.1111/1460-6984.12319
   Dacakis G, 2012, CURR OPIN OTOLARYNGO, V20, P165, DOI 10.1097/MOO.0b013e3283530f85
   Dahl K., 2018, THESIS
   Davies S, 2015, INT J TRANSGENDERISM, V16, P117, DOI 10.1080/15532739.2015.1075931
   Davis A., 1969, PINK NIGHT
   Fitzsimons M, 2001, BRAIN LANG, V78, P94, DOI 10.1006/brln.2000.2448
   Fox CM, 2012, J SPEECH LANG HEAR R, V55, P930, DOI 10.1044/1092-4388(2011/10-0235)
   Martins RHG, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.06.012
   Gelfer MP, 2013, J VOICE, V27, P556, DOI 10.1016/j.jvoice.2012.11.008
   Gelfer MP, 2013, J VOICE, V27, P321, DOI 10.1016/j.jvoice.2012.07.008
   Gelfer MP, 2013, J VOICE, V27, P335, DOI 10.1016/j.jvoice.2012.07.009
   Gelfer MP, 2005, J VOICE, V19, P544, DOI 10.1016/j.jvoice.2004.10.006
   Gelfer MP, 2000, J VOICE, V14, P22, DOI 10.1016/S0892-1997(00)80092-2
   Gorham-Rowan M, 2006, J VOICE, V20, P251, DOI 10.1016/j.jvoice.2005.03.004
   Grant J.M., 2011, INJUSTICE EVERY TURN
   GUNZBURGER D, 1995, ARCH SEX BEHAV, V24, P339, DOI 10.1007/BF01541604
   GUNZBURGER D, 1989, CLIN LINGUIST PHONET, V3, P163, DOI 10.3109/02699208908985279
   GUNZBURGER D, 1993, EUR J DISORDER COMM, V28, P13
   Hancock A, 2014, J VOICE, V28, P203, DOI 10.1016/j.jvoice.2013.08.009
   Hancock AB, 2017, J LANG SOC PSYCHOL, V36, P599, DOI 10.1177/0261927X17704460
   Hancock AB, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.03.013
   Hancock AB, 2013, INT J LANG COMM DIS, V48, P54, DOI 10.1111/j.1460-6984.2012.00185.x
   Hardy TLD, 2016, AM J SPEECH-LANG PAT, V25, DOI 10.1044/2015_AJSLP-14-0098
   Hardy TLD, 2017, GENDER FINDER COMPUT
   Hillenbrand JM, 2009, ATTEN PERCEPT PSYCHO, V71, P1150, DOI 10.3758/APP.71.5.1150
   Hirsch S., 2017, PERSPECTIVES ASHA SP, V2, P74, DOI DOI 10.1044/PERSP2.SIG10.74
   Holmberg EB, 2010, J VOICE, V24, P511, DOI 10.1016/j.jvoice.2009.02.002
   Hosmer DW, 2013, WILEY SER PROBAB ST, P89
   HUNT KW, 1965, ENGL J, V54, P300, DOI 10.2307/811114
   James SE, 2016, The Report of the 2015 U.S. Transgender Survey
   KayPentax, 2008, MULT SOFTWARETM MOD
   Kelley M., 2016, BOLIEK LAB SEGMENTAL
   Kelley M., 2017, FO SCRIPT PRAAT SCRI
   King RS, 2012, INT J TRANSGENDERISM, V13, P117, DOI 10.1080/15532739.2011.664464
   KLATT DH, 1990, J ACOUST SOC AM, V87, P820, DOI 10.1121/1.398894
   Koo TK, 2016, J CHIROPR MED, V15, P155, DOI 10.1016/j.jcm.2016.02.012
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   Leung Y, 2018, J SPEECH LANG HEAR R, V61, P266, DOI 10.1044/2017_JSLHR-S-17-0067
   McNeill EJM, 2008, J VOICE, V22, P727, DOI 10.1016/j.jvoice.2006.12.010
   Mendoza E, 1996, J VOICE, V10, P59, DOI 10.1016/S0892-1997(96)80019-1
   MORDAUNT M, 2006, VOICE COMMUNICATION, P168
   MOUNT KH, 1988, J COMMUN DISORD, V21, P229, DOI 10.1016/0021-9924(88)90031-7
   Oates J, 1997, VENEREOLOGY, V10, P178
   Oates J., 2015, Perspectives on Voice and Voice Disorders, V25, P48, DOI [DOI 10.1044/VVD25.2.48, https://doi.org/10.1044/vvd25.2.48, 10.1044/vvd25.2, DOI 10.1044/VVD25.2]
   OATES JM, 1983, BRIT J DISORD COMMUN, V18, P139
   Owen K, 2010, INT J TRANSGENDERISM, V12, P272, DOI 10.1080/15532739.2010.550767
   Pasricha N, 2008, LOGOP PHONIATR VOCO, V33, P25, DOI 10.1080/14015430701514500
   Porter C. C., 2012, THESIS
   Ratcliff A., 2002, AUGMENT ALTERN COMM, V18, P11, DOI DOI 10.1080/714043393
   Robb MP, 2004, CLIN LINGUIST PHONET, V18, P1, DOI 10.1080/0269920031000105336
   Skuk VG, 2014, J SPEECH LANG HEAR R, V57, P285, DOI 10.1044/1092-4388(2013/12-0314)
   Snow MP, 1998, HUM FACTORS ERGON SO, V40, P1
   SODERSTEN M, 1995, J VOICE, V9, P182, DOI 10.1016/S0892-1997(05)80252-8
   SPENCER LE, 1988, FOLIA PHONIATR, V40, P31, DOI 10.1159/000265881
   Stryker S, 2008, RADICAL HIST REV, P145, DOI 10.1215/01636545-2007-026
   Tabachnick BarbaraG., 2013, Using Multivariate Statistics
   Tsao YC, 1997, J SPEECH LANG HEAR R, V40, P858, DOI 10.1044/jslhr.4004.858
   Tsao YC, 2006, J SPEECH LANG HEAR R, V49, P1156, DOI 10.1044/1092-4388(2006/083)
   Van Borsel J, 2001, J VOICE, V15, P570, DOI 10.1016/S0892-1997(01)00059-5
   Van Borsel J, 2008, J FLUENCY DISORD, V33, P241, DOI 10.1016/j.jfludis.2008.06.004
   Van Borsel J, 2008, CLIN LINGUIST PHONET, V22, P679, DOI 10.1080/02699200801976695
   Van Borsel J, 2009, J VOICE, V23, P291, DOI 10.1016/j.jvoice.2007.08.002
   Verhoeven J, 2004, LANG SPEECH, V47, P297, DOI 10.1177/00238309040470030401
   WENDAHL RW, 1966, FOLIA PHONIATR, V18, P26, DOI 10.1159/000263081
   WENDAHL RW, 1966, FOLIA PHONIATR, V18, P98, DOI 10.1159/000263059
   Whitehill TL, 2002, J SPEECH LANG HEAR R, V45, P80, DOI 10.1044/1092-4388(2002/006)
   WOLFE VI, 1990, J SPEECH HEAR DISORD, V55, P43, DOI 10.1044/jshd.5501.43
NR 81
TC 20
Z9 23
U1 3
U2 17
PU MOSBY-ELSEVIER
PI NEW YORK
PA 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN 0892-1997
EI 1873-4588
J9 J VOICE
JI J. Voice
PD MAR
PY 2020
VL 34
IS 2
DI 10.1016/j.jvoice.2018.10.002
PG 16
WC Audiology & Speech-Language Pathology; Otorhinolaryngology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Otorhinolaryngology
GA KW6NR
UT WOS:000521282000017
PM 30503396
DA 2024-01-09
ER

PT J
AU Chappell, W
   Nix, J
   Parrott, M
AF Chappell, Whitney
   Nix, John
   Parrott, Mackenzie
TI Social and Stylistic Correlates of Vocal Fry in a <i>cappella</i>
   Performances
SO JOURNAL OF VOICE
LA English
DT Article
DE Vocal fry; Matched guise; Social correlates; Stylistic correlates; a
   cappella
ID CREAKY VOICE
AB Objective. To determine the social and stylistic correlates of vocal fry in a cappella performances. Study Design. A matched-guise experiment was used to measure listener evaluations of fry and non-fry guises.
   Methods. Four singers, two male and two female, sang "The Star-Spangled Banner" with onset vocal fry. These recordings were used to create the two guises: (i) an unmodified recording with onset vocal fry on vowel-initial words and (ii) a recording in which the fry had been removed. In total, 253 participants listened to the recordings and evaluated the singers' social and stylistic attributes along a Likert scale, e.g., how confident, sexy, and sincere each singer sounded. A factor analysis was used to conflate correlated variables, and mixed effects linear regression models (n = 1,012) were fitted to each lone or joint factor to determine whether vocal fry significantly influenced listeners' responses to the singers.
   Results. Vocal fry significantly altered listener evaluations of the singers' sincerity/commitment, maturity/sophistication, naturalness, and confidence (P < 0.05). Unlike male singers, who were rated as significantly less sincere/committed with vocal fry, female singers were seen as more sincere/committed with vocal fry and younger listeners also found them less natural, suggesting vocal fry is associated with emotional intensity in female voices. Younger listeners perceived singers with fry as less mature/sophisticated, suggesting an association with youth. Finally, listeners with more musical training rated singers with fry as less confident, while less trained listeners did not exhibit this difference.
   Conclusions. Listeners are highly attuned to vocal fry in music but respond to it differently based upon their age, musical training, and the singer's sex. Vocal fry is evaluated more positively among younger, less musically trained listeners, and it is better received in women's voices, suggesting that the use of fry strategically targets a specific audience, i.e., younger and less trained listeners, who interpret fry as a marker of youth and emotional earnestness. These findings show that a single stylistic feature like vocal fry can be imbued with multiple meanings depending on the singer and audience, and its use can serve to include or exclude particular listener groups.
C1 [Chappell, Whitney; Nix, John] Univ Texas San Antonio, San Antonio, TX 78249 USA.
C3 University of Texas System; University of Texas at San Antonio (UTSA)
RP Chappell, W (corresponding author), Univ Texas San Antonio, Dept Modern Languages & Literatures, 1 UTSA Circle, San Antonio, TX 78249 USA.
EM whitney.chappell@utsa.edu
RI Chappell, Whitney/AAU-4169-2021; Nix, John/JVN-6840-2024
OI Chappell, Whitney/0000-0001-7600-7549
CR Abdelli-Beruh NB, 2014, J VOICE, V28, P185, DOI 10.1016/j.jvoice.2013.08.011
   Anderson RC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0097506
   [Anonymous], 1996, DISCOVER YOUR VOICE
   [Anonymous], R LANG ENV STAT COMP
   Barr DJ, 2013, J MEM LANG, V68, P255, DOI 10.1016/j.jml.2012.11.001
   Bates D, 2013, J STAT SOFTW, V52, P1, DOI 10.18637/jss.v052.i05
   Chapman JL, 2011, SINGING TEACHING HOL, P284
   Gibson TA, 2017, J VOICE, V31, P62, DOI 10.1016/j.jvoice.2016.02.005
   HOLLIEN H, 1968, J ACOUST SOC AM, V43, P506, DOI 10.1121/1.1910858
   HOLLIEN H, 1968, J SPEECH HEAR RES, V11, P600, DOI 10.1044/jshr.1103.600
   LAMBERT WE, 1960, J ABNORM SOC PSYCH, V60, P44, DOI 10.1037/h0044430
   Lefkowitz D, 106 M AM ANTHR ASS W
   Lunte R., VOCAL FRY ONSETS SIN
   Mendoza-Denton N, 2011, J LINGUIST ANTHROPOL, V21, P261, DOI 10.1111/j.1548-1395.2011.01110.x
   Miller R., 1996, STRUCTURE SINGING, P125
   Nix J, 2005, J SINGING, V62, P53
   Parrott ML, P M ACOUST, DOI [DOI 10.1121/2.0000237, 10.1121/2.0000237.]
   Pecknold D, 2016, ROUTL STUD POP MUSIC, P77
   Podesva R.J., 2013, P ANN M BERKELEY LIN, V37, P427
   Riggs S., 1998, SINGING STARS COMPLE, P58
   Riggs S., 1998, SINGING STARS AUDIO, P7
   Thompson Marie, 2016, N PARADOXA, V37, P5
   Titze I., 1994, Principles of Voice Production
   Titze IR, 2012, VOCOLOGY SCI PRACTIC, p[268, 273]
   Vanek C., 2006, SurveyGizmo
   Vennard W., 1967, SINGING MECH TECHNIC
   Wolk L, 2012, J VOICE, V26, pE111, DOI 10.1016/j.jvoice.2011.04.007
   Yuasa IP, 2010, AM SPEECH, V85, P315, DOI 10.1215/00031283-2010-018
NR 28
TC 2
Z9 2
U1 1
U2 4
PU MOSBY-ELSEVIER
PI NEW YORK
PA 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN 0892-1997
EI 1873-4588
J9 J VOICE
JI J. Voice
PD JAN
PY 2020
VL 34
IS 1
DI 10.1016/j.jvoice.2018.06.004
PG 9
WC Audiology & Speech-Language Pathology; Otorhinolaryngology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Audiology & Speech-Language Pathology; Otorhinolaryngology
GA KE4ZO
UT WOS:000508565600026
PM 30131202
DA 2024-01-09
ER

PT J
AU Klopfenstein, M
   Bernard, K
   Heyman, C
AF Klopfenstein, Marie
   Bernard, Kelsey
   Heyman, Claire
TI The study of speech naturalness in communication disorders: A systematic
   review of the literature
SO CLINICAL LINGUISTICS & PHONETICS
LA English
DT Review
DE Speech naturalness; speech disorders; hearing disorders
ID PROSODIC DISTURBANCE; CLEFT-PALATE; RATINGS; INTELLIGIBILITY; CHILDREN;
   APRAXIA; PATTERNS
AB The concept of speech naturalness is used widely in clinic and research applications. Unfortunately, the lack of consistency in research methods means that comparing findings between studies is difficult at best. In order to better understand the state of research on speech naturalness in communication disorders and quantify these impressions, this study looks at publications from the last 18 years in a systematic manner. A literature search for the exact phrase "speech naturalness" of the PubMed/MEDLINE, EBSCO, and ASHAWire databases was conducted. Articles included in the review were studies of communication and communication disorders published between 1990 and the end of 2014, in English, and in a peer-reviewed journal. 63 articles were selected and coded using a coding sheet adapted from a prior systematic review on intelligibility and cleft palate. Speech naturalness is an object of study in many subfields of communication disorders. Several concerns were raised as a result of the review, including the reliability and validity of measures, inadequate definitions of terminology, lack of detail in method descriptions, and the need to address relationships between naturalness and other variables included in the studies. Future studies should more carefully report methods and operational definitions used and more studies examining the relationship between naturalness and other speech variables in a variety of communication disorders are greatly needed.
C1 [Klopfenstein, Marie] Southern Illinois Univ, Dept Appl Hlth, Edwardsville, IL 62026 USA.
   [Bernard, Kelsey] Univ Arizona, Physiol Sci Program, Tucson, AZ USA.
   [Heyman, Claire] Carle Fdn Hosp, In Patient Rehab Dept, Urbana, IL USA.
C3 Southern Illinois University System; Southern Illinois University
   Edwardsville; University of Arizona
RP Klopfenstein, M (corresponding author), Southern Illinois Univ, Dept Appl Hlth, Edwardsville, IL 62026 USA.
EM maklopf@siue.edu
RI Klopfenstein, Marie/T-5448-2019
OI Klopfenstein, Marie/0000-0002-2229-8050
CR Andrews C, 2012, LANG SPEECH HEAR SER, V43, P359, DOI 10.1044/0161-1461(2012/11-0038)
   [Anonymous], 2000, NOISE REDUCTION SCHE
   [Anonymous], J MED SPEECH LANGUAG
   [Anonymous], 1984, Apraxia of speech in adults: The disorder and its management
   BELLAIRE K, 1986, J COMMUN DISORD, V19, P271, DOI 10.1016/0021-9924(86)90033-X
   COOPER WE, 1984, LANG SPEECH, V27, P17, DOI 10.1177/002383098402700102
   Coughlin-Woods S, 2005, PERCEPT MOTOR SKILL, V100, P295
   Craig A, 1996, J SPEECH HEAR RES, V39, P808, DOI 10.1044/jshr.3904.808
   DAntonio L. L., 1995, Cleft palate speech management: A multidisciplinary approach to the management of cleft palate, P176
   DARLEY FL, 1969, J SPEECH HEAR RES, V12, P246, DOI 10.1044/jshr.1202.246
   Darley FL, 1975, Motor speech disorders, V3rd
   Eadie TL, 2008, J VOICE, V22, P43, DOI 10.1016/j.jvoice.2006.08.008
   Eadie TL, 2002, J SPEECH LANG HEAR R, V45, P1088, DOI 10.1044/1092-4388(2002/087)
   Ingham RJ, 2006, J SPEECH LANG HEAR R, V49, P660, DOI 10.1044/1092-4388(2006/048)
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P217, DOI 10.1044/jshd.5002.217
   INGHAM RJ, 1978, J SPEECH HEAR RES, V21, P63, DOI 10.1044/jshr.2101.63
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P261, DOI 10.1044/jshd.5003.261
   KENT RD, 1982, BRAIN LANG, V15, P259, DOI 10.1016/0093-934X(82)90060-8
   KENT RD, 1983, J SPEECH HEAR RES, V26, P231, DOI 10.1044/jshr.2602.231
   Klopfenstein M, 2016, J INTERACT RES COM D, V7, P123, DOI 10.1558/jircd.v7i1.27932
   Lenden JM, 2007, J COMMUN DISORD, V40, P66, DOI 10.1016/j.jcomdis.2006.04.004
   LINEBAUGH CW, 1984, DYSARTHRIAS PHYSL AC, P197
   Manning W. H, 2001, Clinical decision making in fluency disorders, V2nd
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   McLeod S, 2006, AM J SPEECH-LANG PAT, V15, P192, DOI 10.1044/1058-0360(2006/018)
   MCNEIL MR, 1990, BRAIN LANG, V38, P135, DOI 10.1016/0093-934X(90)90106-Q
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   ODELL K, 1990, J SPEECH HEAR DISORD, V55, P345, DOI 10.1044/jshd.5502.345
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P994, DOI 10.1044/jshr.3505.994
   ONSLOW M, 1987, J SPEECH HEAR DISORD, V52, P2, DOI 10.1044/jshd.5201.02
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P274, DOI 10.1044/jshr.3502.274
   OSBERGER MJ, 1987, J SPEECH HEAR RES, V30, P241, DOI 10.1044/jshr.3002.241
   PARKHURST BG, 1978, J COMMUN DISORD, V11, P249, DOI 10.1016/0021-9924(78)90017-5
   Ratcliff A., 2002, AUGMENT ALTERN COMM, V18, P11, DOI DOI 10.1080/714043393
   RUNYAN CM, 1982, J FLUENCY DISORD, V7, P71, DOI 10.1016/0094-730X(82)90040-7
   RUNYAN CM, 1979, J FLUENCY DISORD, V4, P29, DOI 10.1016/0094-730X(79)90029-9
   RUNYAN CM, 1978, J FLUENCY DISORD, V3, P25, DOI 10.1016/0094-730X(78)90004-9
   RYALLS JH, 1981, NEUROPSYCHOLOGIA, V19, P365, DOI 10.1016/0028-3932(81)90066-X
   Sacco P. R., 1992, SPEECH NATURALNESS N
   Schiavetti N, 1998, J SPEECH LANG HEAR R, V41, P5, DOI 10.1044/jslhr.4101.05
   Shikani AH, 2012, INT FORUM ALLERGY RH, V2, P348, DOI 10.1002/alr.21018
   SIMMONS N, 1983, CLIN DYSARTHRIA, P283
   Spencer KA, 2009, J MED SPEECH-LANG PA, V17, P125
   Stocks R, 2009, BRAIN INJURY, V23, P820, DOI 10.1080/02699050902997888
   Strand EA, 1996, J SPEECH HEAR RES, V39, P1018, DOI 10.1044/jshr.3905.1018
   Tamplin J, 2008, NEUROREHABILITATION, V23, P207
   Tjaden K, 2000, CLIN LINGUIST PHONET, V14, P619, DOI 10.1080/026992000750048143
   Tse ACY, 2013, J SPEECH LANG HEAR R, V56, P906, DOI 10.1044/1092-4388(2012/12-0051)
   Whitehill T, 2002, INVESTIGATIONS IN CLINICAL PHONETICS AND LINGUISTICS, P405
   Whitehill TL, 2002, CLEFT PALATE-CRAN J, V39, P50, DOI 10.1597/1545-1569(2002)039<0050:AIISWC>2.0.CO;2
   Witzel M.A., 1995, CLEFT PALATE SPEECH, P137
   Wyatt R, 1996, BRIT J PLAST SURG, V49, P143, DOI 10.1016/S0007-1226(96)90216-7
   Yorkston K.M., 1999, Management of motor speech disorders in children and adults
   Yorkston K. M., 1984, DYSARTHRIAS PHYSL AC, P197
   YORKSTON KM, 1990, J SPEECH HEAR DISORD, V55, P550, DOI 10.1044/jshd.5503.550
   YORKSTON KM, 2010, MANAGEMENT MOTOR SPE
   Youmans G, 2011, AM J SPEECH-LANG PAT, V20, P23, DOI 10.1044/1058-0360(2010/09-0085)
NR 57
TC 11
Z9 11
U1 4
U2 9
PU TAYLOR & FRANCIS INC
PI PHILADELPHIA
PA 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
SN 0269-9206
EI 1464-5076
J9 CLIN LINGUIST PHONET
JI Clin. Linguist. Phon.
PD APR 2
PY 2020
VL 34
IS 4
BP 327
EP 338
DI 10.1080/02699206.2019.1652692
EA AUG 2019
PG 12
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA KR4EG
UT WOS:000484032900001
PM 31455101
DA 2024-01-09
ER

PT J
AU Vogel, AP
   Stoll, LH
   Oettinger, A
   Rommel, N
   Kraus, EM
   Timmann, D
   Scott, D
   Atay, C
   Storey, E
   Schöls, L
   Synofzik, M
AF Vogel, Adam P.
   Stoll, Lisa H.
   Oettinger, Andreas
   Rommel, Natalie
   Kraus, Eva-Maria
   Timmann, Dagmar
   Scott, Dion
   Atay, Christina
   Storey, Elsdon
   Schoels, Ludger
   Synofzik, Matthis
TI Speech treatment improves dysarthria in multisystemic ataxia: a
   rater-blinded, controlled pilot-study in ARSACS
SO JOURNAL OF NEUROLOGY
LA English
DT Article
DE Speech; Dysarthria; Rehabilitation; Acoustics; Ataxic neuropathy; Voice
ID PARKINSONS-DISEASE; INTELLIGIBILITY; ADULTS; CLEAR
AB We aimed to provide proof-of-principle evidence that intensive home-based speech treatment can improve dysarthria in complex multisystemic degenerative ataxias, exemplified by autosomal recessive spastic ataxia Charlevoix-Saguenay (ARSACS). Feasibility and piloting efficacy of speech training specifically tailored to cerebellar dysarthria was examined through a 4-week program in seven patients with rater-blinded assessment of intelligibility (primary outcome) and naturalness and acoustic measures of speech (secondary outcomes) performed 4 weeks before, immediately prior to, and directly after training (intraindividual control design). Speech intelligibility and naturalness improved post treatment. This provides piloting evidence that ataxia-tailored speech treatment might be effective in degenerative cerebellar disease.
C1 [Vogel, Adam P.; Stoll, Lisa H.; Rommel, Natalie; Kraus, Eva-Maria; Schoels, Ludger; Synofzik, Matthis] Hertie Inst Clin Brain Res, Dept Neurodegenerat, Tubingen, Germany.
   [Vogel, Adam P.; Stoll, Lisa H.; Rommel, Natalie; Kraus, Eva-Maria; Schoels, Ludger; Synofzik, Matthis] Univ Tubingen, Univ Hosp Tubingen, Ctr Neurol, Tubingen, Germany.
   [Vogel, Adam P.] Univ Melbourne, Ctr Neurosci Speech, 550 Swanston St, Melbourne, Vic 3010, Australia.
   [Vogel, Adam P.] RedenLab, Melbourne, Vic, Australia.
   [Stoll, Lisa H.; Rommel, Natalie] Univ Hosp Tubingen, Therapiezentrum, Tubingen, Germany.
   [Oettinger, Andreas] Kliniken Schmieder, Neurol & Rehabil, Gailingen, Germany.
   [Timmann, Dagmar] Univ Duisburg Essen, Essen Univ Hosp, Dept Neurol, Essen, Germany.
   [Scott, Dion; Atay, Christina] Univ Queensland, St Lucia, Qld, Australia.
   [Storey, Elsdon] Monash Univ, Dept Med, Melbourne, Vic, Australia.
   [Schoels, Ludger; Synofzik, Matthis] Ctr Neurodegenerat Dis DZNE, Tubingen, Germany.
C3 Eberhard Karls University of Tubingen; Eberhard Karls University
   Hospital; Eberhard Karls University of Tubingen; Eberhard Karls
   University Hospital; University of Melbourne; Eberhard Karls University
   of Tubingen; Eberhard Karls University Hospital; University of Duisburg
   Essen; University of Queensland; Monash University; Helmholtz
   Association; German Center for Neurodegenerative Diseases (DZNE)
RP Vogel, AP (corresponding author), Hertie Inst Clin Brain Res, Dept Neurodegenerat, Tubingen, Germany.; Vogel, AP (corresponding author), Univ Tubingen, Univ Hosp Tubingen, Ctr Neurol, Tubingen, Germany.; Vogel, AP (corresponding author), Univ Melbourne, Ctr Neurosci Speech, 550 Swanston St, Melbourne, Vic 3010, Australia.; Vogel, AP (corresponding author), RedenLab, Melbourne, Vic, Australia.
EM vogela@unimelb.edu.au
RI Vogel, Adam/ABD-7685-2020; Schöls, Ludger/ABB-2482-2021
OI Vogel, Adam/0000-0002-3505-2631; Schöls, Ludger/0000-0001-7774-5025;
   Synofzik, Matthis/0000-0002-2280-7273
FU Ataxia Charlevoix-Saguenay Foundation; European Union [643578]; BMBF
   [01GM1607]; Alexander von Humboldt Foundation; IZKF Promotionskolleg
   Tubingen [IZKF 2016-1-07]
FX This study was supported by the Ataxia Charlevoix-Saguenay Foundation
   and from the European Union's Horizon 2020 research and innovation
   program under the ERA-NET Cofund action No 643578. It was supported by
   the BMBF (01GM1607 to M.S.), under the frame of the E-Rare-3 network
   PREPARE (to M.S.). A.P.V. received salaried support from the National
   Health and Medical Research Council, Australia (Career Development
   Fellowship ID 1082910), and received funding from the Alexander von
   Humboldt Foundation. This study was supported by the IZKF
   Promotionskolleg Tubingen (IZKF 2016-1-07) to M.S. and E.K.
CR Ballard KJ, 2010, J SPEECH
   Constantinescu G, 2011, INT J LANG COMM DIS, V46, P1, DOI 10.3109/13682822.2010.484848
   Hargrove PM, 2013, CLIN LINGUIST PHONET, V27, P647, DOI 10.3109/02699206.2013.777121
   KATZ S, 1983, J AM GERIATR SOC, V31, P721, DOI 10.1111/j.1532-5415.1983.tb03391.x
   Nasreddine ZS, 2005, J AM GERIATR SOC, V53, P695, DOI 10.1111/j.1532-5415.2005.53221.x
   Park S, 2016, AM J SPEECH-LANG PAT, V25, P97, DOI 10.1044/2015_AJSLP-14-0113
   SCHIAVETTI N, 1981, J SPEECH HEAR RES, V24, P441, DOI 10.1044/jshr.2403.441
   Schmitz-Hübsch T, 2006, NEUROLOGY, V66, P1717, DOI 10.1212/01.wnl.0000219042.60538.92
   Synofzik M, 2013, ORPHANET J RARE DIS, V8, DOI 10.1186/1750-1172-8-41
   Tamplin J, 2008, NEUROREHABILITATION, V23, P207
   Tjaden K, 2014, J SPEECH LANG HEAR R, V57, P779, DOI 10.1044/2014_JSLHR-S-12-0372
   Vogel AP, 2018, J NEUROL, V265, P2060, DOI 10.1007/s00415-018-8950-4
   Vogel AP, 2017, MITOCHONDRION, V37, P1, DOI 10.1016/j.mito.2017.06.002
   Vogel AP, 2011, J VOICE, V25, P137, DOI 10.1016/j.jvoice.2009.09.003
   Vogel AP, 2014, COCHRANE DATABASE SY, P10
   Weismer G, 2002, J SPEECH LANG HEAR R, V45, P421, DOI 10.1044/1092-4388(2002/033)
NR 16
TC 17
Z9 18
U1 0
U2 8
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 0340-5354
EI 1432-1459
J9 J NEUROL
JI J. Neurol.
PD MAY
PY 2019
VL 266
IS 5
BP 1260
EP 1266
DI 10.1007/s00415-019-09258-4
PG 7
WC Clinical Neurology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Neurosciences & Neurology
GA HT8RT
UT WOS:000464833000024
PM 30840144
DA 2024-01-09
ER

PT J
AU Czaplicki, B
AF Czaplicki, Bartlomiej
TI Measuring the phonological (un)naturalness of selected alternation
   patterns in Polish
SO LANGUAGE SCIENCES
LA English
DT Article
DE Naturalness; Morphology-phonology interface; Cophonology theory;
   Consonant mutation; Palatalization; Progressive voice assimilation
ID MORPHOLOGY; REGULARITIES; PERSPECTIVE; CONSTRAINTS; INFERENCE; ENGLISH;
   RULES
AB In most generative research phonological naturalness/markedness has served as a synchronic bias that can explain the predominance of certain patterns in the world's languages. In this paper, on the basis of one language, Polish, it is shown that unnatural patterns are far from rare and, therefore, phonological theories need to accommodate them. The two patterns under scrutiny, consonant mutations and progressive devoicing, are to a large degree unnatural but fully productive. Consonant mutations are subjected to a thorough examination using data from 604 outputs of the concatenation of 27 mutation triggering suffixes, both vowel- and consonant-initial, in order to assess the role of various predictors of palatality. As there is no observable effect of naturalness but a strong influence of specific suffix-initial segments, base-final consonants and individual suffixes on the palatality of the output, the data provide support for a framework that does not incorporate phonological naturalness as an active bias in models of grammar. The morphophonological patterns, formalized as source-oriented schemas, are morpheme specific. This discussion provides evidence for a model of grammar comprising multiple morpheme-specific cophonologies. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Czaplicki, Bartlomiej] Univ Warsaw, Inst English Studies, Ul Hoza 69, PL-00681 Warsaw, Poland.
C3 University of Warsaw
RP Czaplicki, B (corresponding author), Univ Warsaw, Inst English Studies, Ul Hoza 69, PL-00681 Warsaw, Poland.
EM bczaplicki@uw.edu.pl
RI Czaplicki, Bartlomiej/AAM-9366-2021
CR Albright A, 2003, COGNITION, V90, P119, DOI 10.1016/S0010-0277(03)00146-X
   Albright A, 2002, LANGUAGE, V78, P684, DOI 10.1353/lan.2003.0002
   ANDERSON SR, 1981, LINGUIST INQ, V12, P493
   Anderson SR, 2008, WORD STRUCT, V1, P109, DOI 10.3366/E1750124508000184
   [Anonymous], 2012, DIRECT INTERFACE ONE
   [Anonymous], 2006, THESIS
   [Anonymous], UCLA WORKING PAPERS
   [Anonymous], 2000, PHONOLOGY
   [Anonymous], 2010, CONSTRUCTION MORPHOL
   [Anonymous], P BERK LING SOC
   [Anonymous], 1976, AUTOSEGMENTAL PHONOL
   Aronoff Mark, 1994, Morphology by itself
   Baayen R.H., 2003, MORPHOLOGICAL STRUCT, P355, DOI [DOI 10.1515/9783110910186, 10.1515/9783110910186]
   Bank M., 2003, INDEKS TERGO UNIWERS
   Barnes J, 2006, STRENGTH WEAKNESS IN
   Becker M, 2016, LINGUIST INQ, V47, P391, DOI 10.1162/ling_a_00217
   Berent I, 2007, COGNITION, V104, P591, DOI 10.1016/j.cognition.2006.05.015
   Blevins J, 2006, THEOR LINGUIST, V32, P117, DOI 10.1515/TL.2006.009
   Blevins Juliette, 2004, Evolutionary Phonology: The Emergence of Sound Patterns
   Booij G., 1995, PHONOLOGY DUTCH
   Booij G, 2017, COGNITIVE SCI, V41, P277, DOI 10.1111/cogs.12323
   Booij G, 2009, CURR STUD LINGUIST, V47, P487
   Booij Geert, 2018, PHONOLOGY FIELDWORK, P13
   Brown J, 2006, THEOR LINGUIST, V32, P175, DOI 10.1515/TL.2006.011
   Bybee J. L., 2001, PHONOLOGY LANGUAGE U
   Cameron-Faulkner T, 2000, NAT LANG LINGUIST TH, V18, P813, DOI 10.1023/A:1006496821412
   CARSTAIRS A, 1990, TREND LIN S, V49, P17
   Carstairs Andrew, 1988, YB MORPHOLOGY 1988, V1988, P68
   Chambers KE, 2003, COGNITION, V87, pB69, DOI 10.1016/S0010-0277(02)00233-0
   Chomsky Noam., 1968, The Sound Pattern of English
   Clements G. N., 1985, Phonology Yearbook, V2, P225, DOI [10.1017/S0952675700000440., 10.1017/S0952675700000440, DOI 10.1017/S0952675700000440]
   Czaplicki B, 2014, POZ STUD CONTEMP LIN, V50, P419, DOI 10.1515/psicl-2014-0022
   Czaplicki B, 2013, LINGUA, V123, P31, DOI 10.1016/j.lingua.2012.10.002
   Czaplicki B, 2010, J SLAV LINGUIST, V18, P259, DOI 10.1353/is1.2010.0002
   Czaplicki B, 2010, POZ STUD CONTEMP LIN, V46, P177, DOI 10.2478/v10010-010-0009-3
   Czaplicki Bartlomiej, 2016, PHONOLOGY ITS FACES, P19
   Czaplicki Bartlomiej, 2014, LEXICON BASED PHONOL
   Czaplicki Bartlomiej, 2018, PHONOLOGY FIELDWORK, P185
   Czaplicki Bartlomiej, 2016, STUDIES LEXICOGRAMMA, P261, DOI [10.1075/hcp.54.14cza, DOI 10.1075/HCP.54.14CZA]
   Dabrowska E, 2004, LANG COGNITIVE PROC, V19, P225, DOI 10.1080/01690960344000170
   Dabrowska Ewa., 2004, LANGUAGE MIND BRAIN
   de Courtenay Baudouin, 1972, BAUDOUIN COURTENAY A
   De Lacy P, 2006, THEOR LINGUIST, V32, P185
   de Lacy P, 2013, NAT LANG LINGUIST TH, V31, P287, DOI 10.1007/s11049-013-9191-y
   de Lacy Paul, 2002, THESIS
   Embick David, 2010, LINGUISTIC INQUIRY M, V60
   Gouskova M, 2015, LINGUA, V167, P41, DOI 10.1016/j.lingua.2015.08.014
   Grzegorczykowa Renata, 1999, The Grammar of Contemporary Polish. Morphology, P389
   Guion Susan G., 1996, THESIS
   Gussmann E., 1992, PHONOLOGICAL INVESTI, P5
   Gussmann E., 2007, The Phonology of Polish
   Hale Mark, 2008, The Phonological Enterprise
   Hamann S., 2002, OTS YB 2002, P105
   Hansson Gunnar O., 2014, HDB PHONOLOGICAL THE, P319
   Haspelmath M, 2006, J LINGUIST, V42, P25, DOI 10.1017/S0022226705003683
   Hayes B, 2013, LINGUIST INQ, V44, P45, DOI 10.1162/LING_a_00119
   Hayes B, 2009, LANGUAGE, V85, P822
   Hayes Bruce, 2004, PHONETICALLY BASED P, DOI DOI 10.1017/CBO9780511486401
   Henderson Eugenie, 1992, MON KHMER STUDIES, V18-19, P61
   Hothorn T, 2006, J COMPUT GRAPH STAT, V15, P651, DOI 10.1198/106186006X133933
   Hyman L.M., 1975, Phonology: Theory and analysis
   Hyman L. M., 2001, ROLE SPEECH PERCEPTI, P141
   Inkelas Sharon, 2014, INTERPLAY MORPHOLOGY
   JACKENDOFF R, 1975, LANGUAGE, V51, P639, DOI 10.2307/412891
   Jackendoff R., 2002, FDN LANGUAGE
   Kiparsky P, 2006, THEOR LINGUIST, V32, P217, DOI 10.1515/TL.2006.015
   Kiparsky Paul, 2008, LINGUISTIC UNIVERSAL, P25, DOI [10.1093/acprof:oso/9780199298495.003.0002, DOI 10.1093/ACPROF:OSO/9780199298495.003.0002]
   Kochetov vanOostendorp., 2011, BLACKWELL COMPANION, V3, P1666, DOI DOI 10.1002/9781444335262.WBCTP0071
   Kutsch Lojenga Constance, 1994, NGITI CENTRAL SUDANI
   Langacker Ronald W., 1987, Foundations of Cognitive Grammar: Theoretical prerequisites, VI
   Langacker Ronald W., 2000, Usage -based models of language, P1
   Lindblom B., 1990, NATO ASI SERIES, V55
   Marcus G., 1992, MONOGRAPHS SOC RES C, V57
   Mielke Jeffrey, 2008, The emergence of distinctive features
   Ohala J. J., 1994, Sound Symbolism, DOI DOI 10.1017/CBO9780511751806.022
   Ohala John, 1983, The production of Speech, P189, DOI [10.1007/978-1-4613-8202-7_9, DOI 10.1007/978-1-4613-8202-7_9]
   Ohala John J., 1981, PAPERS PARASESSION L, P178, DOI DOI 10.1075/CILT.323.05OHA
   Onishi KH, 2002, COGNITION, V83, pB13, DOI 10.1016/S0010-0277(01)00165-2
   Peperkamp Sharon, 2007, Laboratory Phonology 9, V9, P315
   Pierrehumbert, 2006, LAB PHONOLOGY, P81, DOI 10.1515/9783110197211.1.81
   Prince, 2004, OPTIMALITY THEORY CO, DOI 10.1002/97804
   Pycha A., 2003, WCCFL, P101
   Rubach J, 1996, LINGUIST INQ, V27, P69
   Rubach J, 2007, LINGUIST INQ, V38, P85, DOI 10.1162/ling.2007.38.1.85
   Rubach Jerzy, 1984, Cyclic and lexical phonology. The structure of Polish
   Sagey Elizabeth, 1986, THESIS
   Stockwell R., 1972, LINGUISTIC CHANGE GE, P1
   Svantesson Jan-Olof., 2005, PHONOLOGY MONGOLIAN
   Tomasello M., 2003, Constructing a Language: The usage-based theory of language acquisition
   Wilson C, 2006, COGNITIVE SCI, V30, P945, DOI 10.1207/s15516709cog0000_89
   Wilson Colin, 2003, WCCFL, P533
   Yu Alan C. L., 2014, HDB PHONOLOGICAL THE, P291
   Zsiga Elizabeth C., 1993, THESIS
NR 93
TC 3
Z9 3
U1 0
U2 2
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0388-0001
EI 1873-5746
J9 LANG SCI
JI Lang. Sci.
PD MAR
PY 2019
VL 72
BP 160
EP 187
DI 10.1016/j.langsci.2018.10.002
PG 28
WC Linguistics; Language & Linguistics
WE Social Science Citation Index (SSCI); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Linguistics
GA HQ0UR
UT WOS:000462111000011
DA 2024-01-09
ER

PT J
AU Venkatraman, A
   Sivasankar, MP
AF Venkatraman, Anumitha
   Sivasankar, M. Preeti
TI Continuous Vocal Fry Simulated in Laboratory Subjects: A Preliminary
   Report on Voice Production and Listener Ratings
SO AMERICAN JOURNAL OF SPEECH-LANGUAGE PATHOLOGY
LA English
DT Article
ID PHONATION THRESHOLD PRESSURE; YOUNG; QUALITY
AB Purpose: Vocal fry is prevalent in everyday speech. However, whether the use of vocal fry is detrimental to voice production is unclear. This preliminary study assessed the effects of using continuous vocal fry on voice production measures and listener ratings.
   Method: Ten healthy individuals (equal male and female, mean age = 22.4 years) completed 2 counterbalanced sessions. In each session, participants read in continuous vocal fry or habitual voice quality for 30 min at a comfortable intensity. Continuous vocal fry was simulated. Phonation threshold pressure (PTP10 and PTP20), cepstral peak prominence, and vocal effort ratings were obtained before and after the production of each voice quality. Next, 10 inexperienced listeners (equal male and female, mean age = 24.1 years) used visual analog scales to rate paired samples of continuous vocal fry and habitual voice quality for naturalness, employability, and amount of listener concentration.
   Results: PTP10 and vocal effort ratings increased after 30 min of continuous vocal fry. Inexperienced listeners rated continuous vocal fry more negatively than the habitual voice quality.
   Conclusions: Thirty minutes of simulated, continuous vocal fry worsened some voice measures when compared with a habitual voice quality. Samples of continuous vocal fry were rated as significantly less employable, less natural, and requiring greater listener concentration as compared with samples of habitual voice quality. Future studies should include habitual users of vocal fry to investigate speech stimulability and adaptation with cueing to further understand pathogenesis of vocal fry.
C1 [Venkatraman, Anumitha; Sivasankar, M. Preeti] Purdue Univ, Dept Speech Language & Hearing Sci, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University West Lafayette Campus;
   Purdue University
RP Sivasankar, MP (corresponding author), Purdue Univ, Dept Speech Language & Hearing Sci, W Lafayette, IN 47907 USA.
EM msivasan@purdue.edu
FU National Institutes of Health T32 Training Grant [2T32DC000030-26]
FX Funding was provided by a National Institutes of Health T32 Training
   Grant 2T32DC000030-26 to the Department of Speech, Language, & Hearing
   Sciences at Purdue University. This work was based on a thesis by the
   first author submitted in partial fulfillment of the MS-SLP degree from
   the Department of Speech, Language, & Hearing Sciences at Purdue
   University. The authors thank the members of the MS-thesis committee
   Barbara Solomon, Georgia Malandraki, and Christine Weber for their
   insightful comments. Bruce Craig and Ryan Murphy at the Purdue
   University Statistical Consulting Services assisted with the statistical
   analysis. The authors also thank Robert Fujiki, Abigail Chapleau, and
   Sara Loerch for their contributions to the data analysis.
CR Abdelli-Beruh NB, 2014, J VOICE, V28, P185, DOI 10.1016/j.jvoice.2013.08.011
   Anderson RC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0097506
   [Anonymous], 1980, The Phonetic Description of Voice Quality
   Awan SN, 2010, CLIN LINGUIST PHONET, V24, P742, DOI 10.3109/02699206.2010.492446
   Baldner EF, 2015, J VOICE, V29, P530, DOI 10.1016/j.jvoice.2014.08.017
   Blomgren M, 1998, J ACOUST SOC AM, V103, P2649, DOI 10.1121/1.422785
   Borrie SA, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.12.005
   Cantor-Cutiva L. C, 2017, LOGOP PHONIATR VOCO, V43, P1
   Chen Y, 2002, J SPEECH LANG HEAR R, V45, P821, DOI 10.1044/1092-4388(2002/066)
   CHILDERS DG, 1991, J ACOUST SOC AM, V90, P2394, DOI 10.1121/1.402044
   Erickson E, 2010, J SPEECH LANG HEAR R, V53, P75, DOI 10.1044/1092-4388(2009/09-0024)
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   Fisher KV, 1997, J SPEECH LANG HEAR R, V40, P1122, DOI 10.1044/jslhr.4005.1122
   Fujiki RB, 2017, J VOICE, V31, P211, DOI 10.1016/j.jvoice.2016.07.005
   Guzman M, 2013, INT J SPEECH-LANG PA, V15, P127, DOI 10.3109/17549507.2012.702283
   Heman-Ackah YD, 2014, J VOICE, V28, DOI 10.1016/j.jvoice.2014.05.005
   Kempster GB, 2009, AM J SPEECH-LANG PAT, V18, P124, DOI 10.1044/1058-0360(2008/08-0017)
   Laukkanen AM, 2004, FOLIA PHONIATR LOGO, V56, P335, DOI 10.1159/000081081
   MCGLONE RE, 1971, J SPEECH HEAR RES, V14, P769, DOI 10.1044/jshr.1404.769
   Nagle KF, 2012, J COMMUN DISORD, V45, P235, DOI 10.1016/j.jcomdis.2012.01.001
   Oliveira G, 2016, J VOICE, V30, P684, DOI 10.1016/j.jvoice.2015.08.015
   Oppenheimer DM, 2009, J EXP SOC PSYCHOL, V45, P867, DOI 10.1016/j.jesp.2009.03.009
   Parker MA, 2018, J VOICE, V32, P538, DOI 10.1016/j.jvoice.2017.08.002
   Plexico LW, 2017, J VOICE, V31, DOI 10.1016/j.jvoice.2016.10.004
   Raj A, 2010, J VOICE, V24, P363, DOI 10.1016/j.jvoice.2008.10.005
   Remacle A, 2012, J VOICE, V26, pE177, DOI 10.1016/j.jvoice.2011.07.016
   Rosen CA, 2000, J VOICE, V14, P619, DOI 10.1016/S0892-1997(00)80017-X
   Roy N, 2003, J VOICE, V17, P331, DOI 10.1067/S0892-1997(03)00078-X
   Solomon NP, 2007, J VOICE, V21, P541, DOI 10.1016/j.jvoice.2006.04.002
   STEMPLE JC, 1994, J VOICE, V8, P271, DOI 10.1016/S0892-1997(05)80299-1
   VERDOLINIMARSTON K, 1990, J VOICE, V4, P142, DOI 10.1016/S0892-1997(05)80139-0
   Wolk L, 2012, J VOICE, V26, pE111, DOI 10.1016/j.jvoice.2011.04.007
   Yuasa IP, 2010, AM SPEECH, V85, P315, DOI 10.1215/00031283-2010-018
NR 33
TC 2
Z9 2
U1 0
U2 2
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 2200 RESEARCH BLVD, #271, ROCKVILLE, MD 20850-3289 USA
SN 1058-0360
EI 1558-9110
J9 AM J SPEECH-LANG PAT
JI Am. J. Speech-Lang. Pathol.
PD NOV
PY 2018
VL 27
IS 4
BP 1539
EP 1545
DI 10.1044/2018_AJSLP-17-0212
PG 7
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA HB6TX
UT WOS:000451206200017
PM 30178028
OA Green Published
DA 2024-01-09
ER

PT J
AU Rao, MVA
   Victory, JS
   Ghosh, PK
AF Rao, Achuth M., V
   Victory, Shiny J.
   Ghosh, Prasanta Kumar
TI Effect of source filter interaction on isolated vowel-consonant-vowel
   perception
SO JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA
LA English
DT Article
ID QUALITY SPEECH SYNTHESIS; PHONATION; F0
AB Source-filter interaction explains the drop in pitch in voiced consonant due to constriction in the vocal tract during vowel-consonant-vowel (VCV) production. In this work, a perceptual study is conducted where the pitch contour in the voiced consonant region is modified to four different levels and a listening test is performed to assess the naturalness of the VCVs synthesized with the modified pitch contour. The listening test with 30 listeners shows no statistically significant difference between the naturalness of the original and synthesized VCVs with modified pitch indicating that pitch drop due to source-filter interaction may not be critical for the perceived naturalness of VCVs. (C) 2018 Acoustical Society of America
C1 [Rao, Achuth M., V; Ghosh, Prasanta Kumar] Indian Inst Sci, Bangalore, Karnataka, India.
   [Victory, Shiny J.] Mepco Schlenk Engn Coll, Sivakasi, India.
C3 Indian Institute of Science (IISC) - Bangalore; Mepco Schlenk
   Engineering College
RP Rao, MVA (corresponding author), Indian Inst Sci, Bangalore, Karnataka, India.
EM achuthr@iisc.ac.in; jshinyv96@gmail.com; prasantg@iisc.ac.in
CR [Anonymous], 1971, ACOUSTIC THEORY SPEE
   [Anonymous], 2010, IEICE T INFORM SYSTE
   Bavegard M., 2009, WORKING PAPERS LINGU, V43, P38
   Chi XM, 2007, J ACOUST SOC AM, V122, P1735, DOI 10.1121/1.2756793
   EWAN WG, 1979, J ACOUST SOC AM, V66, P358, DOI 10.1121/1.383669
   Fant G, 1979, STL QPSR, V1, P79
   Gelman A, 2005, ANN STAT, V33, P1, DOI 10.1214/009053604000001048
   ITU, 2003, 15341 ITU
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Lucero JC, 2012, J ACOUST SOC AM, V132, P403, DOI 10.1121/1.4728170
   Mittal VK, 2014, J ACOUST SOC AM, V136, P1932, DOI 10.1121/1.4894789
   Morise M, 2016, SPEECH COMMUN, V84, P57, DOI 10.1016/j.specom.2016.09.001
   Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457
   Nord L, 1984, STL QPSR, V25, P25
   Ohala J. J, 1987, EXPLAINING INTRINSIC, P207
   Stevens K. N., 2000, Acoustic Phonetics, V30
   STEVENS KN, 1971, J ACOUST SOC AM, V50, P1180, DOI 10.1121/1.1912751
   Taylor Paul, 2009, TEXT TO SPEECH SYNTH
   Titze I.R., 2006, The Myoelastic Aerodynamic Theory of Phonation
   Titze I, 2008, J ACOUST SOC AM, V123, P1902, DOI 10.1121/1.2832339
   Titze IR, 2008, J ACOUST SOC AM, V123, P2733, DOI 10.1121/1.2832337
   Titze IR, 2016, IEEE-ACM T AUDIO SPE, V24, P2507, DOI [10.1109/taslp.2016.2616543, 10.1109/TASLP.2016.2616543]
   VILKMAN E, 1991, SPEECH COMMUN, V10, P325, DOI 10.1016/0167-6393(91)90001-A
   Yoshimura T., 1999, T I ELECT INF COMMUN, V83, P2099
NR 24
TC 3
Z9 3
U1 0
U2 1
PU ACOUSTICAL SOC AMER AMER INST PHYSICS
PI MELVILLE
PA STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA
SN 0001-4966
EI 1520-8524
J9 J ACOUST SOC AM
JI J. Acoust. Soc. Am.
PD AUG
PY 2018
VL 144
IS 2
BP EL95
EP EL99
DI 10.1121/1.5049510
PG 5
WC Acoustics; Audiology & Speech-Language Pathology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Acoustics; Audiology & Speech-Language Pathology
GA GS4MV
UT WOS:000443620700003
PM 30180658
OA Bronze
DA 2024-01-09
ER

PT J
AU Baird, A
   Jorgensen, SH
   Parada-Cabaleiro, E
   Cummins, N
   Hantke, S
   Schuller, B
AF Baird, Alice
   Jorgensen, Stina Hasse
   Parada-Cabaleiro, Emilia
   Cummins, Nicholas
   Hantke, Simone
   Schuller, Bjoern
TI The Perception of Vocal Traits in Synthesized Voices: Age, Gender, and
   Human Likeness
SO JOURNAL OF THE AUDIO ENGINEERING SOCIETY
LA English
DT Article
ID EMOTION
AB The paralinguistics of the voice are the perceived states and traits that make that voice unique to the human body from which it resonates. In many cases the synthesized voice is produced by concatenated segments of recorded human speech, a complex process that can result in an arguably lifeless voice, which lacks the ability for free-expression among other human qualities. In recent years technology-based companies are developing their own synthesized voice identities, yet seemingly paying little attention to the stereotypical traits being heard. Do such synthetic voice traits differ from the human traits they are modelled on? To explore this, the presented perception study performed by 18 listeners evaluated the paralinguistic traits of gender, age, and human likeness in the IBM voice library. Results herein have shown a similar trend to a previous study by the authors with no voice achieving complete human likeness, no voice being perceived within a single age frequency band, and none tied solidly to their given binary gender-a novel finding as commercially available synthesized voices are typically developed to operate within binary identification structures.
C1 [Baird, Alice; Parada-Cabaleiro, Emilia; Cummins, Nicholas; Hantke, Simone; Schuller, Bjoern] Univ Augsburg, ZDB Chair Embedded Intelligence Hlth Care & Wellb, Augsburg, Germany.
   [Jorgensen, Stina Hasse] Univ Copenhagen, Dept Arts & Cultural Studies, Copenhagen, Denmark.
   [Hantke, Simone] MKK Tech Univ Munchen, MISP Grp, Munich, Germany.
   [Schuller, Bjoern] Imperial Coll London, GLAM, London, England.
C3 University of Augsburg; University of Copenhagen; Technical University
   of Munich; Imperial College London
RP Baird, A (corresponding author), Univ Augsburg, ZDB Chair Embedded Intelligence Hlth Care & Wellb, Augsburg, Germany.
EM alice.baird@informatik.uni-augsburg.de
RI Cummins, Nicholas/AAC-6431-2019; Baird, Alice/AAA-5559-2021; Schuller,
   Björn Wolfgang/D-3241-2011; Parada-Cabaleiro, Emilia/GXF-2079-2022
OI Cummins, Nicholas/0000-0002-1178-917X; Baird, Alice/0000-0002-7003-5650;
   Schuller, Björn Wolfgang/0000-0002-6478-8699; Parada-Cabaleiro,
   Emilia/0000-0003-1843-3632
FU Bavarian State Ministry of Education, Science and the Arts; European
   Union [338164]
FX This work is funded by the Bavarian State Ministry of Education, Science
   and the Arts in the framework of the Centre Digitisation. Bavaria
   (ZD.B), and the European Union's Seventh Framework and Horizon 2020
   Programmes under grant agreement No. 338164 (ERC StG iHEARu).
CR Alku P, 1999, CLIN NEUROPHYSIOL, V110, P1329, DOI 10.1016/S1388-2457(99)00088-7
   [Anonymous], 2006, P LREC
   Arora S.J., 2012, INT J COMPUTER APPL, V60, P34
   Baird Alice, 2017, P 12 INT AUDIO MOSTL, DOI [10.1145/3123514.3123528, DOI 10.1145/3123514.3123528]
   Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008
   Butler J., 1990, GENDER TROUBLE
   Butler J., 2011, BODIES MATTER DISCUR
   COLEMAN RO, 1976, J SPEECH HEAR RES, V19, P168, DOI 10.1044/jshr.1901.168
   Dudley H., 1955, J AUDIO ENG SOC, V3, P170
   FRANCK K, 1949, J CONSULT PSYCHOL, V13, P247
   Hantke S, 2015, INT CONF AFFECT, P891, DOI 10.1109/ACII.2015.7344680
   Hill KT, 2010, CEREB CORTEX, V20, P583, DOI 10.1093/cercor/bhp124
   IBM &REG;, 2017, TEXT TO SPEECH
   IBM Watson Developer Cloud, 2017, SCI SERV
   Johnson K, 1999, J PHONETICS, V27, P359, DOI 10.1006/jpho.1999.0100
   JONES Amelia, 2012, SEEING DIFFERENTLY
   KASHIMA Y, 1995, J PERS SOC PSYCHOL, V69, P925, DOI 10.1037/0022-3514.69.5.925
   KLATT DH, 1990, J ACOUST SOC AM, V87, P820, DOI 10.1121/1.398894
   Latinus M, 2011, CURR BIOL, V21, pR143, DOI 10.1016/j.cub.2010.12.033
   Lattner S, 2005, HUM BRAIN MAPP, V24, P11, DOI 10.1002/hbm.20065
   Laukka P., 2004, THESIS
   Lee EJ, 2003, INT J HUM-COMPUT ST, V58, P347, DOI 10.1016/S1071-5819(03)00009-0
   Lee Eun Ju, 2000, P CHI 00 HUM FACT CO, P289, DOI [10.1145/633292.633461, DOI 10.1145/633292.633461]
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Munoz J. E., 1998, DISIDENTIFCATIONS QU
   Nass C, 1997, J APPL SOC PSYCHOL, V27, P864, DOI 10.1111/j.1559-1816.1997.tb00275.x
   Nass C., 2001, EFFECTS EMOTION VOIC
   Nass C. I., 2010, The Man Who Lied to His Laptop: What Machines Teach Us About Human Relationships
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   OLSON HF, 1966, J AUDIO ENG SOC, V14, P233
   Oord A. V. D., 2016, 160903499 ARXIV, V1609
   Oye O. J., 2016, HCI A GROWING FIELD
   Phan T., 2017, TRANSFORMATIONS, P23
   Pitrelli JF, 2006, IEEE T AUDIO SPEECH, V14, P1099, DOI 10.1109/TASL.2006.876123
   Robertson A., 2016, GOOGLES DEEPMIND AI
   Scherer KR, 2013, COMPUT SPEECH LANG, V27, P40, DOI 10.1016/j.csl.2011.11.003
   Scherer KR, 2001, J CROSS CULT PSYCHOL, V32, P76, DOI 10.1177/0022022101032001009
   Schuller B., 2013, Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing
   Schuller B, 2013, INTERSPEECH, P148
   Schuller B, 2015, COMPUT SPEECH LANG, V29, P100, DOI 10.1016/j.csl.2014.08.003
   Schuller B, 2011, SPEECH COMMUN, V53, P1062, DOI 10.1016/j.specom.2011.01.011
   Sedgwick Eve Kosofsky, 1993, TENDENCIES
   Seppala T. J., 2017, AMAZONS REDESIGNED E
   Sutikno T., 2010, J TELECOMM COMPUTING, V9, P201
   Sycamore Mattilda Bernstein, 2006, Nobody Passes: Rejecting the Rules of Gender and Conformity
   Tamagawa R, 2011, INT J SOC ROBOT, V3, P253, DOI 10.1007/s12369-011-0100-4
   Tokuda K, 2013, P IEEE, V101, P1234, DOI 10.1109/JPROC.2013.2251852
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
NR 49
TC 11
Z9 11
U1 4
U2 13
PU AUDIO ENGINEERING SOC
PI NEW YORK
PA 60 E 42ND ST, NEW YORK, NY 10165-2520 USA
SN 1549-4950
J9 J AUDIO ENG SOC
JI J. Audio Eng. Soc.
PD APR
PY 2018
VL 66
IS 4
BP 277
EP 285
DI 10.17743/jaes.2018.0023
PG 9
WC Acoustics; Engineering, Multidisciplinary
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Acoustics; Engineering
GA GW9GC
UT WOS:000447289400009
DA 2024-01-09
ER

PT J
AU Hubbard, DJ
   Faso, DJ
   Assmann, PF
   Sasson, NJ
AF Hubbard, Daniel J.
   Faso, Daniel J.
   Assmann, Peter F.
   Sasson, Noah J.
TI Production and perception of emotional prosody by adults with autism
   spectrum disorder
SO AUTISM RESEARCH
LA English
DT Article
DE Autism spectrum disorder; emotion; affective prosody; expressive speech;
   vocal affect; speech production; speech perception
ID CHILDREN; COMMUNICATION; EXPRESSION; SPEECH; VOICE; ADOLESCENTS;
   INTONATION; SPEAKERS; MODEL; MIND
AB This study examined production and perception of affective prosody by adults with autism spectrum disorder (ASD). Previous research has reported increased pitch variability in talkers with ASD compared to typically developing (TD) controls in grammatical speaking tasks (e.g., comparing interrogative vs. declarative sentences), but it is unclear whether this pattern extends to emotional speech. In this study, speech recordings in five emotion contexts (angry, happy, interested, sad, and neutral) were obtained from 15 adult males with ASD and 15 controls (Experiment 1), and were later presented to 52 listeners (22 with ASD) who were asked to identify the emotion expressed and rate the level of naturalness of the emotion in each recording (Experiment 2). Compared to the TD group, talkers with ASD produced phrases with greater intensity, longer durations, and increased pitch range for all emotions except neutral, suggesting that their greater pitch variability was specific to emotional contexts. When asked to identify emotion from speech, both groups of listeners were more accurate at identifying the emotion context from speech produced by ASD speakers compared to TD speakers, but rated ASD emotional speech as sounding less natural. Collectively, these results reveal differences in emotional speech production in talkers with ASD that provide an acoustic basis for reported perceptions of oddness in the speech presentation of adults with ASD. Autism Res2017, 10: 1991-2001. (c) 2017 International Society for Autism Research, Wiley Periodicals, Inc.
   Lay SummaryThis study examined emotional speech communication produced and perceived by adults with autism spectrum disorder (ASD) and typically-developing (TD) controls. Compared to the TD group, talkers with ASD produced emotional phrases that were louder, longer, and more variable in pitch. Both ASD and TD listeners were more accurate at identifying emotion in speech produced by ASD speakers compared to TD speakers, but rated ASD emotional speech as sounding less natural.
C1 [Hubbard, Daniel J.; Faso, Daniel J.; Assmann, Peter F.; Sasson, Noah J.] Univ Texas Dallas, Sch Behav & Brain Sci, GR41,800 West Campbell Rd, Dallas, TX 75080 USA.
C3 University of Texas System; University of Texas Dallas
RP Hubbard, DJ (corresponding author), Univ Texas Dallas, Sch Behav & Brain Sci, GR41,800 West Campbell Rd, Dallas, TX 75080 USA.
EM dhubbard@utdallas.edu
OI Sasson, Noah/0000-0002-3676-1253
FU National Institute of Mental Health at National Institutes of Health
   [R15 MH1015945]; National Science Foundation [1124479]; Division Of
   Behavioral and Cognitive Sci; Direct For Social, Behav & Economic Scie
   [1124479] Funding Source: National Science Foundation
FX The authors wish to thank the individuals who participated in the study,
   and the Nonpareil Institute of Plano, TX for helping with participant
   recruitment. We thank the reviewers for their comments on a previous
   version of the manuscript. This research was funded in part by a grant
   from the National Institute of Mental Health at National Institutes of
   Health (R15 MH1015945, PI Sasson). This work is based on a doctoral
   dissertation by DH, who was supported in part by a grant from the
   National Science Foundation (1124479, PI Assmann).
CR Bänziger T, 2012, EMOTION, V12, P1161, DOI 10.1037/a0025827
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Bates D, 2015, J STAT SOFTW, V67, P1, DOI 10.18637/jss.v067.i01
   Begeer S, 2008, DEV REV, V28, P342, DOI 10.1016/j.dr.2007.09.001
   Boersma P., 2014, PRAAT DOING PHONETIC
   Diehl JJ, 2012, RES AUTISM SPECT DIS, V6, P123, DOI 10.1016/j.rasd.2011.03.012
   Fairbanks G, 1939, SPEECH MONOGR, V6, P87, DOI 10.1080/03637753909374863
   Faso DJ, 2015, J AUTISM DEV DISORD, V45, P75, DOI 10.1007/s10803-014-2194-7
   Fosnot S.M., 1999, P 14 INT C PHON SCI, P1925
   Golan O, 2007, J AUTISM DEV DISORD, V37, P1096, DOI 10.1007/s10803-006-0252-5
   Green H., 2009, INT J SPEECH LANGUAG, V11, P308, DOI DOI 10.1080/17549500903003060
   Grossman RB, 2012, RES AUTISM SPECT DIS, V6, P1150, DOI 10.1016/j.rasd.2012.03.006
   Hubbard K, 2007, J PSYCHOLINGUIST RES, V36, P159, DOI 10.1007/s10936-006-9037-4
   Hudenko WJ, 2012, AUTISM, V16, P641, DOI 10.1177/1362361311402856
   Hurley RSE, 2007, J AUTISM DEV DISORD, V37, P1679, DOI 10.1007/s10803-006-0299-3
   Hus V, 2014, J AUTISM DEV DISORD, V44, P1996, DOI 10.1007/s10803-014-2080-3
   Juslin PN, 2003, PSYCHOL BULL, V129, P770, DOI 10.1037/0033-2909.129.5.770
   Kawahara H, 1999, SPEECH COMMUN, V27, P187, DOI 10.1016/S0167-6393(98)00085-5
   Lord C, 2000, J AUTISM DEV DISORD, V30, P205, DOI 10.1023/A:1005592401947
   Lyons M, 2014, AUTISM RES, V7, P181, DOI 10.1002/aur.1355
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   Nadig A, 2012, J AUTISM DEV DISORD, V42, P499, DOI 10.1007/s10803-011-1264-3
   Paul R, 2005, J AUTISM DEV DISORD, V35, P205, DOI 10.1007/s10803-004-1999-1
   Paul R, 2008, RES AUTISM SPECT DIS, V2, P110, DOI 10.1016/j.rasd.2007.04.001
   Peppé S, 2003, CLIN LINGUIST PHONET, V17, P345, DOI 10.1080/0269920031000079994
   Peppé S, 2007, J SPEECH LANG HEAR R, V50, P1015, DOI 10.1044/1092-4388(2007/071)
   R Core Team, 2018, R: A Language and Environment for Statistical Computing
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Russell JA, 2003, PSYCHOL REV, V110, P145, DOI 10.1037/0033-295X.110.1.145
   Rutherford MD, 2002, J AUTISM DEV DISORD, V32, P189, DOI 10.1023/A:1015497629971
   Sasson NJ, 2017, SCI REP-UK, V7, DOI 10.1038/srep40700
   Sasson NJ, 2013, AUTISM RES, V6, P134, DOI 10.1002/aur.1272
   Sasson NJ, 2011, J NEURODEV DISORD, V3, P87, DOI 10.1007/s11689-010-9068-x
   SCHERER KR, 1986, PSYCHOL BULL, V99, P143, DOI 10.1037/0033-2909.99.2.143
   Scherer KR, 2003, SPEECH COMMUN, V40, P227, DOI 10.1016/S0167-6393(02)00084-5
   SCHERER KR, 1979, EMOTIONS PERSONALITY, P495
   Shriberg LD, 2011, J AUTISM DEV DISORD, V41, P405, DOI 10.1007/s10803-010-1117-5
   Stewart ME, 2013, AUTISM, V17, P6, DOI 10.1177/1362361311424572
   Uljarevic M, 2013, J AUTISM DEV DISORD, V43, P1517, DOI 10.1007/s10803-012-1695-5
   Van Bourgondien M. E., 1992, HIGH FUNCTIONING IND, P227, DOI DOI 10.1007/978-1-4899-2456-8_12
   Wang AT, 2007, ARCH GEN PSYCHIAT, V64, P698, DOI 10.1001/archpsyc.64.6.698
   Wechsler D, 1999, WECHSLER ABBREVIATED
   WILLIAMS CE, 1972, J ACOUST SOC AM, V52, P1238, DOI 10.1121/1.1913238
   Wundt W., 1909, GRUNDRISS PSYCHOL AC
NR 44
TC 34
Z9 40
U1 1
U2 41
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1939-3792
EI 1939-3806
J9 AUTISM RES
JI Autism Res.
PD DEC
PY 2017
VL 10
IS 12
BP 1991
EP 2001
DI 10.1002/aur.1847
PG 11
WC Behavioral Sciences; Psychology, Developmental
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Behavioral Sciences; Psychology
GA FQ2ED
UT WOS:000418168800008
PM 28815940
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Rodero, E
AF Rodero, Emma
TI Effectiveness, attention, and recall of human and artificial voices in
   an advertising story. Prosody influence and functions of voices
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Human and artificial voices; Prosody; Effectiveness; Attention; Recall
ID COMPUTER-SYNTHESIZED SPEECH; NATURAL SPEECH; PERCEPTION;
   INTELLIGIBILITY; PERSONALITY; CONSISTENCY; INTONATION; LISTENERS;
   ATTITUDES; EMOTION
AB Many users are exposed every day to artificial voices in their different devices. Because of this, there is a growing interest both in improving the quality of these voices and in analyzing how they are perceived and processed. However, very little research has been conducted to examine nonverbal elements such as prosody. Accordingly, the first purpose of this study is to determine how artificial voices compared to human voices are processed in a narrative advertising story modifying prosody regarding effectiveness, attention, concentration, and recall. The second objective is to evaluate their functions for different applications, advertising among them. The results show that human voices are assessed as more effective and achieved a better level of effectiveness, attention, and recall with less concentration. Concerning the functions, the more important and complex a function is, the more a human voice is preferred over an artificial one. (C) 2017 Elsevier Ltd. All rights reserved.
C1 [Rodero, Emma] Pompeu Fabra Univ UPF, Dept Commun, Roc Boronat 138, Barcelona 08108, Spain.
C3 Pompeu Fabra University
RP Rodero, E (corresponding author), Pompeu Fabra Univ UPF, Dept Commun, Roc Boronat 138, Barcelona 08108, Spain.
EM emma.rodero@upf.edu
RI Rodero, Emma/G-9235-2015
OI Rodero, Emma/0000-0003-0948-3400
CR [Anonymous], 10 IND U SPEECH RES
   [Anonymous], 1997, AUGMENT ALTERN COMM, DOI [DOI 10.1080/07434619712331277878, 10.1080/07434619712331277878]
   [Anonymous], 1997, HDB PHONET SCI
   Boersma  P., 2017, PRAAT DOING PHONETIC
   Brave S, 2005, INT J HUM-COMPUT ST, V62, P161, DOI 10.1016/j.ijhcs.2004.11.002
   Cabral J., 2006, P SPECOM JUN 25 29 S, P536
   Chen F., 2006, Designing human interface in speech technology
   Cohen J, 1988, STAT POWER ANAL BEHA
   Crowell CR, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3735, DOI 10.1109/IROS.2009.5354204
   Delogu C, 1998, SPEECH COMMUN, V24, P153, DOI 10.1016/S0167-6393(98)00009-0
   DUFFY SA, 1992, LANG SPEECH, V35, P351, DOI 10.1177/002383099203500401
   Fleming J. H., 2007, Human sigma: Managing the employee-customer encounter
   GelinasChebat C, 1996, PERCEPT MOTOR SKILL, V83, P243, DOI 10.2466/pms.1996.83.1.243
   GILES H, 1973, SPEECH MONOGR, V40, P330
   Gong L., 2003, INT J SPEECH TECHNOL, V6, P123
   Grice M., 1991, EUROSPEECH 91. 2nd European Conference on Speech Communication and Technology Proceedings, P879
   Gussenhoven C., 2004, The Phonology of Tone and Intonation
   Hennig S., 2012, IEEE RO MAN
   Hinterleitner E, 2014, QUALITY EXPERIENCE A
   Hinterleitner F., 2012, SPOK LANG TECHN WORK
   Hirschberg J, 2005, HDB PRAGMATICS, P515
   Hirst D. J., 2007, P 16 INT C PHONETIC, P1233
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   JENKINS JJ, 1982, B PSYCHONOMIC SOC, V20, P203
   Kamm C., 1997, NSF WORKSH HUM CENTR
   Lai J., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P206, DOI 10.1145/365024.365100
   Lai J., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P321, DOI 10.1145/332040.332451
   Levi S. V., 2007, PSYCHOLINGUISTIC PHE
   Luce P. A., 1981, 7 IND U SPEECH RES L
   LUCE PA, 1983, HUM FACTORS, V25, P17, DOI 10.1177/001872088302500102
   MARICS MA, 1988, HUM FACTORS, V30, P719, DOI 10.1177/001872088803000608
   Mayo C, 2011, SPEECH COMMUN, V53, P311, DOI 10.1016/j.specom.2010.10.003
   Mayor O., 2009, AES 35 INT C LOND UK, P11
   MIRENDA P, 1989, J SPEECH HEAR RES, V32, P175, DOI 10.1044/jshr.3201.175
   Moody T., 1986, P VOIC INP OUT SOC A
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass Clliford, 2005, WIRED FOR SPEECH
   Niebuhr O, 2016, COMPUT HUM BEHAV, V64, P366, DOI 10.1016/j.chb.2016.06.059
   Nusbaum H. C., 1995, International Journal of Speech Technology, V1, P7, DOI 10.1007/BF02277176
   Nye P. W., 1975, SR41 HASK LAB
   PARIS CR, 1995, HUM FACTORS, V37, P335, DOI 10.1518/001872095779064609
   Paris CR, 2000, HUM FACTORS, V42, P421, DOI 10.1518/001872000779698132
   Pauletto S, 2013, LOGOP PHONIATR VOCO, V38, P115, DOI 10.3109/14015439.2013.810303
   Pisoni D. B., 1980, ICASSP 80 Proceedings. IEEE International Conference on Acoustics, Speech and Signal Processing, P572
   Pisoni David B, 1987, Comput Speech Lang, V2, P303, DOI 10.1016/0885-2308(87)90014-3
   Pisoni DB., 1997, Progress in Speech Synthesis, P541, DOI [10.1007/978-1-4612-1894-4_43, DOI 10.1007/978-1-4612-1894-4_43]
   Pittam Jeff, 1994, Voice in social interaction
   Potter RF, 2006, MEDIA PSYCHOL, V8, P395, DOI 10.1207/s1532785xmep0804_4
   Rao K.S., 2012, PREDICTING PROSODY T
   Reeves B., 1996, The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Pla, DOI DOI 10.1007/S42452-020-2192-7
   Rodero E., 2007, ESTUDIOS MENSAJE PER, P523
   Rodero E., 2006, EXLING 2006
   Rodero E, 2016, MEDIA PSYCHOL, V19, P224, DOI 10.1080/15213269.2014.1002942
   Rodero E, 2015, J NONVERBAL BEHAV, V39, P79, DOI 10.1007/s10919-014-0201-5
   Rodero E, 2013, SEX ROLES, V68, P349, DOI 10.1007/s11199-012-0247-y
   Rodero Emma, 2014, J APPL LINGUISTICS P, V11, P89, DOI [10.1558/japl.32411, DOI 10.1558/JAPL.32411]
   Roring RW, 2007, HUM FACTORS, V49, P25, DOI 10.1518/001872007779598055
   Rosenberg A, 2009, SPEECH COMMUN, V51, P640, DOI 10.1016/j.specom.2008.11.001
   Rosenthal-von der Pütten AM, 2013, INT J SOC ROBOT, V5, P17, DOI 10.1007/s12369-012-0173-8
   Salza P. L, 1993, SPEECH TECHNOLOGY AS
   Sanderman AA, 1997, LANG SPEECH, V40, P391, DOI 10.1177/002383099704000405
   Schutz S., 2006, PERCEPTION ANAL SYNT, V47
   Shank DB, 2013, COMPUT HUM BEHAV, V29, P715, DOI 10.1016/j.chb.2012.11.006
   Signorello R., 2012, P 7 GSCP INT C SPEEC, P343
   SLOWIACZEK LM, 1985, HUM FACTORS, V27, P701, DOI 10.1177/001872088502700609
   Stern SE, 2014, REHABIL PSYCHOL, V59, P289, DOI 10.1037/a0036663
   Syrdal A. K., 1994, APPL SPEECH TECHNOLO
   Taake K., 2009, THESIS WASHINGTON U
   TERKEN J, 1988, J PHONETICS, V16, P453, DOI 10.1016/S0095-4470(19)30521-2
   Vainio M., 2002, P IEEE 2002 WORKSH S
   van Wissen A, 2012, COMPUT HUM BEHAV, V28, P23, DOI 10.1016/j.chb.2011.08.006
   Winters S. J., 2004, PROGR REPORT RES SPO, V26
   Wolters MK, 2015, J AM MED INFORM ASSN, V22, P35, DOI 10.1136/amiajnl-2014-002820
NR 74
TC 7
Z9 9
U1 3
U2 20
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD DEC
PY 2017
VL 77
BP 336
EP 346
DI 10.1016/j.chb.2017.08.044
PG 11
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA FM2NB
UT WOS:000414820400035
DA 2024-01-09
ER

PT J
AU Yamasaki, R
   Montagnoli, A
   Murano, EZ
   Gebrim, E
   Hachiya, A
   da Silva, JVL
   Behlau, M
   Tsuji, D
AF Yamasaki, Rosiane
   Montagnoli, Arlindo
   Murano, Emi Z.
   Gebrim, Eloisa
   Hachiya, Adriana
   Lopes da Silva, Jorge Vicente
   Behlau, Mara
   Tsuji, Domingos
TI Perturbation Measurements on the Degree of Naturalness of Synthesized
   Vowels
SO JOURNAL OF VOICE
LA English
DT Article
DE Synthesized voices; Acoustical measurements; Auditory; perceptual
   evaluation; Naturalness perception; Vocal tract model
ID VOICE QUALITY; PERCEPTION; SHIMMER; JITTER; EXPERIENCE; SPEECH
AB Objective. To determine the impact of jitter and shimmer on the degree of naturalness perception of synthesized vowels produced by acoustical simulation with glottal pulses (GP) and with solid model of the vocal tract (SMVT).
   Study Design. Prospective study.
   Methods. Synthesized vowels were produced in three steps: 1. Eighty GP were developed (20 with jitter, 20 with shimmer, 20 with jitter+shimmer, 20 without perturbation); 2. A SMVT was produced based on magnetic resonance imaging (MRI) from a woman during phonation-/epsilon/ and using rapid prototyping technology; 3. Acoustic simulations were performed to obtain eighty synthesized vowels-/epsilon/. Two experiments were performed. First Experiment: three judges rated 120 vowels (20 humans+80 synthesized+20% repetition) as "human" or "synthesized". Second Experiment: twenty PowerPoint slide sequences were created. Each slide had 4 synthesized vowels produced with the four perturbation condition. Evaluators were asked to rate the vowels from the most natural to the most artificial.
   Results. First Experiment: all the human vowels were classified as human; 27 out of eighty synthesized vowels were rated as human, 15 of those were produced with jitter+shimmer, 10 with jitter, 2 without perturbation and none with shimmer. Second Experiment: Vowels produced with jitter+shimmer were considered as the most natural. Vowels with shimmer and without perturbation were considered as the most artificial.
   Conclusions. The association of jitter and shimmer increased the degree of naturalness of synthesized vowels. Acoustic simulations performed with GP and using SMVT demonstrated a possible method to test the effect of the perturbation measurements on synthesized voices.
C1 [Yamasaki, Rosiane; Murano, Emi Z.; Hachiya, Adriana; Tsuji, Domingos] Univ Sao Paulo, Fac Med, Hosp Clin, Dept Otorhinolaryngol, Sao Paulo, Brazil.
   [Montagnoli, Arlindo] Univ Sao Paulo Sao Carlos, Dept Elect Engn, Sao Paulo, Brazil.
   [Gebrim, Eloisa] Univ Sao Paulo, Fac Med, Hosp Clin, Dept Radiol,InRad, Sao Paulo, Brazil.
   [Lopes da Silva, Jorge Vicente] Ctr Tecnol Informacao Renato Archer, Div Tridimens Technol, Campinas, SP, Brazil.
   [Behlau, Mara] Univ Fed Sao Paulo UNIFESP, Dept Speech Language Pathol & Audiol, Sao Paulo, Brazil.
   [Behlau, Mara] CEV, Sao Paulo, Brazil.
C3 Universidade de Sao Paulo; Universidade de Sao Paulo; Universidade de
   Sao Paulo; Universidade Federal de Sao Paulo (UNIFESP)
RP Yamasaki, R (corresponding author), Rua Oscar Freire,2250 5o Andar 502, BR-05409011 Sao Paulo, SP, Brazil.
EM r.yamasaki@uol.com.br
RI Montagnoli, Arlindo Neto/C-3802-2012; YAMASAKI, ROSIANE/ABB-5110-2021;
   da Silva, Jorge Vicente Lopes/C-8502-2012; Behlau, Mara
   Suzana/AAN-1054-2021
OI Montagnoli, Arlindo Neto/0000-0002-4095-3602; da Silva, Jorge Vicente
   Lopes/0000-0002-2347-5215; Behlau, Mara Suzana/0000-0003-4663-4546;
   Tsuji, Domingos/0000-0002-8219-7550; Gebrim, Eloisa Maria M
   Santiago/0000-0002-6514-3825; YAMASAKI, ROSIANE/0000-0002-7960-4143
FU Fundacao de Amparo a Pesquisa do Estado de Sao Paulo [FAPESP:
   2012/17390-3]
FX The authors thank the Fundacao de Amparo a Pesquisa do Estado de Sao
   Paulo (FAPESP: 2012/17390-3) for financial support.
CR Amorim P, 2015, ADV VISUAL COMPUTING, P14
   Behlau M., 2001, VOICE BOOK SPECIALIS, P1
   Brockmann M, 2008, J SPEECH LANG HEAR R, V51, P1152, DOI 10.1044/1092-4388(2008/06-0208)
   Brockmann M, 2011, J VOICE, V25, P44, DOI 10.1016/j.jvoice.2009.07.002
   Dang JW, 1997, J ACOUST SOC AM, V101, P456, DOI 10.1121/1.417990
   Englert M, 2016, J VOICE, V30, DOI 10.1016/j.jvoice.2015.07.017
   Fraj S, 2012, J ACOUST SOC AM, V132, P2603, DOI 10.1121/1.4751536
   Fujita S, 2005, ACOUST SCI TECH, V26
   Gerratt BR, 2001, J ACOUST SOC AM, V110, P2560, DOI 10.1121/1.1409969
   Gonçalves Maria Inês Rebelo, 2009, Braz. j. otorhinolaryngol., V75, P680
   HILLENBRAND J, 1988, J ACOUST SOC AM, V83, P2361, DOI 10.1121/1.396367
   Honda K., 2008, J ACOUST SOC AM, V123, P3731
   KERSTA LG, 1960, J ACOUST SOC AM, V32, P1502, DOI 10.1121/1.1935196
   Kisenwether JS, 2015, J VOICE, V29, P548, DOI 10.1016/j.jvoice.2014.11.006
   Kreiman J, 2005, J ACOUST SOC AM, V117, P2201, DOI 10.1121/1.1858351
   Kreiman J, 2015, J ACOUST SOC AM, V138, P1, DOI 10.1121/1.4922174
   Lopes LW, 2014, CODAS, V26, P382, DOI 10.1590/2317-1782/20142013033
   Mattioli F, 2015, J VOICE, V29, P455, DOI 10.1016/j.jvoice.2014.09.027
   Montagnoli AN., 2015, VOICE ANAL PROGRAM 1
   Murphy PJ, 2000, J ACOUST SOC AM, V107, P978, DOI 10.1121/1.428272
   Nusbaum H. C., 1995, International Journal of Speech Technology, V1, P7, DOI 10.1007/BF02277176
   Oppenheim A. V., 2010, Discrete-time signal processing, V3rd
   Petrovic-Lazic M, 2015, J VOICE, V29, P241, DOI 10.1016/j.jvoice.2014.07.009
   ROSENBERG AE, 1971, J ACOUST SOC AM, V49, P583, DOI 10.1121/1.1912389
   ROZSYPAL AJ, 1979, J PHONETICS, V7, P343, DOI 10.1016/S0095-4470(19)31069-1
   ROZSYPAL AJ, 1975, J ACOUST SOC AM, V58, pS23, DOI 10.1121/1.2002025
   Smruti S, 2015, ADV INTELL SYST, V328, P367, DOI 10.1007/978-3-319-12012-6_40
   Sofranko JL, 2014, J VOICE, V28, P24, DOI 10.1016/j.jvoice.2013.06.001
   Sorensen MK, 2015, J VOICE, V30
   Titze IR, 2000, PRINCIPLES VOICE PRO, P313
   TOPALOGLU I, 2014, OTOLARYNGOL HEAD NEC, V151, P1003
   Yiu EML, 2002, J ACOUST SOC AM, V112, P1091, DOI 10.1121/1.1500753
   Yu ZW, 2014, J VOICE, V28, DOI 10.1016/j.jvoice.2014.03.014
   Zhang Y, 2005, J VOICE, V19, P519, DOI 10.1016/j.jvoice.2004.11.005
NR 34
TC 2
Z9 2
U1 1
U2 6
PU MOSBY-ELSEVIER
PI NEW YORK
PA 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN 0892-1997
EI 1873-4588
J9 J VOICE
JI J. Voice
PD MAY
PY 2017
VL 31
IS 3
AR 389.e1
DI 10.1016/j.jvoice.2016.09.020
PG 8
WC Audiology & Speech-Language Pathology; Otorhinolaryngology
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Audiology & Speech-Language Pathology; Otorhinolaryngology
GA EX9AK
UT WOS:000403543600053
PM 27777057
DA 2024-01-09
ER

PT J
AU Birkholz, P
   Martin, L
   Xu, Y
   Scherbaum, S
   Neuschaefer-Rube, C
AF Birkholz, Peter
   Martin, Lucia
   Xu, Yi
   Scherbaum, Stefan
   Neuschaefer-Rube, Christiane
TI Manipulation of the prosodic features of vocal tract length, nasality
   and articulatory precision using articulatory synthesis
SO COMPUTER SPEECH AND LANGUAGE
LA English
DT Article
DE Prosody; Feature manipulation; Articulatory synthesis
ID SPEECH SYNTHESIS SYSTEM; VOWEL REDUCTION; VOICE QUALITY; EMOTIONS;
   EXPRESSION; PERSONALITY; SPEAKING; STRESS
AB Vocal emotions, as well as different speaking styles and speaker traits, are characterized by a complex interplay of multiple prosodic features. Natural sounding speech synthesis with the ability to control such paralinguistic aspects requires the manipulation of the corresponding prosodic features. With traditional concatenative speech synthesis it is easy to manipulate the "primary" prosodic features pitch, duration, and intensity, but it is very hard to individually control "secondary" prosodic features like phonation type, vocal tract length, articulatory precision and nasality. These secondary features can be controlled more directly with parametric synthesis methods. In the present study we analyze the ability of articulatory speech synthesis to control secondary prosodic features by rule. To this end, nine German words were re-synthesized with the software VocalTractLab 2.1 and then manipulated in different ways at the articulatory level to vary vocal tract length, articulatory precision and degree of nasality. Listening tests showed that most of the intended prosodic manipulations could be reliably identified with recognition rates between 77% and 96%. Only the manipulations to increase articulatory precision were hardly recognized. The results suggest that rule-based manipulations in articulatory synthesis are generally sufficient for the convincing synthesis of secondary prosodic features at the word level. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Birkholz, Peter] Tech Univ Dresden, Inst Acoust & Speech Commun, D-01062 Dresden, Germany.
   [Martin, Lucia; Neuschaefer-Rube, Christiane] Univ Hosp Aachen, Dept Phoniatr Pedaudiol & Commun Disorders, Pauwelsstr 30, D-52074 Aachen, Germany.
   [Martin, Lucia; Neuschaefer-Rube, Christiane] Rhein Westfal TH Aachen, Pauwelsstr 30, D-52074 Aachen, Germany.
   [Xu, Yi] UCL, Dept Speech Hearing & Phonet Sci, Chandler House,2 Wakefield St, London, England.
   [Scherbaum, Stefan] Tech Univ Dresden, Dept Psychol, D-01062 Dresden, Germany.
C3 Technische Universitat Dresden; RWTH Aachen University; RWTH Aachen
   University Hospital; RWTH Aachen University; University of London;
   University College London; Technische Universitat Dresden
RP Birkholz, P (corresponding author), Tech Univ Dresden, Inst Acoust & Speech Commun, D-01062 Dresden, Germany.
EM peter.birkholz@tu-dresden.de
RI Xu, Yi/M-9738-2019; Xu, Yi/C-4013-2008
OI Xu, Yi/0000-0002-8541-2658; Xu, Yi/0000-0002-8541-2658; Scherbaum,
   Stefan/0000-0002-4408-6016
CR Airas M, 2006, PHONETICA, V63, P26, DOI 10.1159/000091405
   [Anonymous], 2011, 1 INT WORKSHOP PERFO
   [Anonymous], 2005, 3D ARTIKULATORISCHE
   Aryal S, 2016, COMPUT SPEECH LANG, V36, P260, DOI 10.1016/j.csl.2015.02.003
   Beller G., 2008, 4 INT C SPEECH PROS
   Birkholz P., 2011, P INT 2011 FLOR IT, P2681
   Birkholz P., 2004, P INTERSPEECH 2004, P1125, DOI [10.21437/Interspeech.2004-409, DOI 10.21437/INTERSPEECH.2004-409]
   Birkholz P., 2007, 8 ANN C INT SPEECH C, P2865
   Birkholz P, 2015, J ACOUST SOC AM, V137, P1503, DOI 10.1121/1.4906836
   Birkholz P, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0060603
   Birkholz P, 2011, IEEE T AUDIO SPEECH, V19, P1422, DOI 10.1109/TASL.2010.2091632
   Black A. W., 2003, EUROSPEECH, P1649
   Boersma P., 2014, PRAAT DOING PHONETIC
   Burkhardt F., 2000, ISCA TUT RES WORKSH
   Burkhardt F, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P2627
   Campbell N., 2003, P 15 INT C PHON SCI, P2417
   Cheng CE, 2013, J ACOUST SOC AM, V134, P4481, DOI 10.1121/1.4824930
   Chuenwattanapranithi S, 2008, PHONETICA, V65, P210, DOI 10.1159/000192793
   Clark R.A.J., 2007, P BLIZZ CHALL WORKSH
   Fant G., 1975, STL QPSR, V16, P1
   FOURAKIS M, 1991, J ACOUST SOC AM, V90, P1816, DOI 10.1121/1.401662
   GIBBS RW, 1986, J EXP PSYCHOL GEN, V115, P3, DOI 10.1037/0096-3445.115.1.3
   Gobl C, 2003, SPEECH COMMUN, V40, P189, DOI 10.1016/S0167-6393(02)00082-1
   Grichkovtsova I., 2009, ROLE PROSODY AFFECTI
   Hunt AJ, 1996, INT CONF ACOUST SPEE, P373, DOI 10.1109/ICASSP.1996.541110
   Iida A, 2003, SPEECH COMMUN, V40, P161, DOI 10.1016/S0167-6393(02)00081-X
   Kane J, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P184
   KLATT DH, 1980, J ACOUST SOC AM, V67, P971, DOI 10.1121/1.383940
   Ladd DR, 2008, CAMB STUD LINGUIST, V79, P1
   LINDBLOM B, 1963, J ACOUST SOC AM, V35, P1773, DOI 10.1121/1.1918816
   LINDBLOM B, 1990, SPEECH PRODUCTION SP, P413, DOI DOI 10.1007/978-94-009-2037-8
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   MURRAY IR, 1995, SPEECH COMMUN, V16, P369, DOI 10.1016/0167-6393(95)00005-9
   Patel S, 2011, BIOL PSYCHOL, V87, P93, DOI 10.1016/j.biopsycho.2011.02.010
   Pfitzinger H.R, 2006, SPEECH PROSODY, P6
   Picart B, 2014, COMPUT SPEECH LANG, V28, P687, DOI 10.1016/j.csl.2013.04.008
   Prom-on S, 2009, J ACOUST SOC AM, V125, P405, DOI 10.1121/1.3037222
   Scherer KR, 2015, COMPUT SPEECH LANG, V29, P218, DOI 10.1016/j.csl.2013.10.002
   Scherer KR, 2003, SER AFFECTIVE SCI, P433
   SCHERER KR, 1978, EUR J SOC PSYCHOL, V8, P467, DOI 10.1002/ejsp.2420080405
   Schroder M., 2010, BLUEPRINT AFFECTIVE, P222
   Schroder Marc, 2001, 7 EUR C SPEECH COMM
   Schuller B, 2015, COMPUT SPEECH LANG, V29, P100, DOI 10.1016/j.csl.2014.08.003
   Sendlmeier W.F., 1998, FORUM PHONETICUM, V66, P1
   Simpson AP, 2001, J ACOUST SOC AM, V109, P2153, DOI 10.1121/1.1356020
   Stevens K., 1998, Acoustic phonetics
   van den Doel K., 2006, 7 INT SEM SPEECH PRO
   VANBERGEM DR, 1993, SPEECH COMMUN, V12, P1, DOI 10.1016/0167-6393(93)90015-D
   Waaramaa T, 2008, FOLIA PHONIATR LOGO, V60, P249, DOI 10.1159/000151762
   Xu Y., 2013, PROSODY ICONICITY, P33, DOI DOI 10.1075/ILL.13.02XU
   Yamagishi J, 2005, IEICE T INF SYST, VE88D, P502, DOI 10.1093/ietisy/e88-d.3.502
   Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004
NR 52
TC 15
Z9 15
U1 0
U2 24
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0885-2308
EI 1095-8363
J9 COMPUT SPEECH LANG
JI Comput. Speech Lang.
PD JAN
PY 2017
VL 41
BP 116
EP 127
DI 10.1016/j.csl.2016.06.004
PG 12
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DY1OI
UT WOS:000384863900006
OA Green Submitted
DA 2024-01-09
ER

PT J
AU Kuriki, S
   Tamura, Y
   Igarashi, M
   Kato, N
   Nakano, T
AF Kuriki, Shinji
   Tamura, Yuri
   Igarashi, Miki
   Kato, Nobumasa
   Nakano, Tamami
TI Similar impressions of humanness for human and artificial singing voices
   in autism spectrum disorders
SO COGNITION
LA English
DT Article
DE Autism; Artificial voice; Humanness; Music
ID MIND PERCEPTION; CHILDREN; MUSIC; ALEXITHYMIA; INSULA; INDIVIDUALS;
   EMOTIONS; ROBOTS
AB People with autism spectrum disorder (ASD) exhibit impairments in the perception of and orientation to social information related to humans, and some people with ASD show higher preference toward human-like robots than other humans. We speculated that this behavioural bias in people with ASD is caused by a weakness in their perception of humanness. To address this issue, we investigated whether people with ASD detect a subtle difference between the same song sung by human and artificial voices even when the lyrics, melody and rhythm are identical. People without ASD answered that the songs sung by a human voice evoked more impressions of humanness (human-likeness, animateness, naturalness, emotion) and more positive feelings (warmth, familiarity, comfort) than those sung by an artificial voice. In contrast, people with ASD had similar impressions of humanness and positive feelings for the songs sung by the human and artificial voices. The evaluations of musical characteristics (complexity, regularity, brightness) did not differ between people with and without ASD. These results suggest that people with ASD are weak in their ability to perceive psychological attributes of humanness. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Kuriki, Shinji; Tamura, Yuri; Nakano, Tamami] Osaka Univ, Sch Med, Dept Brain Physiol, 2-2 Yamadaoka, Suita, Osaka 5650871, Japan.
   [Igarashi, Miki; Kato, Nobumasa] Showa Univ, Sch Med, Dept Psychiat, Tokyo, Japan.
   [Nakano, Tamami] Osaka Univ, Grad Sch Frontiers Biosci, 1-3 Yamadaoka, Suita, Osaka 5650871, Japan.
C3 Osaka University; Showa University; Osaka University
RP Nakano, T (corresponding author), Osaka Univ, Grad Sch Frontiers Biosci, 1-3 Yamadaoka, Suita, Osaka 5650871, Japan.
EM tamami_nakano@fbs.osaka-u.ac.jp
FU Ministry of Education, Culture, Sports, Science and Technology, Japan
   [251195040]; Grants-in-Aid for Scientific Research [15K12620] Funding
   Source: KAKEN
FX This work was supported by the Grant-in-Aid for Scientific Research on
   Innovative Areas 251195040 "Constructive Developmental Science" from the
   Ministry of Education, Culture, Sports, Science and Technology, Japan to
   T.N.
CR Alcántara JI, 2004, J CHILD PSYCHOL PSYC, V45, P1107, DOI 10.1111/j.1469-7610.2004.t01-1-00303.x
   Allen R, 2013, J AUTISM DEV DISORD, V43, P432, DOI 10.1007/s10803-012-1587-8
   Bamiou DE, 2003, BRAIN RES REV, V42, P143, DOI 10.1016/S0165-0173(03)00172-3
   Baron-Cohen S, 2001, J AUTISM DEV DISORD, V31, P5, DOI 10.1023/A:1005653411471
   Baron-Cohen S, 2006, PROG NEURO-PSYCHOPH, V30, P865, DOI 10.1016/j.pnpbp.2006.01.010
   Bird G, 2013, TRANSL PSYCHIAT, V3, DOI 10.1038/tp.2013.61
   Bird G, 2010, BRAIN, V133, P1515, DOI 10.1093/brain/awq060
   Broadbent E, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0072589
   Dawson G, 1998, J AUTISM DEV DISORD, V28, P479, DOI 10.1023/A:1026043926488
   Diehl JJ, 2012, RES AUTISM SPECT DIS, V6, P249, DOI 10.1016/j.rasd.2011.05.006
   Gebauer L, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00192
   Gray HM, 2007, SCIENCE, V315, P619, DOI 10.1126/science.1134475
   Gray K, 2012, COGNITION, V125, P125, DOI 10.1016/j.cognition.2012.06.007
   Gray K, 2011, P NATL ACAD SCI USA, V108, P477, DOI 10.1073/pnas.1015493108
   Haslam N, 2006, PERS SOC PSYCHOL REV, V10, P252, DOI 10.1207/s15327957pspr1003_4
   Heaton P, 1999, PSYCHOL MED, V29, P1405, DOI 10.1017/S0033291799001221
   Heaton P, 2008, BRIT J DEV PSYCHOL, V26, P171, DOI 10.1348/026151007X206776
   Jemel B, 2006, J AUTISM DEV DISORD, V36, P91, DOI 10.1007/s10803-005-0050-5
   Molnar-Szakacs I, 2012, ANN NY ACAD SCI, V1252, P318, DOI 10.1111/j.1749-6632.2012.06465.x
   Molnar-Szakacs Istvan, 2009, Mcgill J Med, V12, P87
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Nakano T, 2010, P ROY SOC B-BIOL SCI, V277, P2935, DOI 10.1098/rspb.2010.0587
   Pierno AC, 2008, NEUROPSYCHOLOGIA, V46, P448, DOI 10.1016/j.neuropsychologia.2007.08.020
   Remedios R, 2009, J NEUROSCI, V29, P1034, DOI 10.1523/JNEUROSCI.4089-08.2009
   Tamura Y, 2015, SCI REP-UK, V5, DOI 10.1038/srep08799
   Thompson JC, 2011, PERCEPTION, V40, P695, DOI 10.1068/p6900
NR 26
TC 12
Z9 13
U1 1
U2 43
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0010-0277
EI 1873-7838
J9 COGNITION
JI Cognition
PD AUG
PY 2016
VL 153
BP 1
EP 5
DI 10.1016/j.cognition.2016.04.004
PG 5
WC Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA DQ9VZ
UT WOS:000379558700001
PM 27107740
DA 2024-01-09
ER

PT J
AU Crumpton, J
   Bethel, CL
AF Crumpton, Joe
   Bethel, Cindy L.
TI A Survey of Using Vocal Prosody to Convey Emotion in Robot Speech
SO INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS
LA English
DT Article
DE Synthesized speech; Emotional robot speech; Human-robot interaction;
   Vocal prosody
ID VOICE; CONVERSION; EXPRESSION; MODEL; PITCH
AB The use of speech for robots to communicate with their human users has been facilitated by improvements in speech synthesis technology. Now that the intelligibility of synthetic speech has advanced to the point that speech synthesizers are a widely accepted and used technology, what are other aspects of speech synthesis that can be used to improve the quality of human-robot interaction? The communication of emotions through changes in vocal prosody is one way to make synthesized speech sound more natural. This article reviews the use of vocal prosody to convey emotions between humans, the use of vocal prosody by agents and avatars to convey emotions to their human users, and previous work within the human-robot interaction (HRI) community addressing the use of vocal prosody in robot speech. The goals of this article are (1) to highlight the ability and importance of using vocal prosody to convey emotions within robot speech and (2) to identify experimental design issues when using emotional robot speech in user studies.
C1 [Crumpton, Joe] Mississippi State Univ, Distributed Analyt & Secur Inst, Starkville, MS 39762 USA.
   [Bethel, Cindy L.] Mississippi State Univ, Social Therapeut & Robot Syst Lab, Dept Comp Sci & Engn, Starkville, MS USA.
C3 Mississippi State University; Mississippi State University
RP Crumpton, J (corresponding author), Mississippi State Univ, Distributed Analyt & Secur Inst, Starkville, MS 39762 USA.
EM crumpton@dasi.msstate.edu; cbethel@cse.msstate.edu
OI Crumpton, Joe/0000-0002-5932-2480
CR Aihara R., 2012, Amer. J. Signal Process., V2, P134, DOI DOI 10.5923/J.AJSP.20120205.06
   Alm CO, 2005, P C HUM LANG TECHN E, P579, DOI [10.3115/1220575.1220648, DOI 10.3115/1220575.1220648]
   Amir N., 2009, PROC IEEE 3 INT C AF, P1
   [Anonymous], 1992, Proceedings of the 1992 International Conference on Spoken Language Processing
   [Anonymous], 2004, EMOTION EVOLUTION RA
   Ashimura K, 2013, VOCABULARIES EMOTION
   Bachorowski J.A., 2008, Handbook of Emotions, V3rd ed., P196
   Baggia P, 2013, EMOTION MARKUP LANGU
   Baggia P., 2010, Speech synthesis markup language (SSML) version 1.1
   Bainbridge WA, 2011, INT J SOC ROBOT, V3, P41, DOI 10.1007/s12369-010-0082-7
   Barrett LF, 2006, PERS SOC PSYCHOL REV, V10, P20, DOI 10.1207/s15327957pspr1001_2
   BATES J, 1994, COMMUN ACM, V37, P122, DOI 10.1145/176789.176803
   Beale R, 2009, INT J HUM-COMPUT ST, V67, P755, DOI 10.1016/j.ijhcs.2009.05.001
   Beckman M., 1994, GUIDELINES TOBI LABE
   Benoit C, 1996, SPEECH COMMUN, V18, P381, DOI 10.1016/0167-6393(96)00026-X
   Bethel CL, 2006, P 2006 AAAI FALL S S
   Black A. W., CMU ARCTIC SPEECH SY
   Boersma P., 2021, Glot International
   Breazeal C., 1999, Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289), P858, DOI 10.1109/IROS.1999.812787
   Breazeal C, 2002, AUTON ROBOT, V12, P83, DOI 10.1023/A:1013215010749
   Brooks DJ, 2012, P WORKSH 26 AAAI C A
   Burkhardt F, 2000, P ISCA TUT RES WORKS
   Cahn J. E., 1990, Journal of the American Voice I/O Society, V8, P1
   Cowie R, 2003, SPEECH COMMUN, V40, P5, DOI 10.1016/S0167-6393(02)00071-7
   Crumpton J, 2015, P 2015 INT C COLL TE
   Crumpton J, 2014, 23 IEEE INT S ROB HU
   Dautenhahn K, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1488
   Dmello S., 2013, ACM T INTERACT INTEL, V2, P1, DOI 10.1145/2395123.2395128
   EKMAN P, 1969, SCIENCE, V164, P86, DOI 10.1126/science.164.3875.86
   Erickson D, 2005, ACOUST SCI TECHNOL, V26, P317, DOI 10.1250/ast.26.317
   Fairbanks G, 1939, SPEECH MONOGR, V6, P87, DOI 10.1080/03637753909374863
   FRICK RW, 1985, PSYCHOL BULL, V97, P412, DOI 10.1037/0033-2909.97.3.412
   Greasley P, 2000, LANG SPEECH, V43, P355, DOI 10.1177/00238309000430040201
   Harnmerschmidt K, 2007, J VOICE, V21, P531, DOI 10.1016/j.jvoice.2006.03.002
   Hennig S., 2012, 2012 RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication, P589, DOI 10.1109/ROMAN.2012.6343815
   HUTTAR GL, 1968, J SPEECH HEAR RES, V11, P481, DOI 10.1044/jshr.1103.481
   IIDA A, 2000, P ISCA WORKSH SPEECH, V1, P167
   Jung Y, 2004, Proceedings: 7th Annual International Workshop on Presence, Valencia, Spain, P80, DOI DOI 10.1145/1349822.1349866
   Khan Z., 1998, TRITANAP9821 KTH
   Kidd C. D., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P3559
   Kim E. S., 2009, 2009 4th ACM/IEEE International Conference on Human-Robot Interaction (HRI), P23
   Laukka P, 2004, THESIS UPPSALA U
   Laukka P, 2011, COMPUT SPEECH LANG, V25, P84, DOI 10.1016/j.csl.2010.03.004
   Leyzberg D., 2012, P ANN M COGNITIVE SC, V34
   Leyzberg D, 2011, ACMIEEE INT CONF HUM, P347, DOI 10.1145/1957656.1957789
   Li XY, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P5009, DOI 10.1109/IROS.2009.5354007
   Ling ZH, 2015, IEEE SIGNAL PROC MAG, V32, P35, DOI 10.1109/MSP.2014.2359987
   MASSARO DW, 1989, BEHAV BRAIN SCI, V12, P778, DOI 10.1017/S0140525X0002584X
   Massaro DW, 1996, PSYCHON B REV, V3, P215, DOI 10.3758/BF03212421
   Mehrabian A, 1996, CURR PSYCHOL, V14, P261, DOI 10.1007/BF02686918
   Microsoft, 2015, PROS EL
   Mitchell WJ, 2011, I-PERCEPTION, V2, P10, DOI 10.1068/i0415
   Nass Clifford, 2005, P HUM FACT COMP SYST, P1973, DOI [DOI 10.1145/1056808.1057070, 10.1145/1056808.1057070]
   Niculescu A, 2013, INT J SOC ROBOT, V5, P171, DOI 10.1007/s12369-012-0171-x
   Niu HN, 2003, INT CONF ACOUST SPEE, P125
   Nuance Communications, 2015, DRAG MOB SDK REF
   Pearson JC, 2000, An Introduction to Human Communication: Understanding and Sharing, V8th
   Pierre-Yves O, 2003, INT J HUM-COMPUT ST, V59, P157, DOI 10.1016/S1071-5819(03)00141-6
   Pitrelli JF, 2006, IEEE T AUDIO SPEECH, V14, P1099, DOI 10.1109/TASL.2006.876123
   Pittarn J., 1993, HDB EMOTIONS, P185
   Powers A., 2007, 2007 2nd Annual Conference on Human-Robot Interaction (HRI), P145
   Prasad R, 2004, ADV ROBOTICS, V18, P533, DOI 10.1163/156855304774195064
   Rani P, 2004, IEEE-RAS INT C HUMAN, P149
   Ray C, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3816, DOI 10.1109/IROS.2008.4650714
   Read R, 2012, P 2012 HRI PION WORK
   Read R, 2014, ACMIEEE INT CONF HUM, P276, DOI 10.1145/2559636.2559836
   Read R, 2013, ACMIEEE INT CONF HUM, P209, DOI 10.1109/HRI.2013.6483575
   Read R, 2012, ACMIEEE INT CONF HUM, P219
   Read Robin G., 2010, Proceedings: 3rd International Workshop on Affective Interaction in Natural Environments (AFFINE), Firenze, Italy, P65
   Roehling S, 2006, P 11 AUSTR INT C SPE
   Rogalla O, 2002, IEEE ROMAN 2002, PROCEEDINGS, P454, DOI 10.1109/ROMAN.2002.1045664
   Russell JA, 1999, J PERS SOC PSYCHOL, V76, P805, DOI 10.1037/0022-3514.76.5.805
   Sander D., 2009, OXFORD COMPANION EMO
   Scherer Klaus, 2000, NEUROPSYCHOLOGY EMOT, P138
   SCHERER KR, 1986, PSYCHOL BULL, V99, P143, DOI 10.1037/0033-2909.99.2.143
   SCHERER KR, 1991, MOTIV EMOTION, V15, P123, DOI 10.1007/BF00995674
   Schroder Marc, 2001, INTERSPEECH, P87
   Schröder M, 2011, LECT NOTES COMPUT SC, V6974, P316, DOI 10.1007/978-3-642-24600-5_35
   Schuller B, 2013, COMPUT SPEECH LANG, V27, P4, DOI 10.1016/j.csl.2012.02.005
   Sobin C, 1999, J PSYCHOLINGUIST RES, V28, P347, DOI 10.1023/A:1023237014909
   Tao JH, 2006, IEEE T AUDIO SPEECH, V14, P1145, DOI 10.1109/TASL.2006.876113
   Tielman M, 2014, ACMIEEE INT CONF HUM, P407, DOI 10.1145/2559636.2559663
   Tokuda K, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON SPEECH SYNTHESIS, P227
   Türk O, 2010, IEEE T AUDIO SPEECH, V18, P965, DOI 10.1109/TASL.2010.2041113
   Veilleux N., 2006, 6 911 TRANSCRIBING P
   Vinciarelli A., 2008, P 16 ACM INT C MULT, P1061, DOI DOI 10.1145/1459359.1459573
   W3C, 2004, SPEECH SYNTH MARK LA
   Wainer Joshua, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P872
   Walker MR, 2001, INT CONF ACOUST SPEE, P965, DOI 10.1109/ICASSP.2001.941077
   Weaver CH, 1964, FUNDAMENTALS SPEECH, P283
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
   Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004
NR 93
TC 53
Z9 56
U1 3
U2 44
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1875-4791
EI 1875-4805
J9 INT J SOC ROBOT
JI Int. J. Soc. Robot.
PD APR
PY 2016
VL 8
IS 2
BP 271
EP 285
DI 10.1007/s12369-015-0329-4
PG 15
WC Robotics
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Robotics
GA DJ7KO
UT WOS:000374390500008
DA 2024-01-09
ER

PT J
AU Goy, H
   Pichora-Fuller, MK
   van Lieshout, P
AF Goy, Huiwen
   Pichora-Fuller, M. Kathleen
   van Lieshout, Pascal
TI Effects of age on speech and voice quality ratings
SO JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA
LA English
DT Article
ID VOCAL AGE; OLDER-ADULTS; PERCEPTION; FEATURES; SOUND; YOUNG;
   INTELLIGIBILITY; COMMUNICATION; IMPRESSIONS; ELDERSPEAK
AB The quality of communication may be affected by listeners' perception of talkers' characteristics. This study examined if there were effects of talker and listener age on the perception of speech and voice qualities. Younger and older listeners judged younger and older talkers' gender and age, then rated speech samples on pleasantness, naturalness, clarity, ease of understanding, loudness, and the talker's suitability to be an audiobook reader. For the same talkers, listeners also rated voice samples on pleasantness, roughness, and power. Younger and older talkers were perceived to be similar on most qualities except age. Younger and older listeners rated talkers similarly, except that younger listeners perceived younger voices to be more pleasant and less rough than older voices. For vowel samples, younger listeners were more accurate than older listeners at age estimation, while older listeners were more accurate than younger listeners at gender identification, suggesting that younger and older listeners differ in their evaluation of specific talker characteristics. Thus, the perception of quality was generally more affected by the age of the listener than the age of the talker, and age-related differences between listeners depended on whether voice or speech samples were used and the rating being made. (C) 2016 Acoustical Society of America.
C1 [Goy, Huiwen; Pichora-Fuller, M. Kathleen] Univ Toronto, Dept Psychol, 3359 Mississauga Rd, Mississauga, ON L5L 1C6, Canada.
   [van Lieshout, Pascal] Univ Toronto, Dept Speech Language Pathol, 500 Univ Ave, Toronto, ON M5G 1V7, Canada.
C3 University of Toronto; University Toronto Mississauga; University of
   Toronto
RP Goy, H (corresponding author), Univ Toronto, Dept Psychol, 3359 Mississauga Rd, Mississauga, ON L5L 1C6, Canada.
EM huiwen.goy@utoronto.ca
RI van Lieshout, Pascal HHM/A-1371-2008
OI van Lieshout, Pascal HHM/0000-0001-8139-8900; Goy,
   Huiwen/0000-0003-4767-7863
FU Natural Sciences and Engineering Research Council of Canada [138472,
   312308]; Canada Research Chair grant [950-213162]; University of Toronto
   Mississauga Work-Study Program
FX The authors wish to thank James Qi for technical assistance and Sumaiya
   Farooq and Ivian Tchakarova for their assistance in stimuli preparation
   and data collection. This project was funded by grants from the Natural
   Sciences and Engineering Research Council of Canada awarded to M. K.
   P.-F. (No. 138472) and P. v. L. (No. 312308), a Canada Research Chair
   grant awarded to P. v. L. (No. 950-213162), and the University of
   Toronto Mississauga Work-Study Program.
CR AMERMAN JD, 1990, BRIT J DISORD COMMUN, V25, P35
   [Anonymous], 1988, J VOICE, DOI [DOI 10.1016/S0892-1997(88)80024-9, 10.1016/S0892-1997(88)80024-9]
   [Anonymous], 1987, Journal of Voice, DOI DOI 10.1016/S0892-1997(87)80024-3
   Bele IV, 2005, J VOICE, V19, P555, DOI 10.1016/j.jvoice.2004.08.008
   Boersma P., 2014, PRAAT DOING PHONETIC
   Bruckert L, 2006, P ROY SOC B-BIOL SCI, V273, P83, DOI 10.1098/rspb.2005.3265
   Bunton K, 2007, J SPEECH LANG HEAR R, V50, P1481, DOI 10.1044/1092-4388(2007/102)
   COHEN G, 1986, LANG COMMUN, V6, P91, DOI 10.1016/0271-5309(86)90008-X
   Dilley LC, 2013, J SPEECH LANG HEAR R, V56, P159, DOI 10.1044/1092-4388(2012/11-0199)
   DUCHIN SW, 1987, J COMMUN DISORD, V20, P245, DOI 10.1016/0021-9924(87)90022-0
   Eadie TL, 2002, J ACOUST SOC AM, V112, P3014, DOI 10.1121/1.1518983
   Eisenberg LS, 1998, J SPEECH LANG HEAR R, V41, P327, DOI 10.1044/jslhr.4102.327
   FAGEL WPF, 1983, SPEECH COMMUN, V2, P315, DOI 10.1016/0167-6393(83)90048-1
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   GABRIELSSON A, 1979, Scandinavian Audiology, V8, P159, DOI 10.3109/01050397909076317
   Gatehouse S, 2006, INT J AUDIOL, V45, pS120, DOI 10.1080/14992020600783103
   Goy H, 2013, J VOICE, V27, P545, DOI 10.1016/j.jvoice.2013.03.002
   Hamsberger JD, 2008, J VOICE, V22, P58, DOI 10.1016/j.jvoice.2006.07.004
   Harnsberger JD, 2010, J VOICE, V24, P523, DOI 10.1016/j.jvoice.2009.01.003
   HARTMAN DE, 1976, J ACOUST SOC AM, V59, P713, DOI 10.1121/1.380894
   HILLENBRAND J, 1988, J ACOUST SOC AM, V83, P2361, DOI 10.1121/1.396367
   HOLLIEN H, 1991, J COMMUN DISORD, V24, P157, DOI 10.1016/0021-9924(91)90019-F
   HOWELL P, 1991, SPEECH COMMUN, V10, P163, DOI 10.1016/0167-6393(91)90039-V
   Hummert ML, 1999, J NONVERBAL BEHAV, V23, P111
   JACQUES RD, 1990, FOLIA PHONIATR, V42, P118, DOI 10.1159/000266055
   KATES JM, 1994, J ACOUST SOC AM, V95, P3586, DOI 10.1121/1.409976
   Kemper S, 1998, AGING NEUROPSYCHOL C, V5, P43, DOI 10.1076/anec.5.1.43.22
   Klofstad CA, 2012, P ROY SOC B-BIOL SCI, V279, P2698, DOI 10.1098/rspb.2012.0311
   Laan GPM, 1997, SPEECH COMMUN, V22, P43, DOI 10.1016/S0167-6393(97)00012-5
   Linville SE, 1996, J VOICE, V10, P190, DOI 10.1016/S0892-1997(96)80046-4
   LINVILLE SE, 1986, J ACOUST SOC AM, V80, P692, DOI 10.1121/1.394013
   LINVILLE SE, 1985, J ACOUST SOC AM, V78, P40, DOI 10.1121/1.392452
   Maryn Y, 2015, LOGOP PHONIATR VOCO, V40, P122, DOI 10.3109/14015439.2014.915981
   McAleer P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0090779
   Medrado R, 2005, J VOICE, V19, P340, DOI 10.1016/j.jvoice.2004.04.008
   Montepare JM, 2014, J LANG SOC PSYCHOL, V33, P241, DOI 10.1177/0261927X13519080
   Mueller Peter B., 1997, Seminars in Speech and Language, V18, P159, DOI 10.1055/s-2008-1064070
   Mulac A, 1996, HEALTH COMMUN, V8, P199, DOI 10.1207/s15327027hc0803_2
   MUNRO MJ, 1995, LANG LEARN, V45, P73, DOI 10.1111/j.1467-1770.1995.tb00963.x
   PREMINGER JE, 1995, J SPEECH HEAR RES, V38, P714, DOI 10.1044/jshr.3803.714
   RAMIG LA, 1986, LANG COMMUN, V6, P25, DOI 10.1016/0271-5309(86)90003-0
   Rothman HB, 2001, J VOICE, V15, P25, DOI 10.1016/S0892-1997(01)00004-2
   RYAN EB, 1978, J GERONTOL, V33, P98, DOI 10.1093/geronj/33.1.98
   RYAN EB, 1990, PSYCHOL AGING, V5, P514, DOI 10.1037/0882-7974.5.4.514
   RYAN EB, 1986, LANG COMMUN, V6, P1, DOI 10.1016/0271-5309(86)90002-9
   RYAN EB, 1991, PSYCHOL AGING, V6, P442
   SCHERER KR, 1995, J VOICE, V9, P235, DOI 10.1016/S0892-1997(05)80231-0
   SCHWARTZ MF, 1968, J ACOUST SOC AM, V44, P1736, DOI 10.1121/1.1911324
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   SINGH S, 1978, J ACOUST SOC AM, V64, P81, DOI 10.1121/1.381958
   Vongpoisal T, 2007, J SPEECH LANG HEAR R, V50, P1139, DOI 10.1044/1092-4388(2007/079)
   Zäske R, 2011, HEARING RES, V282, P283, DOI 10.1016/j.heares.2011.06.008
NR 52
TC 19
Z9 26
U1 1
U2 16
PU ACOUSTICAL SOC AMER AMER INST PHYSICS
PI MELVILLE
PA STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA
SN 0001-4966
EI 1520-8524
J9 J ACOUST SOC AM
JI J. Acoust. Soc. Am.
PD APR
PY 2016
VL 139
IS 4
BP 1648
EP 1659
DI 10.1121/1.4945094
PG 12
WC Acoustics; Audiology & Speech-Language Pathology
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Acoustics; Audiology & Speech-Language Pathology
GA DK5QN
UT WOS:000374974900018
PM 27106312
DA 2024-01-09
ER

PT J
AU Klopfenstein, M
AF Klopfenstein, Marie
TI Relationship between acoustic measures and speech naturalness ratings in
   Parkinson's disease: A within-speaker approach
SO CLINICAL LINGUISTICS & PHONETICS
LA English
DT Article
DE Fundamental frequency; intensity; phrase-final syllable lengthening;
   pitch accent; prosody; speech naturalness; speech rate; vocalic nucleus
ID INTENSIVE VOICE TREATMENT; TREATMENT LSVT(R); INTELLIGIBILITY;
   DYSARTHRIA; LOUDNESS
AB This study investigated the acoustic basis of across-utterance, within-speaker variation in speech naturalness for four speakers with dysarthria secondary to Parkinson's disease (PD). Speakers read sentences and produced spontaneous speech. Acoustic measures of fundamental frequency, phrase-final syllable lengthening, intensity and speech rate were obtained. A group of listeners judged speech naturalness using a nine-point Likert scale. Relationships between judgements of speech naturalness and acoustic measures were determined for individual speakers with PD. Relationships among acoustic measures also were quantified. Despite variability between speakers, measures of mean F0, intensity range, articulation rate, average syllable duration, duration of final syllables, vocalic nucleus length of final unstressed syllables and pitch accent of final syllables emerged as possible acoustic variables contributing to within-speaker variations in speech naturalness. Results suggest that acoustic measures correlate with speech naturalness, but in dysarthric speech they depend on the speaker due to the within-speaker variation in speech impairment.
C1 [Klopfenstein, Marie] So Illinois Univ, Dept Special Educ & Commun Disorders, Edwardsville, IL 62026 USA.
C3 Southern Illinois University System; Southern Illinois University
   Edwardsville
RP Klopfenstein, M (corresponding author), So Illinois Univ, Dept Special Educ & Commun Disorders, Campus Box 1147, Edwardsville, IL 62026 USA.
EM maklopf@siue.edu
RI Klopfenstein, Marie/T-5448-2019
OI Klopfenstein, Marie/0000-0002-2229-8050
CR [Anonymous], J MED SPEECH LANGUAG
   [Anonymous], DYSARTHRIAS
   Awan SN, 2001, VOICE DIAGNOSTIC PRO
   Boersma P., 2014, PRAAT DOING PHONETIC
   CANTER GJ, 1963, J SPEECH HEAR DISORD, V28, P221, DOI 10.1044/jshd.2803.221
   Cohen J, 1988, STAT POWER ANAL BEHA
   Dagenais PA, 2002, INVESTIGATIONS IN CLINICAL PHONETICS AND LINGUISTICS, P363
   DARLEY FL, 1969, J SPEECH HEAR RES, V12, P462, DOI 10.1044/jshr.1203.462
   DARLEY FL, 1969, J SPEECH HEAR RES, V12, P246, DOI 10.1044/jshr.1202.246
   DROMEY C, 1995, J SPEECH HEAR RES, V38, P751, DOI 10.1044/jshr.3804.751
   Duffy J.R., 1995, Motor speech disorders: Substrates, differential diagnosis, and management
   DUNN OJ, 1964, TECHNOMETRICS, V6, P241, DOI 10.2307/1266041
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   Feenaughty L, 2014, CLIN LINGUIST PHONET, V28, P857, DOI 10.3109/02699206.2014.921839
   Flipsen P, 2003, J SPEECH LANG HEAR R, V46, P724, DOI 10.1044/1092-4388(2003/058)
   FORREST K, 1989, J ACOUST SOC AM, V85, P2608, DOI 10.1121/1.397755
   George D, 2002, SPSS for Windows Step by Step: A Simple Guide and Reference, 11.0 Update, V4th
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P217, DOI 10.1044/jshd.5002.217
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P261, DOI 10.1044/jshd.5003.261
   KENT RD, 1982, BRAIN LANG, V15, P259, DOI 10.1016/0093-934X(82)90060-8
   Kim Y, 2011, J SPEECH LANG HEAR R, V54, P417, DOI 10.1044/1092-4388(2010/10-0020)
   Klopfenstein M., 2012, THESIS
   LINEBAUGH CW, 1984, DYSARTHRIAS PHYSL AC, P197
   LUDLOW CL, 1987, BRAIN LANG, V32, P195, DOI 10.1016/0093-934X(87)90124-6
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P994, DOI 10.1044/jshr.3505.994
   ONSLOW M, 1987, J SPEECH HEAR DISORD, V52, P2, DOI 10.1044/jshd.5201.02
   Ramig LO, 2001, MOVEMENT DISORD, V16, P79, DOI 10.1002/1531-8257(200101)16:1<79::AID-MDS1013>3.0.CO;2-H
   Ramig LO, 2001, J NEUROL NEUROSUR PS, V71, P493, DOI 10.1136/jnnp.71.4.493
   RAMIG LO, 1995, J SPEECH HEAR RES, V38, P1232, DOI 10.1044/jshr.3806.1232
   RUNYAN CM, 1982, J FLUENCY DISORD, V7, P71, DOI 10.1016/0094-730X(82)90040-7
   Sacco P. R., 1992, ANN M AM SPEECH LANG
   Sapir S, 2002, FOLIA PHONIATR LOGO, V54, P296, DOI 10.1159/000066148
   Schiavetti N, 1998, J SPEECH LANG HEAR R, V41, P5, DOI 10.1044/jslhr.4101.05
   Skodda S, 2011, J NEUROL SCI, V310, P231, DOI 10.1016/j.jns.2011.07.020
   Snow D, 1998, J SPEECH LANG HEAR R, V41, P1158, DOI 10.1044/jslhr.4105.1158
   Tjaden K, 2011, J COMMUN DISORD, V44, P655, DOI 10.1016/j.jcomdis.2011.06.003
   TURNER GS, 1993, J SPEECH HEAR RES, V36, P1134, DOI 10.1044/jshr.3606.1134
   UZIEL A, 1975, FOLIA PHONIATR, V27, P166, DOI 10.1159/000263984
   WEISMER G, 1985, Journal of the Acoustical Society of America, V78, pS55, DOI 10.1121/1.2022878
   Weismer G, 2001, FOLIA PHONIATR LOGO, V53, P1, DOI 10.1159/000052649
   Weismer G., 1984, DYSARTHRIAS PHYSL AC, P101
   Whitehill T, 2002, INVESTIGATIONS IN CLINICAL PHONETICS AND LINGUISTICS, P405
   Yorkston K. M., 1984, DYSARTHRIAS PHYSL AC, P197
   YORKSTON KM, 2010, MANAGEMENT MOTOR SPE
   Yunusova Y, 2005, J SPEECH LANG HEAR R, V48, P1294, DOI 10.1044/1092-4388(2005/090)
   Yunusova Y, 2012, FOLIA PHONIATR LOGO, V64, P94, DOI 10.1159/000336890
NR 48
TC 7
Z9 7
U1 4
U2 28
PU TAYLOR & FRANCIS INC
PI PHILADELPHIA
PA 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
SN 0269-9206
EI 1464-5076
J9 CLIN LINGUIST PHONET
JI Clin. Linguist. Phon.
PD DEC 2
PY 2015
VL 29
IS 12
BP 938
EP 954
DI 10.3109/02699206.2015.1081293
PG 17
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA CX3KB
UT WOS:000365596600004
PM 26403503
OA Green Submitted
DA 2024-01-09
ER

PT J
AU Lorenzo-Trueba, J
   Barra-Chicote, R
   San-Segundo, R
   Ferreiros, J
   Yamagishi, J
   Montero, JM
AF Lorenzo-Trueba, Jaime
   Barra-Chicote, Roberto
   San-Segundo, Ruben
   Ferreiros, Javier
   Yamagishi, Junichi
   Montero, Juan M.
TI Emotion transplantation through adaptation in HMM-based speech synthesis
SO COMPUTER SPEECH AND LANGUAGE
LA English
DT Article
DE Statistical parametric speech synthesis; Expressive speech synthesis;
   Cascade adaptation; Emotion transplantation
AB This paper proposes an emotion transplantation method capable of modifying a synthetic speech model through the use of CSMAPLR adaptation in order to incorporate emotional information learned from a different speaker model while maintaining the identity of the original speaker as much as possible. The proposed method relies on learning both emotional and speaker identity information by means of their adaptation function from an average voice model, and combining them into a single cascade transform capable of imbuing the desired emotion into the target speaker. This method is then applied to the task of transplanting four emotions (anger, happiness, sadness and surprise) into 3 male speakers and 3 female speakers and evaluated in a number of perceptual tests. The results of the evaluations show how the perceived naturalness for emotional text significantly favors the use of the proposed transplanted emotional speech synthesis when compared to traditional neutral speech synthesis, evidenced by a big increase in the perceived emotional strength of the synthesized utterances at a slight cost in speech quality. A final evaluation with a robotic laboratory assistant application shows how by using emotional speech we can significantly increase the students' satisfaction with the dialog system, proving how the proposed emotion transplantation system provides benefits in real applications. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Lorenzo-Trueba, Jaime; Barra-Chicote, Roberto; San-Segundo, Ruben; Ferreiros, Javier; Montero, Juan M.] Univ Politecn Madrid, ETSI Telecomunicac, Speech Technol Grp, E-28040 Madrid, Spain.
   [Yamagishi, Junichi] Univ Edinburgh, CSTR, Informat Forum, Edinburgh EH8 9AB, Midlothian, Scotland.
C3 Universidad Politecnica de Madrid; University of Edinburgh
RP Lorenzo-Trueba, J (corresponding author), Univ Politecn Madrid, ETSI Telecomunicac, Speech Technol Grp, Ave Complutense 30,Ciudad Univ, E-28040 Madrid, Spain.
EM jaime.lorenzo@die.upm.es; barra@die.upm.es; lapiz@die.upm.es;
   jfl@die.upm.es; yjamagis@inf.ed.ac.uk; juancho@die.upm.es
RI Barra-Chicote, Roberto/L-4963-2014; San-Segundo, Rubén/J-6027-2017;
   Montero, Juan M/K-2381-2014
OI Barra-Chicote, Roberto/0000-0003-0844-7037; San-Segundo,
   Rubén/0000-0001-9659-5464; Montero, Juan M/0000-0002-7908-5400
FU European Union [287678]; TIMPANO project [TIN2011-28169-C05-03]; INAPRA
   (MICINN) project [DPI2010-21247-C02-02]; MA2VICMR (Comunidad Autonoma de
   Madrid) project [S2009/TIC-1542]; Universidad Politecnica de Madrid
   under grant SBUPM-QTKTZHB; Grants-in-Aid for Scientific Research
   [15K12071] Funding Source: KAKEN
FX The work leading to these results has received funding from the European
   Union under grant agreement 287678. It has also been supported by
   TIMPANO (TIN2011-28169-C05-03), INAPRA (MICINN, DPI2010-21247-C02-02),
   and MA2VICMR (Comunidad Autonoma de Madrid, S2009/TIC-1542) projects.
   Jaime Lorenzo has been funded by Universidad Politecnica de Madrid under
   grant SBUPM-QTKTZHB. The authors want to thank the other members of the
   Speech Technology Group, ARABOT and Simple4All projects for the
   continuous and fruitful discussion on these topics. The authors also
   want to thank the Multimedia Technology Group from Universidad de Vigo
   and the Speech Processing Group (VEU) from Universidad Politecnica de
   Cataluna for sharing some of the databases used in this research.
CR Adell J, 2012, SPEECH COMMUN, V54, P459, DOI 10.1016/j.specom.2011.10.010
   Anastasakos T, 1997, INT CONF ACOUST SPEE, P1043, DOI 10.1109/ICASSP.1997.596119
   Andersson S., 2010, SPEECH PROSODY
   Andersson S, 2012, SPEECH COMMUN, V54, P175, DOI 10.1016/j.specom.2011.08.001
   [Anonymous], P 12 ANN C INT SPEEC
   [Anonymous], P ISCA SPEECH PROS C
   [Anonymous], INTERSPEECH
   Banga C.M., 2010, TECHNICAL REPORT GRU
   Barra-Chicote R., 2008, P LREC
   Barra-Chicote R., 2011, THESIS ETSIT UPM
   Barra-Chicote R, 2010, SPEECH COMMUN, V52, P394, DOI 10.1016/j.specom.2009.12.007
   Bonafonte A., 2008, DOCUMENTATION UPC ES, P2781
   Chen L., 2012, INT 2012 13 ANN C IN
   Chesta C., 1999, EUROSPEECH
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Erro D, 2010, IEEE T AUDIO SPEECH, V18, P974, DOI 10.1109/TASL.2009.2038658
   Gales MJF, 1998, COMPUT SPEECH LANG, V12, P75, DOI 10.1006/csla.1998.0043
   Gales MJF, 2000, IEEE T SPEECH AUDI P, V8, P417, DOI 10.1109/89.848223
   Gao L., 2005, LATIN SQURES EXPT DE
   Hsu CY, 2012, EURASIP J AUDIO SPEE, P1, DOI 10.1186/1687-4722-2012-21
   Kawahara H., 2001, P MAVEBA, P13
   King S., 2008, BLIZZARD CHALLENGE, P2008
   Lorenzo-Trueba J., 2013, P WORKSH TECN ACC 4
   Lorenzo-Trueba J., 2012, INT 2012 13 ANN C IN, P9
   Lorenzo-Trueba J., 2013, 8 ISCA SPEECH SYNTH
   Lutfi SL, 2013, SENSORS-BASEL, V13, P10519, DOI 10.3390/s130810519
   Mendez Pazo F., 2010, P 6 JORN TECN HABL 2
   Nose T, 2013, SPEECH COMMUN, V55, P347, DOI 10.1016/j.specom.2012.09.003
   Obin N., 2011, INTERSPEECH
   Qin L, 2006, LECT NOTES COMPUT SC, V4274, P233
   Raitio T., 2013, COMPUT SPEECH LANG
   Rodriguez-Losada D., 2008, ADV SERVICE ROBOTICS, P229, DOI DOI 10.5772/5950
   Schuller B, 2010, IEEE T AFFECT COMPUT, V1, P119, DOI 10.1109/T-AFFC.2010.8
   Seltzer M.L., 2012, INTERSPEECH
   Shinoda K, 1997, 1997 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING, PROCEEDINGS, P381, DOI 10.1109/ASRU.1997.659114
   Takeda S., 2013, INT J AFFECTIVE ENG, V12, P79
   Yamagishi J, 2005, IEICE T INF SYST, VE88D, P502, DOI 10.1093/ietisy/e88-d.3.502
   Yamagishi J, 2003, IEICE T FUND ELECTR, VE86A, P1956
   Yamagishi J, 2009, IEEE T AUDIO SPEECH, V17, P66, DOI 10.1109/TASL.2008.2006647
   Yanagisawa K., 2013, ORDER, V5, P10
   Zovato E., 2004, 5 ISCA WORKSH SPEECH
NR 41
TC 22
Z9 25
U1 0
U2 24
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0885-2308
EI 1095-8363
J9 COMPUT SPEECH LANG
JI Comput. Speech Lang.
PD NOV
PY 2015
VL 34
IS 1
SI SI
BP 292
EP 307
DI 10.1016/j.csl.2015.03.008
PG 16
WC Computer Science, Artificial Intelligence
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CL8JX
UT WOS:000357222100015
OA Green Accepted
DA 2024-01-09
ER

PT J
AU Anand, S
   Stepp, CE
AF Anand, Supraja
   Stepp, Cara E.
TI Listener Perception of Monopitch, Naturalness, and Intelligibility for
   Speakers With Parkinson's Disease
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
ID INTENSIVE VOICE TREATMENT; DYSARTHRIC SPEECH; SENTENCE INTELLIGIBILITY;
   FUNDAMENTAL-FREQUENCY; TREATMENT LSVT(R); INDIVIDUALS; ACCEPTABILITY;
   COMMUNICATION; IMPRESSIONS; PROGRESSION
AB Purpose: Given the potential significance of speech naturalness to functional and social rehabilitation outcomes, the objective of this study was to examine the effect of listener perceptions of monopitch on speech naturalness and intelligibility in individuals with Parkinson's disease (PD).
   Method: Two short utterances were extracted from monologue samples of 16 speakers with PD and 5 age-matched adults without PD. Sixteen listeners evaluated these stimuli for monopitch, speech naturalness and intelligibility using the visual sort and rate method.
   Results: Naive listeners can reliably judge monopitch, speech naturalness, and intelligibility with minimal familiarization. While monopitch and speech intelligibility were only moderately correlated, monopitch and speech naturalness were highly correlated.
   Conclusions: A great deal of attention is currently being paid to improvement of vocal loudness and thus speech intelligibility in PD. Our findings suggest that prosodic characteristics such as monopitch should be explored as adjuncts to this treatment of dysarthria in PD. Development of such prosodic treatments may enhance speech naturalness and thus improve quality of life.
C1 [Anand, Supraja] West Chester Univ, W Chester, PA 19383 USA.
   [Stepp, Cara E.] Boston Univ, Boston, MA 02215 USA.
C3 Pennsylvania State System of Higher Education (PASSHE); West Chester
   University of Pennsylvania; Boston University
RP Anand, S (corresponding author), West Chester Univ, W Chester, PA 19383 USA.
EM sanand@wcupa.edu
RI Stepp, Cara/AAQ-6425-2020
FU National Institute on Deafness and Other Communication Disorders
   [DC012651]; Eunice Kennedy Shriver National Institute of Child Health
   and Human Development [HD065688]
FX This work was supported by Grant DC012651 from the National Institute on
   Deafness and Other Communication Disorders (awarded to C. E. Stepp), and
   the Boston Rehabilitation Outcomes Center was supported by Grant
   HD065688 from the Eunice Kennedy Shriver National Institute of Child
   Health and Human Development (awarded to A. M. Jette). The authors wish
   to thank Jessica Malloy for assistance with participant recruitment and
   the four listeners who provided pilot ratings of the three percepts.
CR Adams S. G., 1998, CANADIAN ACOUSTICS, V26, P86
   ADAMS SG, 1992, EUR J DISORDER COMM, V27, P121
   [Anonymous], J MED SPEECH LANGUAG
   [Anonymous], DYSARTHRIAS
   Aronson A. E., 1990, Clinical Voice Disorders, An Interdisciplinary Approach
   Baumgartner CA, 2001, J VOICE, V15, P105, DOI 10.1016/S0892-1997(01)00010-8
   Berry, 1983, CLIN DYSARTHRIA, P231
   Bowen L. K., 2014, J MED SPEECH-LANG PA, V21, P235
   Bunton K, 2001, CLIN LINGUIST PHONET, V15, P181
   Bunton K, 2007, J SPEECH LANG HEAR R, V50, P1481, DOI 10.1044/1092-4388(2007/102)
   CAEKEBEKE JFV, 1991, J NEUROL NEUROSUR PS, V54, P145, DOI 10.1136/jnnp.54.2.145
   Cannito MP, 2012, J VOICE, V26, P214, DOI 10.1016/j.jvoice.2011.08.014
   Cheang HS, 2007, J NEUROLINGUIST, V20, P221, DOI 10.1016/j.jneuroling.2006.07.001
   Dagenais PA, 2006, CLIN LINGUIST PHONET, V20, P141, DOI 10.1080/02699200400026843
   Dagenais PA, 1999, J MED SPEECH-LANG PA, V7, P91
   DARLEY FL, 1969, J SPEECH HEAR RES, V12, P246, DOI 10.1044/jshr.1202.246
   De Bodt MS, 2002, J COMMUN DISORD, V35, P283, DOI 10.1016/S0021-9924(02)00065-5
   Duffy J.R., 1995, Motor speech disorders: Substrates, differential diagnosis, and management
   Eadie TL, 2006, AM J SPEECH-LANG PAT, V15, P307, DOI 10.1044/1058-0360(2006/030)
   Eadie TL, 2002, J SPEECH LANG HEAR R, V45, P1088, DOI 10.1044/1092-4388(2002/087)
   Fahn S, 2003, ANN NY ACAD SCI, V991, P1, DOI 10.1111/j.1749-6632.2003.tb07458.x
   Fox Cynthia M., 2006, Seminars in Speech and Language, V27, P283, DOI 10.1055/s-2006-955118
   Gamboa J, 1997, J VOICE, V11, P314, DOI 10.1016/S0892-1997(97)80010-0
   Goberman A, 2002, J COMMUN DISORD, V35, P217, DOI 10.1016/S0021-9924(01)00072-7
   Goberman AM, 2005, J COMMUN DISORD, V38, P215, DOI 10.1016/j.jcomdis.2004.10.001
   Granqvist Svante, 2003, Logoped Phoniatr Vocol, V28, P109, DOI 10.1080/14015430310015255
   GREENE MCL, 1968, FOLIA PHONIATR, V20, P250, DOI 10.1159/000263203
   Ho AK, 1999, NEUROPSYCHOLOGIA, V37, P1453, DOI 10.1016/S0028-3932(99)00067-6
   Holmes RJ, 2000, INT J LANG COMM DIS, V35, P407
   Jaywant A, 2010, J INT NEUROPSYCH SOC, V16, P49, DOI 10.1017/S1355617709990919
   Jimenez-Jimenez F J, 1997, Parkinsonism Relat Disord, V3, P111, DOI 10.1016/S1353-8020(97)00007-2
   KENT RD, 1989, J SPEECH HEAR DISORD, V54, P482, DOI 10.1044/jshd.5404.482
   KENT RD, 1982, BRAIN LANG, V15, P259, DOI 10.1016/0093-934X(82)90060-8
   Laures JS, 1999, J SPEECH LANG HEAR R, V42, P1148, DOI 10.1044/jslhr.4205.1148
   Liss J. M., 2007, MOTOR SPEECH DISORDE, P187
   LOGEMANN JA, 1978, J SPEECH HEAR DISORD, V43, P47, DOI 10.1044/jshd.4301.47
   Lowit A, 2010, INT J SPEECH-LANG PA, V12, P426, DOI 10.3109/17549507.2010.497559
   MacPherson MK, 2011, J SPEECH LANG HEAR R, V54, P19, DOI 10.1044/1092-4388(2010/09-0079)
   Meltzner GS, 2005, J SPEECH LANG HEAR R, V48, P766, DOI 10.1044/1092-4388(2005/053)
   METTER EJ, 1986, J COMMUN DISORD, V19, P347, DOI 10.1016/0021-9924(86)90026-2
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   Miller N, 2006, AGE AGEING, V35, P235, DOI 10.1093/ageing/afj053
   Miller N, 2008, CLIN REHABIL, V22, P14, DOI 10.1177/0269215507079096
   Nagle KF, 2012, J COMMUN DISORD, V45, P235, DOI 10.1016/j.jcomdis.2012.01.001
   Patel R, 2013, FOLIA PHONIATR LOGO, V65, P109, DOI 10.1159/000354422
   Pell MD, 2006, BRAIN LANG, V97, P123, DOI 10.1016/j.bandl.2005.08.010
   PITCAIRN TK, 1990, BRIT J DISORD COMMUN, V25, P85
   Plowman-Prine EK, 2009, NEUROREHABILITATION, V24, P131, DOI 10.3233/NRE-2009-0462
   Ramig LO, 2001, J NEUROL NEUROSUR PS, V71, P493, DOI 10.1136/jnnp.71.4.493
   RAMIG LO, 1995, J SPEECH HEAR RES, V38, P1232, DOI 10.1044/jshr.3806.1232
   Ramig LO., 1994, J MED SPEECH-LANG PA, V2, P191
   Ramig Lorraine O, 2008, Expert Rev Neurother, V8, P297, DOI 10.1586/14737175.8.2.297
   Ramig Lorraine Olson, 2004, Seminars in Speech and Language, V25, P169
   RUBOW R, 1985, J SPEECH HEAR DISORD, V50, P178, DOI 10.1044/jshd.5002.178
   Sapir S, 2007, J SPEECH LANG HEAR R, V50, P899, DOI 10.1044/1092-4388(2007/064)
   SCOTT S, 1983, J NEUROL NEUROSUR PS, V46, P140, DOI 10.1136/jnnp.46.2.140
   Skodda S, 2011, J VOICE, V25, pE199, DOI 10.1016/j.jvoice.2010.04.007
   Skodda S, 2009, MOVEMENT DISORD, V24, P716, DOI 10.1002/mds.22430
   Spielman J, 2007, AM J SPEECH-LANG PAT, V16, P95, DOI 10.1044/1058-0360(2007/014)
   Stathopoulos ET, 2014, J COMMUN DISORD, V48, P1, DOI 10.1016/j.jcomdis.2013.12.001
   Teshima S, 2010, J FLUENCY DISORD, V35, P44, DOI 10.1016/j.jfludis.2010.01.001
   Tjaden K, 2004, J SPEECH LANG HEAR R, V47, P766, DOI 10.1044/1092-4388(2004/058)
   Trail M, 2005, NEUROREHABILITATION, V20, P205
   Weismer G, 2002, J SPEECH LANG HEAR R, V45, P421, DOI 10.1044/1092-4388(2002/033)
   Whitehill TL, 2004, J MED SPEECH-LANG PA, V12, P229
   World Health Organization, 2006, Neurological Disorders
   Yorkston K.M., 1999, Management of motor speech disorders in children and adults
   Yorkston KM., 2004, Management of Speech and Swallowing in Degenerative Diseases
NR 68
TC 41
Z9 57
U1 5
U2 16
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 2200 RESEARCH BLVD, #271, ROCKVILLE, MD 20850-3289 USA
SN 1092-4388
EI 1558-9102
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD AUG
PY 2015
VL 58
IS 4
BP 1134
EP 1144
DI 10.1044/2015_JSLHR-S-14-0243
PG 11
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA DX6XJ
UT WOS:000384529000003
PM 26102242
OA Green Published
DA 2024-01-09
ER

PT J
AU Tamura, Y
   Kuriki, S
   Nakano, T
AF Tamura, Yuri
   Kuriki, Shinji
   Nakano, Tamami
TI Involvement of the left insula in the ecological validity of the human
   voice
SO SCIENTIFIC REPORTS
LA English
DT Article
ID LYRICS; SOUNDS; CORTEX; LOBE
AB A subtle difference between a real human and an artificial object that resembles a human evokes an impression of a large qualitative difference between them. This suggests the existence of a neural mechanism that processes the sense of humanness. To examine the presence of such a mechanism, we compared the behavioral and brain responses of participants who listened to human and artificial singing voices created from vocal fragments of a real human voice. The behavioral experiment showed that the song sung by human voices more often elicited positive feelings and feelings of humanness than the same song sung by artificial voices, although the lyrics, melody, and rhythm were identical. Functional magnetic resonance imaging revealed significantly higher activation in the left posterior insula in response to human voices than in response to artificial voices. Insular activation was not merely evoked by differences in acoustic features between the voices. Therefore, these results suggest that the left insula participates in the neural processing of the ecological quality of the human voice.
C1 [Tamura, Yuri; Kuriki, Shinji; Nakano, Tamami] Osaka Univ, Grad Sch Frontiers Biosci, Dynam Brain Network Lab, Osaka, Japan.
   [Nakano, Tamami] Natl Inst Informat & Commun Technol, Ctr Informat & Neural Networks CiNet, Osaka, Japan.
   [Nakano, Tamami] Osaka Univ, Osaka, Japan.
C3 Osaka University; National Institute of Information & Communications
   Technology (NICT) - Japan; Osaka University
RP Nakano, T (corresponding author), Osaka Univ, Grad Sch Frontiers Biosci, Dynam Brain Network Lab, Osaka, Japan.
EM tamami_nakano@fbs.osaka-u.ac.jp
FU Ministry of Education, Culture, Sports, Science and Technology, Japan
   [251195040]; Grants-in-Aid for Scientific Research [25560429, 25700014,
   25119504] Funding Source: KAKEN
FX We are grateful to Shigeru Kitazawa for his comments on the manuscript
   and Hideki Kawahara for his valuable advice on the acoustic analysis.
   This work was supported by Grant-in-Aid for Scientific Research on
   Innovative Areas 251195040 "Constructive Developmental Science'' from
   the Ministry of Education, Culture, Sports, Science and Technology,
   Japan to T. N.
CR Augustine JR, 1996, BRAIN RES REV, V22, P229, DOI 10.1016/S0165-0173(96)00011-2
   Bamiou DE, 2003, BRAIN RES REV, V42, P143, DOI 10.1016/S0165-0173(03)00172-3
   Belin P, 2002, COGNITIVE BRAIN RES, V13, P17, DOI 10.1016/S0926-6410(01)00084-2
   Brattico E, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00308
   Chaminade T, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0011577
   Fecteau S, 2004, NEUROIMAGE, V23, P840, DOI 10.1016/j.neuroimage.2004.09.019
   Kanda T, 2008, IEEE T ROBOT, V24, P725, DOI 10.1109/TRO.2008.921566
   Lattner S, 2003, HUM BRAIN MAPP, V20, P13, DOI 10.1002/hbm.10118
   Leino T, 2009, J VOICE, V23, P671, DOI 10.1016/j.jvoice.2008.03.008
   Lieberman MD, 2009, SOC COGN AFFECT NEUR, V4, P423, DOI 10.1093/scan/nsp052
   MacDorman KF, 2006, INTERACT STUD, V7, P297, DOI 10.1075/is.7.3.03mac
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Remedios R, 2009, J NEUROSCI, V29, P1034, DOI 10.1523/JNEUROSCI.4089-08.2009
   Sammler D, 2010, J NEUROSCI, V30, P3572, DOI 10.1523/JNEUROSCI.2751-09.2010
   Saygin AP, 2012, SOC COGN AFFECT NEUR, V7, P413, DOI 10.1093/scan/nsr025
NR 15
TC 6
Z9 7
U1 0
U2 2
PU NATURE PUBLISHING GROUP
PI LONDON
PA MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND
SN 2045-2322
J9 SCI REP-UK
JI Sci Rep
PD MAR 5
PY 2015
VL 5
AR 8799
DI 10.1038/srep08799
PG 7
WC Multidisciplinary Sciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Science & Technology - Other Topics
GA CC6DE
UT WOS:000350454100001
PM 25739519
OA Green Published, gold
DA 2024-01-09
ER

PT J
AU Ilves, M
   Surakka, V
AF Ilves, Mirja
   Surakka, Veikko
TI Subjective responses to synthesised speech with lexical emotional
   content: the effect of the naturalness of the synthetic voice
SO BEHAVIOUR & INFORMATION TECHNOLOGY
LA English
DT Article
DE emotion; synthesised speech; naturalness
ID AROUSAL; VALENCE; SYSTEM; HABITUATION; PICTURES; RATINGS; REFLEX; AGENTS
AB This study aimed to investigate how the degree of naturalness and lexical emotional content of synthesised speech affects the subjective ratings of emotional experiences and how the naturalness of the voice affects the ratings of voice quality. Twenty-four participants listened to a set of affective words produced by three different speech synthesis techniques: formant synthesis, diphone synthesis and unit selection synthesis. The participants' task was to rate their experiences evoked by the speech samples using three emotion-related bipolar scales for valence, arousal and approachability. The pleasantness, naturalness and clarity of the voices were also rated. The results showed that the affective words produced by the synthesisers evoked congruent emotion-related ratings in the participants. The ratings of the experienced valence and approachability were statistically significantly stronger when the affective words were produced by the more humanlike voices as compared to the more machinelike voice. The more humanlike voices were also rated as statistically significantly more natural, pleasant and clear than the less humanlike voice. Thus, our findings suggest that even machinelike voices can be used to communicate affective messages but that increasing the level of naturalness enhances positive feelings about synthetic voices and strengthens emotional communication between computers and humans.
C1 [Ilves, Mirja; Surakka, Veikko] Univ Tampere, Res Grp Emot Social & Comp, Tampere Unit Comp Human Interact TAUCHI, Sch Informat Sci, FI-33014 Tampere, Finland.
C3 Tampere University
RP Ilves, M (corresponding author), Univ Tampere, Res Grp Emot Social & Comp, Tampere Unit Comp Human Interact TAUCHI, Sch Informat Sci, Kanslerinrinne 1, FI-33014 Tampere, Finland.
EM mirja.ilves@sis.uta.fi
OI Ilves, Mirja/0000-0002-7763-3741; Surakka, Veikko/0000-0003-3986-0713
FU Doctoral Program in User-Centred Information Technology (UCIT);
   University of Tampere
FX The authors would like to thank all the participants of the study. This
   research was supported by the Doctoral Program in User-Centred
   Information Technology (UCIT) and a grant from the University of
   Tampere.
CR Alais D, 2006, P R SOC B, V273, P1339, DOI 10.1098/rspb.2005.3420
   [Anonymous], 1995, AFFECTIVE COMPUTING
   [Anonymous], 1999, TECHNICAL REPORT C 1
   [Anonymous], 1998, Attention
   Anttonen J., 2005, P SIGCHI C HUM FACT, P491, DOI [10.1145/1054972.1055040, DOI 10.1145/1054972.1055040]
   Aula A, 2002, BCS CONF SERIES, P337
   Bartneck C, 2001, USER MODEL USER-ADAP, V11, P279, DOI 10.1023/A:1011811315582
   Beale R, 2009, INT J HUM-COMPUT ST, V67, P755, DOI 10.1016/j.ijhcs.2009.05.001
   Bertels J, 2009, PSYCHOL BELG, V49, P19, DOI 10.5334/pb-49-1-19
   Beskow J., 2000, PROC INT C SPOKEN LA
   BRADLEY MM, 1993, BEHAV NEUROSCI, V107, P970, DOI 10.1037/0735-7044.107.6.970
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Brave S, 2005, INT J HUM-COMPUT ST, V62, P161, DOI 10.1016/j.ijhcs.2004.11.002
   Breiter HC, 1996, NEURON, V17, P875, DOI 10.1016/S0896-6273(00)80219-6
   Buchanan TW, 2006, INT J PSYCHOPHYSIOL, V61, P26, DOI 10.1016/j.ijpsycho.2005.10.022
   Cahn J. E., 1990, Journal of the American Voice I/O Society, V8, P1
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Canli T, 1998, NEUROREPORT, V9, P3233, DOI 10.1097/00001756-199810050-00019
   Christie IC, 2004, INT J PSYCHOPHYSIOL, V51, P143, DOI 10.1016/j.ijpsycho.2003.08.002
   Conati C, 2009, USER MODEL USER-ADAP, V19, P267, DOI 10.1007/s11257-009-9062-8
   Cowie R, 2003, SPEECH COMMUN, V40, P5, DOI 10.1016/S0167-6393(02)00071-7
   Dai L, 2005, BEHAV INFORM TECHNOL, V24, P219, DOI 10.1080/01449290412331328563
   Dehn DM, 2000, INT J HUM-COMPUT ST, V52, P1, DOI 10.1006/ijhc.1999.0325
   Dutoit T., 1997, AN INTRODUCTION TO T
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Elliot AJ, 1999, EDUC PSYCHOL-US, V34, P169, DOI 10.1207/s15326985ep3403_3
   ESTRADA CA, 1994, MOTIV EMOTION, V18, P285, DOI 10.1007/BF02856470
   Fredrickson BL, 2005, COGNITION EMOTION, V19, P313, DOI 10.1080/02699930441000238
   FRIJDA NH, 1988, AM PSYCHOL, V43, P349, DOI 10.1037/0003-066X.43.5.349
   Grühn D, 2008, BEHAV RES METHODS, V40, P512, DOI 10.3758/BRM.40.2.512
   Guthrie S., 1993, Faces in the Clouds: A New Theory of Religion
   Harmon-Jones E, 2001, J PERS SOC PSYCHOL, V80, P797, DOI 10.1037/0022-3514.80.5.797
   Hassenzahl M, 2006, BEHAV INFORM TECHNOL, V25, P91, DOI 10.1080/01449290500330331
   Iida A, 2003, SPEECH COMMUN, V40, P161, DOI 10.1016/S0167-6393(02)00081-X
   Ilves M., 2004, P 17 INT C COMP AN S, P19
   Ilves M., 2009, EMOTIONS HUMAN VOICE, P137
   Ilves M, 2011, LECT NOTES COMPUT SC, V6974, P588, DOI 10.1007/978-3-642-24600-5_62
   ISEN AM, 1987, J PERS SOC PSYCHOL, V52, P1122, DOI 10.1037/0022-3514.52.6.1122
   Kappas A., 1991, FUNDAMENTALS OF NONV, P201
   Klein J, 2002, INTERACT COMPUT, V14, P119, DOI 10.1016/S0953-5438(01)00053-4
   Knoll M.A., 2011, BEHAV INFORM TECHNOL
   Ku J, 2005, CYBERPSYCHOL BEHAV, V8, P493, DOI 10.1089/cpb.2005.8.493
   LAIRD JD, 1982, J PERS SOC PSYCHOL, V42, P646, DOI 10.1037/0022-3514.42.4.646
   LANG PJ, 1993, PSYCHOPHYSIOLOGY, V30, P261, DOI 10.1111/j.1469-8986.1993.tb03352.x
   LANG PJ, 1992, PSYCHOL SCI, V3, P44, DOI 10.1111/j.1467-9280.1992.tb00255.x
   Larsen JT, 2003, PSYCHOPHYSIOLOGY, V40, P776, DOI 10.1111/1469-8986.00078
   Law ELC, 2010, INTERACT COMPUT, V22, P313, DOI 10.1016/j.intcom.2010.04.006
   Mauss I, 2009, COGNITION EMOTION, V23, P209, DOI 10.1080/02699930802204677
   MILLER JD, 1974, J ACOUST SOC AM, V56, P729, DOI 10.1121/1.1903322
   Minnema MT, 2008, EMOTION, V8, P643, DOI 10.1037/a0013441
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   MURRAY IR, 1995, SPEECH COMMUN, V16, P369, DOI 10.1016/0167-6393(95)00005-9
   NASS C, 1994, HUMAN FACTORS IN COMPUTING SYSTEMS, CHI '94 CONFERENCE PROCEEDINGS - CELEBRATING INTERDEPENDENCE, P72, DOI 10.1145/191666.191703
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   Nass C., 2005, WIRED FOR SPEECH HOW
   Nowak K.L., 2004, J COMPUTER MEDIATED, P9
   Nowak KL., 2005, J COMPUT-MEDIAT COMM, V11, P153, DOI [DOI 10.1111/J.1083-6101.2006.TB00308.X, 10.1111/j.1083-6101.2006.tb00308, 10.1111/j.1083-6101.2006.tb00308.x]
   Nowak KL, 2008, COMPUT HUM BEHAV, V24, P1473, DOI 10.1016/j.chb.2007.05.005
   Partala T, 2004, INTERACT COMPUT, V16, P295, DOI 10.1016/j.intcom.2003.12.001
   Prendinger H, 2005, INT J HUM-COMPUT ST, V62, P231, DOI 10.1016/j.ijhcs.2004.11.009
   Reeves B., 1998, MEDIA EQUATION PEOPL
   Saarni T., 2010, THESIS
   Schneider Walter, 2002, E-Prime user's guide
   Schroder M., 2009, CULTURE PERCEPTION, VIII, P307
   Shi Y., 2009, BEHAV INFORM TECHNOL
   Shneiderman Ben, 1997, interactions, V4, P42, DOI [DOI 10.1145/267505.267514, 10.1145/267505.267514]
   Spering M, 2005, COGNITION EMOTION, V19, P1252, DOI 10.1080/02699930500304886
   Surakka V, 1998, INT J PSYCHOPHYSIOL, V29, P23, DOI 10.1016/S0167-8760(97)00088-3
   Viswanathan M, 2005, COMPUT SPEECH LANG, V19, P55, DOI 10.1016/j.csl.2003.12.001
   Waaramaa-Maki-Kulmala T., 2009, THESIS
   Wambacq IJA, 2004, NEUROREPORT, V15, P555, DOI 10.1097/00001756-200403010-00034
   Werner NS, 2009, INT J PSYCHOPHYSIOL, V74, P259, DOI 10.1016/j.ijpsycho.2009.09.010
   Weyers P, 2006, PSYCHOPHYSIOLOGY, V43, P450, DOI 10.1111/j.1469-8986.2006.00451.x
   Wickens C.D., 2002, Theoretical issues in ergonomics science, V3, P159, DOI [10.1080/14639220210123806, DOI 10.1080/14639220210123806]
   ZAJONC RB, 1980, AM PSYCHOL, V35, P151, DOI 10.1037/0003-066X.35.2.151
NR 76
TC 8
Z9 9
U1 1
U2 19
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 0144-929X
EI 1362-3001
J9 BEHAV INFORM TECHNOL
JI Behav. Inf. Technol.
PD FEB 1
PY 2013
VL 32
IS 2
BP 117
EP 131
DI 10.1080/0144929X.2012.702285
PG 15
WC Computer Science, Cybernetics; Ergonomics
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Engineering
GA 109QU
UT WOS:000316384200003
DA 2024-01-09
ER

PT J
AU Tamagawa, R
   Watson, CI
   Kuo, IH
   MacDonald, BA
   Broadbent, E
AF Tamagawa, Rie
   Watson, Catherine I.
   Kuo, I. Han
   MacDonald, Bruce A.
   Broadbent, Elizabeth
TI The Effects of Synthesized Voice Accents on User Perceptions of Robots
SO INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS
LA English
DT Article
DE Human-robot interaction; Acceptability; Robot; Voice; Perception
ID LANGUAGE ATTITUDES; NEGATIVE AFFECT; SPEECH; SPEAKERS
AB Human voice accents have been shown to affect people's perceptions of the speaker, but little research has looked at how synthesized voice accents affect perceptions of robots. This research investigated people's perceptions of three synthesized voice accents. Three male robot voices were generated: British (UK), American (US), and New Zealand (NZ). In study one, twenty adults listened through headphones to a recorded script repeated in the three different accents, rated the nationality, roboticness, and overall impression of each voice, and chose their preferred accent. Study two used these voices on a healthcare robot to investigate the influence of accent on user perceptions of the robot. Ninety-one individuals were randomized to one of three conditions. In each condition they interacted with a healthcare robot that assisted with blood pressure measurement but the conditions differed in the accent the robot spoke with. In study one, each accent was correctly identified. There was no difference in impression ratings of each voice, but the US accent was rated as more robotic than the NZ accent, and the UK accent was preferred to the US accent. Study two showed that people randomized to the NZ accent had more positive feelings towards the robot and rated the robot's overall performance as higher compared to the robot with the US voice. These results suggest that the employment of a less robotic voice with a local accent may positively affect user perceptions of robots.
C1 [Tamagawa, Rie; Broadbent, Elizabeth] Univ Auckland, Dept Psychol Med, Auckland 1, New Zealand.
   [Watson, Catherine I.; Kuo, I. Han; MacDonald, Bruce A.] Univ Auckland, Dept Elect & Comp Engn, Auckland 1, New Zealand.
C3 University of Auckland; University of Auckland
RP Broadbent, E (corresponding author), Univ Auckland, Dept Psychol Med, Auckland 1, New Zealand.
EM e.broadbent@auckland.ac.nz
RI Watson, Catherine/JAD-1936-2023
OI Broadbent, Elizabeth/0000-0003-3626-9100; Watson,
   Catherine/0000-0001-9010-5188
FU University of Auckland
FX This research was funded by a University of Auckland Grant to Elizabeth
   Broadbent.
CR Alamsaputra DM, 2006, AUGMENT ALTERN COMM, V22, P258, DOI 10.1080/00498250600718555
   [Anonymous], 2006, P AUSTRALASIAN INT C
   [Anonymous], NZ LING SOC C PALM N
   [Anonymous], FESTIVAL SPEECH SYNT
   ARONOVITCH CD, 1976, J SOC PSYCHOL, V99, P207, DOI 10.1080/00224545.1976.9924774
   Arras K. O., 2005, TECHNICAL REPORT
   Atrash A, 2009, INT J SOC ROBOT, V1, P345, DOI 10.1007/s12369-009-0032-4
   Ball Peter, 1983, LANG SCI, V5, P163, DOI [10.1016/S0388-0001(83)80021-7, DOI 10.1016/S0388-0001(83)80021-7]
   BAYARD D, 1999, NZ ENGLISH, P297
   Bayard D., 2001, Journal of Sociolinguistics, V5, P22, DOI DOI 10.1111/1467-9481.00136
   Bayard Donn., 1995, KIWITALK SOCIOLINGUI
   Bennewitz Maren, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P1072
   Berry DS, 1996, J PERS SOC PSYCHOL, V71, P796, DOI 10.1037/0022-3514.71.4.796
   BLACK AW, 2007, BUILDING SYNTHETIC V
   Breazeal C, 2001, IROS 2001: PROCEEDINGS OF THE 2001 IEEE/RJS INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1388, DOI 10.1109/IROS.2001.977175
   Broadbent E., 2007, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, P3703, DOI 10.1109/IROS.2007.4398982
   Cargile AC, 1997, LANG COMMUN, V17, P195, DOI 10.1016/S0271-5309(97)00016-5
   Cesta A, 2007, PSYCHNOLOGY J, V5, P229
   FITT Susan, 2000, TECHNICAL REPORT
   GILES H, 1970, EDUC REV, V22, P211, DOI 10.1080/0013191700220301
   GILES H, 1995, LANG COMMUN, V15, P107, DOI 10.1016/0271-5309(94)00019-9
   Goetz J, 2003, RO-MAN 2003: 12TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P55
   HALL JA, 1981, J HEALTH SOC BEHAV, V22, P18, DOI 10.2307/2136365
   Huygens I., 1983, J MULTILING MULTICUL, V4, P207, DOI DOI 10.1080/01434632.1983.9994112
   Igic Aleksandar, 2009, Australas. Lang. Technol. Assoc. Work, P109
   Kuo I. H., 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P214, DOI 10.1109/ROMAN.2009.5326292
   LEBARON S, 1985, J FAM PRACTICE, V21, P56
   Li X, 2009, AUSTR C ROB AUT SYDN
   LUHMAN R, 1990, LANG SOC, V19, P331, DOI 10.1017/S0047404500014548
   Mayer RE, 2003, J EDUC PSYCHOL, V95, P419, DOI 10.1037/0022-0663.95.2.419
   Mullennix JW, 2003, COMPUT HUM BEHAV, V19, P407, DOI 10.1016/S0747-5632(02)00081-X
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Nass C, 2000, COMMUN ACM, V43, P36, DOI 10.1145/348941.348976
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Niculescu Andreea, 2008, P 5 NORDIC C HUMANCO, P523
   Oestreicher Lars, 2007, 16th IEEE International Conference on Robot and Human Interactive Communication, P558
   Pucher M, 2009, LECT NOTES ARTIF INT, V5398, P216
   Robins B, 2004, RO-MAN 2004: 13TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P277, DOI 10.1109/ROMAN.2004.1374773
   Stern SE, 2008, J LANG SOC PSYCHOL, V27, P254, DOI 10.1177/0261927X08318035
   Tusing KJ, 2000, HUM COMMUN RES, V26, P148, DOI 10.1111/j.1468-2958.2000.tb00754.x
   Walters ML, 2008, 2008 17TH IEEE INTERNATIONAL SYMPOSIUM ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, VOLS 1 AND 2, P707, DOI 10.1109/ROMAN.2008.4600750
   WATSON D, 1988, J PERS SOC PSYCHOL, V54, P1063, DOI 10.1037/0022-3514.54.6.1063
NR 42
TC 60
Z9 67
U1 5
U2 16
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1875-4791
EI 1875-4805
J9 INT J SOC ROBOT
JI Int. J. Soc. Robot.
PD AUG
PY 2011
VL 3
IS 3
BP 253
EP 262
DI 10.1007/s12369-011-0100-4
PG 10
WC Robotics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Robotics
GA V31OX
UT WOS:000208894000006
DA 2024-01-09
ER

PT J
AU Mayo, C
   Clark, RAJ
   King, S
AF Mayo, Catherine
   Clark, Robert A. J.
   King, Simon
TI Listeners' weighting of acoustic cues to synthetic speech naturalness: A
   multidimensional scaling analysis
SO SPEECH COMMUNICATION
LA English
DT Article
DE Speech synthesis; Evaluation; Speech perception; Acoustic cue weighting;
   Multidimensional scaling
ID VOICE QUALITY ASSESSMENT; FINAL STOP CONSONANTS; COMPLEX SOUNDS;
   CHILDREN; ADULTS; PERCEPTION; CATEGORIZATION; ENGLISH; RATINGS
AB The quality of current commercial speech synthesis systems is now so high that system improvements are being made at subtle sub- and supra-segmental levels. Human perceptual evaluation of such subtle improvements requires a highly sophisticated level of perceptual attention to specific acoustic characteristics or cues. However, it is not well understood what acoustic cues listeners attend to by default when asked to evaluate synthetic speech. It may, therefore, be potentially quite difficult to design an evaluation method that allows listeners to concentrate on only one dimension of the signal, while ignoring others that are perceptually more important to them.
   The aim of the current study was to determine which acoustic characteristics of unit-selection synthetic speech are most salient to listeners when evaluating the naturalness of such speech. This study made use of multidimensional scaling techniques to analyse listeners' pairwise comparisons of synthetic speech sentences. Results indicate that listeners place a great deal of perceptual importance on the presence of artifacts and discontinuities in the speech, somewhat less importance on aspects of segmental quality, and very little importance on stress/intonation appropriateness. These relative differences in importance will impact on listeners' ability to attend to these different acoustic characteristics of synthetic speech, and should therefore be taken into account when designing appropriate methods of synthetic speech evaluation. (C) 2010 Elsevier B.V. All rights reserved.
C1 [Mayo, Catherine; Clark, Robert A. J.; King, Simon] Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9AB, Midlothian, Scotland.
C3 University of Edinburgh
RP Mayo, C (corresponding author), Univ Edinburgh, Ctr Speech Technol Res, 10 Crichton St, Edinburgh EH8 9AB, Midlothian, Scotland.
EM catherin@ling.ed.ac.uk; robert@cstr.ed.ac.uk; simon.king@ed.ac.uk
OI King, Simon/0000-0002-2694-2843
FU Engineering and Physical Sciences Research Council [EP/C53042X/1]
   Funding Source: researchfish
CR Allen P, 1997, J ACOUST SOC AM, V102, P2255, DOI 10.1121/1.419637
   Allen P, 2002, J ACOUST SOC AM, V112, P211, DOI 10.1121/1.1482075
   [Anonymous], P ICSLP 98 SYND
   BAILLY G, 2003, ISCA SPEC SESS HOT T
   BEST CT, 1981, PERCEPT PSYCHOPHYS, V29, P191, DOI 10.3758/BF03207286
   Black A., 1997, FESTIVAL SPEECH SYNT
   Bradlow AR, 1999, PERCEPT PSYCHOPHYS, V61, P206, DOI 10.3758/BF03206883
   CERNAK M, 2009, P ICSVI INT C SOUND
   Cernak M., 2005, EUR C AC, P2725
   CHEN JD, 1999, P EUROSPEECH, P611
   Christensen LA, 1997, J ACOUST SOC AM, V102, P2297, DOI 10.1121/1.419639
   CLARK RAJ, 1999, P EUR 99 6 EUR C SPE, P1623
   CLARK RAJ, 2003, INT C PHON SCI BARCE, P1141
   Clark RAJ, 2007, SPEECH COMMUN, V49, P317, DOI 10.1016/j.specom.2007.01.014
   CUTLER A, 1994, J MEM LANG, V33, P824, DOI 10.1006/jmla.1994.1039
   *EXP ADV GROUP LAN, 1996, EV NAT LANG PROC SYS
   FALK TH, 2008, P BLIZZ WORKSH BRISB
   Fisher C, 1996, SIGNAL TO SYNTAX: BOOTSTRAPPING FROM SPEECH TO GRAMMAR IN EARLY ACQUISITION, P343
   Francis AL, 2008, J ACOUST SOC AM, V124, P1234, DOI 10.1121/1.2945161
   Garofolo J.S., 1988, Tech. rep.
   GORDON PC, 1993, COGNITIVE PSYCHOL, V25, P1, DOI 10.1006/cogp.1993.1001
   Hall JL, 2001, J ACOUST SOC AM, V110, P2167, DOI 10.1121/1.1397322
   Hazan V, 2000, J PHONETICS, V28, P377, DOI 10.1006/jpho.2000.0121
   Hazan V, 1998, SPEECH COMMUN, V24, P211, DOI 10.1016/S0167-6393(98)00011-9
   HAZAN V, 1998, ICSLP SYD AUSTR, P2163
   HIRST D, 1998, P ESCA COCOSDA WORKS
   *ITU T, 1994, P85 ITUT
   Iverson P, 2003, COGNITION, V87, pB47, DOI 10.1016/S0010-0277(02)00198-1
   Iverson P, 2005, J ACOUST SOC AM, V118, P3267, DOI 10.1121/1.2062307
   Jilka M., 2003, P 15 ICPHS BARC, P2549
   JILKA M, 2005, P INT 2005 LISB PORT, P2393
   JUSCZYK P., 1997, The Discovery of Spoken Language
   KLABBERS E, 2001, IEEE T SPEECH AUDIO, V9
   KLABBERS E, 1998, P ICSLP, P1983
   Kreiman J, 1998, J ACOUST SOC AM, V104, P1598, DOI 10.1121/1.424372
   Kreiman J, 2000, J ACOUST SOC AM, V108, P1867, DOI 10.1121/1.1289362
   KREIMAN J, 2004, J ACOUST SOC AM, V115, P2609
   Kreiman J, 2007, J ACOUST SOC AM, V122, P2354, DOI 10.1121/1.2770547
   Kruskal JB., 1978, SAGE U PAPER SERIES, DOI 10.4135/9781412985130
   LAMEL LF, 1989, P SPEECH I O ASS SPE, P2161
   Marozeau J, 2003, J ACOUST SOC AM, V114, P2946, DOI 10.1121/1.1618239
   Mayo C, 2005, J ACOUST SOC AM, V118, P1730, DOI 10.1121/1.1979451
   Mayo C, 2004, J ACOUST SOC AM, V115, P3184, DOI 10.1121/1.1738838
   MAYO C, 2005, P INT 2005 LISB PORT
   MOLLER S, 2009, P NAG DAGA 2009 ROTT, P1168
   Nittrouer S, 2004, J ACOUST SOC AM, V115, P1777, DOI 10.1121/1.1651192
   PLUMPE M, 1998, P ESCA COCOSDA WORKS
   RABINOV CR, 1995, J SPEECH HEAR RES, V38, P26, DOI 10.1044/jshr.3801.26
   Schnieder W., 2002, E PRIME USERS GUIDE
   STYLIANOU Y, 2001, P ICASSP INT C AC SP
   SYRDAL A, 2004, J ACOUST SOC AM, V115, P2543
   SYRDAL AK, 2001, P EUR, P979
   Turk A., 2012, Methods in Empirical Prosody Research, P1, DOI [10.1515/9783110914641, DOI 10.1515/9783110914641.1, 10.1515/9783110914641.1]
   Vainio M., 2002, P IEEE 2002 WORKSH S
   Vepa J., 2004, TEXT SPEECH SYNTHESI
   WARDRIPFRUIN C, 1982, J ACOUST SOC AM, V71, P187, DOI 10.1121/1.387346
   WARDRIPFRUIN C, 1985, J ACOUST SOC AM, V77, P1907, DOI 10.1121/1.391833
   WATSON J, 1997, THESIS QUEEN MARGARE
   WIGHTMAN CW, 1992, J ACOUST SOC AM, V91, P1707, DOI 10.1121/1.402450
   Zen H, 2007, COMPUT SPEECH LANG, V21, P153, DOI 10.1016/j.csl.2006.01.002
NR 60
TC 20
Z9 23
U1 1
U2 12
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-6393
EI 1872-7182
J9 SPEECH COMMUN
JI Speech Commun.
PD MAR
PY 2011
VL 53
IS 3
BP 311
EP 326
DI 10.1016/j.specom.2010.10.003
PG 16
WC Acoustics; Computer Science, Interdisciplinary Applications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Acoustics; Computer Science
GA 724FK
UT WOS:000287561300004
OA Green Submitted
DA 2024-01-09
ER

PT J
AU Mitchell, WJ
   Szerszen, KA
   Lu, AS
   Schermerhorn, PW
   Scheutz, M
   MacDorman, KF
AF Mitchell, Wade J.
   Szerszen, Kevin A., Sr.
   Lu, Amy Shirong
   Schermerhorn, Paul W.
   Scheutz, Matthias
   MacDorman, Karl F.
TI A mismatch in the human realism of face and voice produces an uncanny
   valley
SO I-PERCEPTION
LA English
DT Article
DE anthropomorphism; facial-vocal mismatch; human realism; Masahiro Mori;
   social perception
AB The uncanny valley has become synonymous with the uneasy feeling of viewing an animated character or robot that looks imperfectly human. Although previous uncanny valley experiments have focused on relations among a character's visual elements, the current experiment examines whether a mismatch in the human realism of a character's face and voice causes it to be evaluated as eerie. The results support this hypothesis.
C1 [Mitchell, Wade J.; Szerszen, Kevin A., Sr.; Lu, Amy Shirong; MacDorman, Karl F.] Indiana Univ, Sch Informat, Indianapolis, IN 46202 USA.
   [Schermerhorn, Paul W.; Scheutz, Matthias] Indiana Univ, Cognit Sci Program, Indianapolis, IN 46202 USA.
   [Scheutz, Matthias] Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.
C3 Indiana University System; Indiana University-Purdue University
   Indianapolis; Indiana University System; Indiana University-Purdue
   University Indianapolis; Tufts University
RP Mitchell, WJ (corresponding author), Indiana Univ, Sch Informat, 535 West Michigan St, Indianapolis, IN 46202 USA.
EM wamitche@iupui.edu; keszersz@iupui.edu; amylu@iupui.edu;
   pscherme@indiana.edu; mscheutz@cs.tufts.edu; kmacdorm@indiana.edu
RI Lu, Amy Shirong/ITV-5174-2023; MacDorman, Karl/AAH-4483-2020; Lu, Amy
   Shirong/AAQ-7559-2020
OI Lu, Amy Shirong/0000-0002-8230-9049; MacDorman,
   Karl/0000-0003-1093-4184; Lu, Amy Shirong/0000-0002-8230-9049; Scheutz,
   Matthias/0000-0002-0064-2789
CR Ho CC, 2010, COMPUT HUM BEHAV, V26, P1508, DOI 10.1016/j.chb.2010.05.015
   Jentsch Ernst, 1906, Psychiatrisch-Neurologische Wochenschrift, P195
   MacDorman KF, 2006, INTERACT STUD, V7, P297, DOI 10.1075/is.7.3.03mac
   MacDorman KF, 2009, AI SOC, V23, P485, DOI 10.1007/s00146-008-0181-2
   MacDorman KF, 2009, COMPUT HUM BEHAV, V25, P695, DOI 10.1016/j.chb.2008.12.026
   Misselhorn C, 2009, MIND MACH, V19, P345, DOI 10.1007/s11023-009-9158-2
   Moosa Mandi Muhammad, 2010, Biology Theory, V5, P12, DOI 10.1162/BIOT_a_00016
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Ramey C.H., 2005, P VIEWS UNC VALL WOR, P8
   Seyama J, 2007, PRESENCE-TELEOP VIRT, V16, P337, DOI 10.1162/pres.16.4.337
   Tinwell A, 2010, GAMRES COMPUTING CRE, V2, P3, DOI [10.1386/jgvw.2.1.3_1J, DOI 10.1386/JGVW.2.1.3_1]
NR 11
TC 126
Z9 152
U1 2
U2 41
PU PION LTD
PI LONDON
PA 207 BRONDESBURY PARK, LONDON NW2 5JN, ENGLAND
SN 2041-6695
J9 I-PERCEPTION
JI I-Perception
PY 2011
VL 2
IS 1
BP 10
EP 12
DI 10.1068/i0415
PG 3
WC Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA V32GU
UT WOS:000208940500002
PM 23145223
OA Green Published, gold
DA 2024-01-09
ER

PT J
AU Lee, EJ
AF Lee, Eun-Ju
TI The more humanlike, the better? How speech type and users' cognitive
   style affect social responses to computers
SO COMPUTERS IN HUMAN BEHAVIOR
LA English
DT Article
DE Anthropomorphism; Computers Are Social Actors (CASA); Experientiality;
   Rationality
ID MODERATING ROLE; GENDER; INTERFACE; ANTHROPOMORPHISM; PERSONALITY;
   COPRESENCE
AB The present experiment investigated if anthropomorphic interfaces facilitate people's tendency to project social expectations onto computers and how such effects might vary depending on users' cognitive style. In a 2 (synthetic vs. recorded speech) x 2 (flattering vs. generic feedback) x 2 (low vs. high rationality) x 2 (low vs. high experientiality) experiment, participants played a trivia game with a computer. Use of recorded speech did not amplify the previously documented flattery effects (Fogg & Nass, 1997), challenging the notion that anthropomorphism will promote social responses to computers. Participants evaluated the human-voiced computer more positively and conformed more to its suggestions than the one using synthetic speech, but such effects were found only among less analytical or more intuition-driven individuals, suggesting dispositional differences in people's susceptibility to anthropomorphic cues embedded in the interface. (C) 2010 Elsevier Ltd. All rights reserved.
C1 Seoul Natl Univ, Dept Commun, Seoul 151742, South Korea.
C3 Seoul National University (SNU)
RP Lee, EJ (corresponding author), Seoul Natl Univ, Dept Commun, San 56-1 Shilim Dong, Seoul 151742, South Korea.
EM eunju0204@snu.ac.kr
RI Lee, Eun-ju/JAN-8749-2023
CR Bailenson JN, 2001, PRESENCE-VIRTUAL AUG, V10, P583, DOI 10.1162/105474601753272844
   Bailenson JN, 2005, PRESENCE-VIRTUAL AUG, V14, P379, DOI 10.1162/105474605774785235
   Byrne D., 1971, The attraction paradigm
   Campbell MC, 2000, J CONSUM RES, V27, P69, DOI 10.1086/314309
   Carli LL, 2001, J SOC ISSUES, V57, P725, DOI 10.1111/0022-4537.00238
   Cassell J, 1999, APPL ARTIF INTELL, V13, P519, DOI 10.1080/088395199117360
   Chen S, 1999, DUAL-PROCESS THEORIES IN SOCIAL PSYCHOLOGY, P73
   Dehn DM, 2000, INT J HUM-COMPUT ST, V52, P1, DOI 10.1006/ijhc.1999.0325
   EAGLY AH, 1975, J PERS SOC PSYCHOL, V32, P136, DOI 10.1037/h0076850
   EPSTEIN S, 1994, AM PSYCHOL, V49, P709, DOI 10.1037/0003-066X.49.8.709
   Epstein S, 1999, DUAL-PROCESS THEORIES IN SOCIAL PSYCHOLOGY, P462
   Fogg B., 2003, PERSUASIVE TECHNOLOG
   Fogg BJ, 1997, INT J HUM-COMPUT ST, V46, P551, DOI 10.1006/ijhc.1996.0104
   HECKMAN CE, 2000, AUTON AGENT MULTI-AG, P435
   Hinds PJ, 2004, HUM-COMPUT INTERACT, V19, P151, DOI 10.1207/s15327051hci1901&2_7
   Lee EJ, 2003, INT J HUM-COMPUT ST, V58, P347, DOI 10.1016/S1071-5819(03)00009-0
   Lee Eun Ju, 2000, P CHI 00 HUM FACT CO, P289, DOI [10.1145/633292.633461, DOI 10.1145/633292.633461]
   Lee EJ, 2008, INT J HUM-COMPUT ST, V66, P789, DOI 10.1016/j.ijhcs.2008.07.009
   Lee EJ, 2008, J COMMUN, V58, P301, DOI 10.1111/j.1460-2466.2008.00386.x
   Lee KM, 2004, HUM COMMUN RES, V30, P182, DOI 10.1093/hcr/30.2.182
   LI G, 2007, HUMAN COMMUNICATION, V33, P163
   MacDorman K. F., 2006, SUBJECTIVE RATINGS R, P25
   Moon Y, 2000, J CONSUM RES, V26, P323, DOI 10.1086/209566
   Moon Y, 1996, COMMUN RES, V23, P651, DOI 10.1177/009365096023006002
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Nass C, 2000, J SOC ISSUES, V56, P81, DOI 10.1111/0022-4537.00153
   Nass Clifford, 2005, Wired for speech: How voice activates and advances the human-computer relationship
   Nowak KL, 2005, J COMPUT-MEDIAT COMM, V11
   Nowak KL, 2003, PRESENCE-TELEOP VIRT, V12, P481, DOI 10.1162/105474603322761289
   Pacini R, 1999, J PERS SOC PSYCHOL, V76, P972, DOI 10.1037/0022-3514.76.6.972
   Parise S, 1999, COMPUT HUM BEHAV, V15, P123, DOI 10.1016/S0747-5632(98)00035-1
   Petty R.E., 1986, The Elaboration Likelihood Model of Persuasion, P1, DOI DOI 10.1007/978-1-4612-4964-1_1
   Powers Aaron, 2006, P 1 ACM SIGCHI SIGAR, P218, DOI [10.1145/1121241.1121280, DOI 10.1145/1121241.1121280, DOI 10.1109/MRA.2018.2833157]
   Reeves B., 1998, MEDIA EQUATION PEOPL
   Sproull L, 1996, HUM-COMPUT INTERACT, V11, P97, DOI 10.1207/s15327051hci1102_1
   Sundar SS, 2004, BEHAV INFORM TECHNOL, V23, P107, DOI 10.1080/01449290310001659222
   Yee N, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1
NR 37
TC 45
Z9 56
U1 2
U2 43
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0747-5632
EI 1873-7692
J9 COMPUT HUM BEHAV
JI Comput. Hum. Behav.
PD JUL
PY 2010
VL 26
IS 4
BP 665
EP 672
DI 10.1016/j.chb.2010.01.003
PG 8
WC Psychology, Multidisciplinary; Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA 605LG
UT WOS:000278348500022
DA 2024-01-09
ER

PT J
AU Eadie, TL
   Doyle, TC
   Hansen, K
   Beaudin, TG
AF Eadie, Tanya L.
   Doyle, Tphilip C.
   Hansen, Kerry
   Beaudin, Tpaul G.
TI Influence of speaker gender on listener judgments of tracheoesophageal
   speech
SO JOURNAL OF VOICE
LA English
DT Article; Proceedings Paper
CT 35th Annual Symposium of the Voice-Foundation
CY JUN, 2006
CL Philadelphia, PA
SP Voice Fdn
DE tracheoesophageal; speech-listener; judgments-acoustic measures
ID QUALITY-OF-LIFE; ESOPHAGEAL SPEECH; PERCEPTUAL EVALUATION;
   FREQUENCY-CHARACTERISTICS; ACOUSTIC CHARACTERISTICS; ACCEPTABILITY
   RATINGS; LARYNGEAL-CANCER; VOICE; FEMALE; RESTORATION
AB The objectives of this prospective and exploratory study are to determine: (1) naive listener preference for gender in tracheoesophageal (TE) speech when speech severity is controlled; (2) the accuracy of identifying TE speaker gender; (3) the effects of gender identification on judgments of speech acceptability (ACC) and naturalness (NAT); and (4) the acoustic basis of ACC and NAT judgments. Six male and six female adult TE speakers were matched for speech severity. Twenty naive listeners made auditory-perceptual judgments of speech samples in three listening sessions. First, listeners performed preference judgments using a paired comparison paradigm. Second, listeners made judgments of speaker gender, speech ACC, and NAT using rating scales. Last, listeners made ACC and NAT judgments when speaker gender was provided coincidentally. Duration, frequency, and spectral measures were performed. No significant differences were found for preference of male or female speakers. All male speakers were accurately identified, but only two of six female speakers were accurately identified. Significant interactions were found between gender and listening condition (gender known) for NAT and ACC judgments. Males were judged more natural when gender was known; female speakers were judged less natural and less acceptable when gender was known. Regression analyses revealed that judgments of female speakers were best predicted with duration measures when gender was unknown, but with spectral measures when gender was known; judgments of males were best predicted with spectral measures. Naive listeners have difficulty identifying the gender of female TE speakers. Listeners show no preference for speaker gender, but when gender is known, female speakers are least acceptable and natural. The nature of the perceptual task may affect the acoustic basis of listener judgments.
C1 [Eadie, Tanya L.] Univ Washington, Dept Speech & Hearing Sci, Seattle, WA 98195 USA.
   [Doyle, Tphilip C.; Hansen, Kerry; Beaudin, Tpaul G.] Univ Western Ontario, Doctoral Program Rehabil Sci, London, ON, Canada.
C3 University of Washington; University of Washington Seattle; Western
   University (University of Western Ontario)
RP Eadie, TL (corresponding author), Univ Washington, Dept Speech & Hearing Sci, 1417 NE 42nd St, Seattle, WA 98195 USA.
EM teadie@u.washington.edu
OI Eadie, Tanya/0000-0002-7697-1298
CR [Anonymous], CANC FACTS FIG 2003
   *AV INN INC, 1998, INT VOIC AN SYST IVA
   Avaaz Innovations Inc, 1998, EXP CONTR GEN WIND E
   Bellandese MH, 2001, J SPEECH LANG HEAR R, V44, P1315, DOI 10.1044/1092-4388(2001/102)
   BENNETT S, 1973, J SPEECH HEAR RES, V16, P608, DOI 10.1044/jshr.1604.608
   BERLIN CI, 1965, J SPEECH HEAR DISORD, V30, P174, DOI 10.1044/jshd.3002.174
   Cervera T, 2001, J SPEECH LANG HEAR R, V44, P988, DOI 10.1044/1092-4388(2001/077)
   Doyle P.C., 2005, CONT CONSIDERATIONS, P113
   Doyle P. C., 1994, Foundations of Voice and Speech Rehabilitation Following Laryngeal Cancer
   Eadie TL, 2004, LARYNGOSCOPE, V114, P753, DOI 10.1097/00005537-200404000-00030
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   FILTER MD, 1975, PERCEPT MOTOR SKILL, V40, P63, DOI 10.2466/pms.1975.40.1.63
   Finizia C, 1999, ARCH OTOLARYNGOL, V125, P157, DOI 10.1001/archotol.125.2.157
   Finizia C, 1998, LARYNGOSCOPE, V108, P1566, DOI 10.1097/00005537-199810000-00027
   Gelfer MP, 2000, J VOICE, V14, P22, DOI 10.1016/S0892-1997(00)80092-2
   Graville DJ, 2004, J MED SPEECH-LANG PA, V12, P107
   Hillman RE, 1998, ANN OTO RHINOL LARYN, V107, P2
   Iversen-Thoburn SK, 2000, J MED SPEECH-LANG PA, V8, P85
   *KAY EL CORP, 2000, COMP SPEECH LAB CSL
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   LANPHER A, 1997, SELF HELP LARYNGECTO, P71
   Meltzner GS, 2005, J SPEECH LANG HEAR R, V48, P766, DOI 10.1044/1092-4388(2005/053)
   Most T, 2000, J COMMUN DISORD, V33, P165, DOI 10.1016/S0021-9924(99)00030-1
   Mullennix JW, 1995, J ACOUST SOC AM, V98, P3080, DOI 10.1121/1.413832
   Parsa V, 2001, J SPEECH LANG HEAR R, V44, P327, DOI 10.1044/1092-4388(2001/027)
   PINDZOLA RH, 1988, LARYNGOSCOPE, V98, P394
   PINDZOLA RH, 1989, ANN OTO RHINOL LARYN, V98, P960, DOI 10.1177/000348948909801208
   ROBBINS J, 1984, J SPEECH HEAR DISORD, V49, P202, DOI 10.1044/jshd.4902.202
   Searl JP, 2002, J COMMUN DISORD, V35, P407, DOI 10.1016/S0021-9924(02)00092-8
   SHANKS J, 1986, LARYNGECTOMEE REHABI, P269
   SHIPP T, 1967, J SPEECH HEAR RES, V10, P417, DOI 10.1044/jshr.1003.417
   SINGER MI, 1980, ANN OTO RHINOL LARYN, V89, P529, DOI 10.1177/000348948008900608
   SISTY NL, 1972, J SPEECH HEAR RES, V15, P439, DOI 10.1044/jshr.1502.439
   SNIDECOR JC, 1975, LARYNGOSCOPE, V85, P640, DOI 10.1288/00005537-197504000-00005
   SNIDECOR JC, 1959, ANN OTO RHINOL LARYN, V68, P1
   TARDYMITZELL S, 1985, ARCH OTOLARYNGOL, V111, P212
   TRUDEAU MD, 1990, J SPEECH HEAR DISORD, V55, P244, DOI 10.1044/jshd.5502.244
   van As CJ, 1998, J VOICE, V12, P239, DOI 10.1016/S0892-1997(98)80044-1
   Van Riper C., 1978, SPEECH CORRECTION PR
   WEINBERG B, 1972, J SPEECH HEAR RES, V15, P211, DOI 10.1044/jshr.1501.211
   WEINBERG B, 1971, J SPEECH HEAR RES, V14, P391, DOI 10.1044/jshr.1402.391
   WILLIAMS SE, 1985, ARCH OTOLARYNGOL, V111, P216
   Yorkston K. M., 1988, Clinical management of dysarthric speakers
NR 43
TC 11
Z9 15
U1 0
U2 6
PU MOSBY-ELSEVIER
PI NEW YORK
PA 360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA
SN 0892-1997
J9 J VOICE
JI J. Voice
PD JAN
PY 2008
VL 22
IS 1
BP 43
EP 57
DI 10.1016/j.jvoice.2006.08.008
PG 15
WC Audiology & Speech-Language Pathology; Otorhinolaryngology
WE Conference Proceedings Citation Index - Science (CPCI-S); Science Citation Index Expanded (SCI-EXPANDED)
SC Audiology & Speech-Language Pathology; Otorhinolaryngology
GA 250HK
UT WOS:000252290400005
PM 17055223
DA 2024-01-09
ER

PT J
AU Gong, L
   Nass, C
AF Gong, Li
   Nass, Clifford
TI When a talking-face computer agent is half-human and half-humanoid:
   Human identity and consistency preference
SO HUMAN COMMUNICATION RESEARCH
LA English
DT Article
ID SELF-DISCLOSURE; EXPERIMENTAL TESTS; COMMUNICATION; INFORMATION;
   REPRESENTATION; INCONSISTENCY; PERSONALITY; ATTENTION; ATTITUDES
AB Computer-generated anthropomorphic characters are a growing type of communicator that is deployed in digital communication environments. An essential theoretical question is how people identify humanlike but clearly artificial, hence humanoid, entities in comparison to natural human ones. This identity categorization inquiry was approached under the framework of consistency and tested through examining inconsistency effects from mismatching categories. Study 1 (N = 80), incorporating a self-disclosure task, tested participants' responses to a talking-face agent, which varied in four combinations of human versus humanoid faces and voices. In line with the literature on inconsistency, the pairing of a human face with a humanoid voice or a humanoid face with a human voice led to longer processing time in making judgment of the agent and less trust than the pairing of a face and a voice from either the human or the humanoid category. Female users particularly showed negative attitudes toward inconsistently paired talking faces. Study 2 (N = 80), using a task that stressed comprehension demand, replicated the inconsistency effects on judging time and females' negative attitudes but not for comprehension-related outcomes. Voice clarity overshadowed the consistency concern for comprehension-related responses. The overall inconsistency effects suggest that people treat humanoid entities in a different category from natural human ones.
C1 Ohio State Univ, Sch Commun, Columbus, OH 43210 USA.
   Stanford Univ, Dept Commun, Stanford, CA 94305 USA.
C3 University System of Ohio; Ohio State University; Stanford University
RP Gong, L (corresponding author), Ohio State Univ, Sch Commun, Columbus, OH 43210 USA.
EM gong.33@osu.edu
CR [Anonymous], P 8 WORLD C ART INT
   [Anonymous], J ABNORMAL SOCIAL PS
   ARGYLE M, 1970, BRIT J SOC CLIN PSYC, V9, P222, DOI 10.1111/j.2044-8260.1970.tb00668.x
   ARGYLE M, 1971, EUR J SOC PSYCHOL, V1, P385, DOI 10.1002/ejsp.2420010307
   Berger C.R., 1974, Human Communication Research, V1, P99, DOI 10.1111/j.1468-2958.1975.tb00258.x
   Berger CR, 2005, J COMMUN, V55, P415, DOI 10.1111/j.1460-2466.2005.tb02680.x
   BERGER CR, 1986, HUM COMMUN RES, V13, P34, DOI 10.1111/j.1468-2958.1986.tb00093.x
   BICKMORE T, 2001, P SIGCHI C HUM FACT, P396, DOI DOI 10.1145/365024.365304
   BURGOON JK, 1993, HUM COMMUN RES, V20, P67, DOI 10.1111/j.1468-2958.1993.tb00316.x
   Burgoon JK, 2000, COMPUT HUM BEHAV, V16, P553, DOI 10.1016/S0747-5632(00)00029-7
   Dindia K, 1997, HUM COMMUN RES, V23, P388, DOI 10.1111/j.1468-2958.1997.tb00402.x
   EKMAN P, 1974, J PERS SOC PSYCHOL, V29, P288, DOI 10.1037/h0036006
   EKMAN P, 1969, PSYCHIATR, V32, P88, DOI 10.1080/00332747.1969.11023575
   FISKE ST, 1990, ADV EXP SOC PSYCHOL, V23, P1, DOI 10.1016/S0065-2601(08)60317-2
   Gibbs JL, 2006, COMMUN RES, V33, P152, DOI 10.1177/0093650205285368
   GOLINKOFF RM, 1976, CHILD DEV, V47, P252, DOI 10.2307/1128308
   Gong L., 2003, INT J SPEECH TECHNOL, V6, P123
   Grabner-Kräuter S, 2003, INT J HUM-COMPUT ST, V58, P783, DOI 10.1016/S1071-5819(03)00043-0
   GRAVES JR, 1976, J COUNS PSYCHOL, V23, P333, DOI 10.1037/0022-0167.23.4.333
   GREEN EJ, 1981, PERCEPT PSYCHOPHYS, V30, P459, DOI 10.3758/BF03204842
   HALL JA, 1978, PSYCHOL BULL, V85, P845, DOI 10.1037/0033-2909.85.4.845
   HAMERS JF, 1972, J VERB LEARN VERB BE, V11, P303, DOI 10.1016/S0022-5371(72)80091-4
   HENDRICK C, 1972, J PERS SOC PSYCHOL, V22, P219, DOI 10.1037/h0032599
   Isbister K, 2000, INT J HUM-COMPUT ST, V53, P251, DOI 10.1006/ijhc.2000.0368
   Kiesler S, 1996, J PERS SOC PSYCHOL, V70, P47, DOI 10.1037/0022-3514.70.1.47
   Lee EJ, 2002, HUM COMMUN RES, V28, P349, DOI 10.1093/hcr/28.3.349
   Lee EJ, 2004, HUM COMMUN RES, V30, P234, DOI 10.1093/hcr/30.2.234
   MACLEOD CM, 1991, PSYCHOL BULL, V109, P163, DOI 10.1037/0033-2909.109.2.163
   MAES P, 1994, COMMUN ACM, V37, P31, DOI 10.1145/176789.176792
   MAZANEC N, 1976, J PSYCHOL, V93, P175, DOI 10.1080/00223980.1976.9915810
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Moon Y, 2000, J CONSUM RES, V26, P323, DOI 10.1086/209566
   Nass C, 2000, COMMUN ACM, V43, P36, DOI 10.1145/348941.348976
   Nass C, 2001, J EXP PSYCHOL-APPL, V7, P171, DOI 10.1037//1076-898X.7.3.171
   OLIVE JP, 1997, HALS LEGACY 2001S CO, P101
   Pomerantz J. R., 1986, HDB PERCEPTION HUMAN, V2, P36
   Rosenblum LD, 1997, PERCEPT PSYCHOPHYS, V59, P347, DOI 10.3758/BF03211902
   ROY L, 1990, J GENET PSYCHOL, V151, P515, DOI 10.1080/00221325.1990.9914636
   SCHUL Y, 1983, EUR J SOC PSYCHOL, V13, P143, DOI 10.1002/ejsp.2420130205
   Sproull L, 1996, HUM-COMPUT INTERACT, V11, P97, DOI 10.1207/s15327051hci1102_1
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   Tajfel H., 1986, PSYCHOL INTERGROUP R, P7, DOI DOI 10.4324/9780203505984-16
   VOLKMAR FR, 1979, J CHILD PSYCHOL PSYC, V20, P139, DOI 10.1111/j.1469-7610.1979.tb00494.x
   Wheeless L. R., 1978, Human Communication Research, V4, P143, DOI DOI 10.1111/J.1468-2958.1978.TB00604.X
   Wheeless Lawrence R, 1977, Human Communication Research, V3, P250, DOI [DOI 10.1111/J.1468-2958.1977.TB00523.X, 10.1111/j.1468-2958.1977.tb00523.x]
   ZAHN GL, 1973, J EXP SOC PSYCHOL, V9, P320, DOI 10.1016/0022-1031(73)90069-3
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 54
TC 75
Z9 90
U1 3
U2 32
PU OXFORD UNIV PRESS INC
PI CARY
PA JOURNALS DEPT, 2001 EVANS RD, CARY, NC 27513 USA
SN 0360-3989
EI 1468-2958
J9 HUM COMMUN RES
JI Hum. Commun. Res.
PD APR
PY 2007
VL 33
IS 2
BP 163
EP 193
DI 10.1111/j.1468-2958.2007.00295.x
PG 31
WC Communication
WE Social Science Citation Index (SSCI)
SC Communication
GA 145SP
UT WOS:000244885100003
DA 2024-01-09
ER

PT J
AU Meltzner, GS
   Hillman, RE
AF Meltzner, GS
   Hillman, RE
TI Impact of aberrant acoustic properties on the perception of sound
   quality in electrolarynx speech
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
DE electrolarynx speech; alaryngeal speech; laryngectomy speech
   rehabilitation; speech quality; artificial larynx speech
ID LARYNGECTOMEE REHABILITATION; COMMUNICATION; MODEL
AB A large percentage of patients who have undergone laryngectomy to treat advanced laryngeal cancer rely on an electrolarynx (EL) to communicate verba Although serviceable, EL speech is plagued by shortcomings in both sound quality This study sought to better quantify the relative contributions of and intelligibility. previously identified acoustic abnormalities to the perception of degraded quality in EL speech. Ten normal listeners evaluated the sound quality of EL speech tokens that had been acoustically enhanced by (a) increased low-frequency energy, (b) EL-noise reduction, and (c) fundamental frequency variation to mimic normal pitch intonation in relation to nonenhanced EL speech, normal speech, and normal monotonous speech (fundamental frequency variation removed). In comparing all possible combinations of token pairs, listeners were asked to identify which one of each pair sounded most like normal natural speech, and then to rate on a visual analog scale how different the chosen token was from normal speech. The results indicate that although EL speech can be most improved by removing the EL noise and providing proper pitch information, the resulting quality is still well below that of normal natural speech or even that of monotonous natural speech. This suggests that, in addition to the widely acknowledged acoustic abnormalities examined in this investigation, there are other attributes that contribute significantly to the unnatural quality of EL speech. Such additional factors need to be clearly identified and remedied before EL speech can be made to more closely approximate the sound quality of normal natural speech.
C1 MIT, Cambridge, MA 02139 USA.
   Massachusetts Gen Hosp, Boston, MA 02114 USA.
   Harvard Univ, Sch Med, Boston, MA 02115 USA.
C3 Massachusetts Institute of Technology (MIT); Harvard University;
   Massachusetts General Hospital; Harvard University; Harvard Medical
   School
RP Meltzner, GS (corresponding author), BAE Syst, 6 New England Execut Pk, Burlington, MA 01803 USA.
EM geoffrey.meltzner@baesystems.com
CR BARNEY HL, 1959, READINGS SPEECH FOLL, P1337
   COLE D, 1997, P IEEE TENCON 97 IEE, V2, P491
   CRYSTAL TH, 1982, J ACOUST SOC AM, V72, P705, DOI 10.1121/1.388251
   DIEDRICH W, 1977, ALARYNGEAL SPEECH
   Edwards A. L., 1983, Techniques of Attitude Scale Construction
   Espy-Wilson CY, 1998, J SPEECH LANG HEAR R, V41, P1253, DOI 10.1044/jslhr.4106.1253
   GATES GA, 1982, AM J OTOLARYNG, V3, P1, DOI 10.1016/S0196-0709(82)80025-2
   GATES GA, 1982, AM J OTOLARYNG, V3, P8, DOI 10.1016/S0196-0709(82)80026-4
   GRAY S, 1976, ARCH PHYS MED REHAB, V57, P140
   Harris R. J., 2001, A primer of multivariate statistics
   HEATON J, 2004, ANN OTOLOGY RHINOLOG, V109, P972
   Hillman R E, 1998, Ann Otol Rhinol Laryngol Suppl, V172, P1
   HOUSE AS, 1958, J SPEECH HEAR RES, V1, P309, DOI 10.1044/jshr.0104.309
   KING PS, 1968, AMER J PHYSICAL MED, V47, P192
   KOMMERS MS, 1979, J COMMUN DISORD, V12, P411, DOI 10.1016/0021-9924(79)90005-4
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   KRUS DJ, 1977, EDUC PSYCHOL MEAS, V37, P189, DOI 10.1177/001316447703700119
   Ma K., 1999, P EUR C SPEECH COMM, P323
   MCCREE AV, 1995, IEEE T SPEECH AUDI P, V3, P242, DOI 10.1109/89.397089
   Meltzner G., 2005, CONT CONSIDERATIONS
   Meltzner GS, 2003, J ACOUST SOC AM, V114, P1035, DOI 10.1121/1.1582440
   MELTZNER GS, 2003, DISS ABSTR INT, V64, P4486
   Mitchell HL, 1996, J SPEECH HEAR RES, V39, P93, DOI 10.1044/jshr.3901.93
   MORRIS HL, 1992, ANN OTO RHINOL LARYN, V101, P503, DOI 10.1177/000348949210100611
   MOSTELLER F, 1951, PSYCHOMETRIKA, V16, P207
   MOULINES E, 1990, SPEECH COMMUN, V9, P453, DOI 10.1016/0167-6393(90)90021-Z
   Poulton E. C., 1989, BIAS QUANTIFYING JUD
   QI YY, 1991, J SPEECH HEAR RES, V34, P1250, DOI 10.1044/jshr.3406.1250
   RICHARDSON J L, 1985, Journal of Psychosocial Oncology, V3, P83
   SISTY NL, 1972, J SPEECH HEAR RES, V15, P439, DOI 10.1044/jshr.1502.439
   Stevens K., 1998, Acoustic phonetics
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   TORGERSON WS, 1957, THEORY METHODS SCALI
   UEMI N, 1994, IEEE WORKSH ROB HUM, P198
   *US DEP DEF, 1999, MILSTD3005 US DEP DE
   WEBSTER PM, 1990, ANN OTO RHINOL LARYN, V99, P197
   WEISS MS, 1979, J ACOUST SOC AM, V65, P1298, DOI 10.1121/1.382697
   WEISS MS, 1985, J SPEECH HEAR RES, V28, P294, DOI 10.1044/jshr.2802.294
   [No title captured]
NR 39
TC 40
Z9 47
U1 0
U2 6
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE, ROCKVILLE, MD 20852-3279 USA
SN 1092-4388
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD AUG
PY 2005
VL 48
IS 4
BP 766
EP 779
DI 10.1044/1092-4388(2005/053)
PG 14
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA 993SI
UT WOS:000233974900005
PM 16378472
DA 2024-01-09
ER

PT J
AU Coughlin-Woods, S
   Lehman, ME
   Cooke, PA
AF Coughlin-Woods, S
   Lehman, ME
   Cooke, PA
TI Ratings of speech naturalness of children ages 8-16 years
SO PERCEPTUAL AND MOTOR SKILLS
LA English
DT Article
ID TREATMENT EFFICACY; STUTTERERS; ADULTS; NONSTUTTERERS; THERAPY
AB The focus of this cross-sectional study was the investigation of Speech Naturalness (speech that Sounds normal or natural to the listener) of 60 normal speaking children and adolescents between the ages of 8 and 16 years. 26 naive adult listeners rated the naturalness of videotaped and computer-presented speech samples, using a 9-point-Likert rating scale (1: highly natural sounding speech and 9: highly unnatural sounding speech). The children and adolescents who participated as speakers were distributed among 5 age groups (8, 10, 12, 14, and 16 yr.) with 6 boys and 6 girls in each group. Each child demonstrated normal articulation, language, voice, and speech fluency skills. Age and sex comparisons indicated boys' and girls' speech was rated comparably; however, 8-yr.-olds' speech was rated as significantly less natural than those of 12-, 14-, and 16-yr.-olds. Preliminary ratings of Speech Naturalness for normal speaking children were presented. Suggestions for the clinical application of the findings as a target criterion in treatment programs with communicatively impaired children were suggested. Replication with a larger and more representative sample is in order.
C1 Cent Michigan Univ, Mt Pleasant, MI 48859 USA.
   Michigan State Univ, E Lansing, MI 48824 USA.
C3 Central Michigan University; Michigan State University
RP Coughlin-Woods, S (corresponding author), Cent Michigan Univ, 2171 Hlth Prof Bldg, Mt Pleasant, MI 48859 USA.
EM Coughlss@cmich.edu
RI Cooke, Philip/GRO-1618-2022
CR CURLEE RF, 1993, J FLUENCY DISORD, V18, P319, DOI 10.1016/0094-730X(93)90012-S
   Dawson LO, 1929, ELEM SCHOOL J, V29, P610, DOI 10.1086/456299
   Dayalu VN, 2002, PERCEPT MOTOR SKILL, V94, P87, DOI 10.2466/PMS.94.1.87-96
   Finn P, 1997, J SPEECH LANG HEAR R, V40, P821, DOI 10.1044/jslhr.4004.821
   Ingham JC, 1998, J SPEECH LANG HEAR R, V41, P753, DOI 10.1044/jslhr.4104.753
   Ingham RJ, 2001, J SPEECH LANG HEAR R, V44, P841, DOI 10.1044/1092-4388(2001/066)
   INGHAM RJ, 1985, J SPEECH HEAR RES, V28, P495, DOI 10.1044/jshr.2804.495
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P261, DOI 10.1044/jshd.5003.261
   JOHNSON L, 1987, THESIS U MINNESOTA
   KALINOWSKI J, 1994, AM J SPEECH-LANG PAT, V3, P61
   KOWAL S, 1975, J PSYCHOLINGUIST RES, V4, P195, DOI 10.1007/BF01066926
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   LOVE LR, 1971, J SPEECH HEAR RES, V14, P229, DOI 10.1044/jshr.1402.229
   MARTIN RR, 1992, J SPEECH HEAR RES, V35, P521, DOI 10.1044/jshr.3503.521
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   NICHOLS AC, 1966, SPEECH MONOGR, V33, P156
   Onslow M, 1996, J SPEECH HEAR RES, V39, P734, DOI 10.1044/jshr.3904.734
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P274, DOI 10.1044/jshr.3502.274
   Parrish WM, 1951, Q J SPEECH, V37, P448, DOI 10.1080/00335635109381697
   Ratcliff A., 2002, AUGMENT ALTERN COMM, V18, P11, DOI DOI 10.1080/714043393
   RUNYAN CM, 1979, J FLUENCY DISORD, V4, P29, DOI 10.1016/0094-730X(79)90029-9
   STARKWEATHER CW, 1985, STUTTERING THERAPY P, V20, P67
   VANRIPER C, 1990, SPEECH CORRECTION
NR 24
TC 6
Z9 7
U1 1
U2 1
PU SAGE PUBLICATIONS INC
PI THOUSAND OAKS
PA 2455 TELLER RD, THOUSAND OAKS, CA 91320 USA
SN 0031-5125
EI 1558-688X
J9 PERCEPT MOTOR SKILL
JI Percept. Mot. Skills
PD APR
PY 2005
VL 100
IS 2
BP 295
EP 304
PG 10
WC Psychology, Experimental
WE Social Science Citation Index (SSCI)
SC Psychology
GA 924UG
UT WOS:000229005400003
PM 15974337
DA 2024-01-09
ER

PT J
AU Moore, BCJ
   Tan, CT
AF Moore, BCJ
   Tan, CT
TI Perceived naturalness of spectrally distorted speech and music
SO JOURNAL OF THE ACOUSTICAL SOCIETY OF AMERICA
LA English
DT Article
ID DIFFERENT FREQUENCY RESPONSES; SOUND QUALITY; LOUDSPEAKER MEASUREMENTS;
   LISTENER PREFERENCES; FIELD; EAR; TRANSFORMATION
AB We determined how the,perceived naturalness of music and speech (male and female talkers) signals was affected by various forms of linear filtering, some of which were intended to mimic the spectral "distortions" introduced by transducers such as microphones, loudspeakers, and earphones. The filters introduced spectral tilts and ripples of various types, variations in upper and lower cutoff frequency, and combinations of these. All of the differently filtered signals (168 conditions) were intermixed in random order within one block of trials. Levels were adjusted to give approximately equal loudness in all conditions. Listeners were required to judge the perceptual quality (naturalness) of the filtered signals on a scale from 1 to 10. For spectral ripples, perceived quality decreased with increasing ripple density up to 0.2 ripple/ERBN and with increasing ripple depth. Spectral tilts also. degraded quality, and the effects were similar for positive and negative tilts. Ripples and/or tilts degraded quality more when they extended over a wide frequency range (87-6981 Hz) than when they extended over subranges. Low- and mid-frequency ranges were roughly equally important for music, but the mid-range was most important for speech. For music, the highest quality was obtained for the broadband signal (55-16854 Hz). Increasing the lower cutoff frequency from 55 Hz resulted in a clear degradation of quality. There was also a distinct degradation as the upper cutoff frequency was decreased from 16 845 Hz. For speech, there was a marked degradation when the lower cutoff frequency was increased from 123 to 208 Hz and when the upper cutoff frequency was decreased from 10 869 Hz. Typical telephone bandwidth (313 to 3547 Hz) gave very poor quality. (C) 2003 Acoustical Society of America.
C1 Univ Cambridge, Dept Expt Psychol, Cambridge CB2 3EB, England.
C3 University of Cambridge
RP Moore, BCJ (corresponding author), Univ Cambridge, Dept Expt Psychol, Downing St, Cambridge CB2 3EB, England.
RI Tan, Chin-Tuan/AAA-9441-2021; Moore, Brian/I-5541-2012
OI Tan, Chin-Tuan/0000-0002-4676-4917; Moore, Brian/0000-0001-7071-0671
CR Aibara R, 2001, HEARING RES, V152, P100, DOI 10.1016/S0378-5955(00)00240-9
   ALLEN JB, 1977, IEEE T ACOUST SPEECH, V25, P235, DOI 10.1109/TASSP.1977.1162950
   Bech S, 2002, J AUDIO ENG SOC, V50, P564
   Bucklein R., 1962, FREQUENZ, V16/1962, P103
   BURKHARD MD, 1975, J ACOUST SOC AM, V58, P214, DOI 10.1121/1.380648
   Cochran W.G., 1967, STAT METHODS, V6th ed.
   FRYER PA, 1977, HI FI NEWS REC, V22, P51
   GABRIELSSON A, 1988, J SPEECH HEAR RES, V31, P166, DOI 10.1044/jshr.3102.166
   GABRIELSSON A, 1991, J ACOUST SOC AM, V90, P707, DOI 10.1121/1.401941
   GABRIELSSON A, 1990, J ACOUST SOC AM, V88, P1359, DOI 10.1121/1.399713
   GABRIELSSON A, 1976, TA83 KAROLINSKA I TE, P1
   GLASBERG BR, 1990, HEARING RES, V47, P103, DOI 10.1016/0378-5955(90)90170-T
   Gockel H, 1997, J ACOUST SOC AM, V102, P2311, DOI 10.1121/1.419640
   GONTCHAROV VP, 1999, AES 106 CONV MUN
   Green D. M., 1988, PROFILE ANAL
   Jacobs J. E., 1964, J. Audio Eng. Soc., V12, P115
   KILLION MC, 1987, J ACOUST SOC AM, V81, pS75, DOI 10.1121/1.2024388
   KUHN GF, 1979, J ACOUST SOC AM, V65, P991, DOI 10.1121/1.382606
   LETOWSKI T, 1975, ACUSTICA, V34, P106
   Moore B. C. J., 2013, An introduction to the psychology of hearing, V6th
   Moore BCJ, 1997, J AUDIO ENG SOC, V45, P224
   MOORE BCJ, 1983, J ACOUST SOC AM, V74, P750, DOI 10.1121/1.389861
   POULTON EC, 1979, PSYCHOL BULL, V86, P777, DOI 10.1037/0033-2909.86.4.777
   Puria S, 1997, J ACOUST SOC AM, V101, P2754, DOI 10.1121/1.418563
   SHAW EAG, 1974, J ACOUST SOC AM, V56, P1848, DOI 10.1121/1.1903522
   TOOLE FE, 1986, J AUDIO ENG SOC, V34, P227
   TOOLE FE, 1988, J AUDIO ENG SOC, V36, P122
   TOOLE FE, 1986, J AUDIO ENG SOC, V34, P323
   ZWICKER E, 1980, J ACOUST SOC AM, V68, P1523, DOI 10.1121/1.385079
NR 29
TC 123
Z9 163
U1 0
U2 9
PU ACOUSTICAL SOC AMER AMER INST PHYSICS
PI MELVILLE
PA STE 1 NO 1, 2 HUNTINGTON QUADRANGLE, MELVILLE, NY 11747-4502 USA
SN 0001-4966
J9 J ACOUST SOC AM
JI J. Acoust. Soc. Am.
PD JUL
PY 2003
VL 114
IS 1
BP 408
EP 419
DI 10.1121/1.1577552
PG 12
WC Acoustics; Audiology & Speech-Language Pathology
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Acoustics; Audiology & Speech-Language Pathology
GA 699ZF
UT WOS:000184087100039
PM 12880052
DA 2024-01-09
ER

PT J
AU Eadie, TL
   Doyle, PC
AF Eadie, TL
   Doyle, PC
TI Direct magnitude estimation and interval scaling of naturalness and
   severity in tracheoesophageal (TE) speakers
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
DE tracheoesophageal speech; perceptual scaling; alaryngeal speech;
   naturalness; severity
ID PERCEPTUAL EVALUATION; SPEECH NATURALNESS; LARYNGEAL-CANCER; VOICE
   QUALITY
AB The purpose of this study was to determine the Psychophysical character and validity of auditory-perceptual ratings of naturalness and overall severity for tracheoesophageal (TE) speech. This was achieved through use of direct magnitude estimation (DME) and equal-appearing interval (EAI) scaling procedures. I Twenty adult listeners judged speech naturalness and overall severity from connected speech samples produced by 20 adult male TE speakers. A comparison of DME- and EAI-scaled judgments yielded a metathetic continuum for naturalness and a prothetic continuum for overall severity. These data provide support for the use of either DME or EAI scales in auditory-perceptual ratings of naturalness, but they provide support only for DME scales in judging overall severity for TE speech. The present results suggest that the nature of perceptual phenomena (prothetic vs. metathetic) for TE speakers is consistent with findings for the same dimensions produced by normal laryngeal speakers. These data also support a need for further study of perceptual dimensions associated with TE voice and speech in order to avoid the inappropriate and invalid use of EAI scales frequently found in diagnosis, assessment, and evaluation of this clinical population.
C1 Univ Western Ontario, Sch Commun Sci & Disorders, Voice Prod Lab, Elborn Coll, London, ON N6G 1H1, Canada.
C3 Western University (University of Western Ontario)
RP Eadie, TL (corresponding author), Univ Western Ontario, Sch Commun Sci & Disorders, Voice Prod Lab, Elborn Coll, London, ON N6G 1H1, Canada.
OI Eadie, Tanya/0000-0002-7697-1298
CR [Anonymous], 1951, Psychometrika
   BARRY SJ, 1981, J SPEECH HEAR RES, V24, P44, DOI 10.1044/jshr.2401.44
   BLOM ED, 1986, ARCH OTOLARYNGOL, V112, P440
   BLOM ED, 1998, TRACHEOESOPHAGEAL VO
   Doyle P. C., 1994, Foundations of Voice and Speech Rehabilitation Following Laryngeal Cancer
   DOYLE PC, IN PRESS CONT CONSID
   Fairbanks G, 1960, Voice and articulation drillbook, V2nd
   Finizia C, 1999, ARCH OTOLARYNGOL, V125, P157, DOI 10.1001/archotol.125.2.157
   GERRATT BR, 1991, DYSARTHRIA AND APRAXIA OF SPEECH, P77
   GESCHEIDER GA, 1988, ANNU REV PSYCHOL, V39, P169, DOI 10.1146/annurev.ps.39.020188.001125
   Hillman RE, 1998, ANN OTO RHINOL LARYN, V107, P2
   KEITH RL, 1994, LARYNGECTOMEE REHAB
   KEITH RL, 1986, LARYNGECTOMEE REHAB
   Kent R. D., 1996, AM J SPEECH-LANG PAT, V5, P7, DOI [DOI 10.1044/1058-0360.0503.07, 10.1044/1058-0360.0503.07]
   KREIMAN J, 1993, J SPEECH HEAR RES, V36, P21, DOI 10.1044/jshr.3601.21
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   MOON JB, 1987, J SPEECH HEAR RES, V30, P387, DOI 10.1044/jshr.3003.387
   PINDZOLA RH, 1988, LARYNGOSCOPE, V98, P394
   PINDZOLA RH, 1989, ANN OTO RHINOL LARYN, V98, P960, DOI 10.1177/000348948909801208
   ROBBINS J, 1984, J SPEECH HEAR DISORD, V49, P202, DOI 10.1044/jshd.4902.202
   SCHIAVETTI N, 1983, J SPEECH HEAR RES, V26, P568, DOI 10.1044/jshr.2604.568
   SCHIAVETTI N, 1981, J SPEECH HEAR RES, V24, P441, DOI 10.1044/jshr.2403.441
   SCHIAVETTI N, 1984, ARTICULATION ASSESSM, P237
   Sewall A, 1999, CONT ISSUES COMM SCI, V26, P168, DOI [10.1044/cicsd_26_F_168., DOI 10.1044/CICSD_26_F_168]
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   SINGER MI, 1980, ANN OTO RHINOL LARYN, V89, P529, DOI 10.1177/000348948008900608
   SNIDECOR JC, 1978, SPEECH REAHB LARYNGE
   Stevens SS., 1975, PSYCHOPHYSICS INTRO
   TARDYMITZELL S, 1985, ARCH OTOLARYNGOL, V111, P212
   TONER MA, 1989, J SPEECH HEAR RES, V32, P78, DOI 10.1044/jshr.3201.78
   TRUDEAU MD, 1990, J SPEECH HEAR DISORD, V55, P244, DOI 10.1044/jshd.5502.244
   van As CJ, 1998, J VOICE, V12, P239, DOI 10.1016/S0892-1997(98)80044-1
   Van Riper C., 1978, SPEECH CORRECTION PR
   Whitehill TL, 2002, J SPEECH LANG HEAR R, V45, P80, DOI 10.1044/1092-4388(2002/006)
   WILLIAMS SE, 1985, ARCH OTOLARYNGOL, V111, P216
   Wuyts FL, 2000, J SPEECH LANG HEAR R, V43, P796, DOI 10.1044/jslhr.4303.796
   Yorkston K. M., 1988, Clinical management of dysarthric speakers
   Zraick RI, 2000, J SPEECH LANG HEAR R, V43, P979, DOI 10.1044/jslhr.4304.979
   [No title captured]
NR 40
TC 32
Z9 41
U1 0
U2 2
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE, ROCKVILLE, MD 20852-3279 USA
SN 1092-4388
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD DEC
PY 2002
VL 45
IS 6
BP 1088
EP 1096
DI 10.1044/1092-4388(2002/087)
PG 9
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA 631KM
UT WOS:000180167000002
PM 12546479
DA 2024-01-09
ER

PT J
AU Alku, P
   Tiitinen, H
   Näätänen, R
AF Alku, P
   Tiitinen, H
   Näätänen, R
TI A method for generating natural-sounding speech stimuli for cognitive
   brain research
SO CLINICAL NEUROPHYSIOLOGY
LA English
DT Article
DE speech production; inverse filtering; speech synthesis; speech
   perception; auditory discrimination; mismatch negativity
ID PHONEME REPRESENTATIONS; MISMATCH NEGATIVITY; RESPONSES
AB Objective: In response to the rapidly increasing interest in using human voice in cognitive brain research, a new method, semisynthetic speech generation (SSG), is presented for generation of speech stimuli.
   Methods: The method synthesizes speech stimuli as a combination of purely artificial processes and processes that originate from the natural human speech production mechanism. SSG first estimates the source of speech, the glottal flow, from a natural utterance using an inverse filtering technique. The glottal flow obtained is then used as an excitation to an artificial digital filter that models the formant structure of speech.
   Results: SSG is superior to commercial voice synthesizers because it yields speech stimuli of a highly natural quality due to the contribution of the man-originating glottal excitation.
   Conclusion: The artificial modelling of the vocal tract enables one to adjust the formant frequencies of the stimuli as desired, thus making SSG suitable for cognitive experiments using speech sounds as stimuli. (C) 1999 Elsevier Science Ireland Ltd. All rights reserved.
C1 Aalto Univ, Acoust Lab, FIN-02015 Helsinki, Finland.
   Univ Helsinki, Dept Psychol, Cognit Brain Res Unit, Helsinki, Finland.
C3 Aalto University; University of Helsinki
RP Alku, P (corresponding author), Aalto Univ, Acoust Lab, POB 3000, FIN-02015 Helsinki, Finland.
EM paavo.alku@hut.fi
RI Alku, Paavo/E-2400-2012
OI Alku, Paavo/0000-0002-8173-9418
CR ALKU P, 1992, SPEECH COMMUN, V11, P109, DOI 10.1016/0167-6393(92)90005-R
   Cheour M, 1998, NAT NEUROSCI, V1, P351, DOI 10.1038/1561
   Fant G., 1971, Acoustic Theory of Speech Production: With Calculations Based on X-ray Studies of Russian Articulations, DOI DOI 10.1515/9783110873429
   Flanagan J. L., 1972, Speech Analysis, Synthesis, and Perception
   GOLD B, 1968, IEEE T ACOUST SPEECH, VAU16, P81, DOI 10.1109/TAU.1968.1161954
   Karjalainen M., 1990, IEEE ASSP Magazine, V7, P21, DOI 10.1109/53.53030
   KRAUS N, 1993, ELECTROEN CLIN NEURO, V88, P123, DOI 10.1016/0168-5597(93)90063-U
   KURIKI S, 1995, EXP BRAIN RES, V104, P144
   Maisch B, 1995, EUR HEART J, V16, P1
   Naatanen R, 1997, NATURE, V385, P432, DOI 10.1038/385432a0
   Naatanen R., 1992, Attention and brain function, DOI DOI 10.4324/9780429487354
   Rabiner L.R., 1978, DIGITAL PROCESSING S
   ROGERS RL, 1990, ELECTROEN CLIN NEURO, V77, P237, DOI 10.1016/0168-5597(90)90043-D
   Sams M, 1990, J Cogn Neurosci, V2, P344, DOI 10.1162/jocn.1990.2.4.344
   SCHROGER E, 1994, ELECTROEN CLIN NEURO, V92, P140, DOI 10.1016/0168-5597(94)90054-X
   Sharma A, 1997, EVOKED POTENTIAL, V104, P540, DOI 10.1016/S0168-5597(97)00050-6
   Shtyrov Y, 1998, NEUROSCI LETT, V251, P141, DOI 10.1016/S0304-3940(98)00529-1
   TIITINEN H, 1994, NATURE, V372, P90, DOI 10.1038/372090a0
   WONG DY, 1979, IEEE T ACOUST SPEECH, V27, P350, DOI 10.1109/TASSP.1979.1163260
   ZATORRE RJ, 1992, SCIENCE, V256, P846, DOI 10.1126/science.1589767
NR 20
TC 113
Z9 115
U1 0
U2 1
PU ELSEVIER IRELAND LTD
PI CLARE
PA ELSEVIER HOUSE, BROOKVALE PLAZA, EAST PARK SHANNON, CO, CLARE, 00000,
   IRELAND
SN 1388-2457
EI 1872-8952
J9 CLIN NEUROPHYSIOL
JI Clin. Neurophysiol.
PD AUG
PY 1999
VL 110
IS 8
BP 1329
EP 1333
DI 10.1016/S1388-2457(99)00088-7
PG 5
WC Clinical Neurology; Neurosciences
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Neurosciences & Neurology
GA 220UN
UT WOS:000081686000002
PM 10454267
DA 2024-01-09
ER

PT J
AU Mackey, LS
   Finn, P
   Ingham, RJ
AF Mackey, LS
   Finn, P
   Ingham, RJ
TI Effect of speech dialect on speech naturalness ratings: A systematic
   replication of Martin, Haroldson, and Triden (1984)
SO JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH
LA English
DT Article
DE speech naturalness; stuttering; reliability; multicultural; assessment
ID AUDIOVISUAL JUDGMENTS; LANGUAGE ATTITUDES; SYLLABLE STRUCTURE;
   FOREIGN-LANGUAGE; ENGLISH; SPEAKERS; STUTTERERS; ACCENT; NONSTUTTERERS;
   RELIABILITY
AB This study investigated the effect of speech dialect on listeners' speech naturalness ratings by systematically replicating Martin, Haroldson, and Triden's (1984) study using three groups of speaker samples. Two groups consisted of speakers with General American dialect-one with persons who stutter and the other with persons who do not stutter. The third group also consisted of speakers who do not stutter but who spoke non-General American dialect. The results showed that speech naturalness ratings distinguished among the three speaker groups. The variables that appeared to influence speech naturalness ratings were type of dialect, speech fluency, and speaking rate, though they differed across speaker groups. The findings also suggested that strength of speech dialect may be a scaleable dimension that judges can rare with acceptable levels of reliability Dialect may also be an important factor that needs to be incorporated or controlled within systems designed to train speech naturalness ratings. It may also be an important factor in determining the extent to which stuttering treatment produces natural sounding speech.
C1 UNIV NEW MEXICO,DEPT COMMUN DISORDERS,ALBUQUERQUE,NM 87131.
   CLAREMORE REG HOSP,HOME HEALTH,OK.
   UNIV CALIF SANTA BARBARA,SANTA BARBARA,CA 93106.
C3 University of New Mexico; University of California System; University of
   California Santa Barbara
FU NIDCD NIH HHS [5 R01 DC 00060-05] Funding Source: Medline
CR ANDERSONHSIEH J, 1992, LANG LEARN, V42, P529, DOI 10.1111/j.1467-1770.1992.tb01043.x
   [Anonymous], 1986, Webster's third new international dictionary of the English language unabridged
   BATTLE DE, 1993, COMMUNICATION DISORD, pR15
   Bloodstein O., 1995, HDB STUTTERING
   Bouchard Ryan E., 1982, Journal of Language and Social Psychology, V1, P51, DOI DOI 10.1177/0261927X8200100104
   Bradac J. J., 1984, Journal of Language and Social Psychology, V3, P239, DOI [DOI 10.1177/0261927X8400300401, https://doi.org/10.1177/0261927X8400300401]
   BRENNAN EM, 1981, J PSYCHOLINGUIST RES, V10, P487, DOI 10.1007/BF01076735
   BROWN BL, 1985, LANG COMMUN, V5, P207, DOI 10.1016/0271-5309(85)90011-4
   CARGILE AC, 1994, LANG COMMUN, V14, P211, DOI 10.1016/0271-5309(94)90001-9
   Cooper E. B., 1993, COMMUNICATION DISORD, P189
   CORDES AK, 1994, J SPEECH HEAR RES, V37, P264, DOI 10.1044/jshr.3702.264
   CROWTHER CS, 1992, J ACOUST SOC AM, V92, P711, DOI 10.1121/1.403996
   Dictionary O.E, 1989, Oxford English dictionary
   EDWARDS JR, 1989, LANAGUGE DISADVANTAG
   FAYER JM, 1987, LANG LEARN, V37, P313, DOI 10.1111/j.1467-1770.1987.tb00573.x
   FINN P, 1994, J SPEECH HEAR RES, V37, P326, DOI 10.1044/jshr.3702.326
   FLEGE JE, 1987, J PHONETICS, V15, P47, DOI 10.1016/S0095-4470(19)30537-6
   FLEGE JE, 1984, J ACOUST SOC AM, V76, P708, DOI 10.1121/1.391257
   Flege JE., 1989, STUDIES 2 LANGUAGE A, V11, P35, DOI DOI 10.1017/S0272263100007828
   Gallois C., 1989, AUST J LINGUIST, V9, P149, DOI DOI 10.1080/07268608908599415
   Gardner R. C., 1985, Social psychology and second language learning: The role of attitudes and motivation
   GERRATT BR, 1993, J SPEECH HEAR RES, V36, P14, DOI 10.1044/jshr.3601.14
   GILES H, 1983, LANG COMMUN, V3, P305, DOI 10.1016/0271-5309(83)90006-X
   GILES H, 1995, LANG COMMUN, V15, P107, DOI 10.1016/0271-5309(94)00019-9
   Giles H., 1987, SOCIOLINGUISTICS INT, P585
   GOW ML, 1992, J SPEECH HEAR RES, V35, P495, DOI 10.1044/jshr.3503.495
   INGHAM JM, 1986, STUTTERING MEASUREME
   INGHAM RJ, 1985, J SPEECH HEAR RES, V28, P495, DOI 10.1044/jshr.2804.495
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P217, DOI 10.1044/jshd.5002.217
   INGHAM RJ, 1985, J SPEECH HEAR DISORD, V50, P261, DOI 10.1044/jshd.5003.261
   JUFFS A, 1990, IRAL-INT REV APPL LI, V28, P99, DOI 10.1515/iral.1990.28.2.99
   KALIN R, 1978, PSYCHOL REP, V43, P1203, DOI 10.2466/pr0.1978.43.3f.1203
   KALINOWSKI J, 1994, AM J SPEECH-LANG PAT, V3, P61
   Kent R. D., 1996, AM J SPEECH-LANG PAT, V5, P7, DOI [DOI 10.1044/1058-0360.0503.07, 10.1044/1058-0360.0503.07]
   KOSTER CJ, 1993, LANG LEARN, V43, P69, DOI 10.1111/j.1467-1770.1993.tb00173.x
   Leith W. R, 1986, ATYPICAL STUTTERER, P9
   LIPPIGREEN R, 1994, LANG SOC, V23, P163, DOI 10.1017/S0047404500017826
   LUDWIG J, 1982, MOD LANG J, V66, P274, DOI 10.2307/326629
   LUHMAN R, 1990, LANG SOC, V19, P331, DOI 10.1017/S0047404500014548
   Martin R., 1981, MAINTENANCE FLUENCY, P1
   MARTIN RR, 1992, J SPEECH HEAR RES, V35, P521, DOI 10.1044/jshr.3503.521
   MARTIN RR, 1984, J SPEECH HEAR DISORD, V49, P53, DOI 10.1044/jshd.4901.53
   MARTIN RR, 1981, J SPEECH HEAR RES, V46, P59
   METZ DE, 1990, J SPEECH HEAR DISORD, V55, P516, DOI 10.1044/jshd.5503.516
   MUNRO MJ, 1993, LANG SPEECH, V36, P39, DOI 10.1177/002383099303600103
   Munro MJ., 1994, Language Testing, V11, P253
   NESDALE AR, 1990, AUST J PSYCHOL, V42, P309, DOI 10.1080/00049539008260128
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P994, DOI 10.1044/jshr.3505.994
   ONSLOW M, 1987, J SPEECH HEAR DISORD, V52, P2, DOI 10.1044/jshd.5201.02
   Onslow M, 1996, J SPEECH HEAR RES, V39, P734, DOI 10.1044/jshr.3904.734
   ONSLOW M, 1992, J SPEECH HEAR RES, V35, P274, DOI 10.1044/jshr.3502.274
   PITTAM J, 1987, LANG SPEECH, V30, P99, DOI 10.1177/002383098703000201
   PORT RF, 1983, J PHONETICS, V11, P219, DOI 10.1016/S0095-4470(19)30823-X
   Ryan E.B., 1982, Attitudes Towards Language Variation: Social and Applied Contexts
   RYAN EB, 1975, J PERS SOC PSYCHOL, V31, P855, DOI 10.1037/h0076704
   SCHIAVETTI N, 1994, J SPEECH HEAR RES, V37, P46, DOI 10.1044/jshr.3701.46
   SCHIAVETTI N, 1997, NATURE TREATMENT STU, P398
   SHAMES GH, 1989, J FLUENCY DISORD, V14, P67, DOI 10.1016/0094-730X(89)90025-9
   Siegel S., 1956, NONPARAMETRIC STAT B
   TAYLOR O, 1993, COMMUNICATION DISORD, pR12
   Taylor O. L., 1973, LANGUAGE ATTITUDES C, P174
   Van Riper C., 1982, NATURE STUTTERING
   Watson Jennifer Barber, 1994, Seminars in Speech and Language, V15, P149, DOI 10.1055/s-2008-1064140
   Winer BJ, 1991, STATISTICAL PRINCIPL
   YOUNG MA, 1994, J SPEECH HEAR RES, V37, P522, DOI 10.1044/jshr.3703.522
NR 65
TC 20
Z9 25
U1 1
U2 7
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE, ROCKVILLE, MD 20852-3279
SN 1092-4388
J9 J SPEECH LANG HEAR R
JI J. Speech Lang. Hear. Res.
PD APR
PY 1997
VL 40
IS 2
BP 349
EP 360
DI 10.1044/jslhr.4002.349
PG 12
WC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
WE Social Science Citation Index (SSCI)
SC Audiology & Speech-Language Pathology; Linguistics; Rehabilitation
GA WV591
UT WOS:A1997WV59100008
PM 9130203
DA 2024-01-09
ER

PT J
AU BURTON, MW
   BLUMSTEIN, SE
AF BURTON, MW
   BLUMSTEIN, SE
TI LEXICAL EFFECTS ON PHONETIC CATEGORIZATION - THE ROLE OF STIMULUS
   NATURALNESS AND STIMULUS QUALITY
SO JOURNAL OF EXPERIMENTAL PSYCHOLOGY-HUMAN PERCEPTION AND PERFORMANCE
LA English
DT Article
ID PERCEPTION; IDENTIFICATION; SPEECH
AB A series of experiments was conducted to determine whether the effects of lexical status on phonetic categorization were influenced by stimulus naturalness (replicating M. W. Burton, S. R. Baum, & S. E. Blumstein, 1989, who manipulated the intrinsic properties of the stimuli) and by stimulus quality (presenting the stimuli in white noise). The experiments compared continua varying in voice onset time (VOT) only to continua covarying VOT and amplitude of the burst and aspiration noise in no-noise and noise conditions. Results overall showed that the emergence of a lexical effect was influenced by stimulus quality but not by stimulus naturalness. Contrary to previous findings, significant lexical effects failed to emerge in the slower reaction time ranges. These results suggest that stimulus quality contributes to lexical effects on phonetic categorization, whereas stimulus naturalness does not.
C1 BROWN UNIV,DEPT COGNIT & LINGUIST SCI,PROVIDENCE,RI.
C3 Brown University
RP BURTON, MW (corresponding author), PENN STATE UNIV,DEPT PSYCHOL,415 MOORE BLDG,UNIVERSITY PK,PA 16802, USA.
FU NIDCD NIH HHS [DC00142] Funding Source: Medline
CR BURTON MW, 1989, J EXP PSYCHOL HUMAN, V15, P567, DOI 10.1037/0096-1523.15.3.567
   CONNINE CM, 1987, J EXP PSYCHOL HUMAN, V13, P291, DOI 10.1037/0096-1523.13.2.291
   CUTLER A, 1987, COGNITIVE PSYCHOL, V19, P141, DOI 10.1016/0010-0285(87)90010-7
   Cutler A., 1979, SENTENCE PROCESSING, P113
   Elman J., 1986, Parallel distributed processing: Explorations in the microstructure of cognition, VII, P58
   Elman JeffreyL., 1986, INVARIANCE VARIABILI, P360
   FOX RA, 1984, J EXP PSYCHOL HUMAN, V10, P526, DOI 10.1037/0096-1523.10.4.526
   GANONG WF, 1980, J EXP PSYCHOL HUMAN, V6, P110, DOI 10.1037/0096-1523.6.1.110
   MCQUEEN JM, 1991, J EXP PSYCHOL HUMAN, V17, P433, DOI 10.1037/0096-1523.17.2.433
   MCQUEEN JM, 1989, EUROSPEECH 89, V2, P581
   MILLER JL, 1988, J EXP PSYCHOL HUMAN, V14, P369, DOI 10.1037/0096-1523.14.3.369
   PITT MA, 1993, J EXP PSYCHOL HUMAN, V19, P699, DOI 10.1037/0096-1523.19.4.699
NR 12
TC 16
Z9 16
U1 0
U2 0
PU AMER PSYCHOLOGICAL ASSOC
PI WASHINGTON
PA 750 FIRST ST NE, WASHINGTON, DC 20002-4242
SN 0096-1523
J9 J EXP PSYCHOL HUMAN
JI J. Exp. Psychol.-Hum. Percept. Perform.
PD OCT
PY 1995
VL 21
IS 5
BP 1230
EP 1235
DI 10.1037/0096-1523.21.5.1230
PG 6
WC Psychology; Psychology, Experimental
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Psychology
GA RY271
UT WOS:A1995RY27100018
PM 7595247
DA 2024-01-09
ER

PT J
AU YORKSTON, KM
   HAMMEN, VL
   BEUKELMAN, DR
   TRAYNOR, CD
AF YORKSTON, KM
   HAMMEN, VL
   BEUKELMAN, DR
   TRAYNOR, CD
TI THE EFFECT OF RATE CONTROL ON THE INTELLIGIBILITY AND NATURALNESS OF
   DYSARTHRIC SPEECH
SO JOURNAL OF SPEECH AND HEARING DISORDERS
LA English
DT Article
C1 UNIV NEBRASKA,LINCOLN,NE 68588.
C3 University of Nebraska System; University of Nebraska Lincoln
RP YORKSTON, KM (corresponding author), UNIV WASHINGTON,DEPT REHABIL MED,RJ-30,SEATTLE,WA 98195, USA.
FU NINDS NIH HHS [5-RO1-NS-19417-03] Funding Source: Medline
CR Berry, 1983, CLIN DYSARTHRIA, P231
   BERRY WR, 1983, CLIN DYSARTHRIA, P253
   Beukelman DR, 1988, PACER TALLY
   CROW E, 1989, RECENT ADVANCES IN CLINICAL DYSARTHRIA, P99
   Darley FL, 1975, Motor speech disorders, V3rd
   FORREST K, 1989, J ACOUST SOC AM, V85, P2606
   HANSON WR, 1980, J SPEECH HEAR DISORD, V45, P268, DOI 10.1044/jshd.4502.268
   HELM NA, 1979, J SPEECH HEAR DISORD, V44, P350, DOI 10.1044/jshd.4403.350
   HYLAND JD, 1988, J SPEECH HEAR DISORD, V53, P271, DOI 10.1044/jshd.5303.271
   MASSEN B, 1986, J SPEECH HEAR RES, V29, P227
   PARKHURST BG, 1978, J COMMUN DISORD, V11, P249, DOI 10.1016/0021-9924(78)90017-5
   Rosenbeck J. C., 1985, Clinical management of neurogenic communication disorders, V2nd, P97
   Yorkston K. M., 1988, Clinical management of dysarthric speakers
   YORKSTON KM, 1981, J SPEECH HEAR DISORD, V46, P398, DOI 10.1044/jshd.4604.398
   YORKSTON KM, 1989, RECENT ADVANCES IN CLINICAL DYSARTHRIA, P85
   YORKSTON KM, 1988, J COMMUN DISORD, V21, P351, DOI 10.1016/0021-9924(88)90038-X
   YORKSTON KM, 1989, ARCH PHYS MED REHAB, V70, P313
   YROKSTON K, 1984, COMPUTERIZED ASSESSM
NR 18
TC 106
Z9 128
U1 0
U2 21
PU AMER SPEECH-LANG-HEARING ASSN
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE RD, ROCKVILLE, MD 20852-3279
SN 0022-4677
J9 J SPEECH HEAR DISORD
PD AUG
PY 1990
VL 55
IS 3
BP 550
EP 560
DI 10.1044/jshd.5503.550
PG 11
WC Linguistics
WE Social Science Citation Index (SSCI)
SC Linguistics
GA DT067
UT WOS:A1990DT06700024
PM 2381196
DA 2024-01-09
ER

PT J
AU MARTIN, RR
   HAROLDSON, SK
   TRIDEN, KA
AF MARTIN, RR
   HAROLDSON, SK
   TRIDEN, KA
TI STUTTERING AND SPEECH NATURALNESS
SO JOURNAL OF SPEECH AND HEARING DISORDERS
LA English
DT Article
RP MARTIN, RR (corresponding author), UNIV MINNESOTA,DEPT COMMUN DISORDERS,MINNEAPOLIS,MN 55455, USA.
CR GOLDIAMOND I, 1965, RES BEHAVIOR MODIFIC
   INGHAM RJ, 1978, J SPEECH HEAR RES, V21, P63, DOI 10.1044/jshr.2101.63
   JONES RJ, 1969, J APPL BEHAV ANAL, V2, P223, DOI 10.1901/jaba.1969.2-223
   Kirk R.E., 2012, EXPT DESIGN PROCEDUR, P302
   MALLARD AR, 1979, J FLUENCY DISORD, V4, P117, DOI 10.1016/0094-730X(79)90010-X
   PERKINS WH, 1974, J SPEECH HEAR DISORD, V39, P416, DOI 10.1044/jshd.3904.416
   RUNYAN CM, 1979, J FLUENCY DISORD, V4, P29, DOI 10.1016/0094-730X(79)90029-9
   RUNYAN CM, 1978, J FLUENCY DISORDERS, V3, P24
   TINSLEY HEA, 1975, J COUNSELING PSYCHOL, V22, P353
   Winer BJ., 1971, STAT PRINCIPLES EXPT
NR 10
TC 171
Z9 188
U1 1
U2 6
PU AMER SPEECH-LANGUAGE-HEARING ASSOC
PI ROCKVILLE
PA 10801 ROCKVILLE PIKE, ROCKVILLE, MD 20852-3279
SN 0022-4677
J9 J SPEECH HEAR DISORD
PY 1984
VL 49
IS 1
BP 53
EP 58
DI 10.1044/jshd.4901.53
PG 6
WC Linguistics
WE Social Science Citation Index (SSCI); Science Citation Index Expanded (SCI-EXPANDED)
SC Linguistics
GA SG450
UT WOS:A1984SG45000007
PM 6700202
DA 2024-01-09
ER

EF